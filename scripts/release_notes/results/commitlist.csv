commit_hash,category,topic,title,files_changed,pr_link,author,accepter_1,accepter_2,accepter_3,merge_into
04da6aeb61f,skip,Untopiced,Add OpInfo entry for alias_copy (#127232) (#128142),aten/src/ATen/functorch/BatchRulesDecompositions.cpp test/distributed/_tensor/test_dtensor_ops.py test/expect/HasDecompTest.test_has_decomposition.expect test/functorch/test_vmap_registrations.py test/onnx/test_fx_op_consistency.py tools/autograd/gen_variable_type.py torch/_decomp/__init__.py torch/_inductor/exc.py torch/_refs/__init__.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/128142,rec,lezcano,,,
c993f1b37fe,inductor,Untopiced,Fix edge cases for gather in inductor (#126893),test/inductor/test_torchinductor_opinfo.py torch/_inductor/lowering.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/126893,isuruf,peterbell10,,,
3b73f5de3a0,skip,Untopiced,"Revert ""Add OpInfo entry for alias_copy (#127232) (#128142)""",aten/src/ATen/functorch/BatchRulesDecompositions.cpp test/distributed/_tensor/test_dtensor_ops.py test/expect/HasDecompTest.test_has_decomposition.expect test/functorch/test_vmap_registrations.py test/onnx/test_fx_op_consistency.py tools/autograd/gen_variable_type.py torch/_decomp/__init__.py torch/_inductor/exc.py torch/_refs/__init__.py torch/testing/_internal/common_methods_invocations.py,,,,,,
d22287d1ad3,skip,Untopiced,"Revert ""Fix 'get_real_value' on placeholder nodes (#127698)""",test/dynamo/test_decorators.py torch/_dynamo/utils.py,,,,,,
ca561d639b4,skip,Untopiced,"Revert ""Fix 'get_attr' call in dynamo 'run_node' (#127696)""",test/dynamo/test_decorators.py torch/_dynamo/utils.py,,,,,,
7b9c5e0e3fb,inductor,Untopiced,Turn on GraphTransformObserver for inductor (#127962),test/inductor/test_graph_transform_observer.py torch/_inductor/fx_passes/ddp_fusion.py torch/_inductor/fx_passes/group_batch_fusion.py torch/_inductor/fx_passes/joint_graph.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/fx_passes/pre_grad.py torch/_inductor/fx_passes/replace_random.py torch/_inductor/pattern_matcher.py torch/_inductor/utils.py torch/fx/passes/graph_transform_observer.py,https://github.com/pytorch/pytorch/pull/127962,shengfukevin,jansel,,,
8e482e909bd,skip,not user facing,Add some guard to size oblivious has_internal_overlap (#128328),aten/src/ATen/MemoryOverlap.cpp,https://github.com/pytorch/pytorch/pull/128328,ezyang,Skylion007,,,
ab3a0b192aa,skip,Untopiced,[RFC] add per-collective timeout value in flight recorder (#128190),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/TraceUtils.h,https://github.com/pytorch/pytorch/pull/128190,c-p-i-o,wconstab,,,
46948300a25,distributed,Untopiced,[c10d] integrate PMI NCCL initialization to NCCL-PG (#128243),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/128243,shengbao-zheng,wconstab,,,
08d038f8a85,inductor,Untopiced,[PT2] Fix a typo and lint problem (#128258),torch/_inductor/fx_passes/decompose_mem_bound_mm.py torch/_inductor/fx_passes/split_cat.py,https://github.com/pytorch/pytorch/pull/128258,mengluy0125,Yuzhen11,dshi7,,
83941482f74,distributed,Untopiced,Add docstring for the torch.distributed.elastic.utils.distributed.get_free_port function (#128133),torch/distributed/elastic/utils/distributed.py,https://github.com/pytorch/pytorch/pull/128133,afrittoli,H-Huang,,,
136bdb96cb6,profiler,improvements,Update Kineto submodule with fix to test_basic_chrome_trace (#128333),test/profiler/test_profiler.py third_party/kineto,https://github.com/pytorch/pytorch/pull/128333,aaronenyeshi,Skylion007,,,
fa8ec8e7189,dynamo,not user facing,[dynamo] handle hashable exceptions in trace_rules lookup (#128078),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/128078,masnesral,anijain2305,,,
093a4ff5f85,skip,not user facing,[export] FIx unflattener for preserving modules containing unused inputs (#128260),test/export/test_unflatten.py torch/export/unflatten.py,https://github.com/pytorch/pytorch/pull/128260,angelayi,pianpwk,,,
db2fa7b827c,skip,Untopiced,"Revert ""[export] FIx unflattener for preserving modules containing unused inputs (#128260)""",test/export/test_unflatten.py torch/export/unflatten.py,,,,,,
9cab5987bde,skip,Untopiced,Introduce int_oo (#127693),test/dynamo/test_exc.py test/dynamo/test_export.py test/dynamo/test_misc.py test/export/test_export.py test/onnx/test_fx_to_onnx_with_onnxruntime.py test/test_dynamic_shapes.py test/test_proxy_tensor.py test/test_sympy_utils.py torch/_decomp/decompositions.py torch/_export/passes/add_runtime_assertions_for_constraints_pass.py torch/_export/serde/serialize.py torch/_inductor/graph.py torch/export/dynamic_shapes.py torch/fx/experimental/symbolic_shapes.py torch/fx/passes/runtime_assert.py torch/utils/_sympy/functions.py torch/utils/_sympy/interp.py torch/utils/_sympy/numbers.py torch/utils/_sympy/value_ranges.py,https://github.com/pytorch/pytorch/pull/127693,ezyang,lezcano,,,
55646554b7f,skip,not user facing,[EZ] Fix typos in SECURITY.md (#128340),SECURITY.md,https://github.com/pytorch/pytorch/pull/128340,malfet,atalman,clee2000,kit1980,
946f554c8fc,inductor,not user facing,Flip default value for mypy disallow_untyped_defs [10+1/11] (#128293),torch/_export/passes/constant_folding.py torch/_inductor/cpp_builder.py torch/_inductor/fx_passes/micro_pipeline_tp.py,https://github.com/pytorch/pytorch/pull/128293,aorenste,oulgen,,,
38e0a0440c2,Uncategorized,Untopiced,[AMD] Default to hipblaslt in gemm (#127944),aten/src/ATen/Context.h,https://github.com/pytorch/pytorch/pull/127944,xw285cornell,aaronenyeshi,houseroad,,
90bb510ece2,skip,Untopiced,"Revert ""Deprecate `torch._utils.is_compiling()` and `torch._dynamo.external_utils.is_compiling()` (#127690)""",test/test_optim.py torch/_dynamo/decorators.py torch/_dynamo/external_utils.py torch/_functorch/apis.py torch/_functorch/eager_transforms.py torch/_higher_order_ops/associative_scan.py torch/_utils.py torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py torch/distributed/tensor/parallel/_utils.py torch/nn/parallel/distributed.py torch/optim/adadelta.py torch/optim/adam.py torch/optim/adamax.py torch/optim/adamw.py torch/optim/asgd.py torch/optim/nadam.py torch/optim/optimizer.py torch/optim/radam.py torch/optim/rmsprop.py torch/optim/rprop.py torch/optim/sgd.py torch/testing/_internal/optests/generate_tests.py,,,,,,
4460e481bca,skip,not user facing,Disable jacrev/jacfwd/hessian if compiling with dynamo (#128255),test/dynamo/test_higher_order_ops.py test/dynamo_expected_failures/TestComposabilityCPU.test_autograd_function_no_setup_context_transform_hessian_cpu test/dynamo_expected_failures/TestComposabilityCPU.test_autograd_function_no_setup_context_transform_jacfwd_cpu test/dynamo_expected_failures/TestHessianCPU.test_jacfwd_different_levels_cpu test/functorch/test_eager_transforms.py torch/_functorch/eager_transforms.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/128255,guilhermeleobas,zou3519,,,
b459713ca75,quantization,not user facing,[aota] compiled forward outputs requires_grad alignment with eager (#128016),test/functorch/test_aotdispatch.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/aot_autograd.py,https://github.com/pytorch/pytorch/pull/128016,IvanKobzarev,bdhirsh,,,
3a2d0755a43,nn_frontend,not user facing,enable test_ParameterList with dynamo if nn module inlining enabled only  (#128308),test/dynamo_expected_failures/TestNN.test_ParameterList test/test_nn.py,https://github.com/pytorch/pytorch/pull/128308,laithsakka,anijain2305,,,
6630dcd53c6,python_frontend,Untopiced,Add docstring for the torch.serialization.default_restore_location function (#128132),torch/serialization.py,https://github.com/pytorch/pytorch/pull/128132,afrittoli,mikaylagawarecki,,,
58083ffb106,composability,not user facing,Improve unbacked reasoning involving has internal overlap (#128332),test/test_dynamic_shapes.py torch/_meta_registrations.py torch/utils/_sympy/functions.py,https://github.com/pytorch/pytorch/pull/128332,ezyang,lezcano,,,
a2d4fea8726,nn_frontend,not user facing,[easy] Move state_dict hooks tests to test_module_hooks and decorate tests that call load_state_dict with swap (#126906),test/nn/test_load_state_dict.py test/nn/test_module_hooks.py test/test_nn.py,https://github.com/pytorch/pytorch/pull/126906,mikaylagawarecki,albanD,,,
c38b3381a12,nn_frontend,improvements,Make nn.Module state_dict load_state_dict pre-hook and state_dict post hook public (#126704),test/nn/test_load_state_dict.py test/nn/test_module_hooks.py torch/ao/quantization/fx/_lower_to_native_backend.py torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py torch/nn/modules/module.py,https://github.com/pytorch/pytorch/pull/126704,mikaylagawarecki,albanD,,,
583a56d5a8e,distributed,Untopiced,DOC: add docstring to construct_and_record_rdzv_event() (#128189),docs/source/elastic/events.rst torch/distributed/elastic/events/__init__.py,https://github.com/pytorch/pytorch/pull/128189,loganthomas,kurman,,,
2176ef7dfaf,skip,not user facing,[compiled autograd] support .backward(inputs=) (#128252),test/inductor/test_compiled_autograd.py test/test_autograd.py torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/128252,xmfan,jansel,,,
4bbadeee8af,skip,Untopiced,"Revert ""Set simdlen based on ATEN_CPU_CAPABILITY (#123514)""",test/inductor/test_cpu_repro.py test/inductor/test_extension_backend.py test/inductor/test_torchinductor.py torch/_dynamo/testing.py torch/_inductor/codecache.py torch/_inductor/codegen/cpp_prefix.h,,,,,,
a287ff75d07,skip,not user facing,Use init_torchbind_implementations in inductor torchbind tests. (#128341),test/inductor/test_torchbind.py,https://github.com/pytorch/pytorch/pull/128341,ydwu4,angelayi,,,
05711eece92,dynamo,Untopiced,[dynamo][inlining inbuilt modules] Ensure BC for nn_module_stack (#128295),test/dynamo/test_repros.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/128295,anijain2305,ydwu4,,,
b2d602306a9,skip,not user facing,[RELAND][dynamo][nn-modules] Trace through nn.Module dunder methods for UnspecializedNNModule (#126578),test/distributed/test_dynamo_distributed.py test/dynamo/test_higher_order_ops.py test/dynamo_expected_failures/FakeTensorTest.test_embedding_bag_meta test/dynamo_expected_failures/TestCompileTransformsCPU.test_compile_vmap_hessian_cpu test/dynamo_expected_failures/TestEmbeddingNN.test_embedding_max_norm test/dynamo_expected_failures/TestEmbeddingNN.test_embedding_sparse_basic test/dynamo_expected_failures/TestEmbeddingNN.test_embedding_sparse_empty_tensor test/dynamo_expected_failures/TestEmbeddingNN.test_embeddingbag_include_last_offset test/dynamo_expected_failures/TestExperimentalUtils.test_profiler_pattern_matcher_json_report test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Bilinear test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Embedding test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_discontiguous test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_max test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_max_padding_idx test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_mean test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_mean_padding_idx test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_sparse test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_sum test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_sum_padding_idx test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Embedding_discontiguous test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Embedding_sparse test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Linear test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Linear_no_batch_dim test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_PReLU_no_batch_dim test/dynamo_expected_failures/TestNN.test_ParameterDict test/dynamo_expected_failures/TestNN.test_Sequential_iadd test/dynamo_expected_failures/TestNN.test_bilinear_broadcasting test/dynamo_expected_failures/TestNN.test_layer_norm_grads_with_create_graph_flag test/dynamo_expected_failures/TestNN.test_linear_autograd_device_cpu_bias_weightCOO test/dynamo_expected_failures/TestNN.test_linear_autograd_device_cpu_bias_weightCSC test/dynamo_expected_failures/TestNN.test_linear_autograd_device_cpu_bias_weightCSR test/dynamo_expected_failures/TestNN.test_linear_broadcasting test/dynamo_expected_failures/TestNN.test_module_apply_inplace_op test/dynamo_expected_failures/TestNN.test_overwrite_module_params_on_conversion test/dynamo_expected_failures/TestNNParametrization.test_errors_unparametrized_tensor_parametrization_swap_False test/dynamo_expected_failures/TestNNParametrization.test_new_spectral_norm_forward_swap_True test/dynamo_expected_failures/TestNNParametrization.test_new_spectral_norm_swap_True test/dynamo_expected_failures/TestNNParametrizationDeviceCPU.test_weight_norm_parametrization_swap_False_cpu test/dynamo_expected_failures/TestNNParametrizationDeviceCPU.test_weight_norm_parametrization_swap_True_cpu test/dynamo_expected_failures/TestNestedTensorDeviceTypeCPU.test_embedding_jagged_cpu test/dynamo_expected_failures/TestPruningNN.test_identity_pruning test/dynamo_expected_failures/TestPruningNN.test_pruning_id_consistency test/dynamo_expected_failures/TestPruningNN.test_random_pruning_0perc test/profiler/test_profiler.py torch/_dynamo/create_parameter_op.py torch/_dynamo/mutation_guard.py torch/_dynamo/side_effects.py torch/_dynamo/symbolic_convert.py torch/_dynamo/utils.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/126578,anijain2305,jansel,,,
739aa224ec1,skip,not user facing,[Fix] Parameter un/lifting issues in the TorchScript to ExportedProgram converter (#127975),test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/127975,jiashenC,angelayi,ydwu4,,
2126ae186e3,build_frontend,not user facing,Remove caffe2/perfkernels files (#128186),BUILD.bazel caffe2/perfkernels/adagrad.cc caffe2/perfkernels/adagrad.h caffe2/perfkernels/adagrad_avx2.cc caffe2/perfkernels/adagrad_avx512.cc caffe2/perfkernels/batch_box_cox.cc caffe2/perfkernels/batch_box_cox.h caffe2/perfkernels/batch_box_cox_avx2.cc caffe2/perfkernels/cvtsh_ss_bugfix.h caffe2/perfkernels/fused_8bit_rowwise_embedding_lookup.cc caffe2/perfkernels/fused_8bit_rowwise_embedding_lookup.h caffe2/perfkernels/fused_8bit_rowwise_embedding_lookup_idx.cc caffe2/perfkernels/fused_8bit_rowwise_embedding_lookup_idx.h caffe2/perfkernels/fused_nbit_rowwise_conversion.cc caffe2/perfkernels/fused_nbit_rowwise_conversion.h caffe2/perfkernels/lstm_unit_cpu-impl.h caffe2/perfkernels/lstm_unit_cpu.h caffe2/perfkernels/lstm_unit_cpu_avx2.cc caffe2/perfkernels/lstm_unit_cpu_common.cc caffe2/perfkernels/lstm_unit_cpu_common.h caffe2/perfkernels/math.h caffe2/perfkernels/math_cpu_avx2.cc caffe2/perfkernels/math_cpu_base.cc caffe2/perfkernels/typed_axpy.cc caffe2/perfkernels/typed_axpy.h caffe2/perfkernels/typed_axpy_avx.cc caffe2/perfkernels/typed_axpy_avx2.cc caffe2/perfkernels/vectorizer.h,https://github.com/pytorch/pytorch/pull/128186,cyyever,ezyang,r-barnes,,
30875953a4b,quantization,Untopiced,[1/N] Remove inclusion of c10/util/string_utils.h (#128300),aten/src/ATen/DLConvertor.cpp aten/src/ATen/TensorIterator.cpp aten/src/ATen/code_template.h aten/src/ATen/native/cuda/jit_utils.cpp aten/src/ATen/native/quantized/cpu/qconv.cpp aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp torch/csrc/api/include/torch/nn/modules/container/modulelist.h torch/csrc/api/include/torch/nn/modules/container/parameterlist.h torch/csrc/api/include/torch/nn/modules/container/sequential.h torch/csrc/autograd/cpp_hook.cpp torch/csrc/autograd/custom_function.h torch/csrc/distributed/rpc/agent_utils.cpp torch/csrc/distributed/rpc/rref_context.cpp torch/csrc/distributed/rpc/tensorpipe_agent.cpp torch/csrc/distributed/rpc/utils.cpp torch/csrc/jit/api/function_impl.cpp torch/csrc/jit/codegen/fuser/codegen.cpp torch/csrc/jit/codegen/fuser/compiler.cpp torch/csrc/jit/frontend/function_schema_parser.cpp torch/csrc/jit/frontend/ir_emitter.cpp torch/csrc/jit/frontend/name_mangler.cpp torch/csrc/jit/frontend/schema_type_parser.cpp torch/csrc/jit/frontend/tree_views.h torch/csrc/jit/ir/ir.h torch/csrc/jit/mobile/compatibility/backport_manager.cpp torch/csrc/jit/mobile/train/export_data.cpp torch/csrc/jit/passes/fixup_trace_scope_blocks.cpp torch/csrc/jit/passes/hoist_conv_packed_params.cpp torch/csrc/jit/passes/onnx/peephole.cpp torch/csrc/jit/passes/prepack_folding.cpp torch/csrc/jit/passes/quantization/dedup_module_uses.cpp torch/csrc/jit/passes/quantization/insert_observers.cpp torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp torch/csrc/jit/passes/quantization/register_packed_params.cpp torch/csrc/jit/passes/utils/subgraph_utils.cpp torch/csrc/jit/runtime/register_ops_utils.cpp torch/csrc/jit/runtime/register_ops_utils.h torch/csrc/jit/runtime/static/impl.cpp torch/csrc/jit/tensorexpr/block_codegen.cpp torch/csrc/jit/tensorexpr/eval.cpp torch/csrc/jit/tensorexpr/eval.h torch/csrc/jit/tensorexpr/ir.cpp torch/csrc/jit/tensorexpr/ir.h torch/csrc/jit/tensorexpr/kernel.cpp torch/csrc/jit/tensorexpr/loopnest.cpp torch/csrc/jit/tensorexpr/operators/misc.cpp torch/csrc/jit/tensorexpr/registerizer.cpp torch/csrc/jit/tensorexpr/unique_name_manager.cpp,https://github.com/pytorch/pytorch/pull/128300,cyyever,eqy,ezyang,,
f843ccbb1ab,skip,not user facing,[MTIA] Add set_device support (#128040),docs/source/mtia.rst torch/mtia/__init__.py,https://github.com/pytorch/pytorch/pull/128040,egienvalue,gnahzg,,,
99f5a85a095,jit,not user facing,[Clang Tidy] Fix misc-header-include-cycle errors in clang-tidy and ignore some files (#127233),.clang-tidy torch/csrc/jit/python/python_custom_class.cpp torch/csrc/jit/python/python_custom_class.h,https://github.com/pytorch/pytorch/pull/127233,cyyever,ezyang,,,
734e8f6ad7e,skip,not user facing,[inductor] enable fx graph cache on torchbench (#128239),benchmarks/dynamo/torchbench.py,https://github.com/pytorch/pytorch/pull/128239,masnesral,oulgen,,,
3b555ba4771,dataloader_frontend,Untopiced,Add docstring for torch.utils.data.datapipes.decoder.basicandlers (#128018),torch/utils/data/datapipes/utils/decoder.py,https://github.com/pytorch/pytorch/pull/128018,arunppsg,andrewkho,,,
841d87177a9,nn_frontend,not user facing,Make sure #126704 is BC for torch.save-ed `nn.Module` (#128344),torch/nn/modules/module.py,https://github.com/pytorch/pytorch/pull/128344,mikaylagawarecki,albanD,,,
d1d9bc7aa65,onnx,Untopiced,init add comment (#128083),torch/onnx/symbolic_opset9.py,https://github.com/pytorch/pytorch/pull/128083,ahoblitz,titaiwangms,,,
793df7b7cb1,skip,not user facing,Prevent expansion of cat indexing to avoid int64 intermediate (#127815),test/inductor/test_cuda_repro.py torch/_inductor/bounds.py torch/_inductor/codegen/common.py torch/_inductor/lowering.py torch/_inductor/utils.py torch/utils/_sympy/functions.py torch/utils/_sympy/interp.py torch/utils/_sympy/value_ranges.py,https://github.com/pytorch/pytorch/pull/127815,eellison,peterbell10,shunting314,,
e4bd0adca5a,quantization,Untopiced,[6/N] Remove unused functions (#128309),aten/src/ATen/native/SoftMax.cpp aten/src/ATen/native/TensorFactories.cpp aten/src/ATen/native/TensorProperties.cpp aten/src/ATen/native/TensorShape.cpp aten/src/ATen/native/mkldnn/Conv.cpp aten/src/ATen/native/quantized/cpu/ReduceOps.cpp aten/src/ATen/native/quantized/cpu/UpSampleBilinear2d.cpp aten/src/ATen/native/sparse/SparseTensorMath.cpp,https://github.com/pytorch/pytorch/pull/128309,cyyever,ezyang,,,
4077cdd589f,distributed,not user facing,[pipelining][doc] Update arg list of pipeline API (#128361),docs/source/distributed.pipelining.rst,https://github.com/pytorch/pytorch/pull/128361,kwen2501,wconstab,,,
665e568381e,inductor,not user facing,[inductor][inlining nn module] Skip batchnorm version check test for inlining (#128268),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/128268,anijain2305,zou3519,,,
ca45649eb58,dynamo,not user facing,[easy][dynamo][inline work] Fix test with inlining inbuilt nn modules (#128254),test/dynamo/test_decorators.py,https://github.com/pytorch/pytorch/pull/128254,anijain2305,williamwen42,,,
7afffdf48b5,skip,not user facing,"[CI] Comment hf_T5_generate, hf_GPT2 and timm_efficientnet in inductor cpu smoketest for performance unstable issue (#127588)",benchmarks/dynamo/expected_ci_speedup_inductor_torchbench_cpu.csv,https://github.com/pytorch/pytorch/pull/127588,zxd1997066,chuanqi129,desertfire,jgong5,
16e67be7f1a,fx,not user facing,Also preserve unbacked SymInts when partitioning as backward inputs (#128338),test/inductor/test_torchinductor_dynamic_shapes.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/128338,ezyang,IvanKobzarev,,,
cba195c8edd,skip,Untopiced,Support aten operations with out tensor (#124926),benchmarks/dynamo/ci_expected_accuracy/aot_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_inductor_torchbench_inference.csv test/inductor/test_torchinductor.py torch/_inductor/compile_fx.py torch/export/_unlift.py,https://github.com/pytorch/pytorch/pull/124926,EikanWang,angelayi,jansel,jgong5,
fe39c07826b,distributed,not user facing,[pipelining][doc] Remove duplicated words (#128368),docs/source/distributed.pipelining.rst,https://github.com/pytorch/pytorch/pull/128368,kwen2501,wconstab,,,
fa88f390a04,skip,Untopiced,"Revert ""[inductor] enable fx graph cache on torchbench (#128239)""",benchmarks/dynamo/torchbench.py,,,,,,
5b5d269d341,fx,Untopiced,Speed up fx graph iteration by implementing it in C++ (#128288),build_variables.bzl torch/_C/__init__.pyi.in torch/csrc/Module.cpp torch/csrc/fx/node.cpp torch/csrc/fx/node.h torch/fx/graph.py torch/fx/node.py,https://github.com/pytorch/pytorch/pull/128288,oulgen,albanD,jansel,,
24e7f290993,inductor,Untopiced,Lowering for avg_pool_3d_backward (Fixes:#127101) (#127722),test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/127722,Lourencom,jansel,,,
a32157c67c1,dynamo,Untopiced,Mark params static if inlining modules and freezing (#128355),torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/128355,mlazos,anijain2305,,,
402b289f3b8,skip,not user facing,Properly register parameter for binary folding test (#128356),test/inductor/test_binary_folding.py,https://github.com/pytorch/pytorch/pull/128356,mlazos,anijain2305,,,
f2d7f235a68,skip,not user facing,[dynamo][yolov3] Track UnspecializedNNModuleVariable for mutation (#128269),benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/aot_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_training.csv torch/_dynamo/output_graph.py,https://github.com/pytorch/pytorch/pull/128269,anijain2305,jansel,,,
a206dcc79e0,inductor,Untopiced,fb_memcache: Move to fbcode from thirdparty (#128174),test/inductor/test_codecache.py test/inductor/test_max_autotune.py torch/_inductor/codecache.py torch/_inductor/compile_fx.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/128174,c00w,oulgen,,,
207c2248a88,inductor,bug fixes,[inductor] Fix lowering full with SymBool value (#128213),test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/_inductor/lowering.py torch/_inductor/sizevars.py torch/_prims_common/__init__.py torch/utils/_sympy/value_ranges.py,https://github.com/pytorch/pytorch/pull/128213,peterbell10,lezcano,,,
648625b230e,skip,Untopiced,Make TraceUtils.h to be device-agnostic (#126969),torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/TraceUtils.h,https://github.com/pytorch/pytorch/pull/126969,FFFrog,c-p-i-o,,,
fc77fdca6f7,skip,not user facing,[guard_size_oblivious] Add gso ExpandUtils:_sym_to (#128224),aten/src/ATen/ExpandUtils.h,https://github.com/pytorch/pytorch/pull/128224,IvanKobzarev,ezyang,,,
55901fb3da5,skip,Untopiced,[fx] Preserve Fx graph node order in partitioner across runs (#115621),test/fx/test_partitioner_order.py torch/fx/passes/infra/partitioner.py,https://github.com/pytorch/pytorch/pull/115621,kareemshaik80,ezyang,,,
9a38cae299e,skip,not user facing,[AOTI] Switch to use shim v2 (#127674),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/127674,hl475,desertfire,,,
053930e1942,mps,not user facing,[MPS][BE] Remove code duplication (#128373),aten/src/ATen/native/mps/operations/UnaryKernel.mm,https://github.com/pytorch/pytorch/pull/128373,malfet,Skylion007,,,
c13e03c8742,dataloader_frontend,not user facing,Flip default value for mypy disallow_untyped_defs [10+2/11] (#128374),torch/_C/return_types.pyi.in torch/distributed/_tensor/examples/display_sharding_example.py torch/nn/functional.pyi.in torch/utils/_sympy/numbers.py torch/utils/data/datapipes/datapipe.py torch/utils/data/datapipes/datapipe.pyi.in,https://github.com/pytorch/pytorch/pull/128374,aorenste,Skylion007,,,
f8c45996d51,mps,bug fixes,[MPS] Make erfinv compilable for bfloat16 (#128375),aten/src/ATen/native/mps/UnaryConstants.h,https://github.com/pytorch/pytorch/pull/128375,malfet,Skylion007,,,
29081059b65,jit,not user facing,[Static Runtime] Fix & run gen_static_runtime_ops (#128299),benchmarks/static_runtime/test_generated_ops.cc torch/csrc/jit/runtime/static/generated_ops.cpp torchgen/static_runtime/config.py torchgen/static_runtime/generator.py,https://github.com/pytorch/pytorch/pull/128299,davidberard98,YuqingJ,,,
a838e909644,skip,not user facing,Add Intel Gaudi device/HPU  to auto load in instantiate_device_type_tests (#126970),torch/testing/_internal/common_device_type.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/126970,ankurneog,albanD,,,
4345d98663d,dynamo,not user facing,[dynamo] Fix for #127696 (#128358),torch/_dynamo/variables/higher_order_ops.py,https://github.com/pytorch/pytorch/pull/128358,angelayi,ydwu4,,,
491c4a5dcba,skip,Untopiced,"Revert ""Make sure #126704 is BC for torch.save-ed `nn.Module` (#128344)""",torch/nn/modules/module.py,,,,,,
1d233b8f500,skip,Untopiced,"Revert ""Make nn.Module state_dict load_state_dict pre-hook and state_dict post hook public (#126704)""",test/nn/test_load_state_dict.py test/nn/test_module_hooks.py torch/ao/quantization/fx/_lower_to_native_backend.py torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py torch/nn/modules/module.py,,,,,,
8a09940a543,inductor,Untopiced,[inductor] fix compile time regression by caching get_gpu_type (#128363),torch/_inductor/comm_analysis.py,https://github.com/pytorch/pytorch/pull/128363,wanchaol,yf225,,,
cac7a22b924,quantization,not user facing,[cuDNN][Quantization] Don't print when plan finalization fails in cuDNN quantization backend (#128177),aten/src/ATen/native/quantized/cudnn/BinaryOps.cpp aten/src/ATen/native/quantized/cudnn/Conv.cpp aten/src/ATen/native/quantized/cudnn/Linear.cpp test/quantization/core/test_quantized_op.py,https://github.com/pytorch/pytorch/pull/128177,eqy,Skylion007,nWEIdia,,
205410cb44e,skip,not user facing,add xpu to torch.tensors (#127280),docs/source/tensors.rst,https://github.com/pytorch/pytorch/pull/127280,jingxu10,svekars,,,
984b1a8c354,dynamo,bug fixes,Fix 'get_attr' call in dynamo 'run_node' (#127696),test/dynamo/test_decorators.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/127696,BowenBao,jansel,,,
61f922c2cab,dynamo,bug fixes,Fix 'get_real_value' on placeholder nodes (#127698),test/dynamo/test_decorators.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/127698,BowenBao,jansel,,,
3e091237974,skip,not user facing,Enable UFMT on test_nestedtensor.py  (#128359),.lintrunner.toml test/test_nestedtensor.py,https://github.com/pytorch/pytorch/pull/128359,YuqingJ,davidberard98,,,
45dccfddcd8,skip,not user facing,"[cuDNN][SDPA] Support different key, value dimension in cuDNN SDPA (#128350)",aten/src/ATen/native/cudnn/MHA.cpp aten/src/ATen/native/cudnn/MHA.h aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/attention_backward.cu test/test_transformers.py,https://github.com/pytorch/pytorch/pull/128350,eqy,Skylion007,,,
adb699189b9,skip,Untopiced,"Revert ""[RELAND][dynamo][nn-modules] Trace through nn.Module dunder methods for UnspecializedNNModule (#126578)""",test/distributed/test_dynamo_distributed.py test/dynamo/test_higher_order_ops.py test/dynamo_expected_failures/FakeTensorTest.test_embedding_bag_meta test/dynamo_expected_failures/TestCompileTransformsCPU.test_compile_vmap_hessian_cpu test/dynamo_expected_failures/TestEmbeddingNN.test_embedding_max_norm test/dynamo_expected_failures/TestEmbeddingNN.test_embedding_sparse_basic test/dynamo_expected_failures/TestEmbeddingNN.test_embedding_sparse_empty_tensor test/dynamo_expected_failures/TestEmbeddingNN.test_embeddingbag_include_last_offset test/dynamo_expected_failures/TestExperimentalUtils.test_profiler_pattern_matcher_json_report test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Bilinear test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Embedding test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_discontiguous test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_max test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_max_padding_idx test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_mean test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_mean_padding_idx test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_sparse test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_sum test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_sum_padding_idx test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Embedding_discontiguous test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Embedding_sparse test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Linear test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Linear_no_batch_dim test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_PReLU_no_batch_dim test/dynamo_expected_failures/TestNN.test_ParameterDict test/dynamo_expected_failures/TestNN.test_Sequential_iadd test/dynamo_expected_failures/TestNN.test_bilinear_broadcasting test/dynamo_expected_failures/TestNN.test_layer_norm_grads_with_create_graph_flag test/dynamo_expected_failures/TestNN.test_linear_autograd_device_cpu_bias_weightCOO test/dynamo_expected_failures/TestNN.test_linear_autograd_device_cpu_bias_weightCSC test/dynamo_expected_failures/TestNN.test_linear_autograd_device_cpu_bias_weightCSR test/dynamo_expected_failures/TestNN.test_linear_broadcasting test/dynamo_expected_failures/TestNN.test_module_apply_inplace_op test/dynamo_expected_failures/TestNN.test_overwrite_module_params_on_conversion test/dynamo_expected_failures/TestNNParametrization.test_errors_unparametrized_tensor_parametrization_swap_False test/dynamo_expected_failures/TestNNParametrization.test_new_spectral_norm_forward_swap_True test/dynamo_expected_failures/TestNNParametrization.test_new_spectral_norm_swap_True test/dynamo_expected_failures/TestNNParametrizationDeviceCPU.test_weight_norm_parametrization_swap_False_cpu test/dynamo_expected_failures/TestNNParametrizationDeviceCPU.test_weight_norm_parametrization_swap_True_cpu test/dynamo_expected_failures/TestNestedTensorDeviceTypeCPU.test_embedding_jagged_cpu test/dynamo_expected_failures/TestPruningNN.test_identity_pruning test/dynamo_expected_failures/TestPruningNN.test_pruning_id_consistency test/dynamo_expected_failures/TestPruningNN.test_random_pruning_0perc test/profiler/test_profiler.py torch/_dynamo/create_parameter_op.py torch/_dynamo/mutation_guard.py torch/_dynamo/side_effects.py torch/_dynamo/symbolic_convert.py torch/_dynamo/utils.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/user_defined.py,,,,,,
70a1e857180,distributed,not user facing,[Traceable FSDP2] Use custom ops for AllGather copy-in / copy-out and ReduceScatter copy-in (#127856),torch/distributed/_composable/fsdp/_fsdp_collectives.py,https://github.com/pytorch/pytorch/pull/127856,yf225,awgu,,,
8c1247cffb7,skip,not user facing,[BE] Fixed CPU autocast warning (#127774),torch/utils/checkpoint.py,https://github.com/pytorch/pytorch/pull/127774,awgu,Skylion007,soulitzer,tianyu-l,
a55d0d9718c,skip,Untopiced,Fix side effect pruning (#128028),test/dynamo/test_higher_order_ops.py torch/_dynamo/side_effects.py,https://github.com/pytorch/pytorch/pull/128028,zou3519,jansel,,,
5fcb5f0c8b1,onnx,Untopiced,init reshape_from_tensor_shape comment (#128171),torch/onnx/operators.py,https://github.com/pytorch/pytorch/pull/128171,ahoblitz,titaiwangms,,,
1dd2431f863,distributed,not user facing,[Test] Add test for only_active flag (#128191),test/distributed/test_c10d_nccl.py,https://github.com/pytorch/pytorch/pull/128191,c-p-i-o,d4l3k,,,
eb567b1f402,skip,Untopiced,Pass params to dump_nccl_trace_pickle (#128307),test/distributed/elastic/test_control_plane.py torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/control_plane/Handlers.hpp torch/csrc/distributed/c10d/control_plane/WorkerServer.cpp,https://github.com/pytorch/pytorch/pull/128307,c-p-i-o,d4l3k,,,
b79d056e76a,skip,not user facing,[export] FIx unflattener for preserving modules containing unused inputs (#128260),test/export/test_unflatten.py torch/export/unflatten.py,https://github.com/pytorch/pytorch/pull/128260,angelayi,pianpwk,,,
447173198b9,fx,Untopiced,Add docstring for the torch.fx.operator_schemas.create_type_hint func… (#128139),torch/fx/operator_schemas.py,https://github.com/pytorch/pytorch/pull/128139,afrittoli,SherlockNoMad,,,
94fea82d664,onnx,Untopiced,init sub comment (#128082),torch/onnx/symbolic_opset9.py,https://github.com/pytorch/pytorch/pull/128082,ahoblitz,titaiwangms,,,
c9c1fed0654,skip,Untopiced,"Revert ""Flip default value for mypy disallow_untyped_defs [10+2/11] (#128374)""",torch/_C/return_types.pyi.in torch/distributed/_tensor/examples/display_sharding_example.py torch/nn/functional.pyi.in torch/utils/_sympy/numbers.py torch/utils/data/datapipes/datapipe.py torch/utils/data/datapipes/datapipe.pyi.in,,,,,,
5d8c7f39d46,skip,Untopiced,"Revert ""Introduce int_oo (#127693)""",test/dynamo/test_exc.py test/dynamo/test_export.py test/dynamo/test_misc.py test/export/test_export.py test/onnx/test_fx_to_onnx_with_onnxruntime.py test/test_dynamic_shapes.py test/test_proxy_tensor.py test/test_sympy_utils.py torch/_decomp/decompositions.py torch/_export/passes/add_runtime_assertions_for_constraints_pass.py torch/_export/serde/serialize.py torch/_inductor/graph.py torch/export/dynamic_shapes.py torch/fx/experimental/symbolic_shapes.py torch/fx/passes/runtime_assert.py torch/utils/_sympy/functions.py torch/utils/_sympy/interp.py torch/utils/_sympy/numbers.py torch/utils/_sympy/value_ranges.py,,,,,,
786c24a4cd8,inductor,not user facing,[inductor] Always realize sigmoid for CPU (#128339),test/inductor/test_cpu_repro.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/128339,desertfire,eellison,helloguo,jansel,
6af4c6acad5,distributed,not user facing,"Migrate test to internal base class, fixes (#128367)",test/distributed/launcher/run_test.py test/distributed/launcher/test_run.py,https://github.com/pytorch/pytorch/pull/128367,kurman,d4l3k,,,
fb013ecb241,skip,not user facing,Remove unused private List::ptr_to_first_element (#128405),aten/src/ATen/core/List.h aten/src/ATen/core/List_inl.h,https://github.com/pytorch/pytorch/pull/128405,cyyever,ezyang,,,
219da29dfd8,mobile,not user facing,[7/N] Remove unused functions (#128407),aten/src/ATen/native/SpectralOps.cpp torch/csrc/jit/mobile/nnc/aot_compiler.cpp torch/csrc/jit/passes/batch_mm.cpp,https://github.com/pytorch/pytorch/pull/128407,cyyever,ezyang,,,
9538bf4e7c5,mps,not user facing,[2/N] Remove inclusion of c10/util/string_utils.h (#128372),aten/src/ATen/native/TensorShape.cpp aten/src/ATen/native/mps/operations/Activation.mm aten/src/ATen/native/mps/operations/Blas.mm aten/src/ATen/native/mps/operations/ConstantOps.mm aten/src/ATen/native/mps/operations/Convolution.mm aten/src/ATen/native/mps/operations/Distributions.mm aten/src/ATen/native/mps/operations/Linear.mm aten/src/ATen/native/mps/operations/LinearAlgebra.mm aten/src/ATen/native/mps/operations/LossOps.mm aten/src/ATen/native/mps/operations/RangeFactories.mm aten/src/ATen/native/mps/operations/ReduceOps.mm aten/src/ATen/native/mps/operations/Shape.mm aten/src/ATen/native/mps/operations/Sort.mm aten/src/ATen/native/mps/operations/TensorCompare.mm aten/src/ATen/native/mps/operations/Unique.mm aten/src/ATen/native/mps/operations/UpSample.mm aten/src/ATen/native/mps/operations/View.mm torch/csrc/autograd/profiler_python.cpp,https://github.com/pytorch/pytorch/pull/128372,cyyever,aaronenyeshi,,,
bb2a9955297,dynamo,Untopiced,"Back out ""[Dynamo] Treat integers stored on nn.Modules as dynamic (#126466)"" (#128432)",test/dynamo/test_modules.py test/onnx/dynamo/test_dynamo_with_onnxruntime_backend.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/128432,doctorweichen,Yuzhen11,ezyang,,
3d55d84ec22,jit,not user facing,[Fix] Check tensor dtype before using torch.allclose in _trace log (#128438),torch/jit/_trace.py,https://github.com/pytorch/pytorch/pull/128438,jiashenC,angelayi,,,
7f6daf289b6,inductor,not user facing,[inductor] parallel compile: set LD_LIBRARY_PATH for sub-processes in internal (#128376),torch/_inductor/compile_worker/subproc_pool.py,https://github.com/pytorch/pytorch/pull/128376,masnesral,eellison,,,
85eeb90d2c4,dynamo,not user facing,[dynamo] Fix graph breaks related to HF ModelOutput (#127780),benchmarks/dynamo/ci_expected_accuracy/aot_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_training.csv test/dynamo/test_model_output.py torch/_dynamo/side_effects.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/127780,williamwen42,anijain2305,jansel,,
3ddec713b81,skip,Untopiced,"Revert ""[cuDNN][Quantization] Don't print when plan finalization fails in cuDNN quantization backend (#128177)""",aten/src/ATen/native/quantized/cudnn/BinaryOps.cpp aten/src/ATen/native/quantized/cudnn/Conv.cpp aten/src/ATen/native/quantized/cudnn/Linear.cpp test/quantization/core/test_quantized_op.py,,,,,,
7c2058338a9,fx,Untopiced,Improve convert fp32 to fp16 fx pass (#127829),torch/fx/experimental/const_fold.py,https://github.com/pytorch/pytorch/pull/127829,trieuat,Skylion007,,,
86b5df3e71e,fx,docs,Documenting the torch.fx.annotate.annotate function (#128337),torch/fx/annotate.py,https://github.com/pytorch/pytorch/pull/128337,kiszk,malfet,,,
8cf302dce4c,skip,not user facing,[5/N] Change static functions in headers to inline (#128406),aten/src/ATen/native/GridSamplerUtils.h aten/src/ATen/native/LossMulti.h aten/src/ATen/native/MaxPooling.h aten/src/ATen/native/UpSample.h aten/src/ATen/native/vol2col.h,https://github.com/pytorch/pytorch/pull/128406,cyyever,ezyang,,,
02e7519ac3c,python_frontend,docs,DOC: strip inaccurate either float32 or float64 statement from set_default_type (#128192),torch/__init__.py,https://github.com/pytorch/pytorch/pull/128192,loganthomas,malfet,,,
c0b87afcade,skip,not user facing,[RELAND2][dynamo][nn-modules] Trace through nn.Module dunder methods for UnspecializedNNModule (#126578),test/distributed/test_dynamo_distributed.py test/dynamo/test_higher_order_ops.py test/dynamo_expected_failures/FakeTensorTest.test_embedding_bag_meta test/dynamo_expected_failures/TestCompileTransformsCPU.test_compile_vmap_hessian_cpu test/dynamo_expected_failures/TestEmbeddingNN.test_embedding_max_norm test/dynamo_expected_failures/TestEmbeddingNN.test_embedding_sparse_basic test/dynamo_expected_failures/TestEmbeddingNN.test_embedding_sparse_empty_tensor test/dynamo_expected_failures/TestEmbeddingNN.test_embeddingbag_include_last_offset test/dynamo_expected_failures/TestExperimentalUtils.test_profiler_pattern_matcher_json_report test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Bilinear test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Embedding test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_discontiguous test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_max test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_max_padding_idx test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_mean test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_mean_padding_idx test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_sparse test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_sum test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_EmbeddingBag_sum_padding_idx test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Embedding_discontiguous test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Embedding_sparse test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Linear test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_Linear_no_batch_dim test/dynamo_expected_failures/TestJitGeneratedModule.test_nn_PReLU_no_batch_dim test/dynamo_expected_failures/TestNN.test_ParameterDict test/dynamo_expected_failures/TestNN.test_Sequential_iadd test/dynamo_expected_failures/TestNN.test_bilinear_broadcasting test/dynamo_expected_failures/TestNN.test_layer_norm_grads_with_create_graph_flag test/dynamo_expected_failures/TestNN.test_linear_autograd_device_cpu_bias_weightCOO test/dynamo_expected_failures/TestNN.test_linear_autograd_device_cpu_bias_weightCSC test/dynamo_expected_failures/TestNN.test_linear_autograd_device_cpu_bias_weightCSR test/dynamo_expected_failures/TestNN.test_linear_broadcasting test/dynamo_expected_failures/TestNN.test_module_apply_inplace_op test/dynamo_expected_failures/TestNN.test_overwrite_module_params_on_conversion test/dynamo_expected_failures/TestNNParametrization.test_errors_unparametrized_tensor_parametrization_swap_False test/dynamo_expected_failures/TestNNParametrization.test_new_spectral_norm_forward_swap_True test/dynamo_expected_failures/TestNNParametrization.test_new_spectral_norm_swap_True test/dynamo_expected_failures/TestNNParametrizationDeviceCPU.test_weight_norm_parametrization_swap_False_cpu test/dynamo_expected_failures/TestNNParametrizationDeviceCPU.test_weight_norm_parametrization_swap_True_cpu test/dynamo_expected_failures/TestNestedTensorDeviceTypeCPU.test_embedding_jagged_cpu test/dynamo_expected_failures/TestPruningNN.test_identity_pruning test/dynamo_expected_failures/TestPruningNN.test_pruning_id_consistency test/dynamo_expected_failures/TestPruningNN.test_random_pruning_0perc test/profiler/test_profiler.py torch/_dynamo/create_parameter_op.py torch/_dynamo/mutation_guard.py torch/_dynamo/side_effects.py torch/_dynamo/symbolic_convert.py torch/_dynamo/utils.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/126578,anijain2305,jansel,,,
77a0ca66e4e,cuda,Untopiced,Add threadfence to 2-stage reduction for correct writes visibility (#128455),aten/src/ATen/native/cuda/Reduce.cuh aten/src/ATen/native/cuda/reduction_template.cuh,https://github.com/pytorch/pytorch/pull/128455,ngimel,eqy,ezyang,,
089f9a116ac,skip,Untopiced,[tp] refactor and fix PrepareModuleInput for DTensor inputs (#128431),test/distributed/tensor/parallel/test_tp_style.py torch/distributed/tensor/parallel/style.py,https://github.com/pytorch/pytorch/pull/128431,wanchaol,awgu,,,
62311257adb,skip,not user facing,Add 1 test case for Convtranspose1D in op microbenchmark (#127216),benchmarks/operator_benchmark/pt/configs.py benchmarks/operator_benchmark/pt/conv_test.py,https://github.com/pytorch/pytorch/pull/127216,DiweiSun,atalman,chuanqi129,jgong5,
dcc0093dba1,dataloader_frontend,Untopiced,[BE][Easy] export explicitly imported public submodules (#127703),torch/__init__.py torch/optim/__init__.py torch/utils/data/sampler.py,https://github.com/pytorch/pytorch/pull/127703,XuehaiPan,ezyang,,,
a4216999982,skip,Untopiced,"Revert ""[tp] refactor and fix PrepareModuleInput for DTensor inputs (#128431)""",test/distributed/tensor/parallel/test_tp_style.py torch/distributed/tensor/parallel/style.py,,,,,,
8b3daf1768f,fx,not user facing,Add FloatTrueDiv and ToFloat to SYMPY_INTERP (#128418),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/128418,masnesral,ezyang,,,
0b331fd5d75,cuda,not user facing,[CUDA] Abate `SoftMax.cu` compiler warning spam (#128468),aten/src/ATen/native/cuda/SoftMax.cu,https://github.com/pytorch/pytorch/pull/128468,eqy,valentinandrei,,,
04037f3d22f,skip,not user facing,[BE] sort imports in `torch/__init__.py` (#127708),torch/__init__.py,https://github.com/pytorch/pytorch/pull/127708,XuehaiPan,ezyang,,,
1602c7d0c86,skip,not user facing,[dynamo] Enable some inlining inbuilt nn module tests (#128440),test/dynamo/test_inline_inbuilt_nn_modules.py,https://github.com/pytorch/pytorch/pull/128440,anijain2305,jansel,williamwen42,,
ebb00a92bd9,dynamo,not user facing,[dynamo] Skip freezing expect failure for inlining inbuilt nn modules (#128470),test/inductor/test_inductor_freezing.py,https://github.com/pytorch/pytorch/pull/128470,anijain2305,mlazos,,,
1edcb31d34e,inductor,not user facing,[RELAND][inductor][cpp] bf16/fp16 gemm template computed with fp32 (#128472),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codecache.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/ir.py torch/_inductor/mkldnn_lowerings.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/128472,jgong5,desertfire,,,
2386045e4f0,skip,Untopiced,Add OpInfo entry for alias_copy (#127232) (#128142),aten/src/ATen/functorch/BatchRulesDecompositions.cpp test/distributed/_tensor/test_dtensor_ops.py test/expect/HasDecompTest.test_has_decomposition.expect test/functorch/test_vmap_registrations.py test/onnx/test_fx_op_consistency.py test/test_mps.py tools/autograd/gen_variable_type.py torch/_decomp/__init__.py torch/_inductor/exc.py torch/_refs/__init__.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/128142,rec,lezcano,,,
26433b86dea,skip,not user facing,[BE][Easy] sort `__all__` in `torch/__init__.py` (#127709),torch/__init__.py,https://github.com/pytorch/pytorch/pull/127709,XuehaiPan,ezyang,,,
46a35a1ed4f,python_frontend,Untopiced,[BE] enable UFMT for `torch/__init__.py` (#127710),.lintrunner.toml torch/__init__.py,https://github.com/pytorch/pytorch/pull/127710,XuehaiPan,ezyang,,,
2e065f2486e,inductor,not user facing,[Quant][Inductor] Bug fix: mutation nodes not handled correctly for QLinearPointwiseBinaryPT2E (#127592),test/inductor/test_flex_attention.py test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/127592,Xia-Weiwen,Chillee,leslie-fang-intel,,
abc3eec22d3,skip,Untopiced,First version of AOTAutogradCache (#126791),test/dynamo/test_aot_autograd_cache.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/aot_autograd.py torch/_functorch/config.py torch/_inductor/codecache.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/126791,jamesjwu,bdhirsh,,,
71f491554c1,skip,Untopiced,"Revert ""First version of AOTAutogradCache (#126791)""",test/dynamo/test_aot_autograd_cache.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/aot_autograd.py torch/_functorch/config.py torch/_inductor/codecache.py torch/_inductor/utils.py,,,,,,
5ef70faaa76,skip,not user facing,"Revert ""Make torch_geometric models compatible with export (#123403)"" (#128377)",benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/128377,chunyuan-w,angelayi,desertfire,jgong5,
15ab6360072,skip,Untopiced,"Revert ""Fix side effect pruning (#128028)""",test/dynamo/test_higher_order_ops.py torch/_dynamo/side_effects.py,,,,,,
3c971d2ef30,distributed,not user facing,Flip default value for mypy disallow_untyped_defs [final] (#127836),.github/scripts/lintrunner.sh mypy.ini test/test_utils.py test/typing/pass/cuda_steam.py torch/_C/return_types.pyi.in torch/distributed/_tensor/examples/display_sharding_example.py torch/distributed/fsdp/_flat_param.py torch/nn/functional.pyi.in torch/nn/parallel/distributed.py torch/utils/data/datapipes/datapipe.pyi.in,https://github.com/pytorch/pytorch/pull/127836,aorenste,Skylion007,oulgen,,
b19c2319e46,skip,Untopiced,[ROCm] TunableOp for gemm_and_bias (#128143),aten/src/ATen/cuda/tunable/GemmCommon.h aten/src/ATen/cuda/tunable/GemmHipblaslt.h aten/src/ATen/cuda/tunable/Tunable.cpp aten/src/ATen/cuda/tunable/TunableGemm.h aten/src/ATen/native/cuda/Blas.cpp,https://github.com/pytorch/pytorch/pull/128143,jeffdaily,Skylion007,,,
8df56afc200,dynamo,Untopiced,Add support in Python API for the recommended max working set size. (#128289),aten/src/ATen/detail/MPSHooksInterface.h aten/src/ATen/mps/MPSAllocator.h aten/src/ATen/mps/MPSAllocator.mm aten/src/ATen/mps/MPSAllocatorInterface.h aten/src/ATen/mps/MPSHooks.h aten/src/ATen/mps/MPSHooks.mm docs/source/mps.rst test/test_mps.py torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/csrc/mps/Module.cpp torch/mps/__init__.py,https://github.com/pytorch/pytorch/pull/128289,kulinseth,malfet,,,
f2dcbe89d66,skip,Untopiced,"Revert ""Prevent expansion of cat indexing to avoid int64 intermediate (#127815)""",test/inductor/test_cuda_repro.py torch/_inductor/bounds.py torch/_inductor/codegen/common.py torch/_inductor/lowering.py torch/_inductor/utils.py torch/utils/_sympy/functions.py torch/utils/_sympy/interp.py torch/utils/_sympy/value_ranges.py,,,,,,
9e39c62908d,quantization,not user facing,correct avx512_vnni isa name. (#128318),aten/src/ATen/cpu/Utils.cpp aten/src/ATen/cpu/Utils.h torch/_C/_cpu.pyi torch/_dynamo/trace_rules.py torch/cpu/__init__.py torch/csrc/cpu/Module.cpp,https://github.com/pytorch/pytorch/pull/128318,xuhancn,desertfire,jgong5,leslie-fang-intel,
c5172b8de85,skip,Untopiced,"Revert ""[AOTI] Switch to use shim v2 (#127674)""",torch/_inductor/config.py,,,,,,
81e4e12f026,skip,Untopiced,"Revert ""Support aten operations with out tensor (#124926)""",benchmarks/dynamo/ci_expected_accuracy/aot_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_inductor_torchbench_inference.csv test/inductor/test_torchinductor.py torch/_inductor/compile_fx.py torch/export/_unlift.py,,,,,,
f89574fa237,skip,Untopiced,"Revert ""Pass params to dump_nccl_trace_pickle (#128307)""",test/distributed/elastic/test_control_plane.py torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/control_plane/Handlers.hpp torch/csrc/distributed/c10d/control_plane/WorkerServer.cpp,,,,,,
5001f41b904,skip,Untopiced,"Revert ""Make TraceUtils.h to be device-agnostic (#126969)""",torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/TraceUtils.h,,,,,,
0f52dc7e51f,cuda,docs,Document `torch.cuda.profiler.stop` (#128196),torch/cuda/profiler.py,https://github.com/pytorch/pytorch/pull/128196,anandptl84,eqy,malfet,,
a70a7337d27,python_frontend,Untopiced,Update torch.nanmean() docstring to mention input dtype requirement (#128155),torch/_torch_docs.py,https://github.com/pytorch/pytorch/pull/128155,sshkhr,malfet,,,
624e8ae491f,skip,not user facing,Documentation for is_dependent function (#128197),torch/distributions/constraints.py,https://github.com/pytorch/pytorch/pull/128197,TharinduRusira,fritzo,malfet,,
d71f92213ce,distributed,Untopiced,[DSD] keep 'exp_avg' as DTensor after torch.distributed.checkpoint.state_dict.set_optimizer_state_dict (#128004),test/distributed/checkpoint/test_state_dict.py torch/distributed/_state_dict_utils.py torch/distributed/checkpoint/state_dict.py,https://github.com/pytorch/pytorch/pull/128004,mori360,fegin,,,
7db501ba2b1,skip,Untopiced,"Revert ""[cuDNN][SDPA] Support different key, value dimension in cuDNN SDPA (#128350)""",aten/src/ATen/native/cudnn/MHA.cpp aten/src/ATen/native/cudnn/MHA.h aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/attention_backward.cu test/test_transformers.py,,,,,,
6d1b1ddd3e8,releng,not user facing,Select Runner Label Dynamically (#127287),.github/scripts/get_workflow_type.py .github/workflows/_runner-determinator.yml,https://github.com/pytorch/pytorch/pull/127287,DanilBaibak,ZainRizvi,,,
817ce6835b3,skip,Untopiced,"Revert ""[cuDNN][SDPA] Remove `TORCH_CUDNN_SDPA_ENABLED=1`, enable cuDNN SDPA by default on H100 and 2nd on other archs >= sm80 (#125343)""",aten/src/ATen/Context.h aten/src/ATen/native/cudnn/MHA.cpp aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/attention_backward.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.h docs/source/backends.rst test/test_flop_counter.py test/test_transformers.py tools/autograd/derivatives.yaml torch/_C/__init__.pyi.in torch/backends/cuda/__init__.py torch/csrc/Module.cpp torch/testing/_internal/common_cuda.py torch/utils/flop_counter.py,,,,,,
ec1fdda196d,skip,not user facing,Fix jagged NT softmax semantics (#119459),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/119459,jbschlosser,soulitzer,,,
7775fee10f3,skip,Untopiced,[tp] refactor and fix PrepareModuleInput for DTensor inputs (#128431),test/distributed/tensor/parallel/test_tp_style.py torch/distributed/tensor/parallel/style.py,https://github.com/pytorch/pytorch/pull/128431,wanchaol,awgu,,,
cc231a8e2bb,skip,Untopiced,First version of AOTAutogradCache (#126791),test/dynamo/test_aot_autograd_cache.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/aot_autograd.py torch/_functorch/config.py torch/_inductor/codecache.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/126791,jamesjwu,bdhirsh,,,
dd143d44cc3,skip,not user facing,[BE] enable UFMT for top-level files `torch/*.py` (#127707),.lintrunner.toml torch/_guards.py torch/_lobpcg.py torch/_lowrank.py torch/_tensor.py torch/functional.py torch/hub.py torch/library.py torch/overrides.py torch/quasirandom.py torch/random.py torch/return_types.py torch/serialization.py torch/torch_version.py torch/types.py,https://github.com/pytorch/pytorch/pull/127707,XuehaiPan,ezyang,,,
67e6c76a185,skip,not user facing,Support apply_(callable) sugar for CPU NJTs (#125416),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/125416,jbschlosser,soulitzer,,,
0444e899318,skip,not user facing,[export] Remove replace_sym_size_ops_pass (#128443),torch/_export/passes/replace_sym_size_ops_pass.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/128443,zhxchen17,angelayi,,,
cba840fde9d,inductor,not user facing,Fix accidental variable shadow (#128460),torch/_inductor/ir.py torch/csrc/autograd/python_variable_indexing.h,https://github.com/pytorch/pytorch/pull/128460,aorenste,Skylion007,oulgen,,
1026b7cfbe1,python_frontend,docs,Add docstring for the torch.typename function (#128129),torch/__init__.py,https://github.com/pytorch/pytorch/pull/128129,afrittoli,malfet,,,
ff3ba993209,skip,not user facing,Disable inline nn modules on unstable ptr test (#128528),test/inductor/test_cudagraph_trees.py,https://github.com/pytorch/pytorch/pull/128528,mlazos,anijain2305,,,
c0ea8fc3a37,skip,not user facing,Disable inlining nn modules on static inputs tests (#128529),test/inductor/test_cudagraph_trees.py,https://github.com/pytorch/pytorch/pull/128529,mlazos,anijain2305,,,
bb13fad7aa7,distributed,Untopiced,Share TCPStore by default when using c10d rdzv handler (#128096),test/distributed/elastic/rendezvous/dynamic_rendezvous_test.py torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py torch/distributed/elastic/rendezvous/dynamic_rendezvous.py,https://github.com/pytorch/pytorch/pull/128096,kurman,shuqiangzhang,,,
c53d65b3d3d,inductor,not user facing,[inductor] fix linear add bias pattern (#128473),test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/fx_passes/mkldnn_fusion.py,https://github.com/pytorch/pytorch/pull/128473,zhuhaozhe,jgong5,,,
87072dcfdb2,dynamo,Untopiced,Change Dynamo's custom ops warning message to be less spammy (#128456),test/dynamo/test_misc.py torch/_dynamo/__init__.py torch/_dynamo/utils.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/128456,zou3519,anijain2305,,,
6aef2052ea4,skip,not user facing,Save backward graphs lazily to cache (#126999),test/dynamo/test_aot_autograd_cache.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py torch/_functorch/_aot_autograd/runtime_wrappers.py,https://github.com/pytorch/pytorch/pull/126999,jamesjwu,bdhirsh,,,
2b28b107dba,skip,not user facing,[dynamo][fsdp] Dont take unspecializedNNModuleVariable path for FSDP modules (#128453),torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/128453,anijain2305,yf225,,,
6206da55ef1,skip,not user facing,Fix lint after #119459 (#128558),test/test_nestedtensor.py,https://github.com/pytorch/pytorch/pull/128558,huydhn,atalman,kit1980,malfet,
55a6b38f526,skip,not user facing,[inductor] enable fx graph cache on torchbench (#128239),benchmarks/dynamo/torchbench.py,https://github.com/pytorch/pytorch/pull/128239,masnesral,oulgen,,,
3008644297b,cpp_frontend,not user facing,[Caffe2] Remove remaining unused perfkernels (#128477),BUILD.bazel caffe2/core/common.cc caffe2/core/common.h caffe2/perfkernels/CMakeLists.txt caffe2/perfkernels/embedding_lookup.cc caffe2/perfkernels/embedding_lookup.h caffe2/perfkernels/embedding_lookup_avx2.cc caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc caffe2/utils/conversions.h,https://github.com/pytorch/pytorch/pull/128477,cyyever,ezyang,r-barnes,,
f39ab8a0fe1,skip,Untopiced,Fix side effect pruning (#128028),test/dynamo/test_higher_order_ops.py torch/_dynamo/side_effects.py,https://github.com/pytorch/pytorch/pull/128028,zou3519,jansel,,,
f4edd67fe7d,distributed,Untopiced,[c10d] fix OSS commSplit bug (#128459),torch/csrc/distributed/c10d/NCCLUtils.cpp,https://github.com/pytorch/pytorch/pull/128459,shengbao-zheng,shuqiangzhang,,,
1f302d68850,skip,Untopiced,Support aten operations with out tensor (#124926),benchmarks/dynamo/ci_expected_accuracy/aot_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_inductor_torchbench_inference.csv test/inductor/test_torchinductor.py torch/_inductor/compile_fx.py torch/export/_unlift.py,https://github.com/pytorch/pytorch/pull/124926,EikanWang,angelayi,jansel,jgong5,
b4a7b543e55,skip,not user facing,Add targeted unit tests for guards-related functions used in the codecache (#128482),test/test_dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/128482,masnesral,ezyang,,,
bb3cf8a3392,inductor,not user facing,Lift inductor lowerings for jagged <-> padded dense kernels (#125968),torch/_inductor/jagged_lowerings.py torch/_inductor/lowering.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/125968,jbschlosser,davidberard98,,,
e2610240f93,skip,not user facing,[ROCm] Enable several inductor UTs (#127761),test/dynamo/test_autograd_function.py test/inductor/test_aot_inductor.py test/inductor/test_benchmark_fusion.py test/inductor/test_codecache.py test/inductor/test_control_flow.py test/inductor/test_cpu_repro.py test/inductor/test_cuda_cpp_wrapper.py test/inductor/test_cuda_repro.py test/inductor/test_cudagraph_trees.py test/inductor/test_flex_attention.py test/inductor/test_inductor_freezing.py test/inductor/test_max_autotune.py test/inductor/test_memory_planning.py test/inductor/test_mkldnn_pattern_matcher.py test/inductor/test_multi_kernel.py test/inductor/test_perf.py test/inductor/test_profiler.py test/inductor/test_select_algorithm.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_dynamic_shapes.py test/inductor/test_triton_kernels.py,https://github.com/pytorch/pytorch/pull/127761,pragupta,peterbell10,pruthvistony,,
8db4a41973e,skip,not user facing,Use computeStorageNbytesContiguous if possible (#128515),aten/src/ATen/native/TensorShape.cpp,https://github.com/pytorch/pytorch/pull/128515,shazqadeer,ezyang,,,
2fa6f80b136,foreach_frontend,bug fixes,Perform reciprocal optimization with foreach_div (#128433),aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu test/test_foreach.py,https://github.com/pytorch/pytorch/pull/128433,ezyang,awgu,,,
3bc2004f912,Uncategorized,Untopiced,[ts_converter] Fix prim::dtype (#128517),test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/128517,angelayi,jiashenC,,,
4423e1bbdc3,skip,not user facing,[release] Increase version 2.4.0->2.5.0 (#128514),version.txt,https://github.com/pytorch/pytorch/pull/128514,atalman,malfet,,,
eb1db6702f6,inductor,not user facing,[2nd try][AOTI] Switch to use shim v2 (#128521),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/128521,hl475,desertfire,,,
25b7537a27b,distributed,not user facing,doc comment typo fixes and improvements (#128512),torch/distributed/checkpoint/staging.py torch/distributed/checkpoint/state_dict_saver.py,https://github.com/pytorch/pytorch/pull/128512,pradeepfn,LucasLLC,,,
c472cec5656,autograd_frontend,new features,[checkpoint] Clean up selective activation checkpoint and make public (#125795),docs/source/checkpoint.rst test/dynamo/test_activation_checkpointing.py test/test_autograd.py torch/_higher_order_ops/wrap.py torch/utils/checkpoint.py,https://github.com/pytorch/pytorch/pull/125795,soulitzer,Chillee,fmassa,,
c1cd9468184,skip,not user facing,[cond] add a set_ and data mutation expected failure test (#128457),test/functorch/test_control_flow.py,https://github.com/pytorch/pytorch/pull/128457,ydwu4,zou3519,,,
ede74940a15,inductor,not user facing,optimize vec isa check dispatch logical. (#128320),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/128320,xuhancn,desertfire,jgong5,,
3f9b8446cf5,sparse_frontend,not user facing,[8/N] Remove unused functions (#128499),aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp aten/src/ATen/native/transformers/attention.cpp torch/csrc/distributed/c10d/Functional.cpp torch/csrc/distributed/c10d/ProcessGroup.cpp torch/csrc/jit/mobile/nnc/aot_compiler.cpp torchgen/dest/register_dispatch_key.py,https://github.com/pytorch/pytorch/pull/128499,cyyever,malfet,,,
7fe9ab9ccc0,skip,not user facing,update amp example to device-agnostic (#127278),docs/source/notes/amp_examples.rst,https://github.com/pytorch/pytorch/pull/127278,jingxu10,EikanWang,dvrogozh,svekars,
d630e1e838e,skip,Untopiced,"Revert ""[dynamo][yolov3] Track UnspecializedNNModuleVariable for mutation (#128269)""",benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/aot_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_training.csv torch/_dynamo/output_graph.py,,,,,,
d3b82306391,profiler,bug fixes,Fix profiler_kineto Clang errors (#128464),torch/csrc/autograd/profiler_kineto.cpp,https://github.com/pytorch/pytorch/pull/128464,sraikund16,aaronenyeshi,,,
2229884102a,skip,Untopiced,Introduce int_oo (#127693),test/dynamo/test_exc.py test/dynamo/test_export.py test/dynamo/test_misc.py test/export/test_export.py test/onnx/test_fx_to_onnx_with_onnxruntime.py test/test_dynamic_shapes.py test/test_proxy_tensor.py test/test_sympy_utils.py torch/_decomp/decompositions.py torch/_export/passes/add_runtime_assertions_for_constraints_pass.py torch/_export/serde/serialize.py torch/_inductor/graph.py torch/export/dynamic_shapes.py torch/fx/experimental/symbolic_shapes.py torch/fx/passes/runtime_assert.py torch/utils/_sympy/functions.py torch/utils/_sympy/interp.py torch/utils/_sympy/numbers.py torch/utils/_sympy/value_ranges.py,https://github.com/pytorch/pytorch/pull/127693,ezyang,lezcano,,,
83bb9b7c53d,nn_frontend,Untopiced,[BE] explicitly export subpackage `torch.utils` (#128342),torch/__init__.py torch/_tensor.py torch/cuda/graphs.py torch/multiprocessing/reductions.py torch/nn/modules/module.py torch/utils/__init__.py,https://github.com/pytorch/pytorch/pull/128342,XuehaiPan,Skylion007,,,
b86b4ace88a,dynamo,not user facing,Invalidate eager params when inlining and freezing nn modules (#128543),torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/128543,mlazos,anijain2305,,,
ade3d07483f,mps,performance,GGML inspired int8 MM Metal shader (#127646),aten/src/ATen/native/mps/operations/Quantized.mm,https://github.com/pytorch/pytorch/pull/127646,larryliu0820,malfet,,,
ea541dd9653,quantization,not user facing,SymIntify cross_entropy_loss_prob_target numel call (#128141),aten/src/ATen/native/LossNLL.cpp,https://github.com/pytorch/pytorch/pull/128141,shazqadeer,ezyang,,,
1f6e84fa685,skip,not user facing,[inductor][mkldnn] Use floats instead of ints for pattern matcher test (#128484),test/inductor/test_mkldnn_pattern_matcher.py,https://github.com/pytorch/pytorch/pull/128484,anijain2305,mlazos,,,
c52eda896eb,skip,not user facing,[dynamo][trace_rules] Remove incorrectly classified Ingraph functions (#128428),test/dynamo/test_repros.py torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/128428,anijain2305,mlazos,yanboliang,,
518c9e64558,inductor,Untopiced,Forward fix lint (#128587),torch/_inductor/jagged_lowerings.py torch/utils/_sympy/numbers.py,https://github.com/pytorch/pytorch/pull/128587,clee2000,huydhn,,,
14c9eb5ed22,releng,not user facing,Add XPU code owners (#128486),.github/merge_rules.yaml CODEOWNERS,https://github.com/pytorch/pytorch/pull/128486,EikanWang,atalman,malfet,,
0678742924e,mps,improvements,[MPS] Add Metal implementation of exp op (#128421),aten/src/ATen/native/mps/OperationUtils.mm aten/src/ATen/native/mps/UnaryConstants.h aten/src/ATen/native/mps/operations/UnaryKernel.mm aten/src/ATen/native/mps/operations/UnaryOps.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/128421,malfet,kulinseth,,,
ce79b09415c,skip,not user facing,[CUDA][Sparse] Change comparison function of `test_sparse_semi_structured.py` and bump tolerances for `sp24_matmuls` (#128553),test/test_sparse_semi_structured.py,https://github.com/pytorch/pytorch/pull/128553,eqy,jcaip,,,
88974fedd06,skip,not user facing,Clean up xpu ut to make CI happy (#128383),test/test_xpu.py,https://github.com/pytorch/pytorch/pull/128383,guangyey,EikanWang,,,
c8976513927,inductor,not user facing,[inductor] Add BackendFeature gating (#128266),test/inductor/test_torchinductor.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cuda/cuda_cpp_scheduling.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_foreach.py torch/_inductor/decomposition.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/runtime/runtime_utils.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/128266,jansel,shunting314,,,
1fd2cd26a04,skip,not user facing,[inductor][cpp] support bf16/fp16 gemm template epilogue fusion (#126545),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_template.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/kernel/mm.py torch/_inductor/mkldnn_lowerings.py,https://github.com/pytorch/pytorch/pull/126545,jgong5,jansel,,,
d0c08926d19,dynamo,not user facing,allow inlining functions in _python_dispatch and _is_make_fx_tracing (#128485),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/128485,laithsakka,anijain2305,,,
2b9465d62ab,skip,not user facing,[aota] Allow some mutations in backward (#128409),test/functorch/test_aotdispatch.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/_aot_autograd/traced_function_transforms.py,https://github.com/pytorch/pytorch/pull/128409,IvanKobzarev,bdhirsh,,,
7cc07a3eb13,composability,Untopiced,[custom_op] stop using nonlocals to store information (#128547),test/test_custom_ops.py torch/_library/autograd.py torch/csrc/autograd/python_function.cpp,https://github.com/pytorch/pytorch/pull/128547,zou3519,soulitzer,williamwen42,,
edb45dce85a,distributed,Untopiced,Add OpInfo entry for as_strided_copy (#127231),aten/src/ATen/functorch/BatchRulesDecompositions.cpp test/distributed/_tensor/test_dtensor_ops.py test/expect/HasDecompTest.test_aten_core_operators.expect test/expect/HasDecompTest.test_has_decomposition.expect test/functorch/test_ops.py test/functorch/test_vmap.py test/functorch/test_vmap_registrations.py test/test_mps.py tools/autograd/gen_variable_type.py torch/_inductor/lowering.py torch/_refs/__init__.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/127231,rec,lezcano,,,
cf7adc2fa1c,inductor,not user facing,[Inductor] Update Intel GPU Triton commit pin. (#124842),.ci/docker/ci_commit_pins/triton-xpu.txt,https://github.com/pytorch/pytorch/pull/124842,etaf,EikanWang,,,
49366b2640d,skip,not user facing,Add test to xfail_list only for abi_compatible (#128506),test/inductor/test_cpu_cpp_wrapper.py,https://github.com/pytorch/pytorch/pull/128506,chunyuan-w,desertfire,jgong5,,
93a14aba6e8,quantization,not user facing,[BE]: Update mypy to 1.10.0 (#127717),.ci/docker/requirements-ci.txt .lintrunner.toml torch/ao/quantization/fx/fuse_handler.py torch/distributed/_composable/fsdp/fully_shard.py torch/distributions/utils.py torch/fx/experimental/rewriter.py,https://github.com/pytorch/pytorch/pull/127717,Skylion007,ezyang,,,
17b45e905a8,inductor,not user facing,Fix get output code when caching is enabled (#128445),test/inductor/test_torchinductor.py torch/_inductor/codecache.py torch/_inductor/graph.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/128445,oulgen,eellison,jansel,masnesral,
c63ccead5ef,skip,Untopiced,"Revert ""[dynamo] Enable some inlining inbuilt nn module tests (#128440)""",test/dynamo/test_inline_inbuilt_nn_modules.py,,,,,,
e9b81e4edfe,quantization,Untopiced,Fakify torch bind input by default (#128454),torch/_export/non_strict_utils.py torch/_functorch/aot_autograd.py,https://github.com/pytorch/pytorch/pull/128454,ydwu4,angelayi,,,
b05b8d39896,releng,not user facing,[EZ][ALI Migration] Add logging for workflow type determination (#128619),.github/scripts/get_workflow_type.py,https://github.com/pytorch/pytorch/pull/128619,ZainRizvi,clee2000,zxiiro,,
7c370d2fb07,distributed,Untopiced,expose set_thread_name to Python and set thread names (#128448),c10/util/thread_name.cpp c10/util/thread_name.h test/test_multiprocessing.py torch/_C/__init__.pyi.in torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/TCPStoreBackend.cpp torch/csrc/distributed/c10d/TCPStoreLibUvBackend.cpp torch/csrc/distributed/c10d/control_plane/WorkerServer.cpp torch/csrc/multiprocessing/init.cpp torch/distributed/run.py torch/multiprocessing/__init__.py torch/utils/data/_utils/pin_memory.py torch/utils/data/_utils/worker.py,https://github.com/pytorch/pytorch/pull/128448,d4l3k,andrewkho,c-p-i-o,rsdcastro,
e2a72313e85,skip,not user facing,Concat namespaces of torch/csrc/profiler code and other fixes (#128606),torch/csrc/profiler/python/init.cpp torch/csrc/profiler/python/init.h torch/csrc/profiler/python/pybind.h torch/csrc/profiler/standalone/execution_trace_observer.cpp torch/csrc/profiler/standalone/execution_trace_observer.h torch/csrc/profiler/standalone/itt_observer.cpp torch/csrc/profiler/standalone/itt_observer.h torch/csrc/profiler/standalone/nvtx_observer.cpp torch/csrc/profiler/standalone/nvtx_observer.h torch/csrc/profiler/standalone/privateuse1_observer.cpp torch/csrc/profiler/standalone/privateuse1_observer.h,https://github.com/pytorch/pytorch/pull/128606,cyyever,Skylion007,aaronenyeshi,,
bdeb9225b0b,fx,Untopiced,Do not call `get_implications` unnecessarily (#128410),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/128410,lezcano,Chillee,,,
0fdd8d84fa5,fx,Untopiced,Do not generate -1* in SymPy expressions when canonicalising (#128411),test/export/test_export.py test/test_dynamic_shapes.py torch/_export/passes/add_runtime_assertions_for_constraints_pass.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/128411,lezcano,ezyang,,,
3e3435678c6,fx,Untopiced,Remove some implications from the static_eval pattern matcher (#128500),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/128500,lezcano,ezyang,,,
a0604193a23,distributed,not user facing,handle call_function with Parameter args in DDPOptimizer splitting (#128034),test/dynamo/test_structured_trace.py torch/_dynamo/backends/distributed.py,https://github.com/pytorch/pytorch/pull/128034,laithsakka,anijain2305,wconstab,,
9a8917fdbdb,nested tensor_frontend,Untopiced,Naive CPU kernels for jagged <-> padded dense conversions (#127007),aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorTransformerFunctions.cpp test/test_nestedtensor.py,https://github.com/pytorch/pytorch/pull/127007,jbschlosser,davidberard98,,,
d5780396c70,skip,not user facing,"Skip debug asserts for mixed dense, subclass views in autograd_not_implemented_fallback (#128057)",torch/csrc/autograd/autograd_not_implemented_fallback.cpp,https://github.com/pytorch/pytorch/pull/128057,jbschlosser,albanD,soulitzer,,
52f529105d7,distributed,not user facing,force_stride_order on fused_all_gather_matmul/fused_matmul_reduce_scatter's operands to avoid a copy due to layout transformation (#127454),test/distributed/test_cuda_p2p.py torch/_inductor/fx_passes/micro_pipeline_tp.py torch/distributed/_cuda_p2p/__init__.py,https://github.com/pytorch/pytorch/pull/127454,yifuwang,Chillee,,,
dd19c9150c4,skip,Untopiced,"Revert ""[aota] compiled forward outputs requires_grad alignment with eager (#128016)""",test/functorch/test_aotdispatch.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/aot_autograd.py,,,,,,
8733c4f4be4,releng,not user facing,docs: Add link to test-infra issue (#128608),.github/workflows/_runner-determinator.yml,https://github.com/pytorch/pytorch/pull/128608,zxiiro,DanilBaibak,ZainRizvi,jeanschmidt,
de9a072ac40,skip,not user facing,Updating the `sigslot` license to Public Domain (#128085),third_party/LICENSES_BUNDLED.txt third_party/build_bundled.py,https://github.com/pytorch/pytorch/pull/128085,jjasghar,janeyx99,,,
a2655563627,inductor,not user facing,inductor fusion logs: make it easier to attribute to aten graph (#127159),torch/_inductor/ir.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/127159,vkuzo,mlazos,,,
9f55c80a9fd,inductor,Untopiced,[AOTI] Fix a minimal_arrayref_interface test failure (#128613),torch/_inductor/codegen/cpp_wrapper_cpu.py,https://github.com/pytorch/pytorch/pull/128613,desertfire,hl475,,,
5a80d2df844,skip,not user facing,[BE] enable UFMT for `torch/nn/utils` (#128595),.lintrunner.toml torch/nn/utils/__init__.py torch/nn/utils/_deprecation_utils.py torch/nn/utils/_expanded_weights/__init__.py torch/nn/utils/_expanded_weights/conv_expanded_weights.py torch/nn/utils/_expanded_weights/conv_utils.py torch/nn/utils/_expanded_weights/embedding_expanded_weights.py torch/nn/utils/_expanded_weights/expanded_weights_impl.py torch/nn/utils/_expanded_weights/expanded_weights_utils.py torch/nn/utils/_expanded_weights/group_norm_expanded_weights.py torch/nn/utils/_expanded_weights/instance_norm_expanded_weights.py torch/nn/utils/_expanded_weights/layer_norm_expanded_weights.py torch/nn/utils/_expanded_weights/linear_expanded_weights.py torch/nn/utils/_per_sample_grad.py torch/nn/utils/clip_grad.py torch/nn/utils/convert_parameters.py torch/nn/utils/fusion.py torch/nn/utils/init.py torch/nn/utils/memory_format.py torch/nn/utils/parametrizations.py torch/nn/utils/parametrize.py torch/nn/utils/prune.py torch/nn/utils/rnn.py torch/nn/utils/rnn.pyi torch/nn/utils/spectral_norm.py torch/nn/utils/stateless.py torch/nn/utils/weight_norm.py,https://github.com/pytorch/pytorch/pull/128595,XuehaiPan,Skylion007,,,
cdc37e4bffc,inductor,not user facing,Add a shape property to IR nodes (#127818),torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/127818,isuruf,peterbell10,,,
c54e358bdb4,inductor,not user facing,enable comprehensive padding internally (#128555),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/128555,shunting314,eellison,,,
39193b10e8b,inductor,not user facing,[inductor] fx graph cache: memoize devices to make cache key calculation more predictable (#128366),test/inductor/test_codecache.py torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/128366,masnesral,eellison,oulgen,,
a3af32c2fbc,distributed,not user facing,Add functionality to make ViewAndMutationData (slightly more) cache safe (#127618),test/dynamo/test_aot_autograd_cache.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/distributed/_tensor/api.py,https://github.com/pytorch/pytorch/pull/127618,jamesjwu,bdhirsh,,,
c0b40ab42e3,jit,docs,doc string for torch.jit.frontend.get_jit_class_def method (#128391),torch/jit/frontend.py,https://github.com/pytorch/pytorch/pull/128391,arunppsg,jgong5,malfet,,
3b28dc6c9d2,distributed,not user facing,Improve the scheduling for fused_matmul_reduce_scatter (#127455),torch/distributed/_cuda_p2p/__init__.py,https://github.com/pytorch/pytorch/pull/127455,yifuwang,Chillee,,,
790138fdc77,distributed,not user facing,Add profiler annotation for fused_all_gather_matmul and fused_matmul_reduce_scatter (#127556),torch/distributed/_cuda_p2p/__init__.py,https://github.com/pytorch/pytorch/pull/127556,yifuwang,awgu,,,
8763d44bf16,skip,not user facing,add xpu to torch.compile (#127279),docs/source/torch.compiler.rst docs/source/torch.compiler_get_started.rst,https://github.com/pytorch/pytorch/pull/127279,jingxu10,dvrogozh,svekars,,
c8e9656a122,skip,Untopiced,"Revert ""Add test to xfail_list only for abi_compatible (#128506)""",test/inductor/test_cpu_cpp_wrapper.py,,,,,,
bf8a05f483c,distributed,Untopiced,[FSDP2] Included module FQN in `FSDPParamGroup` `record_function`s (#128624),torch/distributed/_composable/fsdp/_fsdp_param_group.py,https://github.com/pytorch/pytorch/pull/128624,awgu,ckluk2,,,
6211e67e49f,jit,docs,Document `torch.jit.frontend.get_default_args` (#128408),torch/jit/frontend.py,https://github.com/pytorch/pytorch/pull/128408,ahoblitz,malfet,,,
3a0006ef222,skip,not user facing,"Remove global variable SIZE, and fix linter warning (#128559)",test/test_sort_and_select.py,https://github.com/pytorch/pytorch/pull/128559,yushangdi,Skylion007,kit1980,,
865d7b3424a,skip,not user facing,[Reland][dynamo] Enable some inlining inbuilt nn module tests (#128440),test/dynamo/test_inline_inbuilt_nn_modules.py test/dynamo/test_misc.py,https://github.com/pytorch/pytorch/pull/128440,anijain2305,jansel,williamwen42,,
8c20f53a5e2,skip,not user facing,Try seeding individual foreach tests (#128220),test/test_foreach.py,https://github.com/pytorch/pytorch/pull/128220,janeyx99,ZainRizvi,crcrpar,huydhn,
b72989a2b5a,onnx,new features,[ONNX] Add upsample trilinear to skip decomp (#128259),.ci/docker/common/install_onnx.sh test/onnx/test_fx_op_consistency.py test/onnx/test_fx_to_onnx.py test/onnx/test_fx_to_onnx_decomp_skip.py torch/onnx/_internal/fx/decomposition_skip.py,https://github.com/pytorch/pytorch/pull/128259,titaiwangms,justinchuby,,,
61421c42c0e,composability,Untopiced,[custom_op] don't invoke autograd.Function when unnecessary (#127976),test/test_custom_ops.py torch/_library/autograd.py,https://github.com/pytorch/pytorch/pull/127976,zou3519,williamwen42,,,
c486e2ab64e,dynamo,Untopiced,Add coloring to fx graph print out (#128476),test/dynamo/test_misc.py test/expect/TestFXAPIBackwardCompatibility.test_function_back_compat-fx_backcompat_function_signatures.expect torch/_dynamo/compiled_autograd.py torch/_dynamo/eval_frame.py torch/_dynamo/output_graph.py torch/_dynamo/utils.py torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py torch/_inductor/compile_fx.py torch/_inductor/freezing.py torch/export/exported_program.py torch/fx/graph.py torch/fx/graph_module.py torch/fx/passes/runtime_assert.py,https://github.com/pytorch/pytorch/pull/128476,Chillee,ezyang,,,
afdaa7fc950,export,Untopiced,[while_loop] expose it as torch.while_loop (#128562),torch/__init__.py,https://github.com/pytorch/pytorch/pull/128562,ydwu4,zou3519,,,
6767e38267f,skip,not user facing,Fix manual licensing (#128630),test/test_license.py third_party/LICENSES_BUNDLED.txt third_party/build_bundled.py,https://github.com/pytorch/pytorch/pull/128630,janeyx99,malfet,,,
9ebec1f3455,quantization,not user facing,Enable Wunused-function in torch_cpu (#128576),aten/src/ATen/native/BlasKernel.cpp aten/src/ATen/native/mkldnn/RNN.cpp aten/src/ATen/native/mkldnn/TensorShape.cpp aten/src/ATen/native/quantized/cpu/OnednnUtils.h cmake/public/utils.cmake torch/csrc/jit/passes/mkldnn_rewrite.cpp torch/csrc/profiler/standalone/execution_trace_observer.cpp,https://github.com/pytorch/pytorch/pull/128576,cyyever,ezyang,r-barnes,,
18f5357f4f8,inductor,Untopiced,Introduce heuristic for mixed_mm on A100 (#128232),test/inductor/test_pattern_matcher.py test/inductor/test_torchinductor.py torch/_inductor/config.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/ir.py torch/_inductor/kernel/mm.py,https://github.com/pytorch/pytorch/pull/128232,AlnisM,Chillee,,,
6f181756dca,skip,not user facing,Use by-column algorithm for fp16/bf16 CPUBlas gemm_transb kernels (#127318),aten/src/ATen/native/cpu/BlasKernel.cpp,https://github.com/pytorch/pytorch/pull/127318,swolchok,malfet,peterbell10,,
ae2359638bf,fx,Untopiced,Save DOT file of graph instead of SVG for GraphTranformObserver (#128634),test/fx/test_fx_xform_observer.py test/inductor/test_graph_transform_observer.py torch/fx/passes/graph_transform_observer.py,https://github.com/pytorch/pytorch/pull/128634,shengfukevin,mengluy0125,,,
6564d63e69a,mps,not user facing,Use mv kernel for small M (#128632),aten/src/ATen/native/mps/operations/Quantized.mm,https://github.com/pytorch/pytorch/pull/128632,larryliu0820,malfet,,,
6895a5804c6,skip,Untopiced,"Revert ""[checkpoint] Clean up selective activation checkpoint and make public (#125795)""",docs/source/checkpoint.rst test/dynamo/test_activation_checkpointing.py test/test_autograd.py torch/_higher_order_ops/wrap.py torch/utils/checkpoint.py,,,,,,
41df20c07ca,skip,not user facing,Run all samples for torchinductor tests (#128343),test/inductor/test_torchinductor_opinfo.py,https://github.com/pytorch/pytorch/pull/128343,isuruf,lezcano,,,
f48ca2561da,profiler,Untopiced,Document `torch.cuda.profiler.start` (#128098),torch/cuda/profiler.py,https://github.com/pytorch/pytorch/pull/128098,anandptl84,aaronenyeshi,,,
0186b386cd7,skip,Untopiced,"Revert ""[ONNX] Add upsample trilinear to skip decomp (#128259)""",.ci/docker/common/install_onnx.sh test/onnx/test_fx_op_consistency.py test/onnx/test_fx_to_onnx.py test/onnx/test_fx_to_onnx_decomp_skip.py torch/onnx/_internal/fx/decomposition_skip.py,,,,,,
685fcfb40de,skip,not user facing,Fix docstring in autograd (#128657),torch/autograd/__init__.py torch/autograd/anomaly_mode.py,https://github.com/pytorch/pytorch/pull/128657,spzala,soulitzer,,,
ba3726d02b2,skip,not user facing,[traced-graph][sparse] remove redundant assert in sparse prop test (#128523),test/export/test_sparse.py,https://github.com/pytorch/pytorch/pull/128523,aartbik,huydhn,,,
eff01bce21c,releng,not user facing,Only run inductor A100 perf benchmark smoke test periodically (#128677),.github/workflows/inductor-periodic.yml .github/workflows/inductor.yml,https://github.com/pytorch/pytorch/pull/128677,huydhn,atalman,desertfire,,
9ac08dab1f7,releng,not user facing,Updates diskspace-cleanup for ROCm CI (#127947),.github/actions/diskspace-cleanup/action.yml,https://github.com/pytorch/pytorch/pull/127947,amdfaa,jithunnair-amd,malfet,,
f48f7615dca,skip,not user facing,[easy][subclasses] dynamo.reset() in test_subclass_views (#128659),test/dynamo/test_subclasses.py,https://github.com/pytorch/pytorch/pull/128659,davidberard98,YuqingJ,,,
18f35d9e12b,skip,Untopiced,"Revert ""Run all samples for torchinductor tests (#128343)""",test/inductor/test_torchinductor_opinfo.py,,,,,,
674be9d3be5,Uncategorized,Untopiced,Update cu124 dynamo benchmark expected values (#128589),benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_torchbench_inference.csv,https://github.com/pytorch/pytorch/pull/128589,clee2000,DanilBaibak,nWEIdia,,
4669c6d3ae8,quantization,Untopiced,[quant][pt2e][quantizer] Support `set_module_name_qconfig` in X86InductorQuantizer (#126044),test/quantization/pt2e/test_x86inductor_quantizer.py torch/ao/quantization/quantizer/utils.py torch/ao/quantization/quantizer/x86_inductor_quantizer.py torch/ao/quantization/quantizer/xnnpack_quantizer.py,https://github.com/pytorch/pytorch/pull/126044,yiliu30,jerryzh168,jgong5,leslie-fang-intel,
e886122e981,distributed,not user facing,[dtensor][debug] add module level tracing and readable display (#128369),torch/distributed/_tensor/debug/comm_mode.py torch/distributed/_tensor/examples/display_sharding_example.py,https://github.com/pytorch/pytorch/pull/128369,sinhaanshul,XilunWu,,,
dd3b79a08fe,distributed,not user facing,[dtensor][be] improving readability of comm_mode.py and comm_mode_features_example.py (#128451),torch/distributed/_tensor/debug/comm_mode.py torch/distributed/_tensor/examples/comm_mode_features_example.py torch/distributed/_tensor/examples/display_sharding_example.py,https://github.com/pytorch/pytorch/pull/128451,sinhaanshul,XilunWu,,,
03725a05127,distributed,not user facing,[dtensor][example] added MLPStacked example for printing sharding (#128461),torch/distributed/_tensor/examples/comm_mode_features_example.py,https://github.com/pytorch/pytorch/pull/128461,sinhaanshul,XilunWu,,,
0344f95c2ea,skip,not user facing,Add missing #include <array> to thread_name.cpp (#128664),c10/util/thread_name.cpp,https://github.com/pytorch/pytorch/pull/128664,wconstab,d4l3k,fduwjj,malfet,
43ae3073f92,skip,Untopiced,"Revert ""[traced-graph][sparse] remove redundant assert in sparse prop test (#128523)""",test/export/test_sparse.py,,,,,,
03e8a4cf45e,inductor,not user facing,[Port][Quant][Inductor] Bug fix: mutation nodes not handled correctly for QLinearPointwiseBinaryPT2E (#128591),.ci/pytorch/common_utils.sh .circleci/scripts/binary_populate_env.sh .github/ci_commit_pins/xla.txt .github/scripts/filter_test_configs.py .github/templates/common.yml.j2 .github/templates/linux_binary_build_workflow.yml.j2 .github/templates/macos_binary_build_workflow.yml.j2 .github/templates/windows_binary_build_workflow.yml.j2 .github/workflows/_android-build-test.yml .github/workflows/_android-full-build-test.yml .github/workflows/_bazel-build-test.yml .github/workflows/_binary-build-linux.yml .github/workflows/_binary-test-linux.yml .github/workflows/_binary-upload.yml .github/workflows/_buck-build-test.yml .github/workflows/_docs.yml .github/workflows/_ios-build-test.yml .github/workflows/_linux-build-label.yml .github/workflows/_linux-build-rg.yml .github/workflows/_linux-build.yml .github/workflows/_linux-test-label.yml .github/workflows/_linux-test-rg.yml .github/workflows/_linux-test.yml .github/workflows/_mac-build.yml .github/workflows/_mac-test-mps.yml .github/workflows/_mac-test.yml .github/workflows/_rocm-test.yml .github/workflows/_run_android_tests.yml .github/workflows/_runner-determinator.yml .github/workflows/_win-build.yml .github/workflows/_win-test.yml .github/workflows/_xpu-test.yml .github/workflows/build-triton-wheel.yml .github/workflows/check-labels.yml .github/workflows/close-nonexistent-disable-issues.yml .github/workflows/docker-builds.yml .github/workflows/docker-release.yml .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-conda-nightly.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-main.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-main.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-nightly.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml .github/workflows/generated-macos-arm64-binary-conda-nightly.yml .github/workflows/generated-macos-arm64-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-conda-nightly.yml .github/workflows/generated-windows-binary-libtorch-debug-main.yml .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-main.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml .github/workflows/lint-bc.yml .github/workflows/lint.yml .github/workflows/llm_td_retrieval.yml .github/workflows/nightly-rockset-uploads.yml .github/workflows/nightly.yml .github/workflows/target-determination-indexer.yml .github/workflows/target_determination.yml .github/workflows/update-viablestrict.yml .github/workflows/update_pytorch_labels.yml .github/workflows/upload-alerts.yml .github/workflows/upload-test-stats.yml .github/workflows/upload-torch-dynamo-perf-stats.yml .github/workflows/upload_test_stats_intermediate.yml .github/workflows/weekly.yml tools/stats/import_test_stats.py,https://github.com/pytorch/pytorch/pull/127592,Xia-Weiwen,Chillee,leslie-fang-intel,,
99988be423e,releng,Untopiced,[halide-backend] Add test shard (#127308),.ci/docker/build.sh .ci/docker/ci_commit_pins/halide.txt .ci/docker/common/install_halide.sh .ci/docker/ubuntu-cuda/Dockerfile .ci/docker/ubuntu/Dockerfile .ci/pytorch/test.sh .github/workflows/docker-builds.yml .github/workflows/inductor.yml,https://github.com/pytorch/pytorch/pull/127308,jansel,eellison,shunting314,,
7e734e2d083,inductor,Untopiced,[inductor] Fix nested indirect indexing case for index_propagation (#128378),test/inductor/test_torchinductor.py torch/_inductor/index_propagation.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/128378,ColinPeppler,lezcano,peterbell10,,
e397ad68839,inductor,Untopiced,Improve codegen for ops.masked in triton (#128054),torch/_inductor/codegen/common.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/ops_handler.py,https://github.com/pytorch/pytorch/pull/128054,isuruf,lezcano,peterbell10,,
1f46284f9ed,skip,not user facing,Extended Module Tracker (#128508),test/distributed/_tools/test_mod_tracker.py torch/distributed/_tools/__init__.py torch/distributed/_tools/mod_tracker.py,https://github.com/pytorch/pytorch/pull/128508,sanketpurandare,wanchaol,,,
bfd5ea93e09,Uncategorized,Untopiced,Enable clang-tidy on c10/util/Float8*.h (#120573),.lintrunner.toml c10/util/Float8_e4m3fn-inl.h c10/util/Float8_e4m3fn.h c10/util/Float8_e4m3fnuz.cpp c10/util/Float8_e4m3fnuz.h c10/util/Float8_e5m2.cpp c10/util/Float8_e5m2fnuz.cpp,https://github.com/pytorch/pytorch/pull/120573,cyyever,drisspg,,,
e1dfc612502,skip,not user facing,Document CI/CD security philosophy (#128316),SECURITY.md,https://github.com/pytorch/pytorch/pull/128316,malfet,atalman,seemethere,,
dfc4b608e1c,Uncategorized,Untopiced,Remove leftover warning causing log spew (#128688),aten/src/ATen/native/transformers/cuda/sdp_utils.cpp,https://github.com/pytorch/pytorch/pull/128688,lw,drisspg,,,
a2d9c430b40,xpu,docs,Adding a note for Getting Started with PyTorch on Intel GPUs (#127872),docs/source/notes/get_start_xpu.rst,https://github.com/pytorch/pytorch/pull/127872,ZhaoqiongZ,svekars,,,
9972e5f447e,fx,not user facing,"Rename impl_abstract to register_fake, part 2/2 (#123938)",test/test_custom_ops.py torch/_custom_op/impl.py torch/_library/__init__.py torch/_library/abstract_impl.py torch/_library/fake_class_registry.py torch/_library/fake_impl.py torch/_library/simple_registry.py torch/_subclasses/fake_tensor.py torch/fx/experimental/symbolic_shapes.py torch/library.py,https://github.com/pytorch/pytorch/pull/123938,zou3519,williamwen42,,,
c76a9d13cb5,inductor,Untopiced,Revert D56709309 (#128481),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/128481,hl475,desertfire,,,
c339efaf023,skip,not user facing,[ROCm] Unskip scaled_dot_product_attention tests on ROCm (#127966),test/functorch/test_ops.py,https://github.com/pytorch/pytorch/pull/127966,alugorey,malfet,,,
c1875934181,skip,not user facing,Prevent expansion of cat indexing to avoid int64 intermediate (#127815),test/inductor/test_cuda_repro.py torch/_inductor/bounds.py torch/_inductor/codegen/common.py torch/_inductor/lowering.py torch/_inductor/utils.py torch/utils/_sympy/functions.py torch/utils/_sympy/interp.py torch/utils/_sympy/value_ranges.py,https://github.com/pytorch/pytorch/pull/127815,eellison,peterbell10,shunting314,,
ee140a198fb,skip,Untopiced,"Revert ""[Port][Quant][Inductor] Bug fix: mutation nodes not handled correctly for QLinearPointwiseBinaryPT2E (#128591)""",.ci/pytorch/common_utils.sh .circleci/scripts/binary_populate_env.sh .github/ci_commit_pins/xla.txt .github/scripts/filter_test_configs.py .github/templates/common.yml.j2 .github/templates/linux_binary_build_workflow.yml.j2 .github/templates/macos_binary_build_workflow.yml.j2 .github/templates/windows_binary_build_workflow.yml.j2 .github/workflows/_android-build-test.yml .github/workflows/_android-full-build-test.yml .github/workflows/_bazel-build-test.yml .github/workflows/_binary-build-linux.yml .github/workflows/_binary-test-linux.yml .github/workflows/_binary-upload.yml .github/workflows/_buck-build-test.yml .github/workflows/_docs.yml .github/workflows/_ios-build-test.yml .github/workflows/_linux-build-label.yml .github/workflows/_linux-build-rg.yml .github/workflows/_linux-build.yml .github/workflows/_linux-test-label.yml .github/workflows/_linux-test-rg.yml .github/workflows/_linux-test.yml .github/workflows/_mac-build.yml .github/workflows/_mac-test-mps.yml .github/workflows/_mac-test.yml .github/workflows/_rocm-test.yml .github/workflows/_run_android_tests.yml .github/workflows/_runner-determinator.yml .github/workflows/_win-build.yml .github/workflows/_win-test.yml .github/workflows/_xpu-test.yml .github/workflows/build-triton-wheel.yml .github/workflows/check-labels.yml .github/workflows/close-nonexistent-disable-issues.yml .github/workflows/docker-builds.yml .github/workflows/docker-release.yml .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-conda-nightly.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-main.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-main.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-nightly.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml .github/workflows/generated-macos-arm64-binary-conda-nightly.yml .github/workflows/generated-macos-arm64-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-conda-nightly.yml .github/workflows/generated-windows-binary-libtorch-debug-main.yml .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-main.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml .github/workflows/lint-bc.yml .github/workflows/lint.yml .github/workflows/llm_td_retrieval.yml .github/workflows/nightly-rockset-uploads.yml .github/workflows/nightly.yml .github/workflows/target-determination-indexer.yml .github/workflows/target_determination.yml .github/workflows/update-viablestrict.yml .github/workflows/update_pytorch_labels.yml .github/workflows/upload-alerts.yml .github/workflows/upload-test-stats.yml .github/workflows/upload-torch-dynamo-perf-stats.yml .github/workflows/upload_test_stats_intermediate.yml .github/workflows/weekly.yml tools/stats/import_test_stats.py,,,,,,
8629939a518,skip,not user facing,[torch/c10] Add C10_UBSAN_ENABLED macro and use it to disable SymInt_… (#127967),c10/macros/Macros.h c10/test/core/SymInt_test.cpp test/cpp/jit/test_misc.cpp,https://github.com/pytorch/pytorch/pull/127967,kiukchung,atalman,d4l3k,malfet,
732b4e90740,skip,not user facing,Fix generated vararg types (#128648),torchgen/api/python.py,https://github.com/pytorch/pytorch/pull/128648,aorenste,jamesjwu,,,
f75f5987aa9,skip,Untopiced,"Revert ""Extended Module Tracker (#128508)""",test/distributed/_tools/test_mod_tracker.py torch/distributed/_tools/__init__.py torch/distributed/_tools/mod_tracker.py,,,,,,
4c84af0f5d3,dynamo,not user facing,Fix indexing and slicing of ranges in dynamo (#128567),test/dynamo/test_functions.py test/dynamo_expected_failures/TestIterDataPipeGraphFastForward.test_simple_snapshot_custom_non_generator test/dynamo_expected_failures/TestIterDataPipeGraphFastForward.test_simple_snapshot_custom_self_next torch/_dynamo/variables/lists.py,https://github.com/pytorch/pytorch/pull/128567,laithsakka,anijain2305,,,
1fb4effe7ad,skip,not user facing,"[GPT-fast benchmark] Add MLP, gather + gemv, gemv micro benchmark (#128002)",benchmarks/gpt_fast/benchmark.py,https://github.com/pytorch/pytorch/pull/128002,yanboliang,Chillee,,,
089e76cca35,skip,not user facing,[traced-graph][sparse] remove redundant assert in sparse prop test (#128523),test/export/test_sparse.py,https://github.com/pytorch/pytorch/pull/128523,aartbik,huydhn,,,
d4807da802c,skip,not user facing,Various fixes of torch/csrc files (#127252),torch/csrc/serialization.cpp torch/csrc/utils/tensor_dtypes.cpp,https://github.com/pytorch/pytorch/pull/127252,cyyever,r-barnes,,,
2357490524d,inductor,Untopiced,[PT2] Enable shape_padding multiplier adjustment (#128346),torch/_inductor/fx_passes/pad_mm.py torch/_inductor/fx_passes/split_cat.py,https://github.com/pytorch/pytorch/pull/128346,mengluy0125,jackiexu1992,,,
d7fc871175a,inductor,not user facing,[inductor] Improve superfluous mask handling in triton codegen (#128518),torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/128518,peterbell10,lezcano,,,
2367161e4bb,skip,Untopiced,"Revert ""[ROCm] Unskip scaled_dot_product_attention tests on ROCm (#127966)""",test/functorch/test_ops.py,,,,,,
be0eec9031c,skip,not user facing,[export] Improve static typing in tracer. (#128552),torch/_export/serde/serialize.py torch/export/_trace.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/128552,zhxchen17,angelayi,,,
b94c52dd296,releng,not user facing,[GHF] Refuse merge to non-default branch (#128710),.github/scripts/test_trymerge.py .github/scripts/trymerge.py,https://github.com/pytorch/pytorch/pull/128710,malfet,atalman,seemethere,,
a6bd154a42a,inductor,not user facing,[inductor] Support mm decomps for matrices with unbacked sizes (#128655),test/inductor/test_unbacked_symints.py torch/_inductor/decomposition.py,https://github.com/pytorch/pytorch/pull/128655,ColinPeppler,jansel,,,
27458cc097f,skip,not user facing,[BE] Refactor repeated code in test_weight_norm (#128726),test/test_mps.py,https://github.com/pytorch/pytorch/pull/128726,malfet,kit1980,,,
9035fff2de2,skip,not user facing,[BE] Do not test deprecated `torch.nn.utils.weight_norm` (#128727),test/test_mps.py,https://github.com/pytorch/pytorch/pull/128727,malfet,kit1980,,,
d50712e5e35,inductor,Untopiced,[PT2] add inductor log for unbind_stack_pass (#128684),torch/_inductor/fx_passes/split_cat.py,https://github.com/pytorch/pytorch/pull/128684,mengluy0125,dshi7,,,
2e5366fbc04,skip,not user facing,Extended Module Tracker (#128508),test/distributed/_tools/test_mod_tracker.py torch/distributed/_tools/__init__.py torch/distributed/_tools/mod_tracker.py,https://github.com/pytorch/pytorch/pull/128508,sanketpurandare,wanchaol,,,
9c773321160,Uncategorized,Untopiced,[torch.compile][ci] Flaky models in CI (similar to DISABLED_TEST) (#128715),benchmarks/dynamo/check_accuracy.py,https://github.com/pytorch/pytorch/pull/128715,anijain2305,eellison,,,
1aafb9eb907,skip,not user facing,[dynamo][yolov3] Track UnspecializedNNModuleVariable for mutation (#128269),benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/aot_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_training.csv test/dynamo/test_modules.py torch/_dynamo/output_graph.py,https://github.com/pytorch/pytorch/pull/128269,anijain2305,jansel,,,
4b96575a09b,dynamo,Untopiced,[dynamo][aot autograd] Silently disable default saved tensor hooks during tracing (#123196),aten/src/ATen/SavedTensorHooks.cpp aten/src/ATen/SavedTensorHooks.h test/dynamo/test_higher_order_ops.py test/dynamo/test_repros.py test/dynamo_expected_failures/TestNestedCheckpoint.test_nested_checkpoint_kwargs_early_stop_False test/dynamo_expected_failures/TestNestedCheckpoint.test_nested_checkpoint_non_tensor_inputs_and_outputs_early_stop_False test/dynamo_expected_failures/TestNestedCheckpoint.test_nested_checkpoint_reentrant_backwards_early_stop_False test/dynamo_expected_failures/TestNestedCheckpoint.test_nested_checkpoint_same_graph_early_stop_False test/inductor/test_compiled_autograd.py test/test_autograd.py test/test_functionalization_of_rng_ops.py torch/_C/_autograd.pyi torch/_dynamo/__init__.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/_functorch/aot_autograd.py torch/csrc/autograd/init.cpp torch/csrc/autograd/python_saved_variable_hooks.cpp,https://github.com/pytorch/pytorch/pull/123196,xmfan,soulitzer,,,
11de50f17cc,dynamo,not user facing,[Dynamo] skip some TorchScript tests (#128731),test/test_jit.py,https://github.com/pytorch/pytorch/pull/128731,zou3519,williamwen42,,,
e6e102cf85d,skip,not user facing,Dynamo testing: add some skips (#128734),test/dynamo_skips/TestTorchTidyProfiler.test_optimizer_parameters_sgd,https://github.com/pytorch/pytorch/pull/128734,zou3519,williamwen42,,,
e9a29aaa4a3,onnx,new features,[ONNX] Add upsample trilinear to skip decomp (#128259),.ci/docker/common/install_onnx.sh test/onnx/test_fx_op_consistency.py test/onnx/test_fx_to_onnx.py test/onnx/test_fx_to_onnx_decomp_skip.py torch/onnx/_internal/fx/decomposition_skip.py,https://github.com/pytorch/pytorch/pull/128259,titaiwangms,justinchuby,,,
65d3ddcb8bd,skip,not user facing,Add GLIBC requirements for libtorch to solve #113124 (#128135),docs/cpp/source/installing.rst,https://github.com/pytorch/pytorch/pull/128135,ignaciobartol,jbschlosser,,,
e9c6e8369c5,jit,not user facing,Torchbind call method + effects support (#128397),test/export/test_torchbind.py torch/_export/passes/lift_constants_pass.py torch/_export/serde/serialize.py torch/_higher_order_ops/effects.py torch/_higher_order_ops/torchbind.py torch/_ops.py torch/csrc/jit/python/init.cpp torch/export/_remove_effect_tokens_pass.py torch/export/_trace.py torch/export/graph_signature.py torch/export/unflatten.py torch/fx/graph.py torch/fx/proxy.py,https://github.com/pytorch/pytorch/pull/128397,angelayi,ydwu4,zou3519,,
f103247a142,skip,not user facing,Run all samples for torchinductor tests (#128343),test/inductor/test_torchinductor_opinfo.py,https://github.com/pytorch/pytorch/pull/128343,isuruf,lezcano,,,
bca2cf00edc,onnx,new features,[ONNX] Add dynamic axes support to torchscript exporter with dynamo=True (#128371),test/onnx/dynamo/test_exporter_api.py torch/onnx/utils.py,https://github.com/pytorch/pytorch/pull/128371,titaiwangms,justinchuby,,,
d3a4d9e4fee,Uncategorized,Untopiced,Update cu124 dynamo benchmark expected values (#128737),benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_torchbench_training.csv,https://github.com/pytorch/pytorch/pull/128737,clee2000,Skylion007,,,
fd27138c4a8,skip,not user facing,Update DALLE2_pytorch expected accuracy result on CPU (#128718),benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_inductor_torchbench_inference.csv,https://github.com/pytorch/pytorch/pull/128718,huydhn,malfet,,,
4abecd71028,skip,not user facing,[AOTI] fixed performance issue for AOTI_TORCH_CHECK (#128402),torch/csrc/inductor/aoti_torch/c/shim.h,https://github.com/pytorch/pytorch/pull/128402,chenyang78,desertfire,,,
52d4442a00c,distributed,Untopiced,"[c10d] Socket, TCPStore: add better logging (#128673)",test/distributed/test_store.py torch/csrc/distributed/c10d/TCPStore.cpp torch/csrc/distributed/c10d/TCPStoreBackend.cpp torch/csrc/distributed/c10d/TCPStoreLibUvBackend.cpp torch/csrc/distributed/c10d/socket.cpp torch/csrc/distributed/c10d/socket.h,https://github.com/pytorch/pytorch/pull/128673,d4l3k,c-p-i-o,,,
bd72e28314d,skip,Untopiced,[1/N] Change #include <c10/util/Optional.h> to #include <optional> (#128301),aten/src/ATen/CPUGeneratorImpl.h aten/src/ATen/InferSize.h aten/src/ATen/SavedTensorHooks.cpp aten/src/ATen/SavedTensorHooks.h aten/src/ATen/TensorIndexing.h aten/src/ATen/native/BatchLinearAlgebra.h aten/src/ATen/record_function.h c10/core/ConstantSymNodeImpl.h c10/core/ScalarTypeToTypeMeta.h c10/core/SymBool.h c10/core/SymInt.h c10/core/SymIntArrayRef.h c10/core/SymNodeImpl.h c10/core/SymbolicShapeMeta.cpp c10/core/TensorImpl.cpp c10/core/TensorImpl.h c10/core/TensorOptions.h c10/core/UndefinedTensorImpl.cpp c10/core/impl/InlineDeviceGuard.h c10/core/impl/InlineStreamGuard.h c10/core/impl/PyObjectSlot.h c10/core/impl/TorchDispatchModeTLS.cpp c10/cuda/CUDACachingAllocator.cpp c10/cuda/CUDAFunctions.cpp c10/cuda/CUDAGuard.h c10/cuda/impl/CUDAGuardImpl.h c10/test/core/DeviceGuard_test.cpp c10/test/core/SymInt_test.cpp c10/test/core/impl/InlineDeviceGuard_test.cpp c10/test/core/impl/InlineStreamGuard_test.cpp c10/test/util/optional_test.cpp c10/util/Backtrace.cpp c10/util/OptionalArrayRef.h c10/xpu/test/impl/XPUStreamTest.cpp torch/csrc/Module.cpp torch/csrc/Storage.cpp torch/csrc/api/include/torch/expanding_array.h torch/csrc/api/include/torch/fft.h torch/csrc/api/include/torch/nested.h torch/csrc/api/include/torch/nn/functional/activation.h torch/csrc/api/include/torch/nn/functional/embedding.h torch/csrc/api/include/torch/nn/functional/loss.h torch/csrc/api/include/torch/nn/functional/normalization.h torch/csrc/api/include/torch/nn/functional/pooling.h torch/csrc/api/include/torch/nn/functional/upsampling.h torch/csrc/api/include/torch/nn/modules/batchnorm.h torch/csrc/api/include/torch/nn/modules/conv.h torch/csrc/api/include/torch/nn/modules/pooling.h torch/csrc/api/include/torch/nn/modules/utils.h torch/csrc/api/include/torch/nn/options/activation.h torch/csrc/api/include/torch/nn/options/embedding.h torch/csrc/api/include/torch/nn/options/loss.h torch/csrc/api/include/torch/nn/options/normalization.h torch/csrc/api/include/torch/nn/options/pooling.h torch/csrc/api/include/torch/nn/options/upsampling.h torch/csrc/api/include/torch/nn/options/vision.h torch/csrc/api/include/torch/nn/utils/clip_grad.h torch/csrc/api/include/torch/nn/utils/convert_parameters.h torch/csrc/api/include/torch/optim/lbfgs.h torch/csrc/api/include/torch/optim/optimizer.h torch/csrc/api/include/torch/serialize/input-archive.h torch/csrc/api/include/torch/types.h torch/csrc/api/src/jit.cpp torch/csrc/api/src/nn/modules/activation.cpp torch/csrc/api/src/nn/modules/conv.cpp torch/csrc/api/src/nn/modules/embedding.cpp torch/csrc/api/src/nn/modules/pooling.cpp torch/csrc/api/src/nn/modules/upsampling.cpp torch/csrc/api/src/optim/lbfgs.cpp torch/csrc/api/src/serialize/input-archive.cpp torch/csrc/autograd/FunctionsManual.cpp torch/csrc/autograd/FunctionsManual.h torch/csrc/autograd/TraceTypeManual.cpp torch/csrc/autograd/VariableTypeManual.cpp torch/csrc/autograd/VariableTypeUtils.h torch/csrc/autograd/autograd.h torch/csrc/autograd/autograd_not_implemented_fallback.cpp torch/csrc/autograd/engine.cpp torch/csrc/autograd/function.h torch/csrc/autograd/functions/accumulate_grad.h torch/csrc/autograd/functions/comm.cpp torch/csrc/autograd/functions/comm.h torch/csrc/autograd/init.cpp torch/csrc/autograd/input_buffer.cpp torch/csrc/autograd/input_buffer.h torch/csrc/autograd/profiler_legacy.cpp torch/csrc/autograd/profiler_legacy.h torch/csrc/autograd/profiler_python.cpp torch/csrc/autograd/python_function.cpp torch/csrc/autograd/python_function.h torch/csrc/autograd/python_variable.cpp torch/csrc/autograd/python_variable_indexing.cpp torch/csrc/autograd/record_function_ops.h torch/csrc/autograd/utils/grad_layout_contract.h torch/csrc/autograd/utils/python_arg_parsing.h torch/csrc/autograd/variable.h torch/csrc/cuda/comm.cpp torch/csrc/cuda/comm.h torch/csrc/cuda/memory_snapshot.h torch/csrc/cuda/nccl.h torch/csrc/cuda/python_nccl.cpp torch/csrc/distributed/autograd/engine/dist_engine.cpp torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupCudaP2P.hpp torch/csrc/distributed/c10d/ProcessGroupGloo.cpp torch/csrc/distributed/c10d/ProcessGroupGloo.hpp torch/csrc/distributed/c10d/ProcessGroupMPI.cpp torch/csrc/distributed/c10d/ProcessGroupMPI.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/TCPStore.cpp torch/csrc/distributed/c10d/TCPStore.hpp torch/csrc/distributed/c10d/TraceUtils.h torch/csrc/distributed/c10d/Types.hpp torch/csrc/distributed/c10d/Utils.hpp torch/csrc/distributed/c10d/Work.hpp torch/csrc/distributed/c10d/init.cpp torch/csrc/distributed/c10d/intra_node_comm.hpp torch/csrc/distributed/c10d/logger.cpp torch/csrc/distributed/c10d/reducer.cpp torch/csrc/distributed/c10d/reducer.hpp torch/csrc/distributed/c10d/reducer_cuda.cpp torch/csrc/distributed/c10d/reducer_timer.hpp torch/csrc/distributed/c10d/sequence_num.cpp torch/csrc/distributed/c10d/sequence_num.hpp torch/csrc/distributed/rpc/profiler/remote_profiler_manager.cpp torch/csrc/distributed/rpc/profiler/remote_profiler_manager.h torch/csrc/distributed/rpc/py_rref.cpp torch/csrc/distributed/rpc/python_functions.cpp torch/csrc/distributed/rpc/request_callback_no_python.cpp torch/csrc/distributed/rpc/rref_impl.h torch/csrc/distributed/rpc/script_call.h torch/csrc/distributed/rpc/tensorpipe_cuda.cpp torch/csrc/distributed/rpc/tensorpipe_utils.cpp torch/csrc/dynamo/python_compiled_autograd.cpp torch/csrc/functorch/init.cpp torch/csrc/inductor/aoti_torch/utils.h torch/csrc/jit/api/compilation_unit.h torch/csrc/jit/api/function_impl.h torch/csrc/jit/api/module.cpp torch/csrc/jit/api/module.h torch/csrc/jit/api/object.cpp torch/csrc/jit/api/object.h torch/csrc/jit/codegen/fuser/compiler.cpp torch/csrc/jit/codegen/fuser/cpu/fused_kernel.cpp torch/csrc/jit/codegen/fuser/executor.cpp torch/csrc/jit/codegen/fuser/kernel_spec.h torch/csrc/jit/codegen/onednn/graph_helper.cpp torch/csrc/jit/codegen/onednn/graph_rewriter.cpp torch/csrc/jit/codegen/onednn/prepare_binary.cpp torch/csrc/jit/cuda/cuda.h torch/csrc/jit/frontend/builtin_functions.cpp torch/csrc/jit/frontend/canonicalize_modified_loop.cpp torch/csrc/jit/frontend/concrete_module_type.cpp torch/csrc/jit/frontend/function_schema_parser.cpp torch/csrc/jit/frontend/ir_emitter.cpp torch/csrc/jit/frontend/parse_string_literal.h torch/csrc/jit/frontend/parser.cpp torch/csrc/jit/frontend/schema_matching.cpp torch/csrc/jit/frontend/schema_matching.h torch/csrc/jit/frontend/schema_type_parser.cpp torch/csrc/jit/frontend/script_type_parser.cpp torch/csrc/jit/frontend/source_range.cpp torch/csrc/jit/frontend/source_range.h torch/csrc/jit/frontend/sugared_value.cpp torch/csrc/jit/frontend/sugared_value.h torch/csrc/jit/frontend/tracer.cpp torch/csrc/jit/ir/alias_analysis.cpp torch/csrc/jit/ir/constants.cpp torch/csrc/jit/ir/constants.h torch/csrc/jit/ir/ir.cpp torch/csrc/jit/ir/ir.h torch/csrc/jit/ir/scope.h torch/csrc/jit/mobile/compatibility/backport_manager.cpp torch/csrc/jit/mobile/compatibility/runtime_compatibility.h torch/csrc/jit/mobile/flatbuffer_loader.cpp torch/csrc/jit/mobile/flatbuffer_loader.h torch/csrc/jit/mobile/frame.h torch/csrc/jit/mobile/function.cpp torch/csrc/jit/mobile/import.cpp torch/csrc/jit/mobile/import.h torch/csrc/jit/mobile/import_data.h torch/csrc/jit/mobile/model_tracer/MobileModelRunner.h torch/csrc/jit/mobile/model_tracer/TracerRunner.cpp torch/csrc/jit/mobile/module.cpp torch/csrc/jit/mobile/promoted_prim_ops.cpp torch/csrc/jit/operator_upgraders/upgraders_entry.cpp torch/csrc/jit/operator_upgraders/utils.cpp torch/csrc/jit/operator_upgraders/utils.h torch/csrc/jit/passes/autocast.cpp torch/csrc/jit/passes/canonicalize.cpp torch/csrc/jit/passes/canonicalize_graph_fuser_ops.cpp torch/csrc/jit/passes/constant_propagation.cpp torch/csrc/jit/passes/create_autodiff_subgraphs.cpp torch/csrc/jit/passes/device_type_analysis.cpp torch/csrc/jit/passes/dtype_analysis.cpp torch/csrc/jit/passes/erase_number_types.cpp torch/csrc/jit/passes/freeze_module.cpp torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp torch/csrc/jit/passes/graph_fuser.cpp torch/csrc/jit/passes/graph_rewrite_helper.cpp torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp torch/csrc/jit/passes/integer_value_refinement.cpp torch/csrc/jit/passes/onnx/constant_fold.cpp torch/csrc/jit/passes/onnx/constant_fold.h torch/csrc/jit/passes/onnx/constant_map.cpp torch/csrc/jit/passes/onnx/function_extraction.cpp torch/csrc/jit/passes/onnx/list_model_parameters.cpp torch/csrc/jit/passes/onnx/pattern_conversion/pattern_conversion.cpp torch/csrc/jit/passes/onnx/pattern_conversion/pattern_encapsulation.cpp torch/csrc/jit/passes/onnx/peephole.cpp torch/csrc/jit/passes/onnx/scalar_type_analysis.cpp torch/csrc/jit/passes/onnx/shape_type_inference.cpp torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp torch/csrc/jit/passes/peephole_dict_idioms.cpp torch/csrc/jit/passes/peephole_list_idioms.cpp torch/csrc/jit/passes/quantization/helper.cpp torch/csrc/jit/passes/quantization/helper.h torch/csrc/jit/passes/quantization/insert_observers.cpp torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp torch/csrc/jit/passes/remove_mutation.h torch/csrc/jit/passes/replacement_of_old_operators.cpp torch/csrc/jit/passes/shape_analysis.cpp torch/csrc/jit/passes/symbolic_shape_analysis.cpp torch/csrc/jit/passes/symbolic_shape_cache.cpp torch/csrc/jit/passes/symbolic_shape_runtime_fusion.cpp torch/csrc/jit/passes/tensorexpr_fuser.cpp torch/csrc/jit/passes/utils/check_alias_annotation.cpp torch/csrc/jit/passes/utils/memory_dag.h torch/csrc/jit/passes/utils/subgraph_utils.cpp torch/csrc/jit/python/init.cpp torch/csrc/jit/python/module_python.h torch/csrc/jit/python/pybind_utils.cpp torch/csrc/jit/python/pybind_utils.h torch/csrc/jit/python/python_ir.cpp torch/csrc/jit/python/python_ivalue.h torch/csrc/jit/python/python_list.h torch/csrc/jit/python/python_sugared_value.cpp torch/csrc/jit/python/python_sugared_value.h torch/csrc/jit/python/python_tree_views.cpp torch/csrc/jit/python/script_init.cpp torch/csrc/jit/runtime/autodiff.cpp torch/csrc/jit/runtime/decomposition_registry.cpp torch/csrc/jit/runtime/graph_executor.h torch/csrc/jit/runtime/graph_executor_impl.h torch/csrc/jit/runtime/interpreter.cpp torch/csrc/jit/runtime/interpreter.h torch/csrc/jit/runtime/jit_exception.h torch/csrc/jit/runtime/operator.h torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp torch/csrc/jit/runtime/register_ops_utils.h torch/csrc/jit/runtime/register_prim_ops.cpp torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp torch/csrc/jit/runtime/register_special_ops.cpp torch/csrc/jit/runtime/simple_graph_executor_impl.cpp torch/csrc/jit/runtime/static/fusion.cpp torch/csrc/jit/runtime/static/impl.cpp torch/csrc/jit/runtime/static/ops.cpp torch/csrc/jit/runtime/static/ops.h torch/csrc/jit/runtime/symbolic_script.cpp torch/csrc/jit/runtime/symbolic_script.h torch/csrc/jit/runtime/symbolic_shape_registry.cpp torch/csrc/jit/serialization/callstack_debug_info_serialization.cpp torch/csrc/jit/serialization/export.cpp torch/csrc/jit/serialization/export_bytecode.cpp torch/csrc/jit/serialization/export_module.cpp torch/csrc/jit/serialization/flatbuffer_serializer.cpp torch/csrc/jit/serialization/import.h torch/csrc/jit/serialization/import_source.cpp torch/csrc/jit/serialization/import_source.h torch/csrc/jit/serialization/pickle.cpp torch/csrc/jit/serialization/pickler.cpp torch/csrc/jit/serialization/python_print.cpp torch/csrc/jit/serialization/source_range_serialization.cpp torch/csrc/jit/tensorexpr/codegen.cpp torch/csrc/jit/tensorexpr/eval.cpp torch/csrc/jit/tensorexpr/expr.h torch/csrc/jit/tensorexpr/external_functions.cpp torch/csrc/jit/tensorexpr/external_functions.h torch/csrc/jit/tensorexpr/graph_opt.cpp torch/csrc/jit/tensorexpr/ir.h torch/csrc/jit/tensorexpr/ir_simplifier.cpp torch/csrc/jit/tensorexpr/kernel.cpp torch/csrc/jit/tensorexpr/llvm_codegen.cpp torch/csrc/jit/tensorexpr/llvm_codegen.h torch/csrc/jit/tensorexpr/llvm_jit.h torch/csrc/jit/tensorexpr/operators/conv2d.cpp torch/csrc/jit/tensorexpr/operators/misc.cpp torch/csrc/jit/tensorexpr/operators/pointwise.h torch/csrc/jit/tensorexpr/operators/quantization.cpp torch/csrc/jit/tensorexpr/operators/softmax.cpp torch/csrc/jit/tensorexpr/tensor.cpp torch/csrc/jit/tensorexpr/tensor.h torch/csrc/jit/testing/file_check.cpp torch/csrc/lazy/backend/backend_device.cpp torch/csrc/lazy/backend/backend_device.h torch/csrc/lazy/core/ir_builder.h torch/csrc/lazy/core/ir_dump_util.cpp torch/csrc/lazy/core/lazy_graph_executor.cpp torch/csrc/lazy/core/shape.cpp torch/csrc/lazy/core/shape.h torch/csrc/lazy/core/shape_inference.h torch/csrc/lazy/core/tensor.cpp torch/csrc/lazy/core/unique.h torch/csrc/lazy/core/util.h torch/csrc/lazy/python/python_util.cpp torch/csrc/lazy/python/python_util.h torch/csrc/lazy/ts_backend/ir_builder.h torch/csrc/lazy/ts_backend/ts_eager_fallback.cpp torch/csrc/lazy/ts_backend/ts_native_functions.cpp torch/csrc/profiler/collection.cpp torch/csrc/profiler/collection.h torch/csrc/profiler/python/init.cpp torch/csrc/profiler/unwind/unwind.cpp torch/csrc/profiler/unwind/unwind.h torch/csrc/profiler/unwind/unwind_error.h torch/csrc/profiler/util.h torch/csrc/tensor/python_tensor.cpp torch/csrc/utils/nested.cpp torch/csrc/utils/python_arg_parser.cpp torch/csrc/utils/python_arg_parser.h torch/csrc/utils/python_dispatch.cpp torch/csrc/utils/python_raii.h torch/csrc/utils/python_symnode.h torch/csrc/utils/schema_info.cpp torch/csrc/utils/tensor_new.cpp torch/csrc/utils/torch_dispatch_mode.h torch/custom_class_detail.h torch/library.h,https://github.com/pytorch/pytorch/pull/128301,cyyever,ezyang,r-barnes,,
0492ec460a9,releng,Untopiced,[BE] Remove external testing of torch::deploy (#127952),.ci/pytorch/common_utils.sh .ci/pytorch/test.sh .github/workflows/pull.yml,https://github.com/pytorch/pytorch/pull/127952,PaliC,malfet,,,
574a2cbcb73,skip,not user facing,Enable UFMT on common_device_type.py and common_dtype.py (#128490),.lintrunner.toml torch/testing/_internal/common_device_type.py torch/testing/_internal/common_dtype.py,https://github.com/pytorch/pytorch/pull/128490,dilililiwhy,XuehaiPan,ezyang,,
ba19ed9a1a4,skip,not user facing,FunctionalTensor: dispatch metadata directly to inner tensor (#127927),torch/_subclasses/functional_tensor.py,https://github.com/pytorch/pytorch/pull/127927,bdhirsh,tugsbayasgalan,,,
271852aa7e4,inductor,not user facing,inductor: pre-grad bmm pass shouldn't match if output is mutated (#128570),torch/_inductor/fx_passes/group_batch_fusion.py,https://github.com/pytorch/pytorch/pull/128570,bdhirsh,eellison,mlazos,,
d67923b955c,distributed,Untopiced,Adding kwargs to composable AC API to enable full capabilities (#128516),test/distributed/_composable/test_checkpoint.py torch/distributed/_composable/checkpoint_activation.py,https://github.com/pytorch/pytorch/pull/128516,sanketpurandare,awgu,,,
5d9a609b4f6,skip,Untopiced,[export] Add print_readable to unflattener (#128617),test/dynamo/test_autograd_function.py test/dynamo/test_higher_order_ops.py test/dynamo/test_subclasses.py torch/export/unflatten.py torch/fx/graph_module.py,https://github.com/pytorch/pytorch/pull/128617,angelayi,pianpwk,zhxchen17,,
6616ad030f9,inductor,not user facing,[Inductor] Fix the High Order Op layout issue (#128275),test/higher_order_ops/test_with_effects.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/128275,leslie-fang-intel,eellison,,,
73ba432d32a,jit,not user facing,[custom_op]Fix None return schema (#128667),test/dynamo/test_misc.py torch/_higher_order_ops/auto_functionalize.py,https://github.com/pytorch/pytorch/pull/128667,FindHao,zou3519,,,
3f47c72268c,dataloader_frontend,not user facing,add multiprocessing checks in test_dataloader.py (#128244),test/test_dataloader.py,https://github.com/pytorch/pytorch/pull/128244,Fuzzkatt,eqy,malfet,,
f37121bb74c,skip,not user facing,"Add model name, quantization and device to gpt_fast micro benchmark output (#128091)",benchmarks/gpt_fast/benchmark.py benchmarks/gpt_fast/generate.py,https://github.com/pytorch/pytorch/pull/128091,huydhn,yanboliang,,,
5efe71f1345,skip,Untopiced,"Revert ""[export] Add print_readable to unflattener (#128617)""",test/dynamo/test_autograd_function.py test/dynamo/test_higher_order_ops.py test/dynamo/test_subclasses.py torch/export/unflatten.py torch/fx/graph_module.py,,,,,,
846bb30e13a,skip,Untopiced,"Revert ""[1/N] Change #include <c10/util/Optional.h> to #include <optional> (#128301)""",aten/src/ATen/CPUGeneratorImpl.h aten/src/ATen/InferSize.h aten/src/ATen/SavedTensorHooks.cpp aten/src/ATen/SavedTensorHooks.h aten/src/ATen/TensorIndexing.h aten/src/ATen/native/BatchLinearAlgebra.h aten/src/ATen/record_function.h c10/core/ConstantSymNodeImpl.h c10/core/ScalarTypeToTypeMeta.h c10/core/SymBool.h c10/core/SymInt.h c10/core/SymIntArrayRef.h c10/core/SymNodeImpl.h c10/core/SymbolicShapeMeta.cpp c10/core/TensorImpl.cpp c10/core/TensorImpl.h c10/core/TensorOptions.h c10/core/UndefinedTensorImpl.cpp c10/core/impl/InlineDeviceGuard.h c10/core/impl/InlineStreamGuard.h c10/core/impl/PyObjectSlot.h c10/core/impl/TorchDispatchModeTLS.cpp c10/cuda/CUDACachingAllocator.cpp c10/cuda/CUDAFunctions.cpp c10/cuda/CUDAGuard.h c10/cuda/impl/CUDAGuardImpl.h c10/test/core/DeviceGuard_test.cpp c10/test/core/SymInt_test.cpp c10/test/core/impl/InlineDeviceGuard_test.cpp c10/test/core/impl/InlineStreamGuard_test.cpp c10/test/util/optional_test.cpp c10/util/Backtrace.cpp c10/util/OptionalArrayRef.h c10/xpu/test/impl/XPUStreamTest.cpp torch/csrc/Module.cpp torch/csrc/Storage.cpp torch/csrc/api/include/torch/expanding_array.h torch/csrc/api/include/torch/fft.h torch/csrc/api/include/torch/nested.h torch/csrc/api/include/torch/nn/functional/activation.h torch/csrc/api/include/torch/nn/functional/embedding.h torch/csrc/api/include/torch/nn/functional/loss.h torch/csrc/api/include/torch/nn/functional/normalization.h torch/csrc/api/include/torch/nn/functional/pooling.h torch/csrc/api/include/torch/nn/functional/upsampling.h torch/csrc/api/include/torch/nn/modules/batchnorm.h torch/csrc/api/include/torch/nn/modules/conv.h torch/csrc/api/include/torch/nn/modules/pooling.h torch/csrc/api/include/torch/nn/modules/utils.h torch/csrc/api/include/torch/nn/options/activation.h torch/csrc/api/include/torch/nn/options/embedding.h torch/csrc/api/include/torch/nn/options/loss.h torch/csrc/api/include/torch/nn/options/normalization.h torch/csrc/api/include/torch/nn/options/pooling.h torch/csrc/api/include/torch/nn/options/upsampling.h torch/csrc/api/include/torch/nn/options/vision.h torch/csrc/api/include/torch/nn/utils/clip_grad.h torch/csrc/api/include/torch/nn/utils/convert_parameters.h torch/csrc/api/include/torch/optim/lbfgs.h torch/csrc/api/include/torch/optim/optimizer.h torch/csrc/api/include/torch/serialize/input-archive.h torch/csrc/api/include/torch/types.h torch/csrc/api/src/jit.cpp torch/csrc/api/src/nn/modules/activation.cpp torch/csrc/api/src/nn/modules/conv.cpp torch/csrc/api/src/nn/modules/embedding.cpp torch/csrc/api/src/nn/modules/pooling.cpp torch/csrc/api/src/nn/modules/upsampling.cpp torch/csrc/api/src/optim/lbfgs.cpp torch/csrc/api/src/serialize/input-archive.cpp torch/csrc/autograd/FunctionsManual.cpp torch/csrc/autograd/FunctionsManual.h torch/csrc/autograd/TraceTypeManual.cpp torch/csrc/autograd/VariableTypeManual.cpp torch/csrc/autograd/VariableTypeUtils.h torch/csrc/autograd/autograd.h torch/csrc/autograd/autograd_not_implemented_fallback.cpp torch/csrc/autograd/engine.cpp torch/csrc/autograd/function.h torch/csrc/autograd/functions/accumulate_grad.h torch/csrc/autograd/functions/comm.cpp torch/csrc/autograd/functions/comm.h torch/csrc/autograd/init.cpp torch/csrc/autograd/input_buffer.cpp torch/csrc/autograd/input_buffer.h torch/csrc/autograd/profiler_legacy.cpp torch/csrc/autograd/profiler_legacy.h torch/csrc/autograd/profiler_python.cpp torch/csrc/autograd/python_function.cpp torch/csrc/autograd/python_function.h torch/csrc/autograd/python_variable.cpp torch/csrc/autograd/python_variable_indexing.cpp torch/csrc/autograd/record_function_ops.h torch/csrc/autograd/utils/grad_layout_contract.h torch/csrc/autograd/utils/python_arg_parsing.h torch/csrc/autograd/variable.h torch/csrc/cuda/comm.cpp torch/csrc/cuda/comm.h torch/csrc/cuda/memory_snapshot.h torch/csrc/cuda/nccl.h torch/csrc/cuda/python_nccl.cpp torch/csrc/distributed/autograd/engine/dist_engine.cpp torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupCudaP2P.hpp torch/csrc/distributed/c10d/ProcessGroupGloo.cpp torch/csrc/distributed/c10d/ProcessGroupGloo.hpp torch/csrc/distributed/c10d/ProcessGroupMPI.cpp torch/csrc/distributed/c10d/ProcessGroupMPI.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/TCPStore.cpp torch/csrc/distributed/c10d/TCPStore.hpp torch/csrc/distributed/c10d/TraceUtils.h torch/csrc/distributed/c10d/Types.hpp torch/csrc/distributed/c10d/Utils.hpp torch/csrc/distributed/c10d/Work.hpp torch/csrc/distributed/c10d/init.cpp torch/csrc/distributed/c10d/intra_node_comm.hpp torch/csrc/distributed/c10d/logger.cpp torch/csrc/distributed/c10d/reducer.cpp torch/csrc/distributed/c10d/reducer.hpp torch/csrc/distributed/c10d/reducer_cuda.cpp torch/csrc/distributed/c10d/reducer_timer.hpp torch/csrc/distributed/c10d/sequence_num.cpp torch/csrc/distributed/c10d/sequence_num.hpp torch/csrc/distributed/rpc/profiler/remote_profiler_manager.cpp torch/csrc/distributed/rpc/profiler/remote_profiler_manager.h torch/csrc/distributed/rpc/py_rref.cpp torch/csrc/distributed/rpc/python_functions.cpp torch/csrc/distributed/rpc/request_callback_no_python.cpp torch/csrc/distributed/rpc/rref_impl.h torch/csrc/distributed/rpc/script_call.h torch/csrc/distributed/rpc/tensorpipe_cuda.cpp torch/csrc/distributed/rpc/tensorpipe_utils.cpp torch/csrc/dynamo/python_compiled_autograd.cpp torch/csrc/functorch/init.cpp torch/csrc/inductor/aoti_torch/utils.h torch/csrc/jit/api/compilation_unit.h torch/csrc/jit/api/function_impl.h torch/csrc/jit/api/module.cpp torch/csrc/jit/api/module.h torch/csrc/jit/api/object.cpp torch/csrc/jit/api/object.h torch/csrc/jit/codegen/fuser/compiler.cpp torch/csrc/jit/codegen/fuser/cpu/fused_kernel.cpp torch/csrc/jit/codegen/fuser/executor.cpp torch/csrc/jit/codegen/fuser/kernel_spec.h torch/csrc/jit/codegen/onednn/graph_helper.cpp torch/csrc/jit/codegen/onednn/graph_rewriter.cpp torch/csrc/jit/codegen/onednn/prepare_binary.cpp torch/csrc/jit/cuda/cuda.h torch/csrc/jit/frontend/builtin_functions.cpp torch/csrc/jit/frontend/canonicalize_modified_loop.cpp torch/csrc/jit/frontend/concrete_module_type.cpp torch/csrc/jit/frontend/function_schema_parser.cpp torch/csrc/jit/frontend/ir_emitter.cpp torch/csrc/jit/frontend/parse_string_literal.h torch/csrc/jit/frontend/parser.cpp torch/csrc/jit/frontend/schema_matching.cpp torch/csrc/jit/frontend/schema_matching.h torch/csrc/jit/frontend/schema_type_parser.cpp torch/csrc/jit/frontend/script_type_parser.cpp torch/csrc/jit/frontend/source_range.cpp torch/csrc/jit/frontend/source_range.h torch/csrc/jit/frontend/sugared_value.cpp torch/csrc/jit/frontend/sugared_value.h torch/csrc/jit/frontend/tracer.cpp torch/csrc/jit/ir/alias_analysis.cpp torch/csrc/jit/ir/constants.cpp torch/csrc/jit/ir/constants.h torch/csrc/jit/ir/ir.cpp torch/csrc/jit/ir/ir.h torch/csrc/jit/ir/scope.h torch/csrc/jit/mobile/compatibility/backport_manager.cpp torch/csrc/jit/mobile/compatibility/runtime_compatibility.h torch/csrc/jit/mobile/flatbuffer_loader.cpp torch/csrc/jit/mobile/flatbuffer_loader.h torch/csrc/jit/mobile/frame.h torch/csrc/jit/mobile/function.cpp torch/csrc/jit/mobile/import.cpp torch/csrc/jit/mobile/import.h torch/csrc/jit/mobile/import_data.h torch/csrc/jit/mobile/model_tracer/MobileModelRunner.h torch/csrc/jit/mobile/model_tracer/TracerRunner.cpp torch/csrc/jit/mobile/module.cpp torch/csrc/jit/mobile/promoted_prim_ops.cpp torch/csrc/jit/operator_upgraders/upgraders_entry.cpp torch/csrc/jit/operator_upgraders/utils.cpp torch/csrc/jit/operator_upgraders/utils.h torch/csrc/jit/passes/autocast.cpp torch/csrc/jit/passes/canonicalize.cpp torch/csrc/jit/passes/canonicalize_graph_fuser_ops.cpp torch/csrc/jit/passes/constant_propagation.cpp torch/csrc/jit/passes/create_autodiff_subgraphs.cpp torch/csrc/jit/passes/device_type_analysis.cpp torch/csrc/jit/passes/dtype_analysis.cpp torch/csrc/jit/passes/erase_number_types.cpp torch/csrc/jit/passes/freeze_module.cpp torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp torch/csrc/jit/passes/graph_fuser.cpp torch/csrc/jit/passes/graph_rewrite_helper.cpp torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp torch/csrc/jit/passes/integer_value_refinement.cpp torch/csrc/jit/passes/onnx/constant_fold.cpp torch/csrc/jit/passes/onnx/constant_fold.h torch/csrc/jit/passes/onnx/constant_map.cpp torch/csrc/jit/passes/onnx/function_extraction.cpp torch/csrc/jit/passes/onnx/list_model_parameters.cpp torch/csrc/jit/passes/onnx/pattern_conversion/pattern_conversion.cpp torch/csrc/jit/passes/onnx/pattern_conversion/pattern_encapsulation.cpp torch/csrc/jit/passes/onnx/peephole.cpp torch/csrc/jit/passes/onnx/scalar_type_analysis.cpp torch/csrc/jit/passes/onnx/shape_type_inference.cpp torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp torch/csrc/jit/passes/peephole_dict_idioms.cpp torch/csrc/jit/passes/peephole_list_idioms.cpp torch/csrc/jit/passes/quantization/helper.cpp torch/csrc/jit/passes/quantization/helper.h torch/csrc/jit/passes/quantization/insert_observers.cpp torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp torch/csrc/jit/passes/remove_mutation.h torch/csrc/jit/passes/replacement_of_old_operators.cpp torch/csrc/jit/passes/shape_analysis.cpp torch/csrc/jit/passes/symbolic_shape_analysis.cpp torch/csrc/jit/passes/symbolic_shape_cache.cpp torch/csrc/jit/passes/symbolic_shape_runtime_fusion.cpp torch/csrc/jit/passes/tensorexpr_fuser.cpp torch/csrc/jit/passes/utils/check_alias_annotation.cpp torch/csrc/jit/passes/utils/memory_dag.h torch/csrc/jit/passes/utils/subgraph_utils.cpp torch/csrc/jit/python/init.cpp torch/csrc/jit/python/module_python.h torch/csrc/jit/python/pybind_utils.cpp torch/csrc/jit/python/pybind_utils.h torch/csrc/jit/python/python_ir.cpp torch/csrc/jit/python/python_ivalue.h torch/csrc/jit/python/python_list.h torch/csrc/jit/python/python_sugared_value.cpp torch/csrc/jit/python/python_sugared_value.h torch/csrc/jit/python/python_tree_views.cpp torch/csrc/jit/python/script_init.cpp torch/csrc/jit/runtime/autodiff.cpp torch/csrc/jit/runtime/decomposition_registry.cpp torch/csrc/jit/runtime/graph_executor.h torch/csrc/jit/runtime/graph_executor_impl.h torch/csrc/jit/runtime/interpreter.cpp torch/csrc/jit/runtime/interpreter.h torch/csrc/jit/runtime/jit_exception.h torch/csrc/jit/runtime/operator.h torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp torch/csrc/jit/runtime/register_ops_utils.h torch/csrc/jit/runtime/register_prim_ops.cpp torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp torch/csrc/jit/runtime/register_special_ops.cpp torch/csrc/jit/runtime/simple_graph_executor_impl.cpp torch/csrc/jit/runtime/static/fusion.cpp torch/csrc/jit/runtime/static/impl.cpp torch/csrc/jit/runtime/static/ops.cpp torch/csrc/jit/runtime/static/ops.h torch/csrc/jit/runtime/symbolic_script.cpp torch/csrc/jit/runtime/symbolic_script.h torch/csrc/jit/runtime/symbolic_shape_registry.cpp torch/csrc/jit/serialization/callstack_debug_info_serialization.cpp torch/csrc/jit/serialization/export.cpp torch/csrc/jit/serialization/export_bytecode.cpp torch/csrc/jit/serialization/export_module.cpp torch/csrc/jit/serialization/flatbuffer_serializer.cpp torch/csrc/jit/serialization/import.h torch/csrc/jit/serialization/import_source.cpp torch/csrc/jit/serialization/import_source.h torch/csrc/jit/serialization/pickle.cpp torch/csrc/jit/serialization/pickler.cpp torch/csrc/jit/serialization/python_print.cpp torch/csrc/jit/serialization/source_range_serialization.cpp torch/csrc/jit/tensorexpr/codegen.cpp torch/csrc/jit/tensorexpr/eval.cpp torch/csrc/jit/tensorexpr/expr.h torch/csrc/jit/tensorexpr/external_functions.cpp torch/csrc/jit/tensorexpr/external_functions.h torch/csrc/jit/tensorexpr/graph_opt.cpp torch/csrc/jit/tensorexpr/ir.h torch/csrc/jit/tensorexpr/ir_simplifier.cpp torch/csrc/jit/tensorexpr/kernel.cpp torch/csrc/jit/tensorexpr/llvm_codegen.cpp torch/csrc/jit/tensorexpr/llvm_codegen.h torch/csrc/jit/tensorexpr/llvm_jit.h torch/csrc/jit/tensorexpr/operators/conv2d.cpp torch/csrc/jit/tensorexpr/operators/misc.cpp torch/csrc/jit/tensorexpr/operators/pointwise.h torch/csrc/jit/tensorexpr/operators/quantization.cpp torch/csrc/jit/tensorexpr/operators/softmax.cpp torch/csrc/jit/tensorexpr/tensor.cpp torch/csrc/jit/tensorexpr/tensor.h torch/csrc/jit/testing/file_check.cpp torch/csrc/lazy/backend/backend_device.cpp torch/csrc/lazy/backend/backend_device.h torch/csrc/lazy/core/ir_builder.h torch/csrc/lazy/core/ir_dump_util.cpp torch/csrc/lazy/core/lazy_graph_executor.cpp torch/csrc/lazy/core/shape.cpp torch/csrc/lazy/core/shape.h torch/csrc/lazy/core/shape_inference.h torch/csrc/lazy/core/tensor.cpp torch/csrc/lazy/core/unique.h torch/csrc/lazy/core/util.h torch/csrc/lazy/python/python_util.cpp torch/csrc/lazy/python/python_util.h torch/csrc/lazy/ts_backend/ir_builder.h torch/csrc/lazy/ts_backend/ts_eager_fallback.cpp torch/csrc/lazy/ts_backend/ts_native_functions.cpp torch/csrc/profiler/collection.cpp torch/csrc/profiler/collection.h torch/csrc/profiler/python/init.cpp torch/csrc/profiler/unwind/unwind.cpp torch/csrc/profiler/unwind/unwind.h torch/csrc/profiler/unwind/unwind_error.h torch/csrc/profiler/util.h torch/csrc/tensor/python_tensor.cpp torch/csrc/utils/nested.cpp torch/csrc/utils/python_arg_parser.cpp torch/csrc/utils/python_arg_parser.h torch/csrc/utils/python_dispatch.cpp torch/csrc/utils/python_raii.h torch/csrc/utils/python_symnode.h torch/csrc/utils/schema_info.cpp torch/csrc/utils/tensor_new.cpp torch/csrc/utils/torch_dispatch_mode.h torch/custom_class_detail.h torch/library.h,,,,,,
2d01f877373,quantization,improvements,Enable torch.empty for float8 dtypes + deterministic mode + cpu (#128744),aten/src/ATen/native/TensorFactories.h aten/src/ATen/native/cpu/FillKernel.cpp c10/util/Float8_e5m2-inl.h c10/util/Float8_e5m2fnuz-inl.h test/quantization/core/experimental/test_float8.py,https://github.com/pytorch/pytorch/pull/128744,vkuzo,drisspg,malfet,,
62a0e39ced9,dynamo,not user facing,[dynamo][inlining-nn-modules] Update tests with new expected counts (#128463),test/onnx/dynamo/test_dynamo_with_onnxruntime_backend.py,https://github.com/pytorch/pytorch/pull/128463,anijain2305,yanboliang,,,
7e092a62e61,dynamo,not user facing,[dynamo] Support weakref objects (#128533),test/dynamo/test_repros.py torch/_dynamo/guards.py torch/_dynamo/source.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/user_defined.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/128533,anijain2305,jansel,,,
9ebf77b13b2,inductor,not user facing,Fix windows inductor defination issue (#128686),aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h aten/src/ATen/cpu/vec/vec256/vec256_double.h aten/src/ATen/cpu/vec/vec256/vec256_float.h aten/src/ATen/cpu/vec/vec256/vec256_float_neon.h aten/src/ATen/cpu/vec/vec256/vec256_half_neon.h aten/src/ATen/cpu/vec/vec512/vec512_bfloat16.h aten/src/ATen/cpu/vec/vec512/vec512_double.h aten/src/ATen/cpu/vec/vec512/vec512_float.h aten/src/ATen/cpu/vec/vec_base.h aten/src/ATen/cpu/vec/vec_mask.h torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/128686,xuhancn,jansel,jgong5,,
108adbc7265,dynamo,not user facing,[dynamo][side effects] Raise assertion error if the object is already tracked for mutation (#128590),torch/_dynamo/side_effects.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/128590,anijain2305,mlazos,,,
4ccbf711e2f,optim,docs,Learning Rate Scheduler docstring fix (#128679),torch/optim/lr_scheduler.py,https://github.com/pytorch/pytorch/pull/128679,spzala,janeyx99,,,
472211c97ad,skip,not user facing,Make assert_size_stride to return all errors (#128764),test/dynamo/test_misc.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/128764,oulgen,jansel,,,
e4c32d14a8d,Uncategorized,bc breaking,[3/N] Remove inclusion of c10/util/string_utils.h (#128504),caffe2/core/common.h caffe2/serialize/inline_container.cc test/cpp/jit/test_custom_class_registrations.cpp,https://github.com/pytorch/pytorch/pull/128504,cyyever,malfet,,,
b50c0e94c25,distributed,Untopiced,TCPStoreLibUvBackend: use somaxconn and enable TCP_NODELAY (#128739),torch/csrc/distributed/c10d/TCPStoreLibUvBackend.cpp,https://github.com/pytorch/pytorch/pull/128739,d4l3k,c-p-i-o,kurman,rsdcastro,
de4f379cf29,skip,not user facing,run mkldnn test with inlining (#128749),test/inductor/test_mkldnn_pattern_matcher.py,https://github.com/pytorch/pytorch/pull/128749,mlazos,anijain2305,,,
60bbdc0b406,skip,not user facing,Modularize aten parameter parser and checker (#125308),test/inductor/test_torchinductor.py torch/_C/__init__.pyi.in torch/_inductor/utils.py torch/csrc/dynamo/guards.cpp torch/csrc/dynamo/guards.h torch/csrc/inductor/aoti_eager/kernel_holder.cpp torch/csrc/inductor/aoti_eager/kernel_holder.h torch/csrc/inductor/aoti_eager/kernel_meta_info.cpp torch/csrc/inductor/aoti_eager/kernel_meta_info.h torch/csrc/utils/python_dispatch.cpp,https://github.com/pytorch/pytorch/pull/125308,EikanWang,atalman,jansel,jgong5,
7a39755da28,distributed,not user facing,Introduce a prototype for SymmetricMemory (#128582),.lintrunner.toml BUILD.bazel build_variables.bzl c10/cuda/driver_api.h caffe2/CMakeLists.txt test/distributed/test_symmetric_memory.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/CUDASymmetricMemory.cu torch/csrc/distributed/c10d/CUDASymmetricMemory.cuh torch/csrc/distributed/c10d/ProcessGroupCudaP2P.hpp torch/csrc/distributed/c10d/SymmetricMemory.cpp torch/csrc/distributed/c10d/SymmetricMemory.hpp torch/csrc/distributed/c10d/init.cpp torch/csrc/distributed/c10d/intra_node_comm.cpp torch/csrc/distributed/c10d/intra_node_comm.cu torch/csrc/distributed/c10d/intra_node_comm.hpp,https://github.com/pytorch/pytorch/pull/128582,yifuwang,wanchaol,,,
18634048a1f,skip,not user facing,Separate AOTI Eager utils as a single file (#125819),test/inductor/test_torchinductor.py torch/_inductor/aoti_eager.py torch/_inductor/utils.py torch/csrc/inductor/aoti_eager/kernel_holder.cpp,https://github.com/pytorch/pytorch/pull/125819,EikanWang,desertfire,jansel,jgong5,
f0d68120f4e,skip,not user facing,[subclasses] Handle dynamo inputs that are subclass views with (-1) in the view (#128662),test/dynamo/test_subclasses.py test/dynamo_expected_failures/TestAOTAutograd.test_output_aliases_multiple_inputs_get_correct_one torch/_subclasses/meta_utils.py,https://github.com/pytorch/pytorch/pull/128662,davidberard98,jbschlosser,,,
94c0dcbe1d3,inductor,not user facing,[inductor] Parallel compile: handle crashes in subprocesses (#128757),test/inductor/test_compile_worker.py torch/_inductor/compile_worker/__main__.py torch/_inductor/compile_worker/subproc_pool.py,https://github.com/pytorch/pytorch/pull/128757,masnesral,jansel,,,
6079c509109,inductor,Untopiced,Make config.fx_graph_remote_cache be three-value switch (#128628),test/inductor/test_codecache.py test/inductor/test_torchinductor.py torch/_inductor/compile_fx.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/128628,oulgen,eellison,masnesral,,
ab13980424f,releng,not user facing,"[ONNX] Update 'person_of_interest.rst', 'CODEOWNERS' and 'merge_rules.yaml' (#126364)",.github/merge_rules.yaml CODEOWNERS docs/source/community/persons_of_interest.rst torch/_dynamo/backends/onnxrt.py,https://github.com/pytorch/pytorch/pull/126364,BowenBao,albanD,justinchuby,titaiwangms,
a61939467a5,dynamo,Untopiced,Enable passing dynamo-traced complex test (#128771),torch/testing/_internal/common_optimizers.py,https://github.com/pytorch/pytorch/pull/128771,mlazos,anijain2305,,,
f1ee3589a12,inductor,Untopiced,[Inductor] Emit strided block pointer from ModularIndexing and FloorDiv (#127342),test/inductor/test_codecache.py test/inductor/test_indexing.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_strided_blocks.py torch/_inductor/codegen/triton.py torch/_inductor/runtime/runtime_utils.py torch/_inductor/sizevars.py torch/testing/_internal/inductor_utils.py torch/utils/_sympy/functions.py torch/utils/_sympy/symbol.py,https://github.com/pytorch/pytorch/pull/127342,blaine-rister,jansel,shunting314,,
cc518ebd381,releng,Untopiced,[Inductor Intel GPU backend Upstream] Reuse inductor test for Intel GPU (PART 2) (#124147),test/inductor/test_binary_folding.py test/inductor/test_control_flow.py test/inductor/test_coordinate_descent_tuner.py test/inductor/test_debug_trace.py test/inductor/test_dependencies.py test/inductor/test_indexing.py test/inductor/test_minifier.py test/inductor/test_mmdecomp.py test/inductor/test_smoke.py test/inductor/test_split_cat_fx_passes.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py test/inductor/test_torchinductor_dynamic_shapes.py test/inductor/test_triton_kernels.py torch/testing/_internal/common_device_type.py torch/testing/_internal/common_utils.py torch/testing/_internal/opinfo/core.py torch/testing/_internal/triton_utils.py,https://github.com/pytorch/pytorch/pull/124147,etaf,EikanWang,jansel,,
e4d8aa4d249,skip,not user facing,[torchbench] Enable some models with inline_inbuilt_nn_modules (#128315),benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_training.csv benchmarks/dynamo/common.py benchmarks/dynamo/torchbench.py,https://github.com/pytorch/pytorch/pull/128315,anijain2305,jansel,,,
979edbbe128,distributed,not user facing,[Traceable FSDP2] Dynamo support FSDP2 use_training_state context manager (#127854),test/distributed/_composable/fsdp/test_fully_shard_compile.py torch/_dynamo/guards.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/torch.py torch/distributed/_composable/fsdp/fully_shard.py torch/distributed/utils.py,https://github.com/pytorch/pytorch/pull/127854,yf225,yanboliang,,,
f8d60e0e0a4,inductor,not user facing,[Inductor][CPP] Fix Half data type cse cache issue for CPP Backend (#128498),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/128498,leslie-fang-intel,jerryzh168,jgong5,zhuhaozhe,
6cbdbb6c3c2,skip,not user facing,Remove top lev numpy dependency from fuzzer.py (#128759),torch/utils/benchmark/utils/fuzzer.py,https://github.com/pytorch/pytorch/pull/128759,msaroufim,Skylion007,,,
f9dae86222a,skip,not user facing,Concat namespaces in torch/csrc/utils/* (#128787),torch/csrc/utils/byte_order.cpp torch/csrc/utils/init.cpp torch/csrc/utils/nested.cpp torch/csrc/utils/out_types.cpp torch/csrc/utils/pybind.cpp torch/csrc/utils/schema_info.cpp torch/csrc/utils/structseq.cpp torch/csrc/utils/throughput_benchmark.cpp,https://github.com/pytorch/pytorch/pull/128787,cyyever,Skylion007,,,
74e11a42101,skip,not user facing,Enable clang-tidy on torch/csrc/mps (#128782),.lintrunner.toml torch/csrc/mps/Module.cpp torch/csrc/mps/Module.h,https://github.com/pytorch/pytorch/pull/128782,cyyever,Skylion007,,,
a52c8ace98a,skip,not user facing,[3/N] Non-Tensor: Support string parameter for aten operations (#125831),test/inductor/test_torchinductor.py torch/_inductor/aoti_eager.py torch/csrc/inductor/aoti_eager/kernel_holder.cpp torch/csrc/inductor/aoti_eager/kernel_meta_info.cpp torch/csrc/inductor/aoti_eager/kernel_meta_info.h,https://github.com/pytorch/pytorch/pull/125831,EikanWang,jansel,jgong5,,
b40a033c380,inductor,not user facing,[cpp_extension][inductor] Fix sleef windows depends. (#128770),torch/_inductor/cpp_builder.py torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/128770,xuhancn,jansel,jgong5,,
b0282071c48,dynamo,not user facing,[dynamo] override torch.nn.modules.activation._is_make_fx_tracing (#128748),test/dynamo/test_repros.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/torch.py,https://github.com/pytorch/pytorch/pull/128748,anijain2305,jansel,,,
0f81473d7b4,composability,Untopiced,Update fake tensor error checks for bool tensor subtraction (#128492),test/test_binary_ufuncs.py torch/_refs/__init__.py,https://github.com/pytorch/pytorch/pull/128492,ambareesh1510,soulitzer,,,
e3093849e55,quantization,docs,[Docs] Update links (#128795),torch/ao/nn/quantized/modules/embedding_ops.py,https://github.com/pytorch/pytorch/pull/128795,malfet,atalman,,,
24443fe16ab,inductor,not user facing,[inductor] parallel compile: Print traceback detail when there's an exception in a sub-process (#128775),test/inductor/test_compile_worker.py torch/_inductor/compile_worker/subproc_pool.py,https://github.com/pytorch/pytorch/pull/128775,masnesral,jansel,,,
2a41fc03903,skip,not user facing,Short-term fix to preserve NJT metadata cache in torch.compile (#122836),aten/src/ATen/FunctionalInverses.cpp aten/src/ATen/native/native_functions.yaml test/dynamo/test_subclasses.py test/expect/HasDecompTest.test_has_decomposition.expect test/test_nestedtensor.py tools/autograd/derivatives.yaml torch/nested/_internal/nested_tensor.py torch/nested/_internal/ops.py torch/nested/_internal/sdpa.py,https://github.com/pytorch/pytorch/pull/122836,jbschlosser,soulitzer,,,
bfad0aee446,skip,not user facing,[export] Preserve requires_grad for export inputs. (#128656),test/export/test_export.py test/export/test_serialize.py torch/_export/__init__.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/128656,zhxchen17,tugsbayasgalan,,,
dff6342a0b6,distributed,not user facing,[BE][Easy] enable UFMT for `torch/nn/parallel` (#128596),.lintrunner.toml torch/nn/attention/__init__.py torch/nn/attention/_utils.py torch/nn/attention/bias.py torch/nn/backends/thnn.py torch/nn/parallel/__init__.py torch/nn/parallel/_functions.py torch/nn/parallel/comm.py torch/nn/parallel/data_parallel.py torch/nn/parallel/distributed.py torch/nn/parallel/parallel_apply.py torch/nn/parallel/replicate.py torch/nn/parallel/scatter_gather.py,https://github.com/pytorch/pytorch/pull/128596,XuehaiPan,mikaylagawarecki,,,
95ac2d64827,dataloader_frontend,not user facing,[BE] enable UFMT for `torch/nn/modules` (#128594),.lintrunner.toml torch/nn/modules/__init__.py torch/nn/modules/_functions.py torch/nn/modules/activation.py torch/nn/modules/adaptive.py torch/nn/modules/batchnorm.py torch/nn/modules/channelshuffle.py torch/nn/modules/container.py torch/nn/modules/conv.py torch/nn/modules/distance.py torch/nn/modules/dropout.py torch/nn/modules/flatten.py torch/nn/modules/fold.py torch/nn/modules/instancenorm.py torch/nn/modules/lazy.py torch/nn/modules/linear.py torch/nn/modules/loss.py torch/nn/modules/module.py torch/nn/modules/normalization.py torch/nn/modules/padding.py torch/nn/modules/pixelshuffle.py torch/nn/modules/pooling.py torch/nn/modules/rnn.py torch/nn/modules/sparse.py torch/nn/modules/transformer.py torch/nn/modules/upsampling.py torch/nn/modules/utils.py,https://github.com/pytorch/pytorch/pull/128594,XuehaiPan,mikaylagawarecki,,,
f6e6e55fa7d,skip,not user facing,[BE] enable UFMT for `torch/nn/functional.py` (#128592),.lintrunner.toml tools/pyi/gen_pyi.py torch/__init__.py torch/_jit_internal.py torch/_library/utils.py torch/_linalg_utils.py torch/_lowrank.py torch/_meta_registrations.py torch/_utils.py torch/_utils_internal.py torch/_vmap_internals.py torch/distributed/__init__.py torch/functional.py torch/hub.py torch/library.py torch/nn/functional.py torch/overrides.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/128592,XuehaiPan,mikaylagawarecki,,,
a87d82abd74,skip,not user facing,[BE] enable UFMT for `torch/nn/*.py` (#128593),.lintrunner.toml torch/__init__.py torch/nn/__init__.py torch/nn/_reduction.py torch/nn/common_types.py torch/nn/grad.py torch/nn/init.py torch/nn/parameter.py torch/utils/data/dataloader.py torch/utils/data/datapipes/_hook_iterator.py,https://github.com/pytorch/pytorch/pull/128593,XuehaiPan,mikaylagawarecki,,,
316b7296771,jit,not user facing,[Fix] TS converter constant to tensor (#128442),test/export/test_converter.py torch/_export/converter.py torch/jit/_trace.py,https://github.com/pytorch/pytorch/pull/128442,jiashenC,angelayi,,,
73b78d1cbef,Uncategorized,docs,Document the torch.nn.parallel.scatter_gather.gather function (#128566),torch/nn/parallel/scatter_gather.py,https://github.com/pytorch/pytorch/pull/128566,ahoblitz,kwen2501,,,
fc2913fb808,composability,not user facing,Remove amax return from _scaled_mm (#128683),aten/src/ATen/native/cuda/Blas.cpp aten/src/ATen/native/native_functions.yaml test/forward_backward_compatibility/check_forward_backward_compatibility.py test/test_matmul_cuda.py torch/_meta_registrations.py torch/csrc/inductor/aoti_torch/generated/c_shim_cuda.h torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/128683,drisspg,vkuzo,,,
c6b180a3166,skip,not user facing,Created docs (and example) for cudart function in torch.cuda  (#128741),docs/source/cuda.rst torch/cuda/__init__.py,https://github.com/pytorch/pytorch/pull/128741,ignaciobartol,msaroufim,,,
153362fbc9e,distributed,Untopiced,Support HSDP + Monolith Checkpointing (#128446),torch/distributed/fsdp/_optim_utils.py,https://github.com/pytorch/pytorch/pull/128446,mvpatel2000,fegin,,,
d35cdee97f2,cpp_frontend,deprecation,[Caffe2] Remove caffe2 onnx tests (#128687),test/onnx/debug_embed_params.py test/onnx/pytorch_helper.py test/onnx_caffe2/export_onnx_tests_filter.py test/onnx_caffe2/export_onnx_tests_generator.py test/onnx_caffe2/test_caffe2_common.py test/onnx_caffe2/test_custom_ops.py test/onnx_caffe2/test_pytorch_helper.py test/onnx_caffe2/test_pytorch_onnx_caffe2.py test/onnx_caffe2/test_pytorch_onnx_caffe2_quantized.py test/onnx_caffe2/test_verify.py,https://github.com/pytorch/pytorch/pull/128687,cyyever,r-barnes,,,
5344c41d431,releng,Untopiced,Use forked torchbench branch with pinned numpy (#128856),.ci/pytorch/common_utils.sh .ci/pytorch/perf_test/test_cpu_speed_mini_sequence_labeler.sh .ci/pytorch/perf_test/test_gpu_speed_cudnn_lstm.sh .ci/pytorch/perf_test/test_gpu_speed_lstm.sh .ci/pytorch/perf_test/test_gpu_speed_mlstm.sh .github/ci_commit_pins/torchbench.txt benchmarks/dynamo/Makefile,https://github.com/pytorch/pytorch/pull/128856,eellison,PaliC,huydhn,,
c172b58fe01,skip,Untopiced,"Revert ""Update DALLE2_pytorch expected accuracy result on CPU (#128718)""",benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_inductor_torchbench_inference.csv,,,,,,
213eba7d2e9,releng,not user facing,Configure mergebot via config (#128840),.github/pytorch-probot.yml,https://github.com/pytorch/pytorch/pull/128840,clee2000,huydhn,,,
b181b588574,python_frontend,improvements,Fix Storage.filename to not track the filename when storage was mmap-ed with MAP_PRIVATE (#128725),aten/src/ATen/MapAllocator.h test/test_tensor_creation_ops.py torch/csrc/StorageMethods.cpp torch/storage.py,https://github.com/pytorch/pytorch/pull/128725,mikaylagawarecki,albanD,,,
1577328ea40,releng,not user facing,Set bash shell on Windows (#128854),.github/workflows/_win-build.yml .github/workflows/_win-test.yml,https://github.com/pytorch/pytorch/pull/128854,huydhn,atalman,kit1980,malfet,
0f89e66d174,distributed,not user facing,Validate logs are created by default (#128522),test/distributed/launcher/test_run.py,https://github.com/pytorch/pytorch/pull/128522,kurman,d4l3k,,,
a59766ee058,skip,not user facing,"replace `AT_ERROR(...)` with `TORCH_CHECK(false, ...)` (#128788)",aten/src/ATen/native/TensorShape.cpp,https://github.com/pytorch/pytorch/pull/128788,crcrpar,mikaylagawarecki,,,
8c06eae17eb,skip,not user facing,[GPT-benchmark] Add metric: compilation time for GPT models (#128768),benchmarks/gpt_fast/generate.py,https://github.com/pytorch/pytorch/pull/128768,yanboliang,Chillee,,,
a489792bb2d,skip,not user facing,[GPT-benchmark] Fix memory bandwidth for MoE (#128783),benchmarks/gpt_fast/generate.py,https://github.com/pytorch/pytorch/pull/128783,yanboliang,Chillee,,,
8953725e6d6,inductor,not user facing,[Inductor][FlexAttention] Tune backwards kernel block sizes (#128853),torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/128853,yanboliang,angelayi,,,
163847b1bb5,cpp_frontend,improvements,[1/N] [Caffe2] Remove caffe2_aten_fallback code (#128675),test/onnx/test_export_modes.py test/onnx/test_operators.py test/onnx/test_pytorch_onnx_no_runtime.py test/onnx/test_utility_funs.py test/quantization/core/test_quantized_op.py test/test_jit.py torch/_C/_onnx.pyi torch/csrc/onnx/init.cpp torch/onnx/__init__.py torch/onnx/_internal/jit_utils.py torch/onnx/symbolic_helper.py torch/onnx/symbolic_opset11.py torch/onnx/symbolic_opset12.py torch/onnx/symbolic_opset16.py torch/onnx/symbolic_opset9.py torch/onnx/utils.py torch/onnx/verification.py torch/testing/_internal/common_quantization.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/128675,cyyever,r-barnes,,,
1fd7496ab2e,Uncategorized,bug fixes,[MTIA] Fix synchronize API (#128714),torch/csrc/mtia/Module.cpp torch/mtia/__init__.py,https://github.com/pytorch/pytorch/pull/128714,egienvalue,aaronenyeshi,,,
7baf32b5e74,distributed,Untopiced,[c10d] fix p2p group commsplit (#128803),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/128803,shengbao-zheng,shuqiangzhang,,,
1835e3beab7,releng,Untopiced,Fix the inductor ci (#128879),.ci/pytorch/common_utils.sh .ci/pytorch/perf_test/test_cpu_speed_mini_sequence_labeler.sh .ci/pytorch/perf_test/test_gpu_speed_cudnn_lstm.sh .ci/pytorch/perf_test/test_gpu_speed_lstm.sh .ci/pytorch/perf_test/test_gpu_speed_mlstm.sh .github/ci_commit_pins/torchbench.txt benchmarks/dynamo/Makefile,https://github.com/pytorch/pytorch/pull/128879,xuzhao9,eellison,,,
3b8c9b8ab11,skip,not user facing,[Docker Release] Test if pytorch was compiled with CUDA before pushing to repo (#128852),Dockerfile,https://github.com/pytorch/pytorch/pull/128852,atalman,malfet,,,
8415a4ba98f,skip,not user facing,"Back out ""[ROCm] TunableOp for gemm_and_bias (#128143)"" (#128815)",aten/src/ATen/cuda/tunable/GemmCommon.h aten/src/ATen/cuda/tunable/GemmHipblaslt.h aten/src/ATen/cuda/tunable/Tunable.cpp aten/src/ATen/cuda/tunable/TunableGemm.h aten/src/ATen/native/cuda/Blas.cpp,https://github.com/pytorch/pytorch/pull/128815,xw285cornell,eqy,malfet,mxz297,
95b5ea9cdef,fx,not user facing,Add mark_unbacked (#128638),torch/_dynamo/decorators.py torch/_dynamo/variables/builder.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/128638,ezyang,IvanKobzarev,,,
b70440f0a7f,profiler,docs,Document the torch.cuda.profiler.profile function (#128216),torch/cuda/profiler.py,https://github.com/pytorch/pytorch/pull/128216,awayzjj,eqy,malfet,,
11ff5345d24,fx,not user facing,Changed colored logging to only be turned on if printing to interactive terminal (#128874),test/dynamo/test_misc.py torch/fx/_utils.py,https://github.com/pytorch/pytorch/pull/128874,Chillee,anijain2305,,,
beb29836cd1,skip,not user facing,[Inductor][CPP] Add Min/Max with VecMask (#126841),test/inductor/test_torchinductor_opinfo.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_utils.py,https://github.com/pytorch/pytorch/pull/126841,leslie-fang-intel,isuruf,jgong5,peterbell10,
c35ffaf954f,skip,not user facing,[Inductor][CPP] Add ne with VecMask (#126940),aten/src/ATen/cpu/vec/vec_mask.h test/inductor/test_torchinductor_opinfo.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/126940,leslie-fang-intel,isuruf,jgong5,peterbell10,
fbc7559ceb3,composability,Untopiced,[custom ops] convert string type annotation to real type (#128809),test/custom_operator/test_infer_schema_annotation.py torch/_library/infer_schema.py,https://github.com/pytorch/pytorch/pull/128809,yushangdi,zou3519,,,
9e8443b56f5,skip,not user facing,Remove dtype from gpt-fast micro benchmark experiments model name (#128789),benchmarks/gpt_fast/benchmark.py,https://github.com/pytorch/pytorch/pull/128789,huydhn,yanboliang,,,
e12fa93b8bb,skip,not user facing,add is_big_gpu(0) check to test_select_algorithm tests in tests/inductor/test_cuda_cpp_wrapper.py (#128652),test/inductor/test_cuda_cpp_wrapper.py,https://github.com/pytorch/pytorch/pull/128652,Fuzzkatt,eqy,soulitzer,,
43998711a79,skip,not user facing,[CUDAGraph] add more docs for cudagraph trees (#127963),docs/source/torch.compiler_cudagraph_trees.rst,https://github.com/pytorch/pytorch/pull/127963,BoyuanFeng,eellison,,,
22f1793c0ac,dynamo,not user facing,[dynamo][easy] Use LazyVariableTracker for UserDefinedObject var_getattr (#128877),torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/128877,anijain2305,mlazos,,,
4e97d37fd94,inductor,Untopiced,[inlining-inbuilt-nn-modules][pre-grad] Adjust efficient_conv_bn_eval_graph for inlining (#128878),torch/_inductor/fx_passes/efficient_conv_bn_eval.py,https://github.com/pytorch/pytorch/pull/128878,anijain2305,mlazos,,,
c017c97333d,dynamo,not user facing,[dynamo][inlining-inbuilt-nn-modules] Update test output (#128880),test/dynamo/test_structured_trace.py,https://github.com/pytorch/pytorch/pull/128880,anijain2305,mlazos,,,
4061b3b8225,skip,not user facing,Forward fix to skip ROCm tests for #122836 (#128891),test/test_nestedtensor.py,https://github.com/pytorch/pytorch/pull/128891,jbschlosser,huydhn,,,
17abbafdfc6,inductor,not user facing,[inductor] Fix some windows cpp builder issue (#128765),torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/128765,xuhancn,jansel,jgong5,,
59b4983dc06,distributed,Untopiced,DebugPlane: add dump_traceback handler (#128904),build_variables.bzl test/distributed/elastic/test_control_plane.py torch/csrc/distributed/c10d/control_plane/PythonHandlers.cpp,https://github.com/pytorch/pytorch/pull/128904,d4l3k,c-p-i-o,,,
d9eaa224f25,mps,Untopiced,Fixes #128429: NaN in triu op on MPS (#128575),aten/src/ATen/native/mps/operations/TriangularOps.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/128575,jhavukainen,kulinseth,,,
f7eae279463,distributed,Untopiced,Pass params to dump_nccl_trace_pickle (#128781),test/distributed/elastic/test_control_plane.py torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/control_plane/Handlers.hpp torch/csrc/distributed/c10d/control_plane/WorkerServer.cpp,https://github.com/pytorch/pytorch/pull/128781,c-p-i-o,d4l3k,,,
e3a39d49a0b,fx,not user facing,[Traceable FSDP][Compiled Autograd] Add queue_callback() support (#126366),test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/_dynamo/external_utils.py torch/_dynamo/side_effects.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/126366,yf225,xmfan,,,
60baeee59f7,distributed,not user facing,[BE] Skip the test if CUDA is not available (#128885),test/distributed/_tensor/debug/test_comm_mode.py,https://github.com/pytorch/pytorch/pull/128885,fegin,wz337,,,
6e43897912d,distributed,not user facing,[BE][ptd_fb_test][3/N] Enable TestSlide for MultiThreadedTestCase (#128843),torch/testing/_internal/common_distributed.py,https://github.com/pytorch/pytorch/pull/128843,fegin,wz337,,,
304c9345726,skip,not user facing,Move MKLDNN Specific IR to Separate File (#126504),torch/_inductor/ir.py torch/_inductor/mkldnn_ir.py torch/_inductor/mkldnn_lowerings.py,https://github.com/pytorch/pytorch/pull/126504,leslie-fang-intel,desertfire,jgong5,,
3dd5f0ecbbb,optim,Untopiced,Remove circular import (#128875),torch/optim/__init__.py,https://github.com/pytorch/pytorch/pull/128875,fbgheith,kit1980,,,
f2805a0408c,distributed,Untopiced,[FSDP2] Added APIs for explicit fwd/bwd prefetching (#128884),test/distributed/_composable/fsdp/test_fully_shard_comm.py test/distributed/_composable/fsdp/test_fully_shard_training.py torch/distributed/_composable/fsdp/_fsdp_param_group.py torch/distributed/_composable/fsdp/_fsdp_state.py torch/distributed/_composable/fsdp/fully_shard.py torch/testing/_internal/common_fsdp.py,https://github.com/pytorch/pytorch/pull/128884,awgu,ckluk2,weifengpy,,
e6d4451ae89,distributed,not user facing,"[BE][Easy] enable UFMT for `torch/distributed/{algorithms,autograd,benchmarks,checkpoint,elastic}/` (#128866)",.lintrunner.toml torch/distributed/algorithms/__init__.py torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py torch/distributed/algorithms/_comm_hooks/__init__.py torch/distributed/algorithms/_comm_hooks/default_hooks.py torch/distributed/algorithms/_optimizer_overlap/optimizer_overlap.py torch/distributed/algorithms/_quantization/quantization.py torch/distributed/algorithms/ddp_comm_hooks/__init__.py torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py torch/distributed/algorithms/ddp_comm_hooks/debugging_hooks.py torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py torch/distributed/algorithms/ddp_comm_hooks/mixed_precision_hooks.py torch/distributed/algorithms/ddp_comm_hooks/optimizer_overlap_hooks.py torch/distributed/algorithms/ddp_comm_hooks/post_localSGD_hook.py torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py torch/distributed/algorithms/join.py torch/distributed/algorithms/model_averaging/averagers.py torch/distributed/algorithms/model_averaging/hierarchical_model_averager.py torch/distributed/algorithms/model_averaging/utils.py torch/distributed/autograd/__init__.py torch/distributed/benchmarks/benchmark_ddp_rpc.py torch/distributed/checkpoint/_dedup_save_plans.py torch/distributed/checkpoint/_dedup_tensors.py torch/distributed/checkpoint/_fsspec_filesystem.py torch/distributed/checkpoint/_nested_dict.py torch/distributed/checkpoint/_sharded_tensor_utils.py torch/distributed/checkpoint/_storage_utils.py torch/distributed/checkpoint/_traverse.py torch/distributed/checkpoint/api.py torch/distributed/checkpoint/default_planner.py torch/distributed/checkpoint/examples/fsdp_checkpoint_example.py torch/distributed/checkpoint/filesystem.py torch/distributed/checkpoint/logger.py torch/distributed/checkpoint/metadata.py torch/distributed/checkpoint/optimizer.py torch/distributed/checkpoint/planner.py torch/distributed/checkpoint/planner_helpers.py torch/distributed/checkpoint/resharding.py torch/distributed/checkpoint/staging.py torch/distributed/checkpoint/state_dict.py torch/distributed/checkpoint/state_dict_loader.py torch/distributed/checkpoint/state_dict_saver.py torch/distributed/checkpoint/storage.py torch/distributed/checkpoint/utils.py torch/distributed/elastic/agent/server/api.py torch/distributed/elastic/agent/server/health_check_server.py torch/distributed/elastic/agent/server/local_elastic_agent.py torch/distributed/elastic/control_plane.py torch/distributed/elastic/events/__init__.py torch/distributed/elastic/events/api.py torch/distributed/elastic/metrics/__init__.py torch/distributed/elastic/metrics/api.py torch/distributed/elastic/multiprocessing/__init__.py torch/distributed/elastic/multiprocessing/api.py torch/distributed/elastic/multiprocessing/errors/__init__.py torch/distributed/elastic/multiprocessing/errors/error_handler.py torch/distributed/elastic/multiprocessing/errors/handlers.py torch/distributed/elastic/multiprocessing/redirects.py torch/distributed/elastic/multiprocessing/subprocess_handler/__init__.py torch/distributed/elastic/multiprocessing/subprocess_handler/handlers.py torch/distributed/elastic/multiprocessing/subprocess_handler/subprocess_handler.py torch/distributed/elastic/multiprocessing/tail_log.py torch/distributed/elastic/rendezvous/__init__.py torch/distributed/elastic/rendezvous/api.py torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py torch/distributed/elastic/rendezvous/dynamic_rendezvous.py torch/distributed/elastic/rendezvous/etcd_rendezvous.py torch/distributed/elastic/rendezvous/etcd_rendezvous_backend.py torch/distributed/elastic/rendezvous/etcd_server.py torch/distributed/elastic/rendezvous/etcd_store.py torch/distributed/elastic/rendezvous/registry.py torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py torch/distributed/elastic/rendezvous/utils.py torch/distributed/elastic/timer/__init__.py torch/distributed/elastic/timer/api.py torch/distributed/elastic/timer/debug_info_logging.py torch/distributed/elastic/timer/file_based_local_timer.py torch/distributed/elastic/timer/local_timer.py torch/distributed/elastic/utils/api.py torch/distributed/elastic/utils/distributed.py torch/distributed/elastic/utils/store.py,https://github.com/pytorch/pytorch/pull/128866,XuehaiPan,fegin,,,
22d258427ba,distributed,not user facing,[BE][Easy] enable UFMT for `torch/distributed/_shard/` (#128867),.lintrunner.toml torch/distributed/_shard/__init__.py torch/distributed/_shard/_utils.py torch/distributed/_shard/api.py torch/distributed/_shard/checkpoint/__init__.py torch/distributed/_shard/common_op_utils.py torch/distributed/_shard/metadata.py torch/distributed/_shard/op_registry_utils.py torch/distributed/_shard/sharded_optim/__init__.py torch/distributed/_shard/sharded_optim/api.py torch/distributed/_shard/sharded_tensor/__init__.py torch/distributed/_shard/sharded_tensor/_ops/__init__.py torch/distributed/_shard/sharded_tensor/_ops/_common.py torch/distributed/_shard/sharded_tensor/_ops/binary_cmp.py torch/distributed/_shard/sharded_tensor/_ops/init.py torch/distributed/_shard/sharded_tensor/_ops/misc_ops.py torch/distributed/_shard/sharded_tensor/_ops/tensor_ops.py torch/distributed/_shard/sharded_tensor/api.py torch/distributed/_shard/sharded_tensor/logger.py torch/distributed/_shard/sharded_tensor/logging_handlers.py torch/distributed/_shard/sharded_tensor/metadata.py torch/distributed/_shard/sharded_tensor/reshard.py torch/distributed/_shard/sharded_tensor/shard.py torch/distributed/_shard/sharded_tensor/utils.py torch/distributed/_shard/sharder.py torch/distributed/_shard/sharding_plan/__init__.py torch/distributed/_shard/sharding_plan/api.py torch/distributed/_shard/sharding_spec/__init__.py torch/distributed/_shard/sharding_spec/_internals.py torch/distributed/_shard/sharding_spec/api.py torch/distributed/_shard/sharding_spec/chunk_sharding_spec.py torch/distributed/_spmd/api.py torch/distributed/_spmd/batch_dim_utils.py torch/distributed/_spmd/config.py torch/distributed/_spmd/data_parallel.py torch/distributed/_spmd/distribute.py torch/distributed/_spmd/experimental_ops.py torch/distributed/_spmd/graph_optimization.py torch/distributed/_spmd/parallel_mode.py torch/distributed/_spmd/partial_lower.py,https://github.com/pytorch/pytorch/pull/128867,XuehaiPan,fegin,,,
48171806010,inductor,Untopiced,make fallback for aten.argsort.stable (#128907),test/inductor/test_torchinductor_opinfo.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/128907,isuruf,lezcano,,,
108318ad103,jit,Untopiced,[BE][JIT] Handle case where codegen object can be unset (#128951),torch/csrc/jit/tensorexpr/kernel.h,https://github.com/pytorch/pytorch/pull/128951,davidberard98,aaronenyeshi,,,
ec616da5184,skip,not user facing,RNN API cleanup for cuDNN 9.1 (#122011),aten/src/ATen/native/cudnn/RNN.cpp,https://github.com/pytorch/pytorch/pull/122011,eqy,Skylion007,,,
9818283da18,skip,not user facing,re-enable jacrev/jacfwd/hessian after #128028 landed (#128622),test/dynamo/test_higher_order_ops.py test/dynamo_expected_failures/TestComposabilityCPU.test_autograd_function_no_setup_context_transform_hessian_cpu test/dynamo_expected_failures/TestComposabilityCPU.test_autograd_function_no_setup_context_transform_jacfwd_cpu test/dynamo_expected_failures/TestHessianCPU.test_jacfwd_different_levels_cpu test/functorch/test_eager_transforms.py torch/_functorch/eager_transforms.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/128622,guilhermeleobas,zou3519,,,
26e374e3ca3,skip,not user facing,[EZ] Fix typos in RELEASE.md (#128769),RELEASE.md,https://github.com/pytorch/pytorch/pull/128769,kiszk,mikaylagawarecki,yumium,,
4e03263224a,cuda,not user facing,[CUDA][Convolution] Add missing launch bounds to `vol2col_kernel` (#128740),aten/src/ATen/native/cuda/vol2col.cuh,https://github.com/pytorch/pytorch/pull/128740,eqy,mikaylagawarecki,,,
84c86e56bd8,releng,not user facing,Update tracker issues after successfully cherry-picking a PR (#128924),.github/scripts/cherry_pick.py .github/scripts/github_utils.py,https://github.com/pytorch/pytorch/pull/128924,huydhn,atalman,,,
77830d509fc,skip,Untopiced,"Revert ""Introduce a prototype for SymmetricMemory (#128582)""",.lintrunner.toml BUILD.bazel build_variables.bzl c10/cuda/driver_api.h caffe2/CMakeLists.txt test/distributed/test_symmetric_memory.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/CUDASymmetricMemory.cu torch/csrc/distributed/c10d/CUDASymmetricMemory.cuh torch/csrc/distributed/c10d/ProcessGroupCudaP2P.hpp torch/csrc/distributed/c10d/SymmetricMemory.cpp torch/csrc/distributed/c10d/SymmetricMemory.hpp torch/csrc/distributed/c10d/init.cpp torch/csrc/distributed/c10d/intra_node_comm.cpp torch/csrc/distributed/c10d/intra_node_comm.cu torch/csrc/distributed/c10d/intra_node_comm.hpp,,,,,,
1877b7896c2,autograd_frontend,new features,[checkpoint] Clean up selective activation checkpoint and make public (#125795),docs/source/checkpoint.rst test/dynamo/test_activation_checkpointing.py test/test_autograd.py torch/_higher_order_ops/wrap.py torch/utils/checkpoint.py,https://github.com/pytorch/pytorch/pull/125795,soulitzer,Chillee,fmassa,,
d77a1aaa862,distributed,docs,DOC: add note about same sized tensors to dist.gather() (#128676),torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/128676,loganthomas,wconstab,,,
1a527915a64,distributed,Untopiced,[DSD] Correctly handle shared parameters for optimizer state_dict (#128685),test/distributed/checkpoint/test_state_dict.py torch/distributed/checkpoint/state_dict.py,https://github.com/pytorch/pytorch/pull/128685,fegin,LucasLLC,,,
bdffd9f0c6f,dynamo,not user facing,[export] Graph break on nn.Parameter construction (#128935),torch/_dynamo/variables/torch.py,https://github.com/pytorch/pytorch/pull/128935,anijain2305,angelayi,,,
44483972bdd,composability,not user facing,[EZ] Keep weight_norm var name aligned (#128955),torch/_decomp/decompositions.py,https://github.com/pytorch/pytorch/pull/128955,malfet,Skylion007,albanD,,
04a5d3228ec,skip,not user facing,[ts migration] Support prim::tolist and aten::len (#128894),test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/128894,BoyuanFeng,angelayi,,,
abde6cab4c7,distributed,not user facing,Remove compile_threads=1 in test_inductor_collectives.py (#128580),test/distributed/test_inductor_collectives.py,https://github.com/pytorch/pytorch/pull/128580,masnesral,eellison,,,
fe8558b7aa4,distributed,not user facing,[DSD] Add unittest to verify HSDP1 + broadcast_from_rank0 (#128755),test/distributed/checkpoint/test_state_dict.py,https://github.com/pytorch/pytorch/pull/128755,fegin,Skylion007,wz337,,
9a7e2519d3d,mps,Untopiced,[MPS] Fused Adam & AdamW (#127242),aten/src/ATen/native/mps/OperationUtils.h aten/src/ATen/native/mps/OperationUtils.mm aten/src/ATen/native/mps/operations/FusedAdamAmsgradKernelImpl.h aten/src/ATen/native/mps/operations/FusedAdamAmsgradKernelImpl.mm aten/src/ATen/native/mps/operations/FusedAdamKernel.mm aten/src/ATen/native/mps/operations/FusedAdamKernelImpl.h aten/src/ATen/native/mps/operations/FusedAdamKernelImpl.mm aten/src/ATen/native/mps/operations/FusedAdamWAmsgradKernelImpl.h aten/src/ATen/native/mps/operations/FusedAdamWAmsgradKernelImpl.mm aten/src/ATen/native/mps/operations/FusedAdamWKernel.mm aten/src/ATen/native/mps/operations/FusedAdamWKernelImpl.h aten/src/ATen/native/mps/operations/FusedAdamWKernelImpl.mm aten/src/ATen/native/mps/operations/FusedOptimizerOps.h aten/src/ATen/native/mps/operations/MultiTensorApply.h aten/src/ATen/native/native_functions.yaml test/test_mps.py test/test_optim.py torch/optim/adam.py torch/optim/adamw.py torch/testing/_internal/common_optimizers.py torch/utils/_foreach_utils.py,https://github.com/pytorch/pytorch/pull/127242,qqaatw,janeyx99,kulinseth,,
5bc9835d64e,skip,Untopiced,"Revert ""[dynamo][trace_rules] Remove incorrectly classified Ingraph functions (#128428)""",test/dynamo/test_repros.py torch/_dynamo/trace_rules.py,,,,,,
1babeddbbf3,skip,Untopiced,"Revert ""[inductor][mkldnn] Use floats instead of ints for pattern matcher test (#128484)""",test/inductor/test_mkldnn_pattern_matcher.py,,,,,,
44722c6b108,skip,Untopiced,"Revert ""[dynamo][fsdp] Dont take unspecializedNNModuleVariable path for FSDP modules (#128453)""",torch/_dynamo/variables/builder.py,,,,,,
5dc4f652bc5,nested tensor_frontend,improvements,Backward support for unbind() with NJT (#128032),test/test_nestedtensor.py tools/autograd/derivatives.yaml torch/csrc/autograd/FunctionsManual.cpp torch/csrc/autograd/FunctionsManual.h torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/128032,jbschlosser,soulitzer,,,
4cc3fb5ee22,Uncategorized,Untopiced,Bump urllib3 from 2.2.1 to 2.2.2 in /tools/build/bazel (#128908),tools/build/bazel/requirements.txt,,,,,,
2227da44317,profiler,not user facing,[Profiler] Clean up use_mtia to follow standard use_device instead (#126284),torch/autograd/profiler.py torch/profiler/profiler.py,https://github.com/pytorch/pytorch/pull/126284,aaronenyeshi,fenypatel99,,,
e47603a5495,inductor,bug fixes,Fix weight_norm decomposition behavior (#128956),torch/_decomp/decompositions.py,https://github.com/pytorch/pytorch/pull/128956,malfet,albanD,,,
cec31050b46,distributed,not user facing,"[BE][Easy] enable UFMT for `torch/distributed/{tensor,_tensor}/` (#128868)",.lintrunner.toml torch/distributed/_tensor/_collective_utils.py torch/distributed/_tensor/_dispatch.py torch/distributed/_tensor/_op_schema.py torch/distributed/_tensor/_sharding_prop.py torch/distributed/_tensor/_tp_conv.py torch/distributed/_tensor/api.py torch/distributed/_tensor/debug/__init__.py torch/distributed/_tensor/debug/_op_coverage.py torch/distributed/_tensor/debug/comm_mode.py torch/distributed/_tensor/debug/visualize_sharding.py torch/distributed/_tensor/examples/checkpoint_example.py torch/distributed/_tensor/examples/comm_mode_features_example.py torch/distributed/_tensor/examples/torchrec_sharding_example.py torch/distributed/_tensor/examples/visualize_sharding_example.py torch/distributed/_tensor/experimental/__init__.py torch/distributed/_tensor/experimental/attention.py torch/distributed/_tensor/experimental/local_map.py torch/distributed/_tensor/ops/__init__.py torch/distributed/_tensor/ops/basic_strategy.py torch/distributed/_tensor/ops/conv_ops.py torch/distributed/_tensor/ops/embedding_ops.py torch/distributed/_tensor/ops/experimental_ops.py torch/distributed/_tensor/ops/math_ops.py torch/distributed/_tensor/ops/matrix_ops.py torch/distributed/_tensor/ops/pointwise_ops.py torch/distributed/_tensor/ops/random_ops.py torch/distributed/_tensor/ops/tensor_ops.py torch/distributed/_tensor/ops/view_ops.py torch/distributed/_tensor/placement_types.py torch/distributed/_tensor/random.py torch/distributed/tensor/parallel/__init__.py torch/distributed/tensor/parallel/_utils.py torch/distributed/tensor/parallel/api.py torch/distributed/tensor/parallel/ddp.py torch/distributed/tensor/parallel/fsdp.py torch/distributed/tensor/parallel/input_reshard.py torch/distributed/tensor/parallel/loss.py torch/distributed/tensor/parallel/style.py,https://github.com/pytorch/pytorch/pull/128868,XuehaiPan,fegin,,,
3b798df8534,distributed,not user facing,"[BE][Easy] enable UFMT for `torch/distributed/{fsdp,optim,rpc}/` (#128869)",.lintrunner.toml torch/distributed/fsdp/__init__.py torch/distributed/fsdp/_common_utils.py torch/distributed/fsdp/_debug_utils.py torch/distributed/fsdp/_flat_param.py torch/distributed/fsdp/_init_utils.py torch/distributed/fsdp/_optim_utils.py torch/distributed/fsdp/_runtime_utils.py torch/distributed/fsdp/_state_dict_utils.py torch/distributed/fsdp/_unshard_param_utils.py torch/distributed/fsdp/_wrap_utils.py torch/distributed/fsdp/api.py torch/distributed/fsdp/fully_sharded_data_parallel.py torch/distributed/fsdp/sharded_grad_scaler.py torch/distributed/fsdp/wrap.py torch/distributed/optim/__init__.py torch/distributed/optim/apply_optimizer_in_backward.py torch/distributed/optim/functional_adadelta.py torch/distributed/optim/functional_adagrad.py torch/distributed/optim/functional_adam.py torch/distributed/optim/functional_adamax.py torch/distributed/optim/functional_adamw.py torch/distributed/optim/functional_rmsprop.py torch/distributed/optim/functional_rprop.py torch/distributed/optim/functional_sgd.py torch/distributed/optim/named_optimizer.py torch/distributed/optim/optimizer.py torch/distributed/optim/utils.py torch/distributed/optim/zero_redundancy_optimizer.py torch/distributed/rpc/__init__.py torch/distributed/rpc/_testing/__init__.py torch/distributed/rpc/_testing/faulty_agent_backend_registry.py torch/distributed/rpc/_utils.py torch/distributed/rpc/api.py torch/distributed/rpc/backend_registry.py torch/distributed/rpc/constants.py torch/distributed/rpc/functions.py torch/distributed/rpc/internal.py torch/distributed/rpc/options.py torch/distributed/rpc/rref_proxy.py torch/distributed/rpc/server_process_global_profiler.py,https://github.com/pytorch/pytorch/pull/128869,XuehaiPan,fegin,,,
a0e1e20c415,skip,not user facing,[BE][Easy] enable UFMT for `torch/distributed/` (#128870),.lintrunner.toml torch/distributed/__init__.py torch/distributed/_composable/fsdp/_fsdp_collectives.py torch/distributed/_composable/fsdp/_fsdp_common.py torch/distributed/_composable/fsdp/_fsdp_init.py torch/distributed/_composable/fsdp/_fsdp_param.py torch/distributed/_composable/fsdp/_fsdp_param_group.py torch/distributed/_composable/fsdp/_fsdp_state.py torch/distributed/_composable/fsdp/fully_shard.py torch/distributed/_composable/fully_shard.py torch/distributed/_composable/replicate.py torch/distributed/_cuda_p2p/__init__.py torch/distributed/_functional_collectives.py torch/distributed/_functional_collectives_impl.py torch/distributed/_sharded_tensor/__init__.py torch/distributed/_sharding_spec/__init__.py torch/distributed/_state_dict_utils.py torch/distributed/_tools/memory_tracker.py torch/distributed/c10d_logger.py torch/distributed/collective_utils.py torch/distributed/constants.py torch/distributed/device_mesh.py torch/distributed/distributed_c10d.py torch/distributed/examples/memory_tracker_example.py torch/distributed/launcher/__init__.py torch/distributed/launcher/api.py torch/distributed/logging_handlers.py torch/distributed/nn/__init__.py torch/distributed/nn/api/remote_module.py torch/distributed/nn/functional.py torch/distributed/pipelining/_IR.py torch/distributed/pipelining/__init__.py torch/distributed/remote_device.py torch/distributed/rendezvous.py torch/distributed/run.py torch/distributed/utils.py,https://github.com/pytorch/pytorch/pull/128870,XuehaiPan,fegin,wconstab,,
d9c294c6726,inductor,not user facing,[Inductor] Fix arguments passed to triton kernel launch hooks (#128732),test/inductor/test_profiler.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/128732,Jokeren,bertmaher,shunting314,,
ac5f565fa70,distributed,Untopiced,[FSDP2] Added `set_post_optim_event` (#128975),test/distributed/_composable/fsdp/test_fully_shard_overlap.py test/distributed/_composable/fsdp/test_fully_shard_training.py torch/distributed/_composable/fsdp/_fsdp_state.py torch/distributed/_composable/fsdp/fully_shard.py,https://github.com/pytorch/pytorch/pull/128975,awgu,sanketpurandare,weifengpy,,
cb5e9183c60,quantization,not user facing,[Caffe2] [2/N] Remove Caffe2 from tests (#128911),test/jit/test_tracer.py test/onnx/pytorch_test_common.py test/onnx/test_operators.py test/quantization/core/test_quantized_op.py test/test_determination.py test/test_public_bindings.py test/test_tensorboard.py test/test_torch.py,https://github.com/pytorch/pytorch/pull/128911,cyyever,r-barnes,titaiwangms,,
c5e0b844847,skip,not user facing,[dynamo][trace_rules] Remove incorrectly classified Ingraph functions (#128428),test/dynamo/test_repros.py torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/128428,anijain2305,mlazos,yanboliang,,
670b94c9c82,skip,not user facing,[inductor][mkldnn] Use floats instead of ints for pattern matcher test (#128484),test/inductor/test_mkldnn_pattern_matcher.py,https://github.com/pytorch/pytorch/pull/128484,anijain2305,mlazos,,,
99f042d336b,skip,Untopiced,"Revert ""Forward fix to skip ROCm tests for #122836 (#128891)""",test/test_nestedtensor.py,,,,,,
35c78668b40,skip,not user facing,Improve the debugging message for when foreach mta_called (#128991),test/test_foreach.py,https://github.com/pytorch/pytorch/pull/128991,janeyx99,clee2000,,,
5ffb032be68,skip,Untopiced,"Revert ""Backward support for unbind() with NJT (#128032)""",test/test_nestedtensor.py tools/autograd/derivatives.yaml torch/csrc/autograd/FunctionsManual.cpp torch/csrc/autograd/FunctionsManual.h torch/nested/_internal/ops.py,,,,,,
b0d2fe6299c,skip,Untopiced,"Revert ""Short-term fix to preserve NJT metadata cache in torch.compile (#122836)""",aten/src/ATen/FunctionalInverses.cpp aten/src/ATen/native/native_functions.yaml test/dynamo/test_subclasses.py test/expect/HasDecompTest.test_has_decomposition.expect test/test_nestedtensor.py tools/autograd/derivatives.yaml torch/nested/_internal/nested_tensor.py torch/nested/_internal/ops.py torch/nested/_internal/sdpa.py,,,,,,
2458f79f83e,skip,not user facing,[Inductor UT][Intel GPU] Skip newly added test case test_torchinductor_strided_blocks:test_reduction for Intel GPU (#128881),test/inductor/test_torchinductor_strided_blocks.py,https://github.com/pytorch/pytorch/pull/128881,etaf,EikanWang,blaine-rister,malfet,
eda375a4907,inductor,not user facing,[Inductor] Remove min/max from inductor opinfo test (#128925),test/inductor/test_torchinductor_opinfo.py,https://github.com/pytorch/pytorch/pull/128925,leslie-fang-intel,isuruf,jgong5,peterbell10,
4bc90185fb7,releng,not user facing,fix: Print statements causing parse error (#128969),.github/scripts/get_workflow_type.py,https://github.com/pytorch/pytorch/pull/128969,zxiiro,ZainRizvi,tylertitsworth,,
df85f34a14d,skip,not user facing,Add test to xfail_list only for abi_compatible (#128506),test/inductor/test_cpu_cpp_wrapper.py,https://github.com/pytorch/pytorch/pull/128506,chunyuan-w,desertfire,jgong5,,
ed5b8432cdf,inductor,not user facing,Enable mixed_mm only if casting from lower-bitwidth type to a higher one (#128899),test/inductor/test_pattern_matcher.py torch/_inductor/fx_passes/post_grad.py,https://github.com/pytorch/pytorch/pull/128899,AlnisM,eellison,,,
8771e3429c3,distributed,not user facing,Introduce a prototype for SymmetricMemory (#128582),.lintrunner.toml BUILD.bazel build_variables.bzl c10/cuda/driver_api.h caffe2/CMakeLists.txt test/distributed/test_symmetric_memory.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/CUDASymmetricMemory.cu torch/csrc/distributed/c10d/CUDASymmetricMemory.hpp torch/csrc/distributed/c10d/ProcessGroupCudaP2P.hpp torch/csrc/distributed/c10d/SymmetricMemory.cpp torch/csrc/distributed/c10d/SymmetricMemory.hpp torch/csrc/distributed/c10d/init.cpp torch/csrc/distributed/c10d/intra_node_comm.cpp torch/csrc/distributed/c10d/intra_node_comm.cu torch/csrc/distributed/c10d/intra_node_comm.hpp,https://github.com/pytorch/pytorch/pull/128582,yifuwang,wanchaol,,,
eb9f4da11e8,inductor,not user facing,Modified template indexing to broadcast indices to out instead of mask and some other flexattention micro-opts (#128938),benchmarks/transformer/score_mod.py torch/_inductor/kernel/flex_attention.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/128938,Chillee,drisspg,,,
acefc5c0160,dynamo,not user facing,[torch.compile] Enable bwd compilation metrics (#128973),torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/128973,yanboliang,dshi7,,,
1f0a68b5729,cuda,Untopiced,[ROCm] Fix fp32 atomicAdd for non-MI100 GPUs (#128750),aten/src/ATen/cuda/Atomic.cuh,https://github.com/pytorch/pytorch/pull/128750,jerrymannil,xw285cornell,,,
2f88597aad1,inductor,not user facing,"[inductor] For internal, allow multiple workers if the method is ""subprocess"" (#129002)",torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/129002,masnesral,eellison,,,
fcf2a1378b5,skip,not user facing,"Enable fp8 rowwise scaling kernel on cuda, TAKE 2: #125204  (#128989)",aten/src/ATen/CMakeLists.txt aten/src/ATen/cuda/detail/LazyNVRTC.cpp aten/src/ATen/cuda/nvrtc_stub/ATenNVRTC.h aten/src/ATen/native/cuda/Blas.cpp aten/src/ATen/native/cuda/RowwiseScaledMM.cu aten/src/ATen/native/cuda/RowwiseScaledMM.h test/test_matmul_cuda.py third_party/cutlass.BUILD,https://github.com/pytorch/pytorch/pull/128989,drisspg,vkuzo,yangsiyu007,,
a584b2a389b,skip,Untopiced,"Revert ""Add test to xfail_list only for abi_compatible (#128506)""",test/inductor/test_cpu_cpp_wrapper.py,,,,,,
3a185778edb,skip,not user facing,[aotinductor] Add torch.polar fallback op for shim v2 (#128722),test/inductor/test_cpu_cpp_wrapper.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/csrc/inductor/aoti_torch/generated/c_shim_cuda.h torchgen/aoti/fallback_ops.py,https://github.com/pytorch/pytorch/pull/128722,ColinPeppler,chenyang78,desertfire,,
ba92f5277fe,inductor,not user facing,[inductor][refactor] Unify the use of generate_kernel_call (#128467),torch/_inductor/codegen/cpp_wrapper_cuda.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_foreach.py torch/_inductor/codegen/wrapper.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/128467,desertfire,shunting314,,,
d3e8b8bf472,skip,not user facing,Remove cuda check in the CUDAGraph destructor (#127382),aten/src/ATen/cuda/CUDAGeneratorImpl.cpp,https://github.com/pytorch/pytorch/pull/127382,eee4017,eellison,eqy,,
50567f7081b,distributed,Untopiced,Pass device to is_pinned call inside TensorProperties.create_from_tensor (#128896),test/distributed/checkpoint/test_utils.py torch/distributed/_shard/sharded_tensor/metadata.py torch/distributed/checkpoint/metadata.py,https://github.com/pytorch/pytorch/pull/128896,daulet-askarov,fegin,,,
7fac03aee94,releng,Untopiced,[ALI] Use lf runners for Lint (#128978),.github/workflows/lint.yml,,,,,,
e49525275df,skip,Untopiced,Make TraceUtils.h to be device-agnostic (#126969),torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/TraceUtils.h,https://github.com/pytorch/pytorch/pull/126969,FFFrog,c-p-i-o,,,
118f9ceb7c9,inductor,not user facing,[inductor][ci] Fix torchbench dependency issue with numpy (#128968),.github/ci_commit_pins/torchbench.txt,https://github.com/pytorch/pytorch/pull/128968,xuzhao9,eellison,,,
3397d5ef908,releng,not user facing,"Revert ""[ALI] Use lf runners for Lint"" (#129070)",.github/workflows/lint.yml,https://github.com/pytorch/pytorch/pull/129070,jeanschmidt,atalman,,,
ffb50fb6912,onnx,new features,[ONNX] Add onnx::Gelu support for version 20 (#128773),test/onnx/test_utility_funs.py torch/onnx/symbolic_opset20.py,https://github.com/pytorch/pytorch/pull/128773,YibinLiu666,justinchuby,,,
7d33ff59ba4,releng,not user facing,[Split Build]Use same package (#127934),.ci/pytorch/build.sh setup.py tools/setup_helpers/env.py torch/CMakeLists.txt torch/__init__.py,https://github.com/pytorch/pytorch/pull/127934,PaliC,atalman,,,
236fbcbdf44,releng,not user facing,[Split Build] Test split build in pull CI workflow (#126813),.ci/pytorch/common_utils.sh .ci/pytorch/test.sh .github/actions/linux-build/action.yml .github/workflows/_linux-build-label.yml .github/workflows/_linux-build.yml .github/workflows/pull.yml caffe2/CMakeLists.txt torch/__init__.py,https://github.com/pytorch/pytorch/pull/126813,PaliC,atalman,,,
1b92bdd0ea3,skip,not user facing,[ALI] [Reland] Use LF runners for Lint (#129071),.github/workflows/lint.yml,https://github.com/pytorch/pytorch/pull/129071,jeanschmidt,malfet,,,
0fc603ece42,Uncategorized,Untopiced,[optim] Fused implementation stability table (#129006),docs/source/optim.rst,https://github.com/pytorch/pytorch/pull/129006,qqaatw,malfet,,,
0707811286d,skip,not user facing,[export] experimental joint graph API. (#128847),docs/source/export.rst test/export/test_experimental.py torch/export/experimental/__init__.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/128847,zhxchen17,tugsbayasgalan,,,
bafd68b4fcc,inductor,not user facing,[inductor] fix windows python module ext and func export declaration (#129059),torch/_inductor/codegen/cpp.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/129059,xuhancn,jansel,jgong5,,
b5d541609d1,profiler,not user facing,[Memory Snapshot] Add recordAnnotations to capture record_function annotations (#129072),c10/core/Allocator.h c10/cuda/CUDACachingAllocator.cpp c10/cuda/CUDACachingAllocator.h torch/csrc/cuda/Module.cpp torch/csrc/cuda/memory_snapshot.cpp torch/csrc/profiler/combined_traceback.cpp torch/csrc/profiler/combined_traceback.h,https://github.com/pytorch/pytorch/pull/129072,aaronenyeshi,zdevito,,,
df94d57c0af,skip,Untopiced,"Revert ""[export] experimental joint graph API. (#128847)""",docs/source/export.rst test/export/test_experimental.py torch/export/experimental/__init__.py torch/export/exported_program.py,,,,,,
73f5d2b7876,releng,Untopiced,Run ET unit tests on PT CI (#128560),.ci/docker/ci_commit_pins/executorch.txt .ci/docker/common/install_executorch.sh .ci/pytorch/test.sh test/dynamo/test_misc.py,https://github.com/pytorch/pytorch/pull/128560,huydhn,digantdesai,guangy10,,
fca408fa29d,skip,not user facing,s390x vectorization: rework operators (#129066),aten/src/ATen/cpu/vec/vec256/zarch/vec256_zarch.h,https://github.com/pytorch/pytorch/pull/129066,AlekseiNikiforovIBM,malfet,,,
277f2914a53,skip,not user facing,[9/N] Remove unused functions (#128704),aten/src/ATen/native/mkldnn/Linear.cpp,https://github.com/pytorch/pytorch/pull/128704,cyyever,malfet,,,
571a0db1324,inductor,not user facing,[inductor] Fix logging for run_and_get_cpp_code (#128794),test/inductor/test_cpu_cpp_wrapper.py test/inductor/test_cpu_repro.py test/inductor/test_torchinductor.py torch/_inductor/graph.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/128794,masnesral,leslie-fang-intel,oulgen,,
a6ac6447b55,releng,not user facing,Re-enable py3.12 nightly wheel builds and add triton dependency for ROCm  (#128525),.circleci/scripts/binary_populate_env.sh .github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/128525,jithunnair-amd,malfet,,,
9d06e3783d5,inductor,not user facing,[Inductor][CPP] Fix the symbolic size cast issue in GEMM Benchmark (#128824),torch/_inductor/autotune_process.py torch/_inductor/codegen/cpp_template.py,https://github.com/pytorch/pytorch/pull/128824,leslie-fang-intel,jansel,jgong5,,
a2b1673dfb1,distributed,not user facing,[Horace's PR #126446] Prevent partitioner from ever saving views (#129039),test/distributed/_composable/test_replicate_with_compiler.py test/dynamo/test_repros.py torch/_functorch/config.py torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/129039,yf225,Chillee,,,
859fa183feb,inductor,not user facing,BE: Use future annotations in inductor scheduler and ir (#128892),torch/_inductor/codegen/wrapper.py torch/_inductor/ir.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/128892,peterbell10,lezcano,,,
d8db0749885,dynamo,not user facing,[Traceable FSDP2] [Dynamo] Fix OptimizedModule._initialize to allow tracing into FSDP2 module hooks for module from user-defined module class (#129046),torch/_dynamo/eval_frame.py,https://github.com/pytorch/pytorch/pull/129046,yf225,anijain2305,,,
609ffaf7177,releng,not user facing,Add more shards for slow CPU and ROCm jobs (#128873),.github/workflows/slow.yml,https://github.com/pytorch/pytorch/pull/128873,huydhn,PaliC,,,
19f3abcde45,mps,Untopiced,[Docs][MPS] Add mps environment variable table (#129008),docs/source/mps_environment_variables.rst docs/source/torch_environment_variables.rst,https://github.com/pytorch/pytorch/pull/129008,qqaatw,malfet,,,
ad2593cb862,dynamo,Untopiced,[Animesh's PR #125340] [dynamo][fsdp] Track FSDPNNModuleVariable for mutations (#129045),test/distributed/test_dynamo_distributed.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/129045,yf225,anijain2305,,,
6d2b3c90f14,skip,not user facing,Improved flexattention bwd perf + added configurations for benchmarks (#129013),benchmarks/transformer/score_mod.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/129013,Chillee,drisspg,yanboliang,,
0d25f096c1b,inductor,bug fixes,[CppInductor] Fix erfinv codegen when non-vectorized isa (#129090),torch/_inductor/codegen/cpp_prefix.h,https://github.com/pytorch/pytorch/pull/129090,malfet,jansel,jgong5,,
799acd31b4c,mps,Untopiced,[MPS] Add lu_factor (#99269),aten/src/ATen/native/mps/operations/LinearAlgebra.mm aten/src/ATen/native/native_functions.yaml test/test_mps.py tools/autograd/derivatives.yaml,https://github.com/pytorch/pytorch/pull/99269,qqaatw,kulinseth,lezcano,,
54b0006cb23,inductor,not user facing,Evaluate symexprs on load path of cache not write (#128997),docs/source/conf.py test/allowlist_for_publicAPI.json torch/_guards.py torch/_inductor/codecache.py torch/_inductor/compile_fx.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/128997,oulgen,ezyang,,,
ea47d542ca3,dynamo,not user facing,[dynamo][guards] Remove BOOL_FALSE - not needed after C++ guards (#129098),torch/_dynamo/guards.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/129098,anijain2305,jansel,,,
7178b4e987c,dynamo,Untopiced,[Dynamo x torch_function] fix incorrect source (#128980),test/dynamo/test_subclasses.py torch/_dynamo/variables/torch_function.py,https://github.com/pytorch/pytorch/pull/128980,zou3519,mlazos,,,
2bb8ee602b2,nested tensor_frontend,not user facing,Fix DEBUG=1 asserts with NJT ops (#129014),torch/csrc/autograd/autograd_not_implemented_fallback.cpp,https://github.com/pytorch/pytorch/pull/129014,jbschlosser,YuqingJ,soulitzer,,
e0aa992d73c,releng,not user facing,Fix inductor and deploy jobs timing out (#129108),.github/workflows/inductor-cu124.yml .github/workflows/inductor.yml .github/workflows/periodic.yml,https://github.com/pytorch/pytorch/pull/129108,huydhn,malfet,,,
b2a9b8d4853,inductor,bug fixes,[CpuInductor] Enable NEON ISA detection on Linux ARM (#129075),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/129075,malfet,jansel,,,
fc5b0ff2d7f,releng,not user facing,[BE][Hackaday] deprecate legacy cuda docker image (#128859),.circleci/scripts/binary_populate_env.sh,https://github.com/pytorch/pytorch/pull/128859,PaliC,atalman,,,
65286883d43,skip,not user facing,"[export] reland ""experimental joint graph API."" (#129081)",docs/source/export.rst test/export/test_experimental.py torch/export/experimental/__init__.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/129081,zhxchen17,tugsbayasgalan,,,
832fc352115,skip,Untopiced,"Revert ""Improved flexattention bwd perf + added configurations for benchmarks (#129013)""",benchmarks/transformer/score_mod.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/select_algorithm.py,,,,,,
ddb95dbb0d8,quantization,Untopiced,Fixing equalize with three things and improving functionality (#124632),torch/ao/quantization/_equalize.py,https://github.com/pytorch/pytorch/pull/124632,TiRune,jerryzh168,,,
734891ac22a,jit,Untopiced,Fix export log script (#128967),torch/jit/_trace.py,https://github.com/pytorch/pytorch/pull/128967,tugsbayasgalan,jiashenC,,,
8c2542623bb,dynamo,not user facing,[Traceable FSDP2] [Dynamo] Add tracing support for out-variant custom ops that return None (#129078),test/dynamo/test_misc.py torch/_dynamo/variables/torch.py,https://github.com/pytorch/pytorch/pull/129078,yf225,yanboliang,,,
73340f09094,skip,Untopiced,"Revert ""[3/N] Non-Tensor: Support string parameter for aten operations (#125831)""",test/inductor/test_torchinductor.py torch/_inductor/aoti_eager.py torch/csrc/inductor/aoti_eager/kernel_holder.cpp torch/csrc/inductor/aoti_eager/kernel_meta_info.cpp torch/csrc/inductor/aoti_eager/kernel_meta_info.h,,,,,,
254487f288d,skip,Untopiced,"Revert ""Separate AOTI Eager utils as a single file (#125819)""",test/inductor/test_torchinductor.py torch/_inductor/aoti_eager.py torch/_inductor/utils.py torch/csrc/inductor/aoti_eager/kernel_holder.cpp,,,,,,
e84cf805d2e,skip,Untopiced,"Revert ""Modularize aten parameter parser and checker (#125308)""",test/inductor/test_torchinductor.py torch/_C/__init__.pyi.in torch/_inductor/utils.py torch/csrc/dynamo/guards.cpp torch/csrc/dynamo/guards.h torch/csrc/inductor/aoti_eager/kernel_holder.cpp torch/csrc/inductor/aoti_eager/kernel_holder.h torch/csrc/inductor/aoti_eager/kernel_meta_info.cpp torch/csrc/inductor/aoti_eager/kernel_meta_info.h torch/csrc/utils/python_dispatch.cpp,,,,,,
f2f4dde2d37,dynamo,not user facing,[dynamo] Remove ID_MATCH for FSDPModuleVariable (#129015),test/distributed/test_dynamo_distributed.py torch/_dynamo/guards.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/129015,anijain2305,yf225,,,
aace8ffc002,skip,Untopiced,"Revert ""[BE] enable UFMT for `torch/nn/*.py` (#128593)""",.lintrunner.toml torch/__init__.py torch/nn/__init__.py torch/nn/_reduction.py torch/nn/common_types.py torch/nn/grad.py torch/nn/init.py torch/nn/parameter.py torch/utils/data/dataloader.py torch/utils/data/datapipes/_hook_iterator.py,,,,,,
61fa3de4cb7,releng,not user facing,ci: Hardcode runner-determinator (#128985),.github/workflows/_runner-determinator.yml,https://github.com/pytorch/pytorch/pull/128985,zxiiro,ZainRizvi,,,
adc14adb883,skip,not user facing,Fix flakiness with test_binary_op_list_error_cases (#129003),test/test_foreach.py,https://github.com/pytorch/pytorch/pull/129003,janeyx99,soulitzer,,,
5fba5d83f07,python_frontend,docs,add xpu for amp (#127276),docs/source/amp.rst torch/amp/autocast_mode.py,https://github.com/pytorch/pytorch/pull/127276,jingxu10,albanD,dvrogozh,malfet,
63a724d8e1f,skip,Untopiced,"Revert ""Introduce a prototype for SymmetricMemory (#128582)""",.lintrunner.toml BUILD.bazel build_variables.bzl c10/cuda/driver_api.h caffe2/CMakeLists.txt test/distributed/test_symmetric_memory.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/CUDASymmetricMemory.cu torch/csrc/distributed/c10d/CUDASymmetricMemory.hpp torch/csrc/distributed/c10d/ProcessGroupCudaP2P.hpp torch/csrc/distributed/c10d/SymmetricMemory.cpp torch/csrc/distributed/c10d/SymmetricMemory.hpp torch/csrc/distributed/c10d/init.cpp torch/csrc/distributed/c10d/intra_node_comm.cpp torch/csrc/distributed/c10d/intra_node_comm.cu torch/csrc/distributed/c10d/intra_node_comm.hpp,,,,,,
31d5753247d,skip,not user facing,Short-term fix to preserve NJT metadata cache in torch.compile (#122836),aten/src/ATen/FunctionalInverses.cpp aten/src/ATen/native/native_functions.yaml test/dynamo/test_subclasses.py test/expect/HasDecompTest.test_has_decomposition.expect test/test_nestedtensor.py tools/autograd/derivatives.yaml torch/nested/_internal/nested_tensor.py torch/nested/_internal/ops.py torch/nested/_internal/sdpa.py,https://github.com/pytorch/pytorch/pull/122836,jbschlosser,soulitzer,,,
43060a1dbc0,releng,Untopiced,Add shard support to test_inductor (#129160),.ci/pytorch/test.sh,https://github.com/pytorch/pytorch/pull/129160,huydhn,malfet,seemethere,,
c027c8935b2,skip,not user facing,[distributed] NCCL result code update (#128777),torch/csrc/cuda/nccl.cpp torch/csrc/cuda/nccl.h,https://github.com/pytorch/pytorch/pull/128777,myungjin,Skylion007,,,
9dd8f8cf8b7,skip,not user facing,[cpuinfo][submodule] bump cpuinfo to the latest to support amx isa check (#127505),third_party/cpuinfo,https://github.com/pytorch/pytorch/pull/127505,jgong5,ezyang,,,
9c929f6ce9a,skip,Untopiced,"Revert ""[BE][Easy] enable UFMT for `torch/distributed/` (#128870)""",.lintrunner.toml torch/distributed/__init__.py torch/distributed/_composable/fsdp/_fsdp_collectives.py torch/distributed/_composable/fsdp/_fsdp_common.py torch/distributed/_composable/fsdp/_fsdp_init.py torch/distributed/_composable/fsdp/_fsdp_param.py torch/distributed/_composable/fsdp/_fsdp_param_group.py torch/distributed/_composable/fsdp/_fsdp_state.py torch/distributed/_composable/fsdp/fully_shard.py torch/distributed/_composable/fully_shard.py torch/distributed/_composable/replicate.py torch/distributed/_cuda_p2p/__init__.py torch/distributed/_functional_collectives.py torch/distributed/_functional_collectives_impl.py torch/distributed/_sharded_tensor/__init__.py torch/distributed/_sharding_spec/__init__.py torch/distributed/_state_dict_utils.py torch/distributed/_tools/memory_tracker.py torch/distributed/c10d_logger.py torch/distributed/collective_utils.py torch/distributed/constants.py torch/distributed/device_mesh.py torch/distributed/distributed_c10d.py torch/distributed/examples/memory_tracker_example.py torch/distributed/launcher/__init__.py torch/distributed/launcher/api.py torch/distributed/logging_handlers.py torch/distributed/nn/__init__.py torch/distributed/nn/api/remote_module.py torch/distributed/nn/functional.py torch/distributed/pipelining/_IR.py torch/distributed/pipelining/__init__.py torch/distributed/remote_device.py torch/distributed/rendezvous.py torch/distributed/run.py torch/distributed/utils.py,,,,,,
cc8193c7078,skip,Untopiced,"Revert ""[BE] enable UFMT for `torch/nn/functional.py` (#128592)""",.lintrunner.toml tools/pyi/gen_pyi.py torch/__init__.py torch/_jit_internal.py torch/_library/utils.py torch/_linalg_utils.py torch/_lowrank.py torch/_meta_registrations.py torch/_utils.py torch/_utils_internal.py torch/_vmap_internals.py torch/distributed/__init__.py torch/functional.py torch/hub.py torch/library.py torch/nn/functional.py torch/overrides.py torch/serialization.py,,,,,,
d4022b46586,skip,Untopiced,"Revert ""[BE] enable UFMT for `torch/nn/modules` (#128594)""",.lintrunner.toml torch/nn/modules/__init__.py torch/nn/modules/_functions.py torch/nn/modules/activation.py torch/nn/modules/adaptive.py torch/nn/modules/batchnorm.py torch/nn/modules/channelshuffle.py torch/nn/modules/container.py torch/nn/modules/conv.py torch/nn/modules/distance.py torch/nn/modules/dropout.py torch/nn/modules/flatten.py torch/nn/modules/fold.py torch/nn/modules/instancenorm.py torch/nn/modules/lazy.py torch/nn/modules/linear.py torch/nn/modules/loss.py torch/nn/modules/module.py torch/nn/modules/normalization.py torch/nn/modules/padding.py torch/nn/modules/pixelshuffle.py torch/nn/modules/pooling.py torch/nn/modules/rnn.py torch/nn/modules/sparse.py torch/nn/modules/transformer.py torch/nn/modules/upsampling.py torch/nn/modules/utils.py,,,,,,
5da428d9eba,skip,not user facing,[cpu][flash attention] fix attention mask issue (#128816),aten/src/ATen/native/cpu/FlashAttentionKernel.cpp test/test_transformers.py,https://github.com/pytorch/pytorch/pull/128816,Valentine233,drisspg,jgong5,,
8758fedbfca,Uncategorized,Untopiced,[export] copy sym ops when respecting call module signature (#129153),test/export/test_unflatten.py torch/export/unflatten.py,https://github.com/pytorch/pytorch/pull/129153,pianpwk,angelayi,,,
31c9e3d2f4b,distributed,not user facing,[FSDP][Test] Test save model save with FSDP1 and load into FSDP2 applied model (#129028),test/distributed/checkpoint/fsdp/test_fsdp_dsd.py,https://github.com/pytorch/pytorch/pull/129028,wz337,awgu,fegin,,
0acd09aecd0,distributed,not user facing,[torchrec][pt-d][model store] introduce LocalShardsWrapper for DTensor (#129150),torch/distributed/_checkpointable.py torch/distributed/_tensor/_shards_wrapper.py torch/distributed/_tensor/api.py torch/distributed/checkpoint/planner.py torch/distributed/checkpoint/planner_helpers.py torch/distributed/checkpoint/utils.py,https://github.com/pytorch/pytorch/pull/129150,iamzainhuda,XilunWu,,,
ff89ebc50a7,skip,not user facing,Improved flexattention bwd perf + added configurations for benchmarks (#129013),benchmarks/transformer/score_mod.py test/inductor/test_flex_attention.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/129013,Chillee,drisspg,yanboliang,,
8cd9b104568,composability,not user facing,Fix exp decomp numerics (#129154),test/test_decomp.py torch/_refs/__init__.py,https://github.com/pytorch/pytorch/pull/129154,eellison,bdhirsh,zou3519,,
880e894c39f,dynamo,not user facing,[Brian's PR #128981] fix dynamo isinstance inlining for nn.Parameter + subclasses (#129162),test/dynamo/test_subclasses.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/129162,yf225,bdhirsh,,,
e99a24ce7c0,skip,not user facing,Remove TensorImpl_test.cpp (#129054),.lintrunner.toml aten/src/ATen/CMakeLists.txt aten/src/ATen/core/TensorImpl_test.cpp,https://github.com/pytorch/pytorch/pull/129054,cyyever,albanD,malfet,,
e8dbb45e981,dynamo,not user facing,[dynamo][user-defined-object] Check that object is valid (#129117),test/dynamo_expected_failures/TestComposability.test_convert_without_squash_mask test/dynamo_expected_failures/TestComposability.test_fusion_before_s_prep test/dynamo_expected_failures/TestComposability.test_q_prep_before_s_prep test/dynamo_expected_failures/TestComposability.test_qat_prep_before_s_prep test/dynamo_expected_failures/TestComposability.test_s_prep_before_fusion test/dynamo_expected_failures/TestComposability.test_s_prep_before_q_prep test/dynamo_expected_failures/TestComposability.test_s_prep_before_qat_prep test/dynamo_expected_failures/TestGenerateNumericDebugHandle.test_quantize_pt2e_preserve_handle test/dynamo_expected_failures/TestPT2ERepresentation.test_add test/dynamo_expected_failures/TestPT2ERepresentation.test_add_relu test/dynamo_expected_failures/TestPT2ERepresentation.test_conv2d test/dynamo_expected_failures/TestPT2ERepresentation.test_dynamic_linear test/dynamo_expected_failures/TestPT2ERepresentation.test_maxpool2d test/dynamo_expected_failures/TestPT2ERepresentation.test_qdq test/dynamo_expected_failures/TestPT2ERepresentation.test_qdq_per_channel test/dynamo_expected_failures/TestPT2ERepresentation.test_static_linear test/dynamo_expected_failures/TestQuantizePT2E.test_composable_quantizer_linear_conv test/dynamo_expected_failures/TestQuantizePT2E.test_constant_prop_preserve_metadata test/dynamo_expected_failures/TestQuantizePT2E.test_embedding_conv_linear_quantization test/dynamo_expected_failures/TestQuantizePT2E.test_fold_all_ops_before_quantize test/dynamo_expected_failures/TestQuantizePT2E.test_fold_quantize test/dynamo_expected_failures/TestQuantizePT2E.test_fold_quantize_per_channel test/dynamo_expected_failures/TestQuantizePT2E.test_groupwise_per_channel_quant test/dynamo_expected_failures/TestQuantizePT2E.test_multi_users_without_output_observer test/dynamo_expected_failures/TestQuantizePT2E.test_reentrant test/dynamo_expected_failures/TestQuantizePT2E.test_save_load test/dynamo_expected_failures/TestQuantizePT2E.test_speed test/dynamo_expected_failures/TestQuantizePT2EQATModels.test_qat_resnet18 test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn1d.test_prepare_qat_conv_bn_fusion_getitem_placeholder test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn1d.test_qat_conv_bn_fusion test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn1d.test_qat_conv_bn_fusion_literal_args test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn1d.test_qat_conv_bn_fusion_no_conv_bias test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn1d.test_qat_conv_bn_relu_fusion test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn1d.test_qat_conv_bn_relu_fusion_no_conv_bias test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn1d.test_qat_conv_no_bias test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn1d.test_qat_conv_transpose_bn test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn1d.test_qat_conv_transpose_bn_relu test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn1d.test_qat_inplace_add_relu test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn1d.test_qat_preserve_source_fn_stack test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn1d.test_qat_update_shared_qspec test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn2d.test_prepare_qat_conv_bn_fusion_getitem_placeholder test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn2d.test_qat_conv_bn_fusion test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn2d.test_qat_conv_bn_fusion_literal_args test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn2d.test_qat_conv_bn_fusion_no_conv_bias test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn2d.test_qat_conv_bn_relu_fusion test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn2d.test_qat_conv_bn_relu_fusion_no_conv_bias test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn2d.test_qat_conv_no_bias test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn2d.test_qat_conv_transpose_bn test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn2d.test_qat_conv_transpose_bn_relu test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn2d.test_qat_inplace_add_relu test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn2d.test_qat_preserve_source_fn_stack test/dynamo_expected_failures/TestQuantizePT2EQAT_ConvBn2d.test_qat_update_shared_qspec test/dynamo_expected_failures/TestXNNPACKQuantizer.test_add_and_inplace_add test/dynamo_expected_failures/TestXNNPACKQuantizer.test_add_mul_long test/dynamo_expected_failures/TestXNNPACKQuantizer.test_add_mul_scalar test/dynamo_expected_failures/TestXNNPACKQuantizer.test_conv1d test/dynamo_expected_failures/TestXNNPACKQuantizer.test_conv1d_with_conv2d test/dynamo_expected_failures/TestXNNPACKQuantizer.test_conv2d test/dynamo_expected_failures/TestXNNPACKQuantizer.test_conv_linear test/dynamo_expected_failures/TestXNNPACKQuantizer.test_conv_linear_no_permute test/dynamo_expected_failures/TestXNNPACKQuantizer.test_dynamic_linear test/dynamo_expected_failures/TestXNNPACKQuantizer.test_dynamic_linear_int4_weight test/dynamo_expected_failures/TestXNNPACKQuantizer.test_dynamic_linear_with_conv test/dynamo_expected_failures/TestXNNPACKQuantizer.test_gru test/dynamo_expected_failures/TestXNNPACKQuantizer.test_linear test/dynamo_expected_failures/TestXNNPACKQuantizer.test_linear_gru test/dynamo_expected_failures/TestXNNPACKQuantizer.test_linear_relu test/dynamo_expected_failures/TestXNNPACKQuantizer.test_linear_with_dynamic_shape test/dynamo_expected_failures/TestXNNPACKQuantizer.test_mul_and_inplace_mul test/dynamo_expected_failures/TestXNNPACKQuantizer.test_mul_float32_max test/dynamo_expected_failures/TestXNNPACKQuantizer.test_obs_sharing_ops test/dynamo_expected_failures/TestXNNPACKQuantizer.test_propagate_annotation test/dynamo_expected_failures/TestXNNPACKQuantizer.test_qat_dynamic_linear test/dynamo_expected_failures/TestXNNPACKQuantizerModels.test_resnet18 torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/129117,anijain2305,yf225,,,
b5428250668,skip,not user facing,Enable deterministic support for oneDNN (#127277),aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/native/mkldnn/xpu/detail/Conv.cpp aten/src/ATen/native/mkldnn/xpu/detail/Deconv.cpp aten/src/ATen/native/mkldnn/xpu/detail/Matmul.cpp benchmarks/dynamo/common.py torch/_C/__init__.pyi.in torch/backends/mkldnn/__init__.py torch/csrc/Module.cpp,https://github.com/pytorch/pytorch/pull/127277,weishi-deng,EikanWang,desertfire,gujinghui,
f73b451e78b,skip,Untopiced,"Revert ""Improved flexattention bwd perf + added configurations for benchmarks (#129013)""",benchmarks/transformer/score_mod.py test/inductor/test_flex_attention.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/select_algorithm.py,,,,,,
3a2fdbb142b,dynamo,not user facing,[dynamo] - Add JK killswitch for dynamo compilation. (#128538),torch/_dynamo/eval_frame.py,https://github.com/pytorch/pytorch/pull/128538,c00w,jansel,,,
5c676bb8b30,onnx,bc breaking,Remove Caffe2 handling from onnx_unpack_quantized_weights (#129021),docs/source/conf.py torch/_C/__init__.pyi.in torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp torch/csrc/jit/passes/onnx/unpack_quantized_weights.h torch/csrc/onnx/init.cpp torch/onnx/symbolic_helper.py torch/onnx/utils.py,https://github.com/pytorch/pytorch/pull/129021,cyyever,albanD,justinchuby,,
c008488b9ce,dynamo,not user facing,[dynamo][guards] Dont run TYPE_MATCH for DICT_LENGTH C++ guard (#129163),torch/_dynamo/guards.py,https://github.com/pytorch/pytorch/pull/129163,anijain2305,jansel,williamwen42,,
2b1b055a967,releng,not user facing,[Split Build] Fix libtorch_python RPATH (#129088),.github/workflows/pull.yml torch/CMakeLists.txt,https://github.com/pytorch/pytorch/pull/129088,PaliC,malfet,,,
62e425ab03b,distributed (tools),not user facing,Memory Tracker for tracking Module wise memory (#124688),test/distributed/_tools/test_mem_tracker.py torch/distributed/_tools/__init__.py torch/distributed/_tools/mem_tracker.py,https://github.com/pytorch/pytorch/pull/124688,sanketpurandare,awgu,,,
632910e2a8e,skip,not user facing,Add test to xfail_list only for abi_compatible (#128506),test/inductor/test_cpu_cpp_wrapper.py,https://github.com/pytorch/pytorch/pull/128506,chunyuan-w,desertfire,jgong5,,
914d3ca2ba0,inductor,not user facing,[inductor][cpp] BF16 AMX micro-gemm support (#127195),aten/src/ATen/cpu/Utils.cpp aten/src/ATen/cpu/Utils.h test/inductor/test_cpu_repro.py test/inductor/test_cpu_select_algorithm.py torch/_C/_cpu.pyi torch/_dynamo/trace_rules.py torch/_inductor/codecache.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/codegen/cpp_prefix.h torch/cpu/__init__.py torch/csrc/cpu/Module.cpp,https://github.com/pytorch/pytorch/pull/127195,jgong5,jansel,,,
6b5fbc544e1,dynamo,not user facing,[dynamo] Use polyfill to trace through the attributes of torch.jit.* and lru_cache_wrapper (#128336),test/dynamo/test_functions.py test/dynamo/test_repros.py test/dynamo_expected_failures/TestJit.test_function_default_values test/dynamo_expected_failures/TestOpCPU.test_cat_cpu_bfloat16 test/dynamo_expected_failures/TestOpCPU.test_cat_cpu_float32 test/dynamo_expected_failures/TestScript.test_script_optional_none test/dynamo_expected_failures/TestScript.test_torchscript_multi_head_attn torch/_dynamo/polyfill.py torch/_dynamo/trace_rules.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/128336,anijain2305,jansel,yanboliang,,
aee512cc9d1,distributed,not user facing,[dtensor][op] Fixed stack op strategy (#129018),torch/distributed/_tensor/ops/tensor_ops.py,https://github.com/pytorch/pytorch/pull/129018,sinhaanshul,XilunWu,,,
123812790b7,dynamo,not user facing,[compiled autograd] update benchmarks to use cli flags for fullgraph/dynamic (#127960),benchmarks/dynamo/common.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/127960,xmfan,jansel,,,
68b33453f44,quantization,Untopiced,[aot autograd] collect static parameter metadata when graphs fallback to inference (#128905),test/inductor/test_cudagraph_trees.py torch/_functorch/aot_autograd.py torch/_inductor/cudagraph_trees.py,https://github.com/pytorch/pytorch/pull/128905,xmfan,mlazos,,,
fafa1867d19,dynamo,not user facing,[compiled autograd] use in_compiled_autograd_region instead of compiled_autograd_enabled_count (#128982),test/dynamo/test_aot_autograd_cache.py torch/_dynamo/compiled_autograd.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py,https://github.com/pytorch/pytorch/pull/128982,xmfan,jansel,,,
8f320fd6c6f,skip,not user facing,[compiled autograd] treat input params as static (#128987),test/inductor/test_cudagraph_trees.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py,https://github.com/pytorch/pytorch/pull/128987,xmfan,BoyuanFeng,eellison,,
d97dfe93133,dynamo,not user facing,[compiled autograd] move inputs to cuda with non_blocking=True (#129181),torch/_dynamo/compiled_autograd.py,https://github.com/pytorch/pytorch/pull/129181,xmfan,eellison,jansel,,
f0443ad1743,dynamo,not user facing,[compiled autograd] flatten runtime inputs with fast path (#129116),torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/129116,xmfan,jansel,,,
217aac96d77,distributed,not user facing,Introduce a prototype for SymmetricMemory (#128582),.lintrunner.toml BUILD.bazel build_variables.bzl c10/cuda/driver_api.h caffe2/CMakeLists.txt test/distributed/test_symmetric_memory.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/CUDASymmetricMemory.cu torch/csrc/distributed/c10d/CUDASymmetricMemory.hpp torch/csrc/distributed/c10d/ProcessGroupCudaP2P.hpp torch/csrc/distributed/c10d/SymmetricMemory.cpp torch/csrc/distributed/c10d/SymmetricMemory.hpp torch/csrc/distributed/c10d/init.cpp torch/csrc/distributed/c10d/intra_node_comm.cpp torch/csrc/distributed/c10d/intra_node_comm.cu torch/csrc/distributed/c10d/intra_node_comm.hpp,https://github.com/pytorch/pytorch/pull/128582,yifuwang,wanchaol,,,
311fadb1fbc,composability,Untopiced,[docs] Redirect custom ops landing page to the correct place (#129177),docs/source/notes/custom_operators.rst,https://github.com/pytorch/pytorch/pull/129177,zou3519,albanD,,,
08b616281f1,dynamo,Untopiced,[custom ops] Switch out references from old landing page to new landing page (#129178),aten/src/ATen/core/MetaFallbackKernel.cpp c10/core/StorageImpl.cpp c10/core/TensorImpl.h test/dynamo/test_misc.py torch/_dynamo/output_graph.py torch/_dynamo/variables/functions.py torch/library.py,https://github.com/pytorch/pytorch/pull/129178,zou3519,albanD,,,
5d8e23b49c5,composability,Untopiced,[custom_op] Support string default values in schema (#129179),test/test_custom_ops.py torch/_library/infer_schema.py,https://github.com/pytorch/pytorch/pull/129179,zou3519,albanD,,,
27ae1f981d8,inductor,not user facing,[inductor] fix linear_add_bias for autocast case (#129138),test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/fx_passes/mkldnn_fusion.py,https://github.com/pytorch/pytorch/pull/129138,zhuhaozhe,jgong5,,,
e1c1052829e,nested tensor_frontend,improvements,Backward support for unbind() with NJT (#128032),test/test_nestedtensor.py tools/autograd/derivatives.yaml torch/csrc/autograd/FunctionsManual.cpp torch/csrc/autograd/FunctionsManual.h torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/128032,jbschlosser,soulitzer,,,
b6978080561,skip,not user facing,[BE][Easy] eliminate relative import in `torchgen` (#128872),torchgen/api/autograd.py torchgen/api/cpp.py torchgen/api/dispatcher.py torchgen/api/lazy.py torchgen/api/meta.py torchgen/api/native.py torchgen/api/python.py torchgen/api/structured.py torchgen/api/translate.py torchgen/api/types/__init__.py torchgen/api/types/signatures.py torchgen/api/types/types.py torchgen/api/types/types_base.py torchgen/api/ufunc.py torchgen/api/unboxing.py torchgen/code_template.py torchgen/context.py torchgen/decompositions/gen_jit_decompositions.py torchgen/dest/__init__.py torchgen/dest/native_functions.py torchgen/dest/ufunc.py torchgen/executorch/api/custom_ops.py torchgen/executorch/api/et_cpp.py torchgen/executorch/api/types/__init__.py torchgen/executorch/api/types/signatures.py torchgen/executorch/api/types/types.py torchgen/executorch/api/unboxing.py torchgen/executorch/model.py torchgen/executorch/parse.py torchgen/fuse/gen_patterns.py torchgen/gen.py torchgen/gen_aoti_c_shim.py torchgen/gen_functionalization_type.py torchgen/gen_lazy_tensor.py torchgen/local.py torchgen/model.py torchgen/native_function_generation.py torchgen/operator_versions/gen_mobile_upgraders.py torchgen/static_runtime/gen_static_runtime_ops.py torchgen/static_runtime/generator.py,https://github.com/pytorch/pytorch/pull/128872,XuehaiPan,zou3519,,,
9795dba1e0a,optim,Untopiced,Optim package docstring fix (#129086),torch/optim/_multi_tensor/__init__.py torch/optim/nadam.py torch/optim/optimizer.py torch/optim/radam.py torch/optim/rmsprop.py torch/optim/rprop.py torch/optim/sgd.py torch/optim/swa_utils.py,https://github.com/pytorch/pytorch/pull/129086,spzala,janeyx99,,,
62e5d045c0a,inductor,not user facing,[AOTI] Auto-tune Triton kernels in a seperate block (#129057),torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cuda.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/codegen/wrapper.py torch/_inductor/compile_fx.py torch/_inductor/config.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/129057,desertfire,eellison,jansel,,
53be7ff0e4d,inductor,Untopiced,Make tl.atomic_add relaxed (#129133),torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/129133,lezcano,peterbell10,,,
2c7c286fa4c,mobile,not user facing,[1/N] Fix clang-tidy warnings in torch/csrc/jit/serialization (#129055),.lintrunner.toml torch/csrc/jit/mobile/compatibility/model_compatibility.h torch/csrc/jit/serialization/callstack_debug_info_serialization.h torch/csrc/jit/serialization/export.cpp torch/csrc/jit/serialization/export.h torch/csrc/jit/serialization/export_bytecode.cpp torch/csrc/jit/serialization/export_bytecode.h torch/csrc/jit/serialization/export_module.cpp torch/csrc/jit/serialization/flatbuffer_serializer.cpp torch/csrc/jit/serialization/flatbuffer_serializer.h torch/csrc/jit/serialization/flatbuffer_serializer_jit.h torch/csrc/jit/serialization/import.cpp torch/csrc/jit/serialization/import.h torch/csrc/jit/serialization/import_export_constants.h torch/csrc/jit/serialization/import_export_functions.h torch/csrc/jit/serialization/import_export_helpers.h torch/csrc/jit/serialization/import_read.cpp torch/csrc/jit/serialization/import_read.h torch/csrc/jit/serialization/import_source.h torch/csrc/jit/serialization/onnx.cpp torch/csrc/jit/serialization/onnx.h torch/csrc/jit/serialization/pickle.h torch/csrc/jit/serialization/pickler.cpp torch/csrc/jit/serialization/pickler.h torch/csrc/jit/serialization/python_print.cpp torch/csrc/jit/serialization/python_print.h torch/csrc/jit/serialization/source_range_serialization.cpp torch/csrc/jit/serialization/source_range_serialization.h torch/csrc/jit/serialization/source_range_serialization_impl.h torch/csrc/jit/serialization/storage_context.h torch/csrc/jit/serialization/type_name_uniquer.h torch/csrc/jit/serialization/unpickler.h,https://github.com/pytorch/pytorch/pull/129055,cyyever,r-barnes,,,
479ce5e2f4f,skip,not user facing,Remove outdated CUDA code from CMake (#128801),caffe2/CMakeLists.txt cmake/Dependencies.cmake cmake/TorchConfig.cmake.in cmake/public/cuda.cmake setup.py,https://github.com/pytorch/pytorch/pull/128801,cyyever,malfet,r-barnes,,
715b09ae2d7,skip,Untopiced,"Revert ""Fix DEBUG=1 asserts with NJT ops (#129014)""",torch/csrc/autograd/autograd_not_implemented_fallback.cpp,,,,,,
fb0c51b61c1,skip,not user facing,[Inductor UT] Fix UT failure 'test_polar_dynamic_shapes_xpu' introduced by #128722 (#129124),test/inductor/test_torchinductor_codegen_dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/129124,etaf,EikanWang,desertfire,,
b0ae0db8156,inductor,not user facing,[Inductor][Intel GPU] Support reduction split. (#129120),test/inductor/test_torchinductor.py test/inductor/test_torchinductor_strided_blocks.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/129120,etaf,EikanWang,desertfire,jansel,
bdd11483eab,dynamo,not user facing,[3.13] get C dynamo to compile with python callback and custom frame eval (#129171),torch/_dynamo/convert_frame.py torch/csrc/dynamo/cpython_defs.c torch/csrc/dynamo/cpython_defs.h torch/csrc/dynamo/eval_frame.c torch/csrc/utils/python_compat.h,https://github.com/pytorch/pytorch/pull/129171,williamwen42,albanD,jansel,,
237c4e6163d,skip,not user facing,Improved flexattention bwd perf + added configurations for benchmarks (#129013),benchmarks/transformer/score_mod.py test/inductor/test_flex_attention.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/129013,Chillee,drisspg,yanboliang,,
feb3f3ad772,inductor,not user facing,[inductor] Refactors for Halide backend (#129024),torch/_inductor/codegen/common.py torch/_inductor/codegen/simd.py torch/_inductor/ir.py torch/_inductor/sizevars.py,https://github.com/pytorch/pytorch/pull/129024,jansel,eellison,shunting314,,
bb4ab59651e,inductor,not user facing,[inductor] Run more test on correct device (#129033),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/129033,jansel,eellison,shunting314,,
bdc39eef3b0,inductor,not user facing,[inductor] Add --inductor-config benchmark flag (#129034),benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/129034,jansel,eellison,shunting314,,
8edd4c71c69,inductor,Untopiced,[AOTI][refactor] Remove GridExprCppPrinter (#129142),torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cuda.py,https://github.com/pytorch/pytorch/pull/129142,desertfire,chenyang78,,,
2db33054b30,nn_frontend,not user facing,Disable fast path in `TransformerEncoderLayer` when there are forward (pre-)hooks attached to modules (#128415),test/test_transformers.py torch/nn/modules/transformer.py,https://github.com/pytorch/pytorch/pull/128415,iibrahimli,mikaylagawarecki,,,
58cefaf53b1,inductor,Untopiced,Fix hipify regular expression for AOTI wrapper (#128912),test/inductor/test_cpp_wrapper_hipify.py torch/_inductor/codegen/aoti_hipify_utils.py,https://github.com/pytorch/pytorch/pull/128912,zoranzhao,desertfire,,,
ebf25e128cb,autograd_frontend,bug fixes,[autograd] Do not stash version counter for saved tensor (#128545),test/inductor/test_compiled_autograd.py test/test_autograd.py torch/csrc/autograd/saved_variable.cpp torch/csrc/autograd/saved_variable.h,https://github.com/pytorch/pytorch/pull/128545,soulitzer,albanD,,,
d52684e9a8b,skip,not user facing,[BE]: Update CUDNN_frontend submodule to v1.5.1 (#128612),third_party/cudnn_frontend,https://github.com/pytorch/pytorch/pull/128612,Skylion007,eqy,ezyang,,
92ca17d85de,releng,not user facing,Update triton pin (#126098),.ci/docker/ci_commit_pins/triton.txt torch/_inductor/kernel/conv.py,https://github.com/pytorch/pytorch/pull/126098,amjames,bertmaher,,,
9103b40a472,nn_frontend,docs,Fix small typo in docstring in ParameterList (#129193),torch/nn/modules/container.py,https://github.com/pytorch/pytorch/pull/129193,mashrurmorshed,mikaylagawarecki,,,
40e8675fcbb,skip,Untopiced,[cuDNN] Graph-capturable cuDNN CTCLoss (#128271),aten/src/ATen/cudnn/Descriptors.h aten/src/ATen/native/LossCTC.cpp aten/src/ATen/native/cudnn/LossCTC.cpp test/test_nn.py,https://github.com/pytorch/pytorch/pull/128271,eqy,ezyang,malfet,,
9d1b65b569a,inductor,Untopiced,[PT2][Observability] Change the log logic (#129201),torch/_inductor/fx_passes/pad_mm.py,https://github.com/pytorch/pytorch/pull/129201,mengluy0125,Skylion007,dshi7,,
ef554465383,distributed,Untopiced,[FSDP2] Add 'TORCH_LOGS=+fsdp' to log hooks(pre/post forward/backward) and FQN (_init_fqns) (#128663),test/distributed/_composable/fsdp/test_fully_shard_logging.py torch/_logging/_internal.py torch/_logging/_registrations.py torch/distributed/_composable/fsdp/_fsdp_param_group.py torch/distributed/_composable/fsdp/_fsdp_state.py,https://github.com/pytorch/pytorch/pull/128663,mori360,awgu,weifengpy,,
1c75ddff357,skip,Untopiced,"Revert ""[cuDNN] Graph-capturable cuDNN CTCLoss (#128271)""",aten/src/ATen/cudnn/Descriptors.h aten/src/ATen/native/LossCTC.cpp aten/src/ATen/native/cudnn/LossCTC.cpp test/test_nn.py,,,,,,
c5b9ee7408e,dynamo,not user facing,[easy][dynamo] Remove try except from call_getattr (#129217),torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/129217,anijain2305,lezcano,,,
5b149432134,skip,not user facing,Run TestAOTAutograd test suite with cache (#128222),test/functorch/test_aotdispatch.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_functorch/config.py,https://github.com/pytorch/pytorch/pull/128222,jamesjwu,bdhirsh,,,
2f8b301c326,distributed,Untopiced,Clean up distributed/CONTRIBUTING.md (#128450),docs/source/_static/img/pt_distributed_arch.png torch/distributed/CONTRIBUTING.md,https://github.com/pytorch/pytorch/pull/128450,wconstab,wanchaol,,,
858fb05dac4,distributed,Untopiced,Modify ExternKernelAlloc with NoneLayout to not assign its result to anything (#129188),test/distributed/_tensor/test_dtensor_compile.py test/distributed/test_c10d_functional_native.py test/distributed/test_inductor_collectives.py torch/_inductor/codegen/wrapper.py,https://github.com/pytorch/pytorch/pull/129188,Chillee,yifuwang,,,
f42d5b6dca7,profiler,bug fixes,[Memory Snapshot] Make recordAnnotations callback initialize lazily (#129242),c10/core/Allocator.h torch/csrc/cuda/Module.cpp torch/csrc/cuda/memory_snapshot.cpp,https://github.com/pytorch/pytorch/pull/129242,aaronenyeshi,zdevito,,,
b72ef9df0d0,Uncategorized,Untopiced,Update torchbench model expected accuracy values after pinning numpy (#129213),benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/aot_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_training.csv,https://github.com/pytorch/pytorch/pull/129213,huydhn,desertfire,malfet,xuzhao9,
b0044e2e185,releng,not user facing,[Split Build] Support nightly release (#129011),.circleci/scripts/binary_linux_test.sh .circleci/scripts/binary_populate_env.sh .circleci/scripts/binary_upload.sh .github/actions/test-pytorch-binary/action.yml .github/scripts/generate_binary_build_matrix.py .github/templates/upload.yml.j2 .github/workflows/_binary-build-linux.yml .github/workflows/_binary-test-linux.yml .github/workflows/_binary-upload.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml setup.py,https://github.com/pytorch/pytorch/pull/129011,PaliC,atalman,,,
7661d1220a7,releng,not user facing,[Split Build] Fix typo in pull ci (#129270),.github/workflows/pull.yml,https://github.com/pytorch/pytorch/pull/129270,PaliC,atalman,,,
64743de6d85,releng,not user facing,[Split Build][BE] consolidate pip install commands (#129253),.circleci/scripts/binary_linux_test.sh,https://github.com/pytorch/pytorch/pull/129253,PaliC,atalman,,,
9ffdbb5d128,skip,not user facing,Forward Fix PR for #128683 (#129037),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/129037,drisspg,vkuzo,,,
b57fa8d9c07,releng,not user facing,[BE] Remove JNI from libtorch builds (#124995),.circleci/scripts/binary_populate_env.sh,https://github.com/pytorch/pytorch/pull/124995,PaliC,malfet,,,
cb126711cdf,releng,not user facing,[merge_rule] add more cpp inductor files (#129192),.github/merge_rules.yaml,https://github.com/pytorch/pytorch/pull/129192,jgong5,atalman,leslie-fang-intel,,
73ba226d986,inductor,Untopiced,[inductor] Linear time dead node elimination (#129082),torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/129082,peterbell10,lezcano,,,
88a35b5b641,inductor,Untopiced,BE: User future annotations in _inductor/comms.py (#129083),torch/_inductor/comms.py,https://github.com/pytorch/pytorch/pull/129083,peterbell10,lezcano,,,
72e3aca227a,inductor,not user facing,[inductor] Refactor fusion of inplace operations (#128979),test/inductor/test_compiled_optimizers.py torch/_inductor/comms.py torch/_inductor/dependencies.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/128979,peterbell10,lezcano,,,
920ebccca26,inductor,not user facing,[inductor][cpp] refactor CppTemplateKernel to inherit CppKernel (#129101),torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_template.py torch/_inductor/codegen/cpp_template_kernel.py,https://github.com/pytorch/pytorch/pull/129101,jgong5,leslie-fang-intel,,,
905dfa186c4,fx,Untopiced,Fix ConstraintViolationError exception string when exprs are int (#129271),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/129271,larryliu0820,lezcano,,,
79aabaf626c,dynamo,not user facing,"[3.13, dynamo] codegen PUSH_NULL when callable is codegen'd (#129172)",test/dynamo/test_bytecode_utils.py test/dynamo/test_model_output.py torch/_dynamo/bytecode_transformation.py torch/_dynamo/codegen.py torch/_dynamo/output_graph.py torch/_dynamo/resume_execution.py torch/_dynamo/side_effects.py torch/_dynamo/source.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/sdpa.py torch/_dynamo/variables/tensor.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/129172,williamwen42,jansel,,,
4f9399bd0d2,skip,Untopiced,[halide-backend] Initial implementation of HalideKernel and HalideScheduling (#126417),test/inductor/test_halide.py test/inductor/test_torchinductor.py torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/halide.py torch/_inductor/config.py torch/_inductor/runtime/hints.py,https://github.com/pytorch/pytorch/pull/126417,jansel,eellison,shunting314,,
10c64c3b49e,skip,Untopiced,[halide-backend] Generate standalone runtime (#129025),test/inductor/test_halide.py test/inductor/test_torchinductor.py torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/halide.py torch/_inductor/config.py torch/_inductor/runtime/hints.py,https://github.com/pytorch/pytorch/pull/129025,jansel,eellison,shunting314,,
1afd492d886,distributed,not user facing,[dtensor][example] add functionality allowing users to choose which example they'd to run (#128720),torch/distributed/_tensor/examples/comm_mode_features_example.py torch/distributed/_tensor/examples/comm_mode_features_example_argparser.py,https://github.com/pytorch/pytorch/pull/128720,sinhaanshul,XilunWu,,,
0e6118a68e6,distributed,not user facing,[dtensor][debug] added logging module tracing table to file feature (#128721),torch/distributed/_tensor/debug/comm_mode.py torch/distributed/_tensor/examples/comm_mode_features_example.py,https://github.com/pytorch/pytorch/pull/128721,sinhaanshul,XilunWu,tianyu-l,,
e165a5971fd,distributed,not user facing,[Traceable FSDP2] Fix support for CUDA resize_storage_bytes_ (#129215),build_variables.bzl test/inductor/test_distributed_patterns.py torch/csrc/inductor/inductor_ops.cpp torch/csrc/inductor/resize_storage_bytes.cpp torch/fx/node.py,https://github.com/pytorch/pytorch/pull/129215,yf225,jansel,,,
94dc3253a0f,skip,not user facing,[BE][Easy] enable UFMT for `torch/distributed/` (#128870),.lintrunner.toml torch/distributed/__init__.py torch/distributed/_composable/fsdp/_fsdp_collectives.py torch/distributed/_composable/fsdp/_fsdp_common.py torch/distributed/_composable/fsdp/_fsdp_init.py torch/distributed/_composable/fsdp/_fsdp_param.py torch/distributed/_composable/fsdp/_fsdp_param_group.py torch/distributed/_composable/fsdp/_fsdp_state.py torch/distributed/_composable/fsdp/fully_shard.py torch/distributed/_composable/fully_shard.py torch/distributed/_composable/replicate.py torch/distributed/_cuda_p2p/__init__.py torch/distributed/_functional_collectives.py torch/distributed/_functional_collectives_impl.py torch/distributed/_sharded_tensor/__init__.py torch/distributed/_sharding_spec/__init__.py torch/distributed/_state_dict_utils.py torch/distributed/_tools/__init__.py torch/distributed/_tools/memory_tracker.py torch/distributed/c10d_logger.py torch/distributed/collective_utils.py torch/distributed/constants.py torch/distributed/device_mesh.py torch/distributed/distributed_c10d.py torch/distributed/examples/memory_tracker_example.py torch/distributed/launcher/__init__.py torch/distributed/launcher/api.py torch/distributed/logging_handlers.py torch/distributed/nn/__init__.py torch/distributed/nn/api/remote_module.py torch/distributed/nn/functional.py torch/distributed/pipelining/_IR.py torch/distributed/pipelining/__init__.py torch/distributed/remote_device.py torch/distributed/rendezvous.py torch/distributed/run.py torch/distributed/utils.py,https://github.com/pytorch/pytorch/pull/128870,XuehaiPan,fegin,wconstab,,
3e02ecd7408,skip,not user facing,Test only one sample with huber_loss (#129245),test/inductor/test_torchinductor_opinfo.py,https://github.com/pytorch/pytorch/pull/129245,isuruf,huydhn,,,
856541c701f,jit,Untopiced,[custom_op] support default dtype values (#129189),c10/core/ScalarType.cpp c10/core/ScalarType.h test/test_custom_ops.py torch/_library/infer_schema.py torch/csrc/TypeInfo.cpp torch/csrc/jit/frontend/function_schema_parser.cpp torch/csrc/jit/frontend/schema_type_parser.cpp torch/csrc/utils/tensor_dtypes.cpp,https://github.com/pytorch/pytorch/pull/129189,zou3519,albanD,,,
749c03406cf,mps,Untopiced,"[metal] Add int4mm weight packing mps kernel, and improved int4mm shader (#128965)",aten/src/ATen/native/mps/operations/Quantized.mm aten/src/ATen/native/native_functions.yaml test/test_mps.py,https://github.com/pytorch/pytorch/pull/128965,manuelcandales,malfet,,,
cac6f99d41b,skip,not user facing,Fix Windows CUDA periodic inductor/test_pattern_matcher test (#129198),test/inductor/test_pattern_matcher.py,https://github.com/pytorch/pytorch/pull/129198,huydhn,ZainRizvi,kit1980,malfet,
17d1723aeea,dynamo,not user facing,[dynamo][unspecialized-nn-modules] Remove dead (also incorrect) code (#129316),torch/_dynamo/guards.py torch/_dynamo/variables/nn_module.py,https://github.com/pytorch/pytorch/pull/129316,anijain2305,jansel,,,
440d8fbd4a6,distributed (tools),not user facing,FSDP2 Memory Tracker (#125323),test/distributed/_tools/test_fsdp2_mem_tracker.py torch/distributed/_tools/__init__.py torch/distributed/_tools/fsdp2_mem_tracker.py,https://github.com/pytorch/pytorch/pull/125323,sanketpurandare,awgu,,,
62ccf6d7cde,dataloader_frontend,not user facing,[BE] enable UFMT for `torch/nn/modules` (#128594),.lintrunner.toml torch/nn/modules/__init__.py torch/nn/modules/_functions.py torch/nn/modules/activation.py torch/nn/modules/adaptive.py torch/nn/modules/batchnorm.py torch/nn/modules/channelshuffle.py torch/nn/modules/container.py torch/nn/modules/conv.py torch/nn/modules/distance.py torch/nn/modules/dropout.py torch/nn/modules/flatten.py torch/nn/modules/fold.py torch/nn/modules/instancenorm.py torch/nn/modules/lazy.py torch/nn/modules/linear.py torch/nn/modules/loss.py torch/nn/modules/module.py torch/nn/modules/normalization.py torch/nn/modules/padding.py torch/nn/modules/pixelshuffle.py torch/nn/modules/pooling.py torch/nn/modules/rnn.py torch/nn/modules/sparse.py torch/nn/modules/transformer.py torch/nn/modules/upsampling.py torch/nn/modules/utils.py,https://github.com/pytorch/pytorch/pull/128594,XuehaiPan,mikaylagawarecki,,,
b91a9dc328d,distributed,not user facing,"[Brian's PR #128754] Use torch.ops.fsdp.set_ for FSDP2 storage resize; dont functionalize resize_, set_, split_with_sizes_copy.out (#129203)",test/dynamo/test_repros.py test/functorch/test_aotdispatch.py test/inductor/test_distributed_patterns.py torch/_dynamo/repro/after_aot.py torch/_functorch/_aot_autograd/functional_utils.py torch/_higher_order_ops/auto_functionalize.py torch/csrc/inductor/resize_storage_bytes.cpp torch/distributed/_composable/fsdp/_fsdp_collectives.py torch/distributed/_composable/fsdp/_fsdp_param.py torch/fx/node.py,https://github.com/pytorch/pytorch/pull/129203,yf225,bdhirsh,,,
dadc0ed4c8e,distributed,not user facing,[Traceable FSDP2] Add `aot_eager` backend E2E tests for transformer model (#129157),test/distributed/_composable/fsdp/test_fully_shard_compile.py test/dynamo/test_trace_rules.py torch/_dynamo/config.py torch/distributed/_composable/fsdp/_fsdp_param_group.py torch/distributed/_composable/fsdp/_fsdp_state.py,https://github.com/pytorch/pytorch/pull/129157,yf225,awgu,,,
f85d1e845a5,skip,not user facing,[BE] enable UFMT for `torch/nn/*.py` (#128593),.lintrunner.toml torch/__init__.py torch/_jit_internal.py torch/_library/utils.py torch/_linalg_utils.py torch/_lowrank.py torch/_meta_registrations.py torch/_utils.py torch/_utils_internal.py torch/_vmap_internals.py torch/functional.py torch/hub.py torch/library.py torch/nn/__init__.py torch/nn/_reduction.py torch/nn/common_types.py torch/nn/grad.py torch/nn/init.py torch/nn/parameter.py torch/overrides.py torch/serialization.py torch/utils/data/dataloader.py torch/utils/data/datapipes/_hook_iterator.py,https://github.com/pytorch/pytorch/pull/128593,XuehaiPan,mikaylagawarecki,,,
7b9e6430eda,releng,not user facing,[Split Build] Add periodic and trunk CI for cuda builds (#129269),.github/workflows/_linux-build.yml .github/workflows/periodic.yml .github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/129269,PaliC,atalman,,,
287c68c5eca,inductor,not user facing,[Inductor][Quant] Use output dtype torch.uint8 explicitly (#128804),torch/_inductor/fx_passes/quantization.py torch/_inductor/mkldnn_ir.py,https://github.com/pytorch/pytorch/pull/128804,leslie-fang-intel,Xia-Weiwen,jgong5,,
8a2fed7e6ab,inductor,not user facing,[Inductor][CPP] Fallback QLinear Binaryfusion from postop sum to binary add when others is view (#128808),torch/_inductor/fx_passes/quantization.py,https://github.com/pytorch/pytorch/pull/128808,leslie-fang-intel,jgong5,,,
662e9e10766,skip,not user facing,[BE] enable UFMT for `torch/nn/functional.py` (#128592),.lintrunner.toml tools/pyi/gen_pyi.py torch/nn/functional.py,https://github.com/pytorch/pytorch/pull/128592,XuehaiPan,mikaylagawarecki,,,
d21f311af88,distributed,not user facing,[Easy][Traceable FSDP2] Skip rocm for the E2E tests (#129339),test/distributed/_composable/fsdp/test_fully_shard_compile.py,https://github.com/pytorch/pytorch/pull/129339,yf225,msaroufim,,,
90942480907,distributed,Untopiced,[FSDP2] Fixed `unshard` without lazy init (#129241),test/distributed/_composable/fsdp/test_fully_shard_comm.py torch/distributed/_composable/fsdp/_fsdp_param_group.py torch/distributed/_composable/fsdp/_fsdp_state.py,https://github.com/pytorch/pytorch/pull/129241,awgu,Skylion007,weifengpy,,
c89a9f5d178,autograd_frontend,Untopiced,Allow SAC policy_fn to return bool for backward compatibility (#129262),test/test_autograd.py torch/utils/checkpoint.py,https://github.com/pytorch/pytorch/pull/129262,soulitzer,Chillee,fmassa,,
e53d9590287,skip,not user facing,[BE] update type annotations for basic utilities in `torch/__init__.py` (#129001),.lintrunner.toml test/test_fake_tensor.py test/test_public_bindings.py tools/pyi/gen_pyi.py torch/_C/__init__.pyi.in torch/__init__.py torch/_higher_order_ops/__init__.py torch/_higher_order_ops/associative_scan.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/effects.py torch/_higher_order_ops/flex_attention.py torch/_higher_order_ops/map.py torch/_higher_order_ops/out_dtype.py torch/_higher_order_ops/strict_mode.py torch/_higher_order_ops/torchbind.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_higher_order_ops/while_loop.py torch/_higher_order_ops/wrap.py torch/masked/maskedtensor/core.py torch/nn/parallel/distributed.py torch/types.py torchgen/fuse/gen_patterns.py,https://github.com/pytorch/pytorch/pull/129001,XuehaiPan,malfet,,,
df51d0b623d,inductor,Untopiced,[aotinductor][UserDefinedTritonKernel] use appropriate expr printer when printing args (#129301),test/inductor/test_aot_inductor.py torch/_inductor/codegen/wrapper.py,https://github.com/pytorch/pytorch/pull/129301,ColinPeppler,chenyang78,desertfire,,
7b910285db8,skip,Untopiced,"Revert ""[inductor] Refactor fusion of inplace operations (#128979)""",test/inductor/test_compiled_optimizers.py torch/_inductor/comms.py torch/_inductor/dependencies.py torch/_inductor/scheduler.py,,,,,,
cb4919344a6,skip,Untopiced,"Revert ""[BE] update type annotations for basic utilities in `torch/__init__.py` (#129001)""",.lintrunner.toml test/test_fake_tensor.py test/test_public_bindings.py tools/pyi/gen_pyi.py torch/_C/__init__.pyi.in torch/__init__.py torch/_higher_order_ops/__init__.py torch/_higher_order_ops/associative_scan.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/effects.py torch/_higher_order_ops/flex_attention.py torch/_higher_order_ops/map.py torch/_higher_order_ops/out_dtype.py torch/_higher_order_ops/strict_mode.py torch/_higher_order_ops/torchbind.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_higher_order_ops/while_loop.py torch/_higher_order_ops/wrap.py torch/masked/maskedtensor/core.py torch/nn/parallel/distributed.py torch/types.py torchgen/fuse/gen_patterns.py,,,,,,
c888ee36325,skip,not user facing,Remove test_mps_allocator_module XFAIL (#129340),test/test_mps.py,https://github.com/pytorch/pytorch/pull/129340,huydhn,kit1980,malfet,,
063facf352a,skip,Untopiced,"Revert ""[halide-backend] Generate standalone runtime (#129025)""",test/inductor/test_halide.py test/inductor/test_torchinductor.py torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/halide.py torch/_inductor/config.py torch/_inductor/runtime/hints.py,,,,,,
1a54bb0f968,skip,Untopiced,"Revert ""[halide-backend] Initial implementation of HalideKernel and HalideScheduling (#126417)""",test/inductor/test_halide.py test/inductor/test_torchinductor.py torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/halide.py torch/_inductor/config.py torch/_inductor/runtime/hints.py,,,,,,
93a33bf3ac0,skip,not user facing,[BE] update type annotations for basic utilities in `torch/__init__.py` (#129001),.lintrunner.toml test/test_fake_tensor.py test/test_public_bindings.py tools/pyi/gen_pyi.py torch/_C/__init__.pyi.in torch/__init__.py torch/_higher_order_ops/__init__.py torch/_higher_order_ops/associative_scan.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/effects.py torch/_higher_order_ops/flex_attention.py torch/_higher_order_ops/map.py torch/_higher_order_ops/out_dtype.py torch/_higher_order_ops/strict_mode.py torch/_higher_order_ops/torchbind.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_higher_order_ops/while_loop.py torch/_higher_order_ops/wrap.py torch/masked/maskedtensor/core.py torch/nn/parallel/distributed.py torch/types.py torchgen/fuse/gen_patterns.py,https://github.com/pytorch/pytorch/pull/129001,XuehaiPan,malfet,,,
18fdc0ae5b9,releng,not user facing,[executorch hash update] update the pinned executorch hash (#129099),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/129099,pytorchupdatebot,pytorchbot,,,
30bfdf1afcb,python_frontend,Untopiced,Errors when 0-dim tensor of complex or bool type passed to aminmax. (#128404),aten/src/ATen/native/cpu/TensorCompareKernel.cpp test/test_reductions.py,https://github.com/pytorch/pytorch/pull/128404,ajbrent,janeyx99,,,
8edb7b96b1a,releng,not user facing,Enable dynamic rollout for pull workflow (#129243),.github/workflows/_bazel-build-test.yml .github/workflows/pull.yml .github/workflows/slow.yml .github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/129243,ZainRizvi,huydhn,malfet,zxiiro,
fbca70718f6,inductor,Untopiced,Fix scatter lowering when src is a Number (#129096),test/inductor/test_torchinductor_opinfo.py torch/_inductor/lowering.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/129096,isuruf,peterbell10,,,
e096faaf30c,composability,Untopiced,Fix rot90 decomposition for no rotation (#129097),test/inductor/test_torchinductor_opinfo.py torch/_refs/__init__.py,https://github.com/pytorch/pytorch/pull/129097,isuruf,peterbell10,,,
5f912f480c0,inductor,Untopiced,Fix max_pool2d decomposition for empty list and integer limits (#129106),aten/src/ATen/native/cpu/MaxPoolKernel.cpp test/inductor/test_torchinductor_opinfo.py test/nn/test_pooling.py test/test_mps.py torch/_inductor/decomposition.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/129106,isuruf,lezcano,malfet,peterbell10,
7b7f3570427,nested tensor_frontend,not user facing,Fix DEBUG=1 asserts with NJT ops (#129014),torch/csrc/autograd/autograd_not_implemented_fallback.cpp,https://github.com/pytorch/pytorch/pull/129014,jbschlosser,YuqingJ,soulitzer,,
00f675bb4c2,nested tensor_frontend,Untopiced,[Nested Tensor]fix sdpa backward for the special case with ragged second batch dim and constant length (#128349),test/test_nestedtensor.py torch/nested/_internal/sdpa.py,https://github.com/pytorch/pytorch/pull/128349,YuqingJ,jbschlosser,,,
41bb81b5827,inductor,not user facing,Add Strided Input test for flex attention (#128915),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/128915,joydddd,Chillee,drisspg,,
b22f0f5f51d,quantization,Untopiced,[torchbind] fix bug of mutating FakeScriptObjects twice in aot_export (#128844),test/cpp/jit/test_custom_class_registrations.cpp test/export/test_torchbind.py torch/_functorch/aot_autograd.py torch/_higher_order_ops/torchbind.py torch/_library/fake_class_registry.py torch/export/graph_signature.py,https://github.com/pytorch/pytorch/pull/128844,ydwu4,angelayi,,,
7b57ddd38c6,skip,not user facing,[inductor] Fix TORCHINDUCTOR_FORCE_DISABLE_CACHES (#129257),torch/_inductor/async_compile.py torch/_inductor/compile_fx.py torch/_inductor/runtime/compile_tasks.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/129257,masnesral,oulgen,,,
4c1e4c5f307,skip,not user facing,[inductor] Enable FX graph caching in OSS by default (#125863),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/125863,masnesral,eellison,oulgen,,
14dc08ddc7d,inductor,Untopiced,Inductor to fail gracefully on Voltas for bf16 tensors (#129288),torch/_inductor/compile_fx.py torch/cuda/__init__.py,https://github.com/pytorch/pytorch/pull/129288,malfet,eqy,jansel,,
81de71fdc5b,inductor,not user facing,[inductor] fix a double clone in coordesc tuning (#129399),torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/129399,shunting314,Chillee,jansel,,
8b6391ee591,distributed,not user facing,[Test][DTensor] Temporarily skip gloo test for test_depthwise_convolution (#129391),test/distributed/_tensor/test_convolution_ops.py,https://github.com/pytorch/pytorch/pull/129391,wz337,awgu,,,
78e40b271b3,cuda,Untopiced,Change index_put on GPU to accept FP8 inputs (#128758),aten/src/ATen/native/cpu/IndexKernel.cpp aten/src/ATen/native/cuda/IndexKernel.cu aten/src/ATen/native/cuda/Indexing.cu test/test_indexing.py,https://github.com/pytorch/pytorch/pull/128758,ani300,drisspg,eqy,,
1315be4893d,inductor,Untopiced,[aotinductor] only autotune at compile time when enabled via config (#129413),torch/_inductor/compile_fx.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/129413,ColinPeppler,desertfire,jingsh,,
c012013aa6c,skip,Untopiced,"Revert ""Add Strided Input test for flex attention (#128915)""",test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py,,,,,,
514f9279f82,dynamo,Untopiced,[dynamo][compile-time] Manually implement nn.Module.__getattr__ to reduce compile time (#129315),torch/_dynamo/utils.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/129315,anijain2305,jansel,,,
c4dd752d976,dynamo,not user facing,[dynamo][compile-time][inlining-inbuilt-nn-modules] Manually implement nn.Module._call_impl (#129285),torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/nn_module.py,https://github.com/pytorch/pytorch/pull/129285,anijain2305,jansel,,,
533395e2043,skip,not user facing,Fix build error on s390x (#129326),aten/src/ATen/cpu/Utils.cpp,https://github.com/pytorch/pytorch/pull/129326,kiszk,Skylion007,eqy,,
4d04203852b,releng,not user facing,[BE] Runner determinator: Expect usernames to be prefixed with '@' (#129246),.github/scripts/get_workflow_type.py .github/workflows/_runner-determinator.yml,https://github.com/pytorch/pytorch/pull/129246,ZainRizvi,huydhn,zxiiro,,
e6bfa2958bd,skip,Untopiced,Add aten._unsafe_masked_index (#116491),aten/src/ATen/functorch/BatchRulesIndexing.cpp aten/src/ATen/native/TensorAdvancedIndexing.cpp aten/src/ATen/native/native_functions.yaml test/distributed/_tensor/test_dtensor_ops.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_opinfo.py test/onnx/test_fx_op_consistency.py test/test_fx_experimental.py test/test_mps.py tools/autograd/derivatives.yaml tools/autograd/gen_variable_type.py torch/_decomp/__init__.py torch/_decomp/decompositions.py torch/_dynamo/trace_rules.py torch/_inductor/codegen/triton.py torch/_inductor/decomposition.py torch/_inductor/lowering.py torch/_inductor/utils.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/116491,isuruf,lezcano,peterbell10,,
b24787b7576,distributed,not user facing,[Traceable FSDP2] Don't decompose fsdp.split_with_sizes_copy (#129414),test/distributed/_composable/fsdp/test_fully_shard_compile.py torch/_functorch/_aot_autograd/functional_utils.py torch/distributed/_composable/fsdp/_fsdp_collectives.py torch/fx/node.py,https://github.com/pytorch/pytorch/pull/129414,yf225,bdhirsh,,,
aa4ee2cb9e1,dynamo,not user facing,[Traceable FSDP2] Add Dynamo support for run_with_rng_state HOP (#127247),test/inductor/test_compiled_autograd.py torch/_dynamo/variables/higher_order_ops.py torch/_prims/rng_prims.py,https://github.com/pytorch/pytorch/pull/127247,yf225,bdhirsh,,,
1bb1e3463c9,python_frontend,bug fixes,Fix allowlisting of builtins for weights_only unpickler (#129244),test/test_serialization.py torch/_weights_only_unpickler.py,https://github.com/pytorch/pytorch/pull/129244,mikaylagawarecki,albanD,,,
c5f7755e867,distributed,improvements,Allow BUILD/NEWOBJ instruction for items added via torch.serialization.add_safe_globals (#129251),test/distributed/_tensor/test_dtensor.py test/test_serialization.py torch/_weights_only_unpickler.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/129251,mikaylagawarecki,albanD,,,
381ce0821c3,python_frontend,improvements,Add warning for weights_only (#129239),test/test_nn.py test/test_serialization.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/129239,mikaylagawarecki,albanD,malfet,,
f18becaaf1c,python_frontend,docs,Add example for torch.serialization.add_safe_globals (#129396),torch/serialization.py,https://github.com/pytorch/pytorch/pull/129396,mikaylagawarecki,albanD,malfet,,
0e1e2890335,skip,not user facing,[ONNX] Benchmark refactored ONNX export (#129427),benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/129427,titaiwangms,justinchuby,,,
665dbc2f52f,distributed,not user facing,[easy][DCP] Fix test_fine_tuning.py for get/set_state_dict API changes (#129365),test/distributed/checkpoint/e2e/test_fine_tuning.py,https://github.com/pytorch/pytorch/pull/129365,cdzhan,fegin,,,
533c4190f96,inductor,not user facing,[inductor][cpp] support nested kernel with indirect indexing (#129223),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/common.py,https://github.com/pytorch/pytorch/pull/129223,jgong5,jansel,leslie-fang-intel,,
8bfd9e98154,skip,Untopiced,[cuDNN] Graph-capturable cuDNN CTCLoss (#128271),aten/src/ATen/cudnn/Descriptors.h aten/src/ATen/native/LossCTC.cpp aten/src/ATen/native/cudnn/LossCTC.cpp test/test_nn.py,https://github.com/pytorch/pytorch/pull/128271,eqy,ezyang,malfet,,
4ca8eecca4c,skip,not user facing,skip test_graph_capture_oom for jetson (#128661),test/test_cuda.py,https://github.com/pytorch/pytorch/pull/128661,Fuzzkatt,atalman,eqy,,
0314c4c101c,releng,not user facing,"[BE][Easy] use `pathlib.Path` instead of `dirname` / `""..""` / `pardir` (#129374)",.circleci/codegen_validation/normalize_yaml_fragment.py .github/scripts/delete_old_branches.py .github/scripts/ensure_actions_will_cancel.py .github/scripts/generate_binary_build_matrix.py .github/scripts/gitutils.py .github/scripts/lint_native_functions.py .github/scripts/test_gitutils.py benchmarks/dynamo/ci_expected_accuracy/cu124/update_expected.py benchmarks/dynamo/ci_expected_accuracy/update_expected.py docs/source/scripts/build_opsets.py docs/source/scripts/build_quantization_configs.py docs/source/scripts/exportdb/generate_example_rst.py scripts/compile_tests/update_failures.py test/jit/fixtures_srcs/generate_models.py test/jit/test_backend_nnapi.py test/mobile/test_bytecode.py test/mobile/test_upgrader_codegen.py test/mobile/test_upgraders.py test/onnx/onnx_test_common.py test/quantization/core/test_docs.py test/test_typing.py tools/amd_build/build_amd.py tools/build_libtorch.py tools/code_coverage/package/oss/utils.py tools/code_coverage/package/util/setting.py tools/gen_vulkan_spv.py tools/linter/adapters/s3_init.py tools/onnx/update_default_opset_version.py tools/setup_helpers/cmake.py tools/setup_helpers/gen.py tools/setup_helpers/gen_unboxing.py tools/setup_helpers/generate_code.py tools/stats/export_test_times.py tools/stats/import_test_stats.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_gen_backend_stubs.py tools/test/test_test_run.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/testing/discover_tests.py tools/testing/do_target_determination_for_s3.py tools/testing/explicit_ci_jobs.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/gen_artifact.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/llm.py tools/testing/target_determination/heuristics/previously_failed_in_pr.py tools/testing/target_determination/heuristics/utils.py tools/testing/test_selections.py tools/vscode_settings.py torch/_inductor/runtime/compile_tasks.py torch/testing/_internal/common_utils.py torchgen/gen_backend_stubs.py torchgen/gen_lazy_tensor.py,https://github.com/pytorch/pytorch/pull/129374,XuehaiPan,justinchuby,malfet,,
63474620ab7,skip,not user facing,test_jit: Replace plain assert by test assert (#128950),test/test_jit.py,https://github.com/pytorch/pytorch/pull/128950,Flamefire,zou3519,,,
6508f0f5d48,distributed (tools),not user facing,"Improved backward tracking and attribution, fixed typing for python < 3.10 (#129400)",test/distributed/_tools/test_mod_tracker.py torch/distributed/_tools/fsdp2_mem_tracker.py torch/distributed/_tools/mem_tracker.py torch/distributed/_tools/mod_tracker.py,https://github.com/pytorch/pytorch/pull/129400,sanketpurandare,awgu,huydhn,,
bbdeff76fc1,inductor,Untopiced,fix add decomposition for complex numbers (#129044),test/inductor/test_torchinductor.py torch/_inductor/decomposition.py,https://github.com/pytorch/pytorch/pull/129044,yushangdi,lezcano,zou3519,,
71ebe5121ad,mps,Untopiced,[MPS] Fast math env var (#129007),aten/src/ATen/native/mps/OperationUtils.mm docs/source/mps_environment_variables.rst,https://github.com/pytorch/pytorch/pull/129007,qqaatw,malfet,,,
cb1c56caba2,skip,not user facing,Set target dependencies to always build for sm90a on rowwise scaling (#129402),cmake/Codegen.cmake,https://github.com/pytorch/pytorch/pull/129402,drisspg,malfet,,,
fd4af878558,mps,not user facing,Fix non-portable path warning (#129474),aten/src/ATen/native/mps/operations/MultiTensorApply.h,https://github.com/pytorch/pytorch/pull/129474,malfet,albanD,atalman,qqaatw,
0e6bb7f1ce9,caffe2,Untopiced,[caffe2][be] migrate gloabl static initializer (#128784),c10/core/StorageImpl.cpp,https://github.com/pytorch/pytorch/pull/128784,klein-meta,aaronenyeshi,,,
e364290718b,nested tensor_frontend,improvements,Support linear backward for NJT with dim > 3 (#129393),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/129393,jbschlosser,soulitzer,,,
665d6ea05b5,skip,not user facing,[export] Fix IR canonlization. (#129401),torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/129401,zhxchen17,tugsbayasgalan,,,
f8db12a5389,skip,not user facing,Fix logic to find sbgemm in BLAS library (#125227),cmake/Modules/FindBLAS.cmake,https://github.com/pytorch/pytorch/pull/125227,vinithakv,malfet,,,
b38f6d4cd23,skip,Untopiced,"Revert ""[inductor] Enable FX graph caching in OSS by default (#125863)""",torch/_inductor/config.py,,,,,,
ad76da6c16c,skip,Untopiced,"Revert ""[inductor] Fix TORCHINDUCTOR_FORCE_DISABLE_CACHES (#129257)""",torch/_inductor/async_compile.py torch/_inductor/compile_fx.py torch/_inductor/runtime/compile_tasks.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/utils.py,,,,,,
fb40ba6fc2a,skip,Untopiced,"Revert ""[Traceable FSDP2] Add Dynamo support for run_with_rng_state HOP (#127247)""",test/inductor/test_compiled_autograd.py torch/_dynamo/variables/higher_order_ops.py torch/_prims/rng_prims.py,,,,,,
45b2931b7ed,skip,Untopiced,"Revert ""[Traceable FSDP2] Don't decompose fsdp.split_with_sizes_copy (#129414)""",test/distributed/_composable/fsdp/test_fully_shard_compile.py torch/_functorch/_aot_autograd/functional_utils.py torch/distributed/_composable/fsdp/_fsdp_collectives.py torch/fx/node.py,,,,,,
e317a8b2647,Uncategorized,Untopiced,Add guard to use AMX for x86_64 only (#129479),aten/src/ATen/cpu/Utils.cpp,https://github.com/pytorch/pytorch/pull/129479,atalman,malfet,nWEIdia,,
53f462c5062,quantization,Untopiced,Write dynamo benchmarks performance result to csv when throw exceptions (#126764),benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_inductor_torchbench_inference.csv benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/126764,WeizhuoZhang-intel,jansel,,,
dd00f5e78d4,skip,not user facing,Fixes T192448049 (#129146),torch/_export/passes/replace_set_grad_with_hop_pass.py torch/_export/utils.py torch/export/_remove_effect_tokens_pass.py torch/export/_trace.py torch/export/graph_signature.py,https://github.com/pytorch/pytorch/pull/129146,ydwu4,angelayi,,,
7ebffef4d02,skip,Untopiced,[FSDP2] Ran post-acc-grad hooks manually (#129450),test/distributed/_composable/fsdp/test_fully_shard_autograd.py test/distributed/_composable/fsdp/test_fully_shard_memory.py torch/distributed/_composable/fsdp/_fsdp_collectives.py,https://github.com/pytorch/pytorch/pull/129450,awgu,weifengpy,yf225,,
b045878f81d,skip,Untopiced,"Revert ""Remove test_mps_allocator_module XFAIL (#129340)""",test/test_mps.py,,,,,,
816e8a3f217,releng,not user facing,[MacOS] Improve libomp packaging (#129473),setup.py,https://github.com/pytorch/pytorch/pull/129473,malfet,atalman,,,
0298560ca2f,distributed,Untopiced,TCPStore: improve connect and retry logic (#129261),build_variables.bzl test/cpp/c10d/BackoffTest.cpp test/cpp/c10d/CMakeLists.txt torch/csrc/distributed/c10d/Backoff.cpp torch/csrc/distributed/c10d/Backoff.hpp torch/csrc/distributed/c10d/TCPStore.cpp torch/csrc/distributed/c10d/TCPStore.hpp torch/csrc/distributed/c10d/socket.cpp torch/csrc/distributed/c10d/socket.h,https://github.com/pytorch/pytorch/pull/129261,d4l3k,XilunWu,c-p-i-o,kurman,
7cf454ec52e,skip,Untopiced,"Revert ""Add example for torch.serialization.add_safe_globals (#129396)""",torch/serialization.py,,,,,,
b1f486aff9f,skip,Untopiced,"Revert ""Add warning for weights_only (#129239)""",test/test_nn.py test/test_serialization.py torch/serialization.py,,,,,,
1865fe282f2,distributed,Untopiced,Log whenever we sleep (#129197),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/129197,c-p-i-o,Skylion007,d4l3k,wconstab,
e68ee2cadb7,skip,not user facing,TunableOp hotfix (#129281),aten/src/ATen/cuda/tunable/GemmCommon.h aten/src/ATen/cuda/tunable/TunableOp.h,https://github.com/pytorch/pytorch/pull/129281,jeffdaily,eqy,mxz297,xw285cornell,
45f28769348,skip,not user facing,[Fix] NumToTensor resulted from numel() and size() in TSCovnerter (#128761),test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/128761,jiashenC,angelayi,,,
ae0f84d89c6,releng,not user facing,[CI] Enable amp accuracy check for inductor cpu (#127758),.ci/pytorch/test.sh .github/workflows/inductor.yml benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_huggingface_amp_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_timm_amp_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_amp_freezing_inference.csv,https://github.com/pytorch/pytorch/pull/127758,DiweiSun,desertfire,jgong5,,
79959d707cc,inductor,not user facing,[Inductor][ROCm] Composable Kernel backend for Inductor (#125453),test/inductor/test_ck_backend.py torch/_inductor/async_compile.py torch/_inductor/codecache.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/rocm/__init__.py torch/_inductor/codegen/rocm/ck_template.py torch/_inductor/codegen/rocm/ck_universal_gemm_template.py torch/_inductor/codegen/rocm/compile_command.py torch/_inductor/codegen/rocm/rocm_benchmark_request.py torch/_inductor/codegen/rocm/rocm_cpp_scheduling.py torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/codegen/rocm/rocm_template.py torch/_inductor/codegen/rocm/rocm_template_buffer.py torch/_inductor/config.py torch/_inductor/kernel/mm.py torch/_inductor/select_algorithm.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/125453,tenpercent,eellison,jansel,,
551e4127185,skip,Untopiced,[CUDA][Inductor][CI] Revert PR#127150 since cu124 is now behaving similar enough to cu121 (#128423),.ci/pytorch/test.sh benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/cpu_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/cpu_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_cpu_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_cpu_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/update_expected.py,https://github.com/pytorch/pytorch/pull/128423,nWEIdia,atalman,eqy,,
e58ef5b65f4,Uncategorized,Untopiced,[export] Rewrite exportdb formatting. (#129260),docs/source/scripts/exportdb/generate_example_rst.py torch/_export/db/case.py torch/_export/db/examples/__init__.py torch/_export/db/examples/assume_constant_result.py torch/_export/db/examples/autograd_function.py torch/_export/db/examples/class_method.py torch/_export/db/examples/cond_branch_class_method.py torch/_export/db/examples/cond_branch_nested_function.py torch/_export/db/examples/cond_branch_nonlocal_variables.py torch/_export/db/examples/cond_closed_over_variable.py torch/_export/db/examples/cond_operands.py torch/_export/db/examples/cond_predicate.py torch/_export/db/examples/constrain_as_size_example.py torch/_export/db/examples/constrain_as_value_example.py torch/_export/db/examples/decorator.py torch/_export/db/examples/dictionary.py torch/_export/db/examples/dynamic_shape_assert.py torch/_export/db/examples/dynamic_shape_constructor.py torch/_export/db/examples/dynamic_shape_if_guard.py torch/_export/db/examples/dynamic_shape_map.py torch/_export/db/examples/dynamic_shape_round.py torch/_export/db/examples/dynamic_shape_slicing.py torch/_export/db/examples/dynamic_shape_view.py torch/_export/db/examples/fn_with_kwargs.py torch/_export/db/examples/list_contains.py torch/_export/db/examples/list_unpack.py torch/_export/db/examples/model_attr_mutation.py torch/_export/db/examples/nested_function.py torch/_export/db/examples/null_context_manager.py torch/_export/db/examples/optional_input.py torch/_export/db/examples/pytree_flatten.py torch/_export/db/examples/scalar_output.py torch/_export/db/examples/specialized_attribute.py torch/_export/db/examples/static_for_loop.py torch/_export/db/examples/static_if.py torch/_export/db/examples/tensor_setattr.py torch/_export/db/examples/torch_sym_min.py torch/_export/db/examples/type_reflection_method.py torch/_export/db/examples/user_input_mutation.py torch/_export/db/gen_example.py,https://github.com/pytorch/pytorch/pull/129260,zhxchen17,tugsbayasgalan,,,
e1499f63420,distributed,Untopiced,[C10D] Make new_group eager when used with comm_split (#129284),test/distributed/test_c10d_nccl.py torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/129284,wconstab,d4l3k,fduwjj,nvcastet,
fd414d61892,inductor,not user facing,[inductor] don't materialize the large sparse matrix in CE bwd (#129043),test/inductor/test_scatter_optimization.py torch/_inductor/config.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/metrics.py,https://github.com/pytorch/pytorch/pull/129043,shunting314,Chillee,jansel,,
1c5df9107d2,distributed,not user facing,[BE] Fix several incorrect skip tests (#129488),test/distributed/test_c10d_common.py test/distributed/test_c10d_nccl.py test/distributed/test_functional_api.py,https://github.com/pytorch/pytorch/pull/129488,fegin,fduwjj,wz337,,
bbd47f7b2fd,releng,not user facing,Remove ProcessGroupCudaP2P and change async-TP to use SymmetricMemory (#128762),.ci/pytorch/multigpu-test.sh benchmarks/distributed/intra_node_comm/allgather_matmul.py build_variables.bzl test/distributed/tensor/parallel/test_micro_pipeline_tp.py test/distributed/test_cuda_p2p.py test/distributed/test_symmetric_memory.py torch/_C/_distributed_c10d.pyi torch/_inductor/fx_passes/micro_pipeline_tp.py torch/csrc/distributed/c10d/ProcessGroupCudaP2P.cpp torch/csrc/distributed/c10d/ProcessGroupCudaP2P.hpp torch/csrc/distributed/c10d/init.cpp torch/distributed/_cuda_p2p/__init__.py torch/distributed/_symmetric_memory/__init__.py torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/128762,yifuwang,wanchaol,,,
52341c28e81,skip,Untopiced,"Revert ""[FSDP2] Ran post-acc-grad hooks manually (#129450)""",test/distributed/_composable/fsdp/test_fully_shard_autograd.py test/distributed/_composable/fsdp/test_fully_shard_memory.py torch/distributed/_composable/fsdp/_fsdp_collectives.py,,,,,,
87ebd627a78,skip,not user facing,RS migration - upload sccache stats to s3 instead of rockset (#129490),tools/stats/upload_sccache_stats.py,https://github.com/pytorch/pytorch/pull/129490,clee2000,ZainRizvi,huydhn,,
f389541ce06,inductor,not user facing,Add Strided Input test for flex attention (#128915),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/128915,joydddd,Chillee,drisspg,,
575bc1e3afc,dynamo,Untopiced,"[Reopen #114036] Allow ""must recompute"" in torch.compile + selective checkpointing (SAC) (#129295)",test/dynamo/test_activation_checkpointing.py torch/_dynamo/config.py torch/_functorch/partitioners.py torch/_higher_order_ops/wrap.py torch/_inductor/pattern_matcher.py torch/fx/proxy.py torch/utils/checkpoint.py,https://github.com/pytorch/pytorch/pull/129295,yf225,Chillee,,,
d1f9e822dd9,distributed,not user facing,[DTensor][Test] Update implicit replication unit tests for tensor arg being the first in args list (#127803),test/distributed/_tensor/test_dtensor.py,https://github.com/pytorch/pytorch/pull/127803,wz337,XilunWu,,,
112ef79f299,inductor,not user facing,[inductor] Remove comm-specific node attributes from scheduler (#129084),torch/_inductor/comms.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/129084,peterbell10,lezcano,,,
0d0d42c4a7f,skip,not user facing,test_qat_mobilenet_v2 succeeding on dynamo (#129532),test/dynamo_expected_failures/TestQuantizePT2EQATModels.test_qat_mobilenet_v2,https://github.com/pytorch/pytorch/pull/129532,clee2000,kit1980,malfet,,
9554a9af878,skip,not user facing,[GPT-benchmark] Distinguish LLM models and mirco-benchmarks (#129498),benchmarks/gpt_fast/benchmark.py benchmarks/gpt_fast/generate.py,https://github.com/pytorch/pytorch/pull/129498,yanboliang,huydhn,,,
b7e7a4cb01d,skip,Untopiced,"[cuDNN][SDPA] Remove `TORCH_CUDNN_SDPA_ENABLED=1`, enable cuDNN SDPA by default on H100 and 2nd on other archs >= sm80 (#125343)",aten/src/ATen/Context.h aten/src/ATen/native/cudnn/MHA.cpp aten/src/ATen/native/cudnn/MHA.h aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/attention_backward.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.h docs/source/backends.rst test/dynamo/test_activation_checkpointing.py test/inductor/test_cuda_repro.py test/test_flop_counter.py test/test_transformers.py tools/autograd/derivatives.yaml torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_meta_registrations.py torch/_subclasses/fake_impls.py torch/backends/cuda/__init__.py torch/csrc/Module.cpp torch/csrc/inductor/aoti_torch/generated/c_shim_cuda.h torch/testing/_internal/common_cuda.py torch/utils/flop_counter.py torchgen/aoti/fallback_ops.py torchgen/gen_aoti_c_shim.py,https://github.com/pytorch/pytorch/pull/125343,eqy,Skylion007,,,
90d5a6f001e,inductor,Untopiced,[inductor] Add lowering and codegen for aten.sort (#128458),test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py test/inductor/test_torchinductor_dynamic_shapes.py test/test_testing.py torch/_inductor/codegen/common.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/dependencies.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/ops_handler.py torch/_inductor/runtime/triton_helpers.py torch/_inductor/sizevars.py,https://github.com/pytorch/pytorch/pull/128458,peterbell10,eellison,lezcano,,
a89a1ed072a,distributed,not user facing,[easy][DCP] make BroadcastingTorchSaveReader device generic (#129231),torch/distributed/checkpoint/format_utils.py,https://github.com/pytorch/pytorch/pull/129231,cdzhan,fegin,,,
555f71a15b8,skip,not user facing,Fix test_auto_simd in machine with AMX support (#129444),test/inductor/test_cpu_repro.py,https://github.com/pytorch/pytorch/pull/129444,CaoE,jgong5,peterbell10,,
54f27b886e9,distributed,not user facing,[Inductor UT] Reuse test_distributed_patterns.py for Intel GPU (#129437),test/inductor/test_distributed_patterns.py,https://github.com/pytorch/pytorch/pull/129437,etaf,EikanWang,jansel,,
c718e2f43b4,distributed,Untopiced,[pytorch][logging] add empty wait counter implementation (#128466),build_variables.bzl torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/monitor/instrumentation.cpp torch/csrc/monitor/instrumentation.h,https://github.com/pytorch/pytorch/pull/128466,jamesperng,c-p-i-o,,,
cda4d4887de,releng,not user facing,Skip signals from older runs of the same workflows (#129291),.github/scripts/drci_mocks.json.gz .github/scripts/gql_mocks.json.gz .github/scripts/test_trymerge.py .github/scripts/trymerge.py,https://github.com/pytorch/pytorch/pull/129291,huydhn,ZainRizvi,,,
6181e65cd81,dynamo,Untopiced,Nested tensor subclass support (#127431),test/functorch/test_aotdispatch.py torch/_dynamo/variables/builder.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/_aot_autograd/subclass_utils.py torch/_subclasses/fake_tensor.py torch/_subclasses/meta_utils.py torch/testing/_internal/custom_tensor.py,https://github.com/pytorch/pytorch/pull/127431,tugsbayasgalan,bdhirsh,,,
bd3a11776fb,distributed,not user facing,[dtensor][test] test case suite for comm_mode features (#128729),test/distributed/_tensor/debug/test_comm_mode_features.py,https://github.com/pytorch/pytorch/pull/128729,sinhaanshul,XilunWu,,,
c04cec609df,distributed,not user facing,[dtensor][debug] fixing CommDebugMode module collective tracing  (#128887),torch/distributed/_tensor/debug/comm_mode.py,https://github.com/pytorch/pytorch/pull/128887,sinhaanshul,XilunWu,,,
7420bad74c5,distributed,not user facing,[BE] Do not assert if the barrier is not created (#129497),test/distributed/test_distributed_spawn.py,https://github.com/pytorch/pytorch/pull/129497,fegin,fduwjj,wz337,,
d02bba519ce,Uncategorized,Untopiced,[export] match fake mode for _decompose_exported_program() (#129421),test/export/test_experimental.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/129421,pianpwk,tugsbayasgalan,zhxchen17,,
610894e9789,mps,Untopiced,[MPS][BE] Generalize Fused optimizers (#129105),aten/src/ATen/native/mps/operations/FusedAdamAmsgradKernelImpl.mm aten/src/ATen/native/mps/operations/FusedAdamKernelImpl.mm aten/src/ATen/native/mps/operations/FusedAdamWAmsgradKernelImpl.mm aten/src/ATen/native/mps/operations/FusedAdamWKernelImpl.mm aten/src/ATen/native/mps/operations/MultiTensorApply.h,https://github.com/pytorch/pytorch/pull/129105,qqaatw,malfet,,,
000f2d637bf,Uncategorized,Untopiced,Refactoring the code to make it lint clean (#129424),torch/csrc/profiler/standalone/execution_trace_observer.cpp,https://github.com/pytorch/pytorch/pull/129424,shengfukevin,aaronenyeshi,,,
8b8e2fcdda4,distributed (checkpoint),bug fixes,[DCP] Fix Optimizer Learning Rate not being loaded correctly (#129398),test/distributed/checkpoint/e2e/test_e2e_save_and_load.py torch/distributed/checkpoint/planner_helpers.py,https://github.com/pytorch/pytorch/pull/129398,wz337,fegin,,,
c422a9549d2,distributed,not user facing,[easy][DCP] Fix test_fsdp_ep.py for _MeshEnv.create_child_mesh API ch… (#129445),test/distributed/checkpoint/e2e/test_fsdp_ep.py,https://github.com/pytorch/pytorch/pull/129445,cdzhan,fegin,wz337,,
ead97ee4863,Uncategorized,Untopiced,[Compile+SAC] Only warn for in-place ops once (#129397),torch/_higher_order_ops/wrap.py,https://github.com/pytorch/pytorch/pull/129397,yf225,tianyu-l,,,
f2840bb2207,skip,not user facing,"[nn-module] Use standard dict for _parameters, _modules and _buffers (#129164)",torch/nn/modules/module.py,https://github.com/pytorch/pytorch/pull/129164,anijain2305,mikaylagawarecki,,,
92be3403ea2,distributed,not user facing,Fix an issue in oneShotAllReduce where different ranks perform reduction in different order (#129501),torch/csrc/distributed/c10d/intra_node_comm.cu,https://github.com/pytorch/pytorch/pull/129501,yifuwang,Chillee,,,
211f38e7420,skip,Untopiced,"Revert ""[ALI] [Reland] Use LF runners for Lint (#129071)""",.github/workflows/lint.yml,,,,,,
53fafdd0c35,releng,not user facing,[BE] Runner determinator: more resilient user matching (#129462),.github/scripts/get_workflow_type.py .github/workflows/_runner-determinator.yml .github/workflows/pull.yml .github/workflows/slow.yml .github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/129462,jeanschmidt,ZainRizvi,zxiiro,,
42d490d41d0,inductor,not user facing,[AOTI][refactor] Move generate_user_defined_triton_kernel (#129267),torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cuda.py,https://github.com/pytorch/pytorch/pull/129267,desertfire,chenyang78,,,
52009068bc3,skip,not user facing,[AOTI][refactor] Unify UserDefinedTritonKernel.codegen (#129378),torch/_inductor/codegen/cpp_wrapper_cuda.py torch/_inductor/codegen/wrapper.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/129378,desertfire,chenyang78,oulgen,,
303ad8d7f54,python_frontend,improvements,Add warning for weights_only (#129239),test/test_nn.py test/test_serialization.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/129239,mikaylagawarecki,albanD,malfet,,
3b531eace74,python_frontend,docs,Add example for torch.serialization.add_safe_globals (#129396),torch/serialization.py,https://github.com/pytorch/pytorch/pull/129396,mikaylagawarecki,albanD,malfet,,
25cec43678e,skip,not user facing,Remove dependency on private _compat_pickle in CPython (#129509),torch/_utils.py torch/_weights_only_unpickler.py,https://github.com/pytorch/pytorch/pull/129509,mikaylagawarecki,malfet,,,
474d743dba2,Uncategorized,Untopiced,[torchao][benchmark] Skip all accuracy tests by returning `pass_due_to_skip` (#129545),benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/129545,xuzhao9,HDCharles,,,
f7708ffebb4,skip,Untopiced,"Revert ""[AOTI][refactor] Unify UserDefinedTritonKernel.codegen (#129378)""",torch/_inductor/codegen/cpp_wrapper_cuda.py torch/_inductor/codegen/wrapper.py torch/_inductor/ir.py,,,,,,
9cf8e5dd32f,quantization,Untopiced,chore(quantization): Enable PT2E symmetric dynamic quantization (#124615),torch/ao/quantization/pt2e/port_metadata_pass.py,https://github.com/pytorch/pytorch/pull/124615,pkluska,kimishpatel,malfet,,
bc68907caae,skip,not user facing,[EZ][BE] Replace `assertTrue` with more appropriate checks (#129569),test/test_mps.py,https://github.com/pytorch/pytorch/pull/129569,malfet,Skylion007,jeanschmidt,,
1b1fd0f4fe6,releng,not user facing,[ROCm] Use additional shard for inductor workflow to resolve timeouts (#129480),.github/workflows/inductor.yml,https://github.com/pytorch/pytorch/pull/129480,jataylo,jeffdaily,jithunnair-amd,malfet,
7373492c9b6,inductor,Untopiced,Use _unsafe_masked_index in masked_scatter decomposition (#123667),torch/_inductor/decomposition.py torch/_inductor/inductor_prims.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/123667,isuruf,peterbell10,,,
8e4f7f742fc,distributed,Untopiced,"[DCP] Capture reader, writer and planner components in the DCP API logger (#129548)",torch/distributed/checkpoint/filesystem.py torch/distributed/checkpoint/logger.py,https://github.com/pytorch/pytorch/pull/129548,saumishr,fegin,wz337,,
b9a1c2c991a,inductor,Untopiced,[ROCm] Enable F8 Inductor Unit tests (#128353),test/inductor/test_fp8.py test/test_matmul_cuda.py torch/_inductor/codegen/triton.py torch/testing/_internal/common_cuda.py,https://github.com/pytorch/pytorch/pull/128353,alugorey,eellison,jansel,,
61bf1452a3b,releng,not user facing,Add one more shard for CPU jobs (#129299),.github/workflows/pull.yml,https://github.com/pytorch/pytorch/pull/129299,huydhn,ZainRizvi,kit1980,,
87d14ad419f,skip,not user facing,[inductor] Fix TORCHINDUCTOR_FORCE_DISABLE_CACHES (#129257),torch/_inductor/async_compile.py torch/_inductor/compile_fx.py torch/_inductor/runtime/compile_tasks.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/129257,masnesral,oulgen,,,
cca85c96cdb,skip,not user facing,[export] minor typo fix (#129543),docs/source/export.rst,https://github.com/pytorch/pytorch/pull/129543,yushangdi,angelayi,,,
e9aefad641f,skip,Untopiced,"Revert ""[CUDA][Inductor][CI] Revert PR#127150 since cu124 is now behaving similar enough to cu121 (#128423)""",.ci/pytorch/test.sh benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/cpu_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/cpu_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_cpu_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_cpu_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/update_expected.py,,,,,,
895316119d7,skip,Untopiced,"Revert ""[BE][Easy] use `pathlib.Path` instead of `dirname` / `""..""` / `pardir` (#129374)""",.circleci/codegen_validation/normalize_yaml_fragment.py .github/scripts/delete_old_branches.py .github/scripts/ensure_actions_will_cancel.py .github/scripts/generate_binary_build_matrix.py .github/scripts/gitutils.py .github/scripts/lint_native_functions.py .github/scripts/test_gitutils.py benchmarks/dynamo/ci_expected_accuracy/cu124/update_expected.py benchmarks/dynamo/ci_expected_accuracy/update_expected.py docs/source/scripts/build_opsets.py docs/source/scripts/build_quantization_configs.py docs/source/scripts/exportdb/generate_example_rst.py scripts/compile_tests/update_failures.py test/jit/fixtures_srcs/generate_models.py test/jit/test_backend_nnapi.py test/mobile/test_bytecode.py test/mobile/test_upgrader_codegen.py test/mobile/test_upgraders.py test/onnx/onnx_test_common.py test/quantization/core/test_docs.py test/test_typing.py tools/amd_build/build_amd.py tools/build_libtorch.py tools/code_coverage/package/oss/utils.py tools/code_coverage/package/util/setting.py tools/gen_vulkan_spv.py tools/linter/adapters/s3_init.py tools/onnx/update_default_opset_version.py tools/setup_helpers/cmake.py tools/setup_helpers/gen.py tools/setup_helpers/gen_unboxing.py tools/setup_helpers/generate_code.py tools/stats/export_test_times.py tools/stats/import_test_stats.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_gen_backend_stubs.py tools/test/test_test_run.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/testing/discover_tests.py tools/testing/do_target_determination_for_s3.py tools/testing/explicit_ci_jobs.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/gen_artifact.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/llm.py tools/testing/target_determination/heuristics/previously_failed_in_pr.py tools/testing/target_determination/heuristics/utils.py tools/testing/test_selections.py tools/vscode_settings.py torch/_inductor/runtime/compile_tasks.py torch/testing/_internal/common_utils.py torchgen/gen_backend_stubs.py torchgen/gen_lazy_tensor.py,,,,,,
90f82426b96,releng,not user facing,RS migration - trymerge to upload merge records to s3 (#129503),.github/scripts/trymerge.py .github/workflows/trymerge.yml .gitignore,https://github.com/pytorch/pytorch/pull/129503,clee2000,ZainRizvi,huydhn,malfet,
d3d67640825,Uncategorized,Untopiced,[pytorch][logging] add fb internal ODS implementation of wait counter (#128605),torch/csrc/monitor/instrumentation.cpp,https://github.com/pytorch/pytorch/pull/128605,jamesperng,c-p-i-o,,,
28480dd7dce,releng,not user facing,[CI] Fix runner determinator for ciflow (#129500),.github/workflows/_runner-determinator.yml .github/workflows/slow.yml .github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/129500,afrittoli,malfet,zxiiro,,
795db809755,releng,not user facing,Upload release tag source code to s3 (#128842),.github/workflows/create_release.yml,https://github.com/pytorch/pytorch/pull/128842,clee2000,atalman,malfet,,
7b1988f9222,releng,not user facing,[ez] Give trymerge id token write permissions after #129503 (#129594),.github/workflows/trymerge.yml,https://github.com/pytorch/pytorch/pull/129594,clee2000,huydhn,,,
13316a8d464,profiler,not user facing,[Profiler] Add Rank to NCCL Debug Info (#129528),torch/csrc/profiler/util.cpp torch/csrc/profiler/util.h,https://github.com/pytorch/pytorch/pull/129528,sraikund16,aaronenyeshi,,,
b8e5678ad21,distributed,not user facing,Delete lazy ddp optimizer (#120727),torch/_dynamo/backends/distributed.py torch/_dynamo/config.py,https://github.com/pytorch/pytorch/pull/120727,eellison,jansel,yf225,,
5ad2ad59211,distributed,Untopiced,"Update start_, end_  and retired only for the right entry when retire a work (#128948)",test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/NCCLUtils.hpp,https://github.com/pytorch/pytorch/pull/128948,HOOLoLo,c-p-i-o,wconstab,,
64f1111d382,skip,not user facing,Expose nholmann json to torch (#129570),BUILD.bazel WORKSPACE c10/CMakeLists.txt caffe2/CMakeLists.txt cmake/Dependencies.cmake third_party/nlohmann.BUILD torch/CMakeLists.txt,https://github.com/pytorch/pytorch/pull/129570,c-p-i-o,d4l3k,malfet,,
90f6043368c,fx,Untopiced,Don't decompose functional composite ops in export inference IR (#128077),test/export/test_export.py test/export/testing.py test/functorch/test_aotdispatch.py torch/_higher_order_ops/utils.py torch/_subclasses/functional_tensor.py torch/export/__init__.py torch/export/_trace.py torch/export/experimental/__init__.py torch/export/exported_program.py torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/128077,tugsbayasgalan,bdhirsh,,,
b6689e0fb83,Uncategorized,Untopiced,[ts migration] add logging as part of torch logging system (#129405),torch/_export/converter.py torch/_logging/_registrations.py,https://github.com/pytorch/pytorch/pull/129405,jiashenC,angelayi,,,
7bb558fd6e6,nested tensor_frontend,not user facing,add _flash_attention_forward and _efficient_attention_forward to compute intensive ops in partitioner (#129533),torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/129533,YuqingJ,drisspg,,,
0b7e8df7d88,inductor,not user facing,[CUDAGraph Trees] Enable input mutation support in OSS (#129184),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/129184,BoyuanFeng,eellison,,,
27a14405d3b,skip,not user facing,enable device index check for all device types (#126767),torch/csrc/autograd/python_variable.cpp,https://github.com/pytorch/pytorch/pull/126767,garfield1997,albanD,,,
cdbd6542d01,releng,Untopiced,Fix inductor benchmarks (#129620),.ci/pytorch/test.sh,https://github.com/pytorch/pytorch/pull/129620,malfet,huydhn,kit1980,,
b9697eacd36,skip,not user facing,[torchbind] support tensor ops inside of __obj_flatten__ (#129605),test/cpp/jit/test_custom_class_registrations.cpp test/export/test_torchbind.py torch/_library/fake_class_registry.py,https://github.com/pytorch/pytorch/pull/129605,ydwu4,angelayi,,,
c9ceae3fac1,distributed,Untopiced,Use JK for mast rdzv handler tcpstore handling and additional logging (#129603),torch/distributed/elastic/agent/server/local_elastic_agent.py,https://github.com/pytorch/pytorch/pull/129603,kurman,d4l3k,,,
9450e198aa0,skip,Untopiced,Conversions between strided and jagged layouts for Nested Tensors (#115749),aten/src/ATen/FunctionalInverses.cpp aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorFactories.cpp aten/src/ATen/native/nested/NestedTensorMath.cpp test/expect/HasDecompTest.test_has_decomposition.expect test/test_nestedtensor.py tools/autograd/derivatives.yaml tools/autograd/gen_inplace_or_view_type.py torch/_subclasses/fake_impls.py torch/nested/__init__.py torch/nested/_internal/nested_tensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/115749,ani300,jbschlosser,,,
e19042481b6,skip,not user facing,[cuDNN][cuDNN Frontend] Bump cuDNN FE submodule to 1.5.2 (#129592),third_party/cudnn_frontend,https://github.com/pytorch/pytorch/pull/129592,eqy,Skylion007,,,
84ad5452f6b,mps,Untopiced,[MPS] Fused SGD optimizer (#129350),aten/src/ATen/native/cuda/FusedSgdKernel.cu aten/src/ATen/native/mps/operations/FusedOptimizerOps.h aten/src/ATen/native/mps/operations/FusedSgdKernel.mm aten/src/ATen/native/mps/operations/MultiTensorApply.h aten/src/ATen/native/native_functions.yaml docs/source/optim.rst torch/testing/_internal/common_optimizers.py,https://github.com/pytorch/pytorch/pull/129350,qqaatw,janeyx99,,,
ea588d7fd36,distributed,not user facing,[SymmetricMemory] use SCM_RIGHTS socket control message to share exported cumem handle (#129412),torch/csrc/distributed/c10d/CUDASymmetricMemory.cu,https://github.com/pytorch/pytorch/pull/129412,yifuwang,Chillee,,,
9b5b93c58f6,skip,Untopiced,[CUDA][Inductor][CI] Revert PR#127150 since cu124 is now behaving similar enough to cu121 (#128423),.ci/pytorch/test.sh benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/aot_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/cpu_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/cpu_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_cpu_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_cpu_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamo_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/update_expected.py,https://github.com/pytorch/pytorch/pull/128423,nWEIdia,atalman,eqy,,
5ee893a84ac,skip,not user facing,Add inductor support for conv3d transpose (#129458),aten/src/ATen/native/mkldnn/MKLDNNConversions.cpp test/inductor/test_mkldnn_pattern_matcher.py,https://github.com/pytorch/pytorch/pull/129458,yanbing-j,jansel,jgong5,,
4082759925a,skip,not user facing,[Inductor] FlexAttention supports block sparse mask (#129216),test/inductor/test_flex_attention.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/_flex_attention.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/129216,yanboliang,Chillee,,,
cf392d8a89a,cuda,Untopiced,[pytorch][cuda] Generate kernels for 5x5 filters on depth wise convolution backward (#129609),aten/src/ATen/native/cuda/DepthwiseConv2d.cu,https://github.com/pytorch/pytorch/pull/129609,valentinandrei,eqy,ezyang,,
c9798d123b4,dynamo,not user facing,[dynamo][compile-time] Manually trace torch.nn.Module.parameters (#129583),test/dynamo/test_modules.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/129583,anijain2305,jansel,,,
9f29a2291c5,nn_frontend,new features,Feat: Updated torch.nn.Modules.set_submodules() (#127714),test/test_nn.py torch/nn/modules/module.py,https://github.com/pytorch/pytorch/pull/127714,Gunale0926,mikaylagawarecki,,,
ff026f3d0a6,composability,not user facing,Fix an issue in meta_scaled_mm (#129521),test/test_matmul_cuda.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/129521,y-sq,drisspg,,,
a028e5862d7,profiler,bug fixes,[profiler] Directly use end_ns to create the FunctionEvent instead of using start_ns + duration_ns in pytorch profiler post processing for checking parent-child precisely (#129554),torch/_C/_autograd.pyi torch/autograd/profiler.py torch/csrc/autograd/init.cpp torch/csrc/autograd/profiler_kineto.cpp torch/csrc/autograd/profiler_kineto.h,https://github.com/pytorch/pytorch/pull/129554,zejun-chen,aaronenyeshi,gujinghui,,
ad607b91f4c,dynamo,not user facing,[dynamo][onnx] Skip some dynamic=True test with inlining in built nn modules (#129610),test/onnx/dynamo/test_dynamo_with_onnxruntime_backend.py,https://github.com/pytorch/pytorch/pull/129610,anijain2305,yanboliang,,,
0680e6cd1c5,profiler,not user facing,[Profiler] Add sraikund16 to profiler paths in CODEOWNERS (#129591),CODEOWNERS,https://github.com/pytorch/pytorch/pull/129591,aaronenyeshi,sraikund16,,,
2d9012ad252,Uncategorized,Untopiced,Forward fix internal pyre failure from D58983461 (#129525),torch/types.py,https://github.com/pytorch/pytorch/pull/129525,huydhn,XuehaiPan,clee2000,malfet,
483dbfcf2af,distributed,not user facing,[BE] Correctly catch skip signals emitting from sys.exit (#129581),test/distributed/test_c10d_nccl.py,https://github.com/pytorch/pytorch/pull/129581,fegin,fduwjj,,,
82c8fc3a2b5,inductor,not user facing,[inductor] Add size_hint to conv dilation (#129631),test/inductor/test_torchinductor.py torch/_inductor/kernel/conv.py,https://github.com/pytorch/pytorch/pull/129631,aakhundov,chenyang78,,,
5ceba6a3cbd,skip,Untopiced,"Revert ""[Inductor] FlexAttention supports block sparse mask (#129216)""",test/inductor/test_flex_attention.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/_flex_attention.py torch/testing/_internal/hop_db.py,,,,,,
5c6af2b5837,python_frontend,bug fixes,"[cpu] Fix div with rounding_mode=""floor"" when division overflows (#129536)",aten/src/ATen/native/cpu/BinaryOpsKernel.cpp test/test_binary_ufuncs.py,https://github.com/pytorch/pytorch/pull/129536,peterbell10,lezcano,,,
04206d1898e,linalg_frontend,not user facing,"TunableOp hotfix, unit test follow-up (#129606)",aten/src/ATen/cuda/tunable/Tunable.cpp aten/src/ATen/cuda/tunable/TunableOp.h test/test_linalg.py,https://github.com/pytorch/pytorch/pull/129606,jeffdaily,atalman,,,
321bdcb372d,autograd_frontend,Untopiced,Fix device propagation for checkpointing (#128671),test/test_cuda.py test/test_xpu.py torch/utils/checkpoint.py,https://github.com/pytorch/pytorch/pull/128671,dvrogozh,guangyey,soulitzer,,
e0bba37d664,cpp_frontend,not user facing,[codemod] Add `[[noreturn]]` to 2 files inc caffe2/c10/util/TypeCast.cpp (#129575),c10/util/TypeCast.cpp c10/util/TypeCast.h,https://github.com/pytorch/pytorch/pull/129575,r-barnes,Skylion007,,,
9174d14551c,build_frontend,not user facing,Don't install remaining caffe2 python files (#129067),caffe2/CMakeLists.txt caffe2/core/__init__.py caffe2/perfkernels/__init__.py,https://github.com/pytorch/pytorch/pull/129067,AlekseiNikiforovIBM,cyyever,r-barnes,,
a4d7aa498bf,distributed,not user facing,[Traceable FSDP2] Add auto-functionalize support for mutable list[Tensor] (copy from Brian's PR #127347); enable E2E inductor unit test for transformer model (#129502),test/distributed/_composable/fsdp/test_fully_shard_compile.py test/dynamo/test_misc.py test/inductor/test_torchinductor.py torch/_functorch/_aot_autograd/functional_utils.py torch/_higher_order_ops/auto_functionalize.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/distributed/_composable/fsdp/_fsdp_collectives.py torch/distributed/_composable/fsdp/_fsdp_param.py torch/fx/node.py,https://github.com/pytorch/pytorch/pull/129502,yf225,zou3519,,,
389492e2640,releng,not user facing,Fix runner determinator bug (#129612),.github/scripts/runner_determinator.py .github/workflows/_runner-determinator.yml,https://github.com/pytorch/pytorch/pull/129612,ZainRizvi,jeanschmidt,zxiiro,,
4dcc1ceff30,dynamo,Untopiced,[dynamo] Fakify result of delegate (#128752),torch/_dynamo/variables/higher_order_ops.py,https://github.com/pytorch/pytorch/pull/128752,angelayi,ydwu4,,,
ec284d3a74e,Uncategorized,Untopiced,Prototype for export_for_training (#129092),test/export/test_export.py torch/_export/verifier.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/129092,tugsbayasgalan,zhxchen17,,,
dabaebd3399,Uncategorized,Untopiced,Make run_decomp work (#129249),test/export/test_export.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/129249,tugsbayasgalan,zhxchen17,,,
71f5ecd1ee0,distributed (tools),not user facing,Fixed Memory Leaks in tests (#129640),test/distributed/_tools/test_fsdp2_mem_tracker.py test/distributed/_tools/test_mem_tracker.py torch/distributed/_tools/mem_tracker.py,https://github.com/pytorch/pytorch/pull/129640,sanketpurandare,clee2000,,,
23adf166e16,skip,not user facing,[cond] inlining into one of the branches when pred is a python constant (#128709),test/dynamo/test_export.py test/dynamo/test_higher_order_ops.py test/export/test_export.py test/export/test_verifier.py test/functorch/test_aotdispatch.py test/functorch/test_control_flow.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/cond.py,https://github.com/pytorch/pytorch/pull/128709,ydwu4,zou3519,,,
39427288f4e,Uncategorized,Untopiced,Taskify training IR + run_decomp flow failures (#129547),test/export/test_export.py test/export/test_export_training_ir_to_run_decomp.py test/export/testing.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/129547,tugsbayasgalan,avikchaudhuri,,,
602b5cb2183,inductor,not user facing,[inductor] switch HalideCodeCache to new cpp_builder. (#129441),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/129441,xuhancn,desertfire,jgong5,,
1d0efedc853,profiler,performance,[Profiler] Add TSC Clock Callback to CUPTI (#125036),buckbuild.bzl cmake/Dependencies.cmake test/cpp/jit/CMakeLists.txt third_party/kineto torch/csrc/autograd/profiler_kineto.cpp,https://github.com/pytorch/pytorch/pull/125036,sraikund16,aaronenyeshi,,,
b8398b771c5,releng,not user facing,Upload test stats when workflow regardless of conclusion (#129694),.github/workflows/upload-test-stats.yml,https://github.com/pytorch/pytorch/pull/129694,clee2000,huydhn,,,
5e7ac69a672,fx,Untopiced,[Dynamic Shapes] fixed dynamic shape inference  (#128807),test/dynamo/test_misc.py test/dynamo/test_subclasses.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/128807,zero000064,ezyang,,,
83a4a8b5109,distributed,Untopiced,[C10D] clean up pointless 'or None' clause (#129522),torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/129522,wconstab,awgu,,,
305ba629060,optimizer_frontend,bc breaking,Add support to `GradScaler` for respecting an already set `grad_scale` value  (#123429),test/test_cuda.py torch/amp/grad_scaler.py,https://github.com/pytorch/pytorch/pull/123429,yousufmo,ezyang,,,
67416a29963,distributed,Untopiced,[c10d] Introduce a util for detecting DMA connectivity among devices (#129510),.lintrunner.toml build_variables.bzl caffe2/CMakeLists.txt test/distributed/test_symmetric_memory.py torch/csrc/distributed/c10d/CudaDMAConnectivity.cpp torch/csrc/distributed/c10d/DMAConnectivity.cpp torch/csrc/distributed/c10d/DMAConnectivity.hpp torch/csrc/distributed/c10d/init.cpp,https://github.com/pytorch/pytorch/pull/129510,yifuwang,weifengpy,,,
d80939e5e93,dataloader_frontend,not user facing,[BE] enable UFMT for `torch/storage.py` (#127706),.lintrunner.toml test/profiler/test_profiler.py test/test_dataloader.py test/test_openmp.py torch/_inductor/runtime/runtime_utils.py torch/amp/autocast_mode.py torch/fx/passes/graph_drawer.py torch/onnx/_internal/fx/op_validation.py torch/storage.py torch/testing/_comparison.py torch/testing/_internal/common_device_type.py torch/types.py torch/utils/benchmark/utils/compile.py torch/utils/data/datapipes/utils/decoder.py,https://github.com/pytorch/pytorch/pull/127706,XuehaiPan,ezyang,,,
f9119575738,dataloader_frontend,not user facing,[BE] sort imports in `torch.utils.data` (#127704),torch/utils/data/__init__.py torch/utils/data/_utils/__init__.py torch/utils/data/_utils/collate.py torch/utils/data/_utils/pin_memory.py torch/utils/data/_utils/signal_handling.py torch/utils/data/_utils/worker.py torch/utils/data/dataloader.py torch/utils/data/datapipes/__init__.py torch/utils/data/datapipes/_decorator.py torch/utils/data/datapipes/_typing.py torch/utils/data/datapipes/dataframe/__init__.py torch/utils/data/datapipes/dataframe/dataframe_wrapper.py torch/utils/data/datapipes/dataframe/dataframes.py torch/utils/data/datapipes/dataframe/datapipes.py torch/utils/data/datapipes/dataframe/structures.py torch/utils/data/datapipes/datapipe.py torch/utils/data/datapipes/datapipe.pyi.in torch/utils/data/datapipes/iter/__init__.py torch/utils/data/datapipes/iter/callable.py torch/utils/data/datapipes/iter/combinatorics.py torch/utils/data/datapipes/iter/combining.py torch/utils/data/datapipes/iter/filelister.py torch/utils/data/datapipes/iter/fileopener.py torch/utils/data/datapipes/iter/grouping.py torch/utils/data/datapipes/iter/routeddecoder.py torch/utils/data/datapipes/iter/selecting.py torch/utils/data/datapipes/iter/sharding.py torch/utils/data/datapipes/iter/streamreader.py torch/utils/data/datapipes/iter/utils.py torch/utils/data/datapipes/map/__init__.py torch/utils/data/datapipes/map/callable.py torch/utils/data/datapipes/map/combinatorics.py torch/utils/data/datapipes/map/combining.py torch/utils/data/datapipes/map/grouping.py torch/utils/data/datapipes/map/utils.py torch/utils/data/datapipes/utils/common.py torch/utils/data/dataset.py torch/utils/data/distributed.py torch/utils/data/graph.py torch/utils/data/graph_settings.py torch/utils/data/sampler.py,https://github.com/pytorch/pytorch/pull/127704,XuehaiPan,ezyang,,,
7cf0b90e496,dataloader_frontend,not user facing,[BE] enable UFMT in `torch.utils.data` (#127705),.lintrunner.toml torch/utils/__init__.py torch/utils/data/__init__.py torch/utils/data/_utils/__init__.py torch/utils/data/_utils/collate.py torch/utils/data/_utils/pin_memory.py torch/utils/data/_utils/worker.py torch/utils/data/dataloader.py torch/utils/data/datapipes/_decorator.py torch/utils/data/datapipes/_hook_iterator.py torch/utils/data/datapipes/_typing.py torch/utils/data/datapipes/dataframe/__init__.py torch/utils/data/datapipes/dataframe/dataframe_wrapper.py torch/utils/data/datapipes/dataframe/dataframes.py torch/utils/data/datapipes/dataframe/datapipes.py torch/utils/data/datapipes/datapipe.py torch/utils/data/datapipes/gen_pyi.py torch/utils/data/datapipes/iter/__init__.py torch/utils/data/datapipes/iter/callable.py torch/utils/data/datapipes/iter/combinatorics.py torch/utils/data/datapipes/iter/combining.py torch/utils/data/datapipes/iter/filelister.py torch/utils/data/datapipes/iter/fileopener.py torch/utils/data/datapipes/iter/grouping.py torch/utils/data/datapipes/iter/routeddecoder.py torch/utils/data/datapipes/iter/selecting.py torch/utils/data/datapipes/iter/sharding.py torch/utils/data/datapipes/iter/streamreader.py torch/utils/data/datapipes/map/__init__.py torch/utils/data/datapipes/map/callable.py torch/utils/data/datapipes/map/combinatorics.py torch/utils/data/datapipes/map/combining.py torch/utils/data/datapipes/map/grouping.py torch/utils/data/datapipes/utils/common.py torch/utils/data/datapipes/utils/decoder.py torch/utils/data/datapipes/utils/snapshot.py torch/utils/data/distributed.py torch/utils/data/graph.py torch/utils/data/graph_settings.py torch/utils/data/sampler.py,https://github.com/pytorch/pytorch/pull/127705,XuehaiPan,ezyang,,,
3fc279633b4,inductor,not user facing,[ATen] Make argsort.stable CompositeImplicitAutograd (#129529),aten/src/ATen/functorch/BatchRulesDecompositions.cpp aten/src/ATen/native/Sorting.cpp aten/src/ATen/native/native_functions.yaml test/expect/HasDecompTest.test_has_decomposition.expect test/functorch/test_vmap.py torch/_inductor/lowering.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/129529,peterbell10,lezcano,,,
fb5888c7190,cpp_frontend,not user facing,Remove unused type traits in  torch/csrc/utils (#128799),test/cpp/api/static.cpp torch/csrc/utils/variadic.h,https://github.com/pytorch/pytorch/pull/128799,cyyever,ezyang,,,
169b4ca07eb,skip,Untopiced,add uuid in cudaDeviceProperties (#125083),test/test_cuda.py torch/_C/__init__.pyi.in torch/csrc/cuda/Module.cpp,https://github.com/pytorch/pytorch/pull/125083,jeffdaily,albanD,eqy,malfet,
0ffb17547ef,skip,not user facing,[Simple FSDP] Add unit test for torch.compile + reparameterization + SAC (#129641),test/dynamo/test_activation_checkpointing.py,https://github.com/pytorch/pytorch/pull/129641,yf225,tianyu-l,,,
d4b6ff6fbe3,releng,not user facing,Disable llm-td step (#129722),.github/workflows/pull.yml .github/workflows/slow.yml .github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/129722,malfet,clee2000,kit1980,,
9e1f3ecaa71,releng,not user facing,"[BE][Easy] use `pathlib.Path` instead of `dirname` / `""..""` / `pardir` (#129374)",.circleci/codegen_validation/normalize_yaml_fragment.py .github/scripts/delete_old_branches.py .github/scripts/ensure_actions_will_cancel.py .github/scripts/generate_binary_build_matrix.py .github/scripts/gitutils.py .github/scripts/lint_native_functions.py .github/scripts/test_gitutils.py benchmarks/dynamo/ci_expected_accuracy/update_expected.py docs/source/scripts/build_opsets.py docs/source/scripts/build_quantization_configs.py docs/source/scripts/exportdb/generate_example_rst.py scripts/compile_tests/update_failures.py test/jit/fixtures_srcs/generate_models.py test/jit/test_backend_nnapi.py test/mobile/test_bytecode.py test/mobile/test_upgrader_codegen.py test/mobile/test_upgraders.py test/onnx/onnx_test_common.py test/quantization/core/test_docs.py test/test_typing.py tools/amd_build/build_amd.py tools/build_libtorch.py tools/code_coverage/package/oss/utils.py tools/code_coverage/package/util/setting.py tools/gen_vulkan_spv.py tools/linter/adapters/s3_init.py tools/onnx/update_default_opset_version.py tools/setup_helpers/cmake.py tools/setup_helpers/gen.py tools/setup_helpers/gen_unboxing.py tools/setup_helpers/generate_code.py tools/stats/export_test_times.py tools/stats/import_test_stats.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_gen_backend_stubs.py tools/test/test_test_run.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/testing/discover_tests.py tools/testing/do_target_determination_for_s3.py tools/testing/explicit_ci_jobs.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/gen_artifact.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/llm.py tools/testing/target_determination/heuristics/previously_failed_in_pr.py tools/testing/target_determination/heuristics/utils.py tools/testing/test_selections.py tools/vscode_settings.py torch/_inductor/runtime/compile_tasks.py torch/testing/_internal/common_utils.py torchgen/gen_backend_stubs.py torchgen/gen_lazy_tensor.py,https://github.com/pytorch/pytorch/pull/129374,XuehaiPan,justinchuby,malfet,,
8ba0f6c7c2e,skip,Untopiced,"Revert ""[nn-module] Use standard dict for _parameters, _modules and _buffers (#129164)""",torch/nn/modules/module.py,,,,,,
deaab33f3fe,composability,Untopiced,[custom op] add error message (#129417),test/test_custom_ops.py torch/_library/infer_schema.py,https://github.com/pytorch/pytorch/pull/129417,yushangdi,zou3519,,,
36b9d9cfcd3,skip,not user facing,[Inductor UT] Generalize device-bias code in newly added UT `test_scatter_optimization.py` (#129622),test/inductor/test_scatter_optimization.py test/inductor/test_torchinductor_dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/129622,etaf,EikanWang,peterbell10,,
c07a799ed55,dynamo,not user facing,[Traceable FSDP2] Add Dynamo support for run_with_rng_state HOP (#127247),test/inductor/test_compiled_autograd.py torch/_dynamo/variables/higher_order_ops.py torch/_prims/rng_prims.py,https://github.com/pytorch/pytorch/pull/127247,yf225,bdhirsh,,,
b9d3cedd648,skip,not user facing,[Inductor] FlexAttention supports block sparse mask (#129216),test/inductor/test_flex_attention.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/_flex_attention.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/129216,yanboliang,Chillee,,,
4b598d87d38,Uncategorized,Untopiced,Fix  FindBLAS.cmake (#129713),cmake/Modules/FindBLAS.cmake,https://github.com/pytorch/pytorch/pull/129713,malfet,kit1980,,,
4b8a5e03745,skip,not user facing,[export] make with_effect mark op has_effect to prevent them from DCEed. (#129680),test/export/test_torchbind.py torch/_higher_order_ops/effects.py torch/export/_remove_effect_tokens_pass.py,https://github.com/pytorch/pytorch/pull/129680,ydwu4,angelayi,,,
8d4216af8c5,skip,not user facing,Fix compile error with Intel oneAPI compiler (#129589),aten/src/ATen/test/rng_test.h,https://github.com/pytorch/pytorch/pull/129589,iskunk,colesbury,,,
7854d84acbf,skip,not user facing,[cuDNN][SDPA] Bail out of dispatching to cuDNN for head dim > 128 on Ampere (#129587),aten/src/ATen/native/transformers/cuda/sdp_utils.cpp test/test_transformers.py,https://github.com/pytorch/pytorch/pull/129587,eqy,Skylion007,drisspg,,
b019f38fdd9,inductor,not user facing,[inductor] Fix pattern replacements with multiple users (#129689),test/inductor/test_torchinductor.py torch/_inductor/pattern_matcher.py,https://github.com/pytorch/pytorch/pull/129689,peterbell10,shunting314,,,
73eb4503cc0,skip,not user facing,"Enable UFMT for numpy_test files, test_xnnpack_integration.py (#129023)",.lintrunner.toml test/test_xnnpack_integration.py,https://github.com/pytorch/pytorch/pull/129023,zeshengzong,ezyang,,,
c43923a1169,skip,Untopiced,"Revert ""[Inductor] FlexAttention supports block sparse mask (#129216)""",test/inductor/test_flex_attention.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/_flex_attention.py torch/testing/_internal/hop_db.py,,,,,,
d21993bbb8b,skip,Untopiced,"Revert ""[cuDNN][SDPA] Bail out of dispatching to cuDNN for head dim > 128 on Ampere (#129587)""",aten/src/ATen/native/transformers/cuda/sdp_utils.cpp test/test_transformers.py,,,,,,
999eec8deab,skip,Untopiced,"Revert ""[cuDNN][SDPA] Remove `TORCH_CUDNN_SDPA_ENABLED=1`, enable cuDNN SDPA by default on H100 and 2nd on other archs >= sm80 (#125343)""",aten/src/ATen/Context.h aten/src/ATen/native/cudnn/MHA.cpp aten/src/ATen/native/cudnn/MHA.h aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/attention_backward.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.h docs/source/backends.rst test/dynamo/test_activation_checkpointing.py test/inductor/test_cuda_repro.py test/test_flop_counter.py test/test_transformers.py tools/autograd/derivatives.yaml torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_meta_registrations.py torch/_subclasses/fake_impls.py torch/backends/cuda/__init__.py torch/csrc/Module.cpp torch/csrc/inductor/aoti_torch/generated/c_shim_cuda.h torch/testing/_internal/common_cuda.py torch/utils/flop_counter.py torchgen/aoti/fallback_ops.py torchgen/gen_aoti_c_shim.py,,,,,,
a0dac3de31b,skip,not user facing,Noise tensor using same size/stride with input to promote performance when channel last situation. (#129467),aten/src/ATen/native/Dropout.cpp,https://github.com/pytorch/pytorch/pull/129467,Shan19900305,ezyang,,,
424068d0d22,skip,not user facing,[Windows] remove mkl shared library dependency. (#129493),setup.py,https://github.com/pytorch/pytorch/pull/129493,xuhancn,malfet,,,
22a06869f24,skip,not user facing,include jit/*.pyi (#129654),setup.py,https://github.com/pytorch/pytorch/pull/129654,randolf-scholz,ezyang,,,
98d34d849d0,skip,not user facing,Add a XPU UT to ensure lazy init (#129638),test/test_xpu.py,https://github.com/pytorch/pytorch/pull/129638,guangyey,gujinghui,,,
fe4032fe204,build_frontend,improvements,[BE][CMake] Do not use `EXEC_PROGRAM` (#129714),cmake/Modules/FindARM.cmake,https://github.com/pytorch/pytorch/pull/129714,malfet,kit1980,,,
635d6c9d66c,skip,Untopiced,[FSDP2] Ran post-acc-grad hooks manually (#129450),test/distributed/_composable/fsdp/test_fully_shard_autograd.py test/distributed/_composable/fsdp/test_fully_shard_memory.py torch/distributed/_composable/fsdp/_fsdp_collectives.py,https://github.com/pytorch/pytorch/pull/129450,awgu,weifengpy,yf225,,
eabe6574c0b,mps,Untopiced,"[metal] Parameterize group_size in int4_mm test, fix int4mm shader for group_size > 128 (#129628)",aten/src/ATen/native/mps/operations/Quantized.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/129628,manuelcandales,kimishpatel,,,
2e3ff394bf9,skip,not user facing,[inductor] optimize cpp builder configuration code (#129577),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/129577,xuhancn,jansel,jgong5,,
59eb2897f17,vulkan,not user facing,[BE][Easy] enable postponed annotations in `tools` (#129375),tools/alerts/create_alerts.py tools/autograd/gen_annotated_fn_args.py tools/autograd/gen_autograd.py tools/autograd/gen_autograd_functions.py tools/autograd/gen_inplace_or_view_type.py tools/autograd/gen_python_functions.py tools/autograd/gen_trace_type.py tools/autograd/gen_variable_factories.py tools/autograd/gen_variable_type.py tools/autograd/gen_view_funcs.py tools/autograd/load_derivatives.py tools/build_pytorch_libs.py tools/build_with_debinfo.py tools/code_analyzer/gen_op_registration_allowlist.py tools/code_analyzer/gen_operators_yaml.py tools/code_analyzer/gen_oplist.py tools/code_coverage/package/oss/init.py tools/code_coverage/package/oss/utils.py tools/code_coverage/package/tool/clang_coverage.py tools/code_coverage/package/tool/gcc_coverage.py tools/code_coverage/package/tool/parser/coverage_record.py tools/code_coverage/package/tool/parser/gcov_coverage_parser.py tools/code_coverage/package/tool/parser/llvm_coverage_parser.py tools/code_coverage/package/tool/parser/llvm_coverage_segment.py tools/code_coverage/package/tool/print_report.py tools/code_coverage/package/tool/summarize_jsons.py tools/code_coverage/package/util/setting.py tools/code_coverage/package/util/utils.py tools/coverage_plugins_package/setup.py tools/coverage_plugins_package/src/coverage_plugins/jit_plugin.py tools/download_mnist.py tools/dynamo/verify_dynamo.py tools/extract_scripts.py tools/gen_vulkan_spv.py tools/generate_torch_version.py tools/github/github_utils.py tools/iwyu/fixup.py tools/jit/gen_unboxing.py tools/linter/adapters/actionlint_linter.py tools/linter/adapters/bazel_linter.py tools/linter/adapters/black_linter.py tools/linter/adapters/clangformat_linter.py tools/linter/adapters/clangtidy_linter.py tools/linter/adapters/cmake_linter.py tools/linter/adapters/constexpr_linter.py tools/linter/adapters/exec_linter.py tools/linter/adapters/flake8_linter.py tools/linter/adapters/grep_linter.py tools/linter/adapters/lintrunner_version_linter.py tools/linter/adapters/mypy_linter.py tools/linter/adapters/nativefunctions_linter.py tools/linter/adapters/newlines_linter.py tools/linter/adapters/no_merge_conflict_csv_linter.py tools/linter/adapters/pip_init.py tools/linter/adapters/ruff_linter.py tools/linter/adapters/shellcheck_linter.py tools/linter/adapters/test_has_main_linter.py tools/linter/adapters/testowners_linter.py tools/linter/adapters/ufmt_linter.py tools/linter/adapters/workflow_consistency_linter.py tools/linter/clang_tidy/generate_build_files.py tools/lite_interpreter/gen_selected_mobile_ops_header.py tools/lldb/deploy_debugger.py tools/nightly.py tools/nvcc_fix_deps.py tools/onnx/gen_diagnostics.py tools/pyi/gen_pyi.py tools/render_junit.py tools/setup_helpers/__init__.py tools/setup_helpers/cmake.py tools/setup_helpers/cmake_utils.py tools/setup_helpers/env.py tools/setup_helpers/gen_version_header.py tools/setup_helpers/generate_code.py tools/stats/check_disabled_tests.py tools/stats/import_test_stats.py tools/stats/monitor.py tools/stats/test_dashboard.py tools/stats/upload_artifacts.py tools/stats/upload_dynamo_perf_stats.py tools/stats/upload_external_contrib_stats.py tools/stats/upload_metrics.py tools/stats/upload_sccache_stats.py tools/stats/upload_stats_lib.py tools/stats/upload_test_stat_aggregates.py tools/stats/upload_test_stats.py tools/stats/upload_test_stats_intermediate.py tools/test/gen_operators_yaml_test.py tools/test/gen_oplist_test.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_cmake.py tools/test/test_codegen.py tools/test/test_create_alerts.py tools/test/test_executorch_custom_ops.py tools/test/test_executorch_gen.py tools/test/test_executorch_signatures.py tools/test/test_gen_backend_stubs.py tools/test/test_selective_build.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/test/test_upload_test_stats.py tools/test/test_vulkan_codegen.py tools/testing/discover_tests.py tools/testing/explicit_ci_jobs.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/determinator.py tools/testing/target_determination/gen_artifact.py tools/testing/target_determination/heuristics/__init__.py tools/testing/target_determination/heuristics/correlated_with_historical_failures.py tools/testing/target_determination/heuristics/edited_by_pr.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/historical_class_failure_correlation.py tools/testing/target_determination/heuristics/historical_edited_files.py tools/testing/target_determination/heuristics/interface.py tools/testing/target_determination/heuristics/llm.py tools/testing/target_determination/heuristics/mentioned_in_pr.py tools/testing/target_determination/heuristics/previously_failed_in_pr.py tools/testing/target_determination/heuristics/profiling.py tools/testing/target_determination/heuristics/utils.py tools/testing/test_run.py tools/testing/test_selections.py,https://github.com/pytorch/pytorch/pull/129375,XuehaiPan,malfet,,,
494057d6d4e,skip,not user facing,[BE][Easy] enable postponed annotations in `torchgen` (#129376),torchgen/api/autograd.py torchgen/api/cpp.py torchgen/api/dispatcher.py torchgen/api/functionalization.py torchgen/api/lazy.py torchgen/api/native.py torchgen/api/python.py torchgen/api/structured.py torchgen/api/translate.py torchgen/api/types/signatures.py torchgen/api/types/types.py torchgen/api/types/types_base.py torchgen/api/ufunc.py torchgen/api/unboxing.py torchgen/code_template.py torchgen/context.py torchgen/dest/lazy_ir.py torchgen/dest/native_functions.py torchgen/dest/register_dispatch_key.py torchgen/dest/ufunc.py torchgen/executorch/api/custom_ops.py torchgen/executorch/api/et_cpp.py torchgen/executorch/api/types/signatures.py torchgen/executorch/api/types/types.py torchgen/executorch/api/unboxing.py torchgen/executorch/model.py torchgen/executorch/parse.py torchgen/gen.py torchgen/gen_aoti_c_shim.py torchgen/gen_backend_stubs.py torchgen/gen_executorch.py torchgen/gen_functionalization_type.py torchgen/gen_lazy_tensor.py torchgen/gen_vmap_plumbing.py torchgen/local.py torchgen/model.py torchgen/native_function_generation.py torchgen/operator_versions/gen_mobile_upgraders.py torchgen/selective_build/operator.py torchgen/selective_build/selector.py torchgen/shape_functions/gen_jit_shape_functions.py torchgen/static_runtime/config.py torchgen/static_runtime/gen_static_runtime_ops.py torchgen/static_runtime/generator.py torchgen/utils.py,https://github.com/pytorch/pytorch/pull/129376,XuehaiPan,ezyang,,,
e40f50cb87b,distributed,not user facing,Use Generic TypeAlias (PEP 585) and Union Type (PEP 604) in `.pyi` stub files (#129419),torch/_C/_aoti.pyi torch/_C/_autograd.pyi torch/_C/_distributed_autograd.pyi torch/_C/_distributed_c10d.pyi torch/_C/_distributed_rpc.pyi torch/_C/_distributed_rpc_testing.pyi torch/_C/_dynamo/compiled_autograd.pyi torch/_C/_dynamo/eval_frame.pyi torch/_C/_dynamo/guards.pyi torch/_C/_functions.pyi torch/_C/_functorch.pyi torch/_C/_lazy.pyi torch/_C/_lazy_ts_backend.pyi torch/_C/_monitor.pyi torch/_C/_profiler.pyi torch/distributed/optim/zero_redundancy_optimizer.pyi torch/fx/__init__.pyi torch/jit/_script.pyi torch/nn/parameter.pyi torch/nn/utils/rnn.pyi torch/utils/_config_typing.pyi,https://github.com/pytorch/pytorch/pull/129419,XuehaiPan,ezyang,,,
04264efab60,inductor,not user facing,Add structured logging on FXGraphCache hit (#129588),test/dynamo/test_structured_trace.py torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/129588,jamesjwu,oulgen,xmfan,,
b84036e3fb8,inductor,not user facing,[AOTI] Fix test_dynamic_scalar_abi_compatible_cpu_with_stack_allocation (#129173),test/inductor/test_aot_inductor.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/csrc/inductor/aoti_runtime/arrayref_tensor.h,https://github.com/pytorch/pytorch/pull/129173,hydeparksnow,chenyang78,,,
6897631cebb,dynamo,not user facing,Guard on inner tensor names for traceable wrapper subclasses (#129618),test/dynamo/test_subclasses.py torch/_dynamo/guards.py torch/_dynamo/source.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/129618,jbschlosser,anijain2305,,,
c12a4f2e65a,composability,Untopiced,Add decomposition for slice_scatter (#123744),test/expect/HasDecompTest.test_aten_core_operators.expect test/expect/HasDecompTest.test_has_decomposition.expect torch/_decomp/decompositions.py torch/_inductor/decomposition.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/123744,isuruf,peterbell10,,,
26d633b7213,distributed,not user facing,[BE] Correctly catch skip signals emitting from sys.exit in Sandcastle (#129731),test/distributed/test_c10d_nccl.py,https://github.com/pytorch/pytorch/pull/129731,fegin,wz337,,,
59e4e925569,autograd_frontend,not user facing,sdp::SDPBackend::flash_attention support PrivateUse1 (#126392),aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.h test/cpp_extensions/open_registration_extension.cpp test/expect/HasDecompTest.test_has_decomposition.expect test/test_namedtuple_return_api.py test/test_transformers.py tools/autograd/derivatives.yaml torch/_C/__init__.pyi.in torch/csrc/Module.cpp,https://github.com/pytorch/pytorch/pull/126392,1274085042,drisspg,,,
db4c7bb7fc2,skip,not user facing,Refine typing annotation for compile (#129136),torch/__init__.py,https://github.com/pytorch/pytorch/pull/129136,zrr1999,ezyang,,,
6120aa37187,skip,not user facing,"[nn-module] Use standard dict for _parameters, _modules and _buffers (#129164)",torch/nn/modules/module.py,https://github.com/pytorch/pytorch/pull/129164,anijain2305,mikaylagawarecki,,,
99456a612b1,inductor,Untopiced,[AOTI] Properly indent launchKernel calls in AOTInductor (#129616),torch/_inductor/codegen/cpp_wrapper_cuda.py,https://github.com/pytorch/pytorch/pull/129616,YUNQIUGUO,ColinPeppler,,,
45f3e20527c,python_frontend,improvements,Improve error message for weights_only load (#129705),test/test_serialization.py torch/_weights_only_unpickler.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/129705,mikaylagawarecki,albanD,vmoens,,
bc8883a7c44,distributed,not user facing,fix the error msg in device_mesh (#129747),torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/129747,zhouzaida,awgu,wconstab,,
5b96a552dfd,mps,Untopiced,Add a check and error message for no support on MPS for conv with output_channels > 2^16 (#129484),aten/src/ATen/native/mps/operations/Convolution.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/129484,jhavukainen,kulinseth,,,
f5ff1a3ab9e,package/deploy,not user facing,[BE] enforce style for empty lines in import segments (#129751),.lintrunner.toml tools/linter/adapters/ufmt_linter.py torch/_numpy/__init__.py torch/package/_package_pickler.py,https://github.com/pytorch/pytorch/pull/129751,XuehaiPan,malfet,,,
0a337613f81,Uncategorized,Untopiced,Fix typo in stack_module_state doc (#129126),torch/_functorch/functional_call.py,https://github.com/pytorch/pytorch/pull/129126,mKabouri,zou3519,,,
7bda23ef849,skip,not user facing,[BE]: Update ruff to 0.5.0 (#129744),.lintrunner.toml pyproject.toml test/test_autograd.py test/test_binary_ufuncs.py tools/linter/adapters/ruff_linter.py,https://github.com/pytorch/pytorch/pull/129744,Skylion007,ezyang,,,
f06e3a15699,releng,not user facing,[Split Build] Make script not crash if split build is not set (#129774),.circleci/scripts/binary_upload.sh,https://github.com/pytorch/pytorch/pull/129774,PaliC,atalman,,,
24f69eef6ad,distributed,Untopiced,[FSDP2] Ran reduce-scatter copy-in in default stream (#129721),test/distributed/_composable/fsdp/test_fully_shard_comm.py test/distributed/_composable/fsdp/test_fully_shard_memory.py torch/distributed/_composable/fsdp/_fsdp_collectives.py torch/distributed/_composable/fsdp/_fsdp_param_group.py torch/distributed/_composable/fsdp/_fsdp_state.py,https://github.com/pytorch/pytorch/pull/129721,awgu,weifengpy,yifuwang,,
fa6c0fe3e4e,skip,Untopiced,"Revert ""Conversions between strided and jagged layouts for Nested Tensors (#115749)""",aten/src/ATen/FunctionalInverses.cpp aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorFactories.cpp aten/src/ATen/native/nested/NestedTensorMath.cpp test/expect/HasDecompTest.test_has_decomposition.expect test/test_nestedtensor.py tools/autograd/derivatives.yaml tools/autograd/gen_inplace_or_view_type.py torch/_subclasses/fake_impls.py torch/nested/__init__.py torch/nested/_internal/nested_tensor.py torch/nested/_internal/ops.py,,,,,,
00d7bba2fa8,skip,Untopiced,"Revert ""[BE] enforce style for empty lines in import segments (#129751)""",.lintrunner.toml tools/linter/adapters/ufmt_linter.py torch/_numpy/__init__.py torch/package/_package_pickler.py,,,,,,
83caf4960f3,skip,Untopiced,"Revert ""Use Generic TypeAlias (PEP 585) and Union Type (PEP 604) in `.pyi` stub files (#129419)""",torch/_C/_aoti.pyi torch/_C/_autograd.pyi torch/_C/_distributed_autograd.pyi torch/_C/_distributed_c10d.pyi torch/_C/_distributed_rpc.pyi torch/_C/_distributed_rpc_testing.pyi torch/_C/_dynamo/compiled_autograd.pyi torch/_C/_dynamo/eval_frame.pyi torch/_C/_dynamo/guards.pyi torch/_C/_functions.pyi torch/_C/_functorch.pyi torch/_C/_lazy.pyi torch/_C/_lazy_ts_backend.pyi torch/_C/_monitor.pyi torch/_C/_profiler.pyi torch/distributed/optim/zero_redundancy_optimizer.pyi torch/fx/__init__.pyi torch/jit/_script.pyi torch/nn/parameter.pyi torch/nn/utils/rnn.pyi torch/utils/_config_typing.pyi,,,,,,
6063bb9d45b,skip,Untopiced,"Revert ""[BE][Easy] enable postponed annotations in `torchgen` (#129376)""",torchgen/api/autograd.py torchgen/api/cpp.py torchgen/api/dispatcher.py torchgen/api/functionalization.py torchgen/api/lazy.py torchgen/api/native.py torchgen/api/python.py torchgen/api/structured.py torchgen/api/translate.py torchgen/api/types/signatures.py torchgen/api/types/types.py torchgen/api/types/types_base.py torchgen/api/ufunc.py torchgen/api/unboxing.py torchgen/code_template.py torchgen/context.py torchgen/dest/lazy_ir.py torchgen/dest/native_functions.py torchgen/dest/register_dispatch_key.py torchgen/dest/ufunc.py torchgen/executorch/api/custom_ops.py torchgen/executorch/api/et_cpp.py torchgen/executorch/api/types/signatures.py torchgen/executorch/api/types/types.py torchgen/executorch/api/unboxing.py torchgen/executorch/model.py torchgen/executorch/parse.py torchgen/gen.py torchgen/gen_aoti_c_shim.py torchgen/gen_backend_stubs.py torchgen/gen_executorch.py torchgen/gen_functionalization_type.py torchgen/gen_lazy_tensor.py torchgen/gen_vmap_plumbing.py torchgen/local.py torchgen/model.py torchgen/native_function_generation.py torchgen/operator_versions/gen_mobile_upgraders.py torchgen/selective_build/operator.py torchgen/selective_build/selector.py torchgen/shape_functions/gen_jit_shape_functions.py torchgen/static_runtime/config.py torchgen/static_runtime/gen_static_runtime_ops.py torchgen/static_runtime/generator.py torchgen/utils.py,,,,,,
a32ce5ce344,skip,Untopiced,"Revert ""[BE][Easy] enable postponed annotations in `tools` (#129375)""",tools/alerts/create_alerts.py tools/autograd/gen_annotated_fn_args.py tools/autograd/gen_autograd.py tools/autograd/gen_autograd_functions.py tools/autograd/gen_inplace_or_view_type.py tools/autograd/gen_python_functions.py tools/autograd/gen_trace_type.py tools/autograd/gen_variable_factories.py tools/autograd/gen_variable_type.py tools/autograd/gen_view_funcs.py tools/autograd/load_derivatives.py tools/build_pytorch_libs.py tools/build_with_debinfo.py tools/code_analyzer/gen_op_registration_allowlist.py tools/code_analyzer/gen_operators_yaml.py tools/code_analyzer/gen_oplist.py tools/code_coverage/package/oss/init.py tools/code_coverage/package/oss/utils.py tools/code_coverage/package/tool/clang_coverage.py tools/code_coverage/package/tool/gcc_coverage.py tools/code_coverage/package/tool/parser/coverage_record.py tools/code_coverage/package/tool/parser/gcov_coverage_parser.py tools/code_coverage/package/tool/parser/llvm_coverage_parser.py tools/code_coverage/package/tool/parser/llvm_coverage_segment.py tools/code_coverage/package/tool/print_report.py tools/code_coverage/package/tool/summarize_jsons.py tools/code_coverage/package/util/setting.py tools/code_coverage/package/util/utils.py tools/coverage_plugins_package/setup.py tools/coverage_plugins_package/src/coverage_plugins/jit_plugin.py tools/download_mnist.py tools/dynamo/verify_dynamo.py tools/extract_scripts.py tools/gen_vulkan_spv.py tools/generate_torch_version.py tools/github/github_utils.py tools/iwyu/fixup.py tools/jit/gen_unboxing.py tools/linter/adapters/actionlint_linter.py tools/linter/adapters/bazel_linter.py tools/linter/adapters/black_linter.py tools/linter/adapters/clangformat_linter.py tools/linter/adapters/clangtidy_linter.py tools/linter/adapters/cmake_linter.py tools/linter/adapters/constexpr_linter.py tools/linter/adapters/exec_linter.py tools/linter/adapters/flake8_linter.py tools/linter/adapters/grep_linter.py tools/linter/adapters/lintrunner_version_linter.py tools/linter/adapters/mypy_linter.py tools/linter/adapters/nativefunctions_linter.py tools/linter/adapters/newlines_linter.py tools/linter/adapters/no_merge_conflict_csv_linter.py tools/linter/adapters/pip_init.py tools/linter/adapters/ruff_linter.py tools/linter/adapters/shellcheck_linter.py tools/linter/adapters/test_has_main_linter.py tools/linter/adapters/testowners_linter.py tools/linter/adapters/ufmt_linter.py tools/linter/adapters/workflow_consistency_linter.py tools/linter/clang_tidy/generate_build_files.py tools/lite_interpreter/gen_selected_mobile_ops_header.py tools/lldb/deploy_debugger.py tools/nightly.py tools/nvcc_fix_deps.py tools/onnx/gen_diagnostics.py tools/pyi/gen_pyi.py tools/render_junit.py tools/setup_helpers/__init__.py tools/setup_helpers/cmake.py tools/setup_helpers/cmake_utils.py tools/setup_helpers/env.py tools/setup_helpers/gen_version_header.py tools/setup_helpers/generate_code.py tools/stats/check_disabled_tests.py tools/stats/import_test_stats.py tools/stats/monitor.py tools/stats/test_dashboard.py tools/stats/upload_artifacts.py tools/stats/upload_dynamo_perf_stats.py tools/stats/upload_external_contrib_stats.py tools/stats/upload_metrics.py tools/stats/upload_sccache_stats.py tools/stats/upload_stats_lib.py tools/stats/upload_test_stat_aggregates.py tools/stats/upload_test_stats.py tools/stats/upload_test_stats_intermediate.py tools/test/gen_operators_yaml_test.py tools/test/gen_oplist_test.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_cmake.py tools/test/test_codegen.py tools/test/test_create_alerts.py tools/test/test_executorch_custom_ops.py tools/test/test_executorch_gen.py tools/test/test_executorch_signatures.py tools/test/test_gen_backend_stubs.py tools/test/test_selective_build.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/test/test_upload_test_stats.py tools/test/test_vulkan_codegen.py tools/testing/discover_tests.py tools/testing/explicit_ci_jobs.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/determinator.py tools/testing/target_determination/gen_artifact.py tools/testing/target_determination/heuristics/__init__.py tools/testing/target_determination/heuristics/correlated_with_historical_failures.py tools/testing/target_determination/heuristics/edited_by_pr.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/historical_class_failure_correlation.py tools/testing/target_determination/heuristics/historical_edited_files.py tools/testing/target_determination/heuristics/interface.py tools/testing/target_determination/heuristics/llm.py tools/testing/target_determination/heuristics/mentioned_in_pr.py tools/testing/target_determination/heuristics/previously_failed_in_pr.py tools/testing/target_determination/heuristics/profiling.py tools/testing/target_determination/heuristics/utils.py tools/testing/test_run.py tools/testing/test_selections.py,,,,,,
c0782e7c818,profiler,new features,Kineto profiler: collecting observer traces from C++ child threads (#128743),test/profiler/test_cpp_thread.cpp test/profiler/test_cpp_thread.py torch/csrc/autograd/profiler_kineto.cpp torch/csrc/autograd/profiler_kineto.h,https://github.com/pytorch/pytorch/pull/128743,LESSuseLESS,aaronenyeshi,,,
3d962178917,skip,Untopiced,"Revert ""[BE][Easy] use `pathlib.Path` instead of `dirname` / `""..""` / `pardir` (#129374)""",.circleci/codegen_validation/normalize_yaml_fragment.py .github/scripts/delete_old_branches.py .github/scripts/ensure_actions_will_cancel.py .github/scripts/generate_binary_build_matrix.py .github/scripts/gitutils.py .github/scripts/lint_native_functions.py .github/scripts/test_gitutils.py benchmarks/dynamo/ci_expected_accuracy/update_expected.py docs/source/scripts/build_opsets.py docs/source/scripts/build_quantization_configs.py docs/source/scripts/exportdb/generate_example_rst.py scripts/compile_tests/update_failures.py test/jit/fixtures_srcs/generate_models.py test/jit/test_backend_nnapi.py test/mobile/test_bytecode.py test/mobile/test_upgrader_codegen.py test/mobile/test_upgraders.py test/onnx/onnx_test_common.py test/quantization/core/test_docs.py test/test_typing.py tools/amd_build/build_amd.py tools/build_libtorch.py tools/code_coverage/package/oss/utils.py tools/code_coverage/package/util/setting.py tools/gen_vulkan_spv.py tools/linter/adapters/s3_init.py tools/onnx/update_default_opset_version.py tools/setup_helpers/cmake.py tools/setup_helpers/gen.py tools/setup_helpers/gen_unboxing.py tools/setup_helpers/generate_code.py tools/stats/export_test_times.py tools/stats/import_test_stats.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_gen_backend_stubs.py tools/test/test_test_run.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/testing/discover_tests.py tools/testing/do_target_determination_for_s3.py tools/testing/explicit_ci_jobs.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/gen_artifact.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/llm.py tools/testing/target_determination/heuristics/previously_failed_in_pr.py tools/testing/target_determination/heuristics/utils.py tools/testing/test_selections.py tools/vscode_settings.py torch/_inductor/runtime/compile_tasks.py torch/testing/_internal/common_utils.py torchgen/gen_backend_stubs.py torchgen/gen_lazy_tensor.py,,,,,,
dfd55d1714c,skip,Untopiced,"Revert ""[cond] inlining into one of the branches when pred is a python constant (#128709)""",test/dynamo/test_export.py test/dynamo/test_higher_order_ops.py test/export/test_export.py test/export/test_verifier.py test/functorch/test_aotdispatch.py test/functorch/test_control_flow.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/cond.py,,,,,,
2bc6f329b29,python_frontend,improvements,Make PyTorch argparser understand complex (#129580),torch/csrc/Dtype.h torch/csrc/utils/python_arg_parser.h,https://github.com/pytorch/pytorch/pull/129580,malfet,albanD,,,
065c386990d,skip,Untopiced,Allow get attributes on DDP similar to FSDP (#128620),torch/nn/parallel/distributed.py,https://github.com/pytorch/pytorch/pull/128620,mayank31398,,,,
7b5a8424a10,skip,not user facing,[GPT-fast] Update micro benchmark numbers as A100-50G (#129799),benchmarks/gpt_fast/benchmark.py benchmarks/gpt_fast/generate.py,https://github.com/pytorch/pytorch/pull/129799,yanboliang,Chillee,,,
ec47d4d9a84,skip,not user facing,[Inductor] FlexAttention supports block sparse mask (#129216),test/inductor/test_flex_attention.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/_flex_attention.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/129216,yanboliang,Chillee,,,
3ef44df6670,skip,not user facing,[ts-migration] support prim::SetAttr and fix prim::GetAttr (#129440),test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/129440,BoyuanFeng,angelayi,,,
89696db4b05,jit,Untopiced,"Revert ""[LLVM/TensorExpr] Update for an API change in LLVM 18."" (#129797)",torch/csrc/jit/tensorexpr/llvm_codegen.cpp,https://github.com/pytorch/pytorch/pull/129797,WenleiHe,dcci,,,
5d1763d1593,Uncategorized,Untopiced,Add lcnet to the inline_inbuilt_nn_module list (#129775),benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_timm_amp_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_amp_freezing_inference.csv benchmarks/dynamo/timm_models.py benchmarks/dynamo/torchbench.py,https://github.com/pytorch/pytorch/pull/129775,anijain2305,mlazos,,,
a676b7c5f3c,skip,not user facing,Add XGLMForCausalLM to the flaky model list (#129776),benchmarks/dynamo/check_accuracy.py,https://github.com/pytorch/pytorch/pull/129776,anijain2305,mlazos,,,
58f346c874a,skip,not user facing,[inductor] split cpu vec isa to dedicate file (keep git history) (#129789),test/inductor/test_cpu_repro.py test/inductor/test_cpu_select_algorithm.py test/inductor/test_extension_backend.py torch/_inductor/codecache.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/cpp_builder.py torch/_inductor/cpu_vec_isa.py,https://github.com/pytorch/pytorch/pull/129789,xuhancn,jansel,jgong5,,
8a67daf2834,vulkan,not user facing,[BE][Easy] enable postponed annotations in `tools` (#129375),tools/alerts/create_alerts.py tools/autograd/gen_annotated_fn_args.py tools/autograd/gen_autograd.py tools/autograd/gen_autograd_functions.py tools/autograd/gen_inplace_or_view_type.py tools/autograd/gen_python_functions.py tools/autograd/gen_trace_type.py tools/autograd/gen_variable_factories.py tools/autograd/gen_variable_type.py tools/autograd/gen_view_funcs.py tools/autograd/load_derivatives.py tools/build_pytorch_libs.py tools/build_with_debinfo.py tools/code_analyzer/gen_op_registration_allowlist.py tools/code_analyzer/gen_operators_yaml.py tools/code_analyzer/gen_oplist.py tools/code_coverage/package/oss/init.py tools/code_coverage/package/oss/utils.py tools/code_coverage/package/tool/clang_coverage.py tools/code_coverage/package/tool/gcc_coverage.py tools/code_coverage/package/tool/parser/coverage_record.py tools/code_coverage/package/tool/parser/gcov_coverage_parser.py tools/code_coverage/package/tool/parser/llvm_coverage_parser.py tools/code_coverage/package/tool/parser/llvm_coverage_segment.py tools/code_coverage/package/tool/print_report.py tools/code_coverage/package/tool/summarize_jsons.py tools/code_coverage/package/util/setting.py tools/code_coverage/package/util/utils.py tools/coverage_plugins_package/setup.py tools/coverage_plugins_package/src/coverage_plugins/jit_plugin.py tools/download_mnist.py tools/dynamo/verify_dynamo.py tools/extract_scripts.py tools/gen_vulkan_spv.py tools/generate_torch_version.py tools/github/github_utils.py tools/iwyu/fixup.py tools/jit/gen_unboxing.py tools/linter/adapters/actionlint_linter.py tools/linter/adapters/bazel_linter.py tools/linter/adapters/black_linter.py tools/linter/adapters/clangformat_linter.py tools/linter/adapters/clangtidy_linter.py tools/linter/adapters/cmake_linter.py tools/linter/adapters/constexpr_linter.py tools/linter/adapters/exec_linter.py tools/linter/adapters/flake8_linter.py tools/linter/adapters/grep_linter.py tools/linter/adapters/lintrunner_version_linter.py tools/linter/adapters/mypy_linter.py tools/linter/adapters/nativefunctions_linter.py tools/linter/adapters/newlines_linter.py tools/linter/adapters/no_merge_conflict_csv_linter.py tools/linter/adapters/pip_init.py tools/linter/adapters/ruff_linter.py tools/linter/adapters/shellcheck_linter.py tools/linter/adapters/test_has_main_linter.py tools/linter/adapters/testowners_linter.py tools/linter/adapters/ufmt_linter.py tools/linter/adapters/workflow_consistency_linter.py tools/linter/clang_tidy/generate_build_files.py tools/lite_interpreter/gen_selected_mobile_ops_header.py tools/lldb/deploy_debugger.py tools/nightly.py tools/nvcc_fix_deps.py tools/onnx/gen_diagnostics.py tools/pyi/gen_pyi.py tools/render_junit.py tools/setup_helpers/__init__.py tools/setup_helpers/cmake.py tools/setup_helpers/cmake_utils.py tools/setup_helpers/env.py tools/setup_helpers/gen_version_header.py tools/setup_helpers/generate_code.py tools/stats/check_disabled_tests.py tools/stats/import_test_stats.py tools/stats/monitor.py tools/stats/test_dashboard.py tools/stats/upload_artifacts.py tools/stats/upload_dynamo_perf_stats.py tools/stats/upload_external_contrib_stats.py tools/stats/upload_metrics.py tools/stats/upload_sccache_stats.py tools/stats/upload_stats_lib.py tools/stats/upload_test_stat_aggregates.py tools/stats/upload_test_stats.py tools/stats/upload_test_stats_intermediate.py tools/test/gen_operators_yaml_test.py tools/test/gen_oplist_test.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_cmake.py tools/test/test_codegen.py tools/test/test_create_alerts.py tools/test/test_executorch_custom_ops.py tools/test/test_executorch_gen.py tools/test/test_executorch_signatures.py tools/test/test_gen_backend_stubs.py tools/test/test_selective_build.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/test/test_upload_test_stats.py tools/test/test_vulkan_codegen.py tools/testing/discover_tests.py tools/testing/explicit_ci_jobs.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/determinator.py tools/testing/target_determination/gen_artifact.py tools/testing/target_determination/heuristics/__init__.py tools/testing/target_determination/heuristics/correlated_with_historical_failures.py tools/testing/target_determination/heuristics/edited_by_pr.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/historical_class_failure_correlation.py tools/testing/target_determination/heuristics/historical_edited_files.py tools/testing/target_determination/heuristics/interface.py tools/testing/target_determination/heuristics/llm.py tools/testing/target_determination/heuristics/mentioned_in_pr.py tools/testing/target_determination/heuristics/previously_failed_in_pr.py tools/testing/target_determination/heuristics/profiling.py tools/testing/target_determination/heuristics/utils.py tools/testing/test_run.py tools/testing/test_selections.py,https://github.com/pytorch/pytorch/pull/129375,XuehaiPan,malfet,,,
9120992c729,skip,not user facing,[BE][Easy] enable postponed annotations in `torchgen` (#129376),torchgen/api/autograd.py torchgen/api/cpp.py torchgen/api/dispatcher.py torchgen/api/functionalization.py torchgen/api/lazy.py torchgen/api/native.py torchgen/api/python.py torchgen/api/structured.py torchgen/api/translate.py torchgen/api/types/signatures.py torchgen/api/types/types.py torchgen/api/types/types_base.py torchgen/api/ufunc.py torchgen/api/unboxing.py torchgen/code_template.py torchgen/context.py torchgen/dest/lazy_ir.py torchgen/dest/native_functions.py torchgen/dest/register_dispatch_key.py torchgen/dest/ufunc.py torchgen/executorch/api/custom_ops.py torchgen/executorch/api/et_cpp.py torchgen/executorch/api/types/signatures.py torchgen/executorch/api/types/types.py torchgen/executorch/api/unboxing.py torchgen/executorch/model.py torchgen/executorch/parse.py torchgen/gen.py torchgen/gen_aoti_c_shim.py torchgen/gen_backend_stubs.py torchgen/gen_executorch.py torchgen/gen_functionalization_type.py torchgen/gen_lazy_tensor.py torchgen/gen_vmap_plumbing.py torchgen/local.py torchgen/model.py torchgen/native_function_generation.py torchgen/operator_versions/gen_mobile_upgraders.py torchgen/selective_build/operator.py torchgen/selective_build/selector.py torchgen/shape_functions/gen_jit_shape_functions.py torchgen/static_runtime/config.py torchgen/static_runtime/gen_static_runtime_ops.py torchgen/static_runtime/generator.py torchgen/utils.py,https://github.com/pytorch/pytorch/pull/129376,XuehaiPan,ezyang,,,
56935684c3d,distributed,not user facing,Use Generic TypeAlias (PEP 585) and Union Type (PEP 604) in `.pyi` stub files (#129419),torch/_C/_aoti.pyi torch/_C/_autograd.pyi torch/_C/_distributed_autograd.pyi torch/_C/_distributed_c10d.pyi torch/_C/_distributed_rpc.pyi torch/_C/_distributed_rpc_testing.pyi torch/_C/_dynamo/compiled_autograd.pyi torch/_C/_dynamo/eval_frame.pyi torch/_C/_dynamo/guards.pyi torch/_C/_functions.pyi torch/_C/_functorch.pyi torch/_C/_lazy.pyi torch/_C/_lazy_ts_backend.pyi torch/_C/_monitor.pyi torch/_C/_profiler.pyi torch/distributed/optim/zero_redundancy_optimizer.pyi torch/fx/__init__.pyi torch/jit/_script.pyi torch/nn/parameter.pyi torch/nn/utils/rnn.pyi torch/utils/_config_typing.pyi,https://github.com/pytorch/pytorch/pull/129419,XuehaiPan,ezyang,,,
b0e5c9514df,skip,not user facing,use shutil.which in check_compiler_ok_for_platform (#129069),torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/129069,imba-tjd,ezyang,,,
18ae3bab2fa,distributed,Untopiced,[Pipelining] Support separate dw_runner for PipelineStage (#128983),test/distributed/pipelining/test_stage.py torch/distributed/pipelining/stage.py,https://github.com/pytorch/pytorch/pull/128983,wconstab,H-Huang,,,
a6da01bd01b,distributed,Untopiced,[pipelining] Support arbitrary stage ordering on ranks (#128976),test/distributed/pipelining/schedule_registry.py test/distributed/pipelining/test_schedule.py torch/distributed/pipelining/schedules.py torch/distributed/pipelining/stage.py,https://github.com/pytorch/pytorch/pull/128976,H-Huang,wconstab,,,
13d4be1dc72,distributed,Untopiced,[pipelining] Support W action for schedules (#129233),test/distributed/pipelining/schedule_registry.py test/distributed/pipelining/test_schedule.py torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/129233,H-Huang,wconstab,,,
e34b7e6af39,skip,Untopiced,[halide-backend] Initial implementation of HalideKernel and HalideScheduling (#126417),test/inductor/test_halide.py test/inductor/test_torchinductor.py torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/halide.py torch/_inductor/config.py torch/_inductor/runtime/hints.py,https://github.com/pytorch/pytorch/pull/126417,jansel,eellison,shunting314,,
da5f37515eb,skip,Untopiced,[halide-backend] Generate standalone runtime (#129025),test/inductor/test_halide.py test/inductor/test_torchinductor.py torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/halide.py torch/_inductor/config.py torch/_inductor/runtime/hints.py,https://github.com/pytorch/pytorch/pull/129025,jansel,eellison,shunting314,,
86cadc63851,inductor,Untopiced,[halide-backend] Dimension-based indexing (#129026),test/inductor/test_torchinductor.py torch/_inductor/codegen/halide.py torch/_inductor/sizevars.py torch/utils/_sympy/symbol.py,https://github.com/pytorch/pytorch/pull/129026,jansel,eellison,shunting314,,
b93bf55b6a2,inductor,Untopiced,[halide-backend] Add GPU support (#127506),test/inductor/test_halide.py test/inductor/test_torchinductor.py torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/wrapper.py torch/_inductor/config.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/kernel/mm.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/127506,jansel,eellison,shunting314,,
4cb8cb04a75,inductor,Untopiced,[halide-backend] Enable bfloat16 support (#129036),test/inductor/test_torchinductor.py torch/_inductor/codegen/halide.py torch/_inductor/runtime/hints.py,https://github.com/pytorch/pytorch/pull/129036,jansel,eellison,shunting314,,
a18eb651d35,skip,Untopiced,[halide-backend] Disable split reductions for Halide (#129320),torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/halide.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/129320,jansel,eellison,shunting314,,
9ae78a578ca,skip,Untopiced,[halide-backend] Support manual schedules (#129321),test/inductor/test_halide.py torch/_inductor/codecache.py torch/_inductor/codegen/halide.py torch/_inductor/runtime/hints.py,https://github.com/pytorch/pytorch/pull/129321,jansel,shunting314,,,
7837a12474c,package/deploy,not user facing,[BE] enforce style for empty lines in import segments (#129751),.lintrunner.toml pyproject.toml tools/linter/adapters/ufmt_linter.py torch/_numpy/__init__.py torch/package/_package_pickler.py,https://github.com/pytorch/pytorch/pull/129751,XuehaiPan,malfet,,,
6d75604ef13,skip,Untopiced,[BE][Easy] replace `import pathlib` with `from pathlib import Path` (#129426),aten/src/ATen/nnapi/codegen.py benchmarks/dynamo/ci_expected_accuracy/update_expected.py benchmarks/dynamo/common.py scripts/compile_tests/update_failures.py test/distributed/nn/jit/test_instantiator.py test/export/test_serialize.py test/inductor/test_debug_trace.py test/jit/test_save_load.py test/lazy/test_ts_opinfo.py test/run_test.py test/test_serialization.py test/test_tensorboard.py test/torch_np/check_tests_conform.py test/torch_np/numpy_tests/core/test_multiarray.py tools/onnx/update_default_opset_version.py tools/setup_helpers/generate_code.py tools/stats/export_test_times.py tools/stats/import_test_stats.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_test_run.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/testing/do_target_determination_for_s3.py tools/testing/explicit_ci_jobs.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/gen_artifact.py torch/jit/_monkeytype_config.py torch/utils/data/datapipes/gen_pyi.py torch/utils/model_dump/__init__.py torchgen/gen_backend_stubs.py torchgen/gen_lazy_tensor.py,https://github.com/pytorch/pytorch/pull/129426,XuehaiPan,malfet,,,
3fec0efd344,inductor,not user facing,[Inductor][CPP] Support vectorization of bitwise fn (#129733),test/inductor/test_cpu_repro.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/129733,leslie-fang-intel,Skylion007,jgong5,,
67c9ec2b6d1,skip,not user facing,[inductor] unificate toolchain code. (#129816),torch/_inductor/codecache.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/cpp_builder.py torch/_inductor/cpu_vec_isa.py,https://github.com/pytorch/pytorch/pull/129816,xuhancn,jansel,,,
2effbcfcd8a,skip,Untopiced,"Revert ""[BE][Easy] replace `import pathlib` with `from pathlib import Path` (#129426)""",aten/src/ATen/nnapi/codegen.py benchmarks/dynamo/ci_expected_accuracy/update_expected.py benchmarks/dynamo/common.py scripts/compile_tests/update_failures.py test/distributed/nn/jit/test_instantiator.py test/export/test_serialize.py test/inductor/test_debug_trace.py test/jit/test_save_load.py test/lazy/test_ts_opinfo.py test/run_test.py test/test_serialization.py test/test_tensorboard.py test/torch_np/check_tests_conform.py test/torch_np/numpy_tests/core/test_multiarray.py tools/onnx/update_default_opset_version.py tools/setup_helpers/generate_code.py tools/stats/export_test_times.py tools/stats/import_test_stats.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_test_run.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/testing/do_target_determination_for_s3.py tools/testing/explicit_ci_jobs.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/gen_artifact.py torch/jit/_monkeytype_config.py torch/utils/data/datapipes/gen_pyi.py torch/utils/model_dump/__init__.py torchgen/gen_backend_stubs.py torchgen/gen_lazy_tensor.py,,,,,,
4ee1cb9b955,skip,Untopiced,[BE][Easy] replace `import pathlib` with `from pathlib import Path` (#129426),aten/src/ATen/nnapi/codegen.py benchmarks/dynamo/ci_expected_accuracy/update_expected.py benchmarks/dynamo/common.py scripts/compile_tests/update_failures.py test/distributed/nn/jit/test_instantiator.py test/export/test_serialize.py test/inductor/test_debug_trace.py test/jit/test_save_load.py test/lazy/test_ts_opinfo.py test/run_test.py test/test_serialization.py test/test_tensorboard.py test/torch_np/check_tests_conform.py test/torch_np/numpy_tests/core/test_multiarray.py tools/onnx/update_default_opset_version.py tools/setup_helpers/generate_code.py tools/stats/export_test_times.py tools/stats/import_test_stats.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_test_run.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/testing/do_target_determination_for_s3.py tools/testing/explicit_ci_jobs.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/gen_artifact.py torch/jit/_monkeytype_config.py torch/utils/model_dump/__init__.py torchgen/gen_backend_stubs.py torchgen/gen_lazy_tensor.py,https://github.com/pytorch/pytorch/pull/129426,XuehaiPan,malfet,,,
fe5424d0f86,skip,not user facing,Enable UFMT on test/test_public_bindings.py (#128389),.lintrunner.toml test/test_public_bindings.py,https://github.com/pytorch/pytorch/pull/128389,dilililiwhy,malfet,,,
35a197defa1,inductor,not user facing,[Inductor][CPP] Enable Quantized Linear GEMM Template with FP32 output (#128825),test/inductor/test_cpu_select_algorithm.py test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/mkldnn_lowerings.py torch/_inductor/utils.py torch/testing/_internal/common_quantization.py,https://github.com/pytorch/pytorch/pull/128825,leslie-fang-intel,jansel,jgong5,,
1a689ea38cd,inductor,not user facing,[Inductor][CPP] Enable Quantized Linear GEMM Template with INT8 output and Unary Post Op (#129048),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/mkldnn_lowerings.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/129048,leslie-fang-intel,jansel,jgong5,,
8a5fda03772,skip,not user facing,added type hints for  __contains__ (#129653),test/typing/reveal/tensor_constructors.py tools/pyi/gen_pyi.py torch/_tensor.py,https://github.com/pytorch/pytorch/pull/129653,randolf-scholz,ezyang,,,
fff633f087f,releng,Untopiced,[CI] Enable AOT inductor FP32 accuracy test for CPU (#129040),.ci/pytorch/test.sh .github/workflows/inductor.yml benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_huggingface_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_timm_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_torchbench_freezing_inference.csv,https://github.com/pytorch/pytorch/pull/129040,WeizhuoZhang-intel,chuanqi129,desertfire,malfet,
68484621fe6,functorch,not user facing,[cuDNN][functorch] Bump tolerances for `nn.functional.conv2d` in `test_vmap_autograd_grad` (#129796),test/functorch/test_ops.py,https://github.com/pytorch/pytorch/pull/129796,eqy,Skylion007,,,
4dd3cff234b,skip,not user facing,[CUDA] Fix more `DeviceIndex` printing (#128540),aten/src/ATen/cuda/CUDAContext.cpp,https://github.com/pytorch/pytorch/pull/128540,eqy,Skylion007,nWEIdia,,
8755e035d28,cuda,bug fixes,[CUDA][Pooling] Fix 64-bit indexing in `avg_pool_2d` backward attempt 2 (#129818),aten/src/ATen/native/cuda/AveragePool2d.cu test/test_nn.py,https://github.com/pytorch/pytorch/pull/129818,eqy,Chillee,Skylion007,,
7b0e9a27baf,skip,not user facing,Restore `allowed_info` in OOM message when applicable (#129546),c10/cuda/CUDACachingAllocator.cpp,https://github.com/pytorch/pytorch/pull/129546,eqy,Skylion007,,,
f845a7a91a6,skip,Untopiced,"[cuDNN][SDPA] Remove `TORCH_CUDNN_SDPA_ENABLED=1`, enable cuDNN SDPA by default on H100 and 2nd on other archs >= sm80 (#125343)",aten/src/ATen/Context.h aten/src/ATen/native/cudnn/MHA.cpp aten/src/ATen/native/cudnn/MHA.h aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/attention_backward.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.h docs/source/backends.rst test/dynamo/test_activation_checkpointing.py test/inductor/test_cuda_repro.py test/test_flop_counter.py test/test_transformers.py tools/autograd/derivatives.yaml torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_meta_registrations.py torch/_subclasses/fake_impls.py torch/backends/cuda/__init__.py torch/csrc/Module.cpp torch/csrc/inductor/aoti_torch/generated/c_shim_cuda.h torch/testing/_internal/common_cuda.py torch/utils/flop_counter.py torchgen/aoti/fallback_ops.py torchgen/gen_aoti_c_shim.py,https://github.com/pytorch/pytorch/pull/125343,eqy,Skylion007,,,
24b6c5a41fa,skip,not user facing,[cuDNN][SDPA] Bail out of dispatching to cuDNN for head dim > 128 on Ampere (#129587),aten/src/ATen/native/transformers/cuda/sdp_utils.cpp test/test_transformers.py,https://github.com/pytorch/pytorch/pull/129587,eqy,Skylion007,drisspg,,
e62073d7997,dynamo,not user facing,[dynamo] Skip FUNCTION_MATCH on method-wrapper objects (#129830),test/dynamo/test_misc.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/129830,anijain2305,jansel,,,
eb1583dbc1a,jit,Untopiced,[2/N] Fix clang-tidy warnings in torch/csrc/jit/serialization (#129300),torch/csrc/jit/serialization/callstack_debug_info_serialization.cpp torch/csrc/jit/serialization/callstack_debug_info_serialization.h torch/csrc/jit/serialization/export.cpp torch/csrc/jit/serialization/export_bytecode.cpp torch/csrc/jit/serialization/export_module.cpp torch/csrc/jit/serialization/flatbuffer_serializer.cpp torch/csrc/jit/serialization/import.cpp torch/csrc/jit/serialization/import_export_helpers.cpp torch/csrc/jit/serialization/pickler.cpp torch/csrc/jit/serialization/pickler.h,https://github.com/pytorch/pytorch/pull/129300,cyyever,ezyang,,,
04a0d856207,releng,Untopiced,[BE] Print all pip packages installed on the system after TorchChat (#129809),.ci/pytorch/common_utils.sh,https://github.com/pytorch/pytorch/pull/129809,malfet,atalman,kit1980,,
f6a0be5023d,rocm,improvements,Add warpSize to Device properties (#128449),torch/_C/__init__.pyi.in torch/_inductor/codegen/codegen_device_driver.py torch/csrc/cuda/Module.cpp,https://github.com/pytorch/pytorch/pull/128449,ramcherukuri,eqy,jataylo,jithunnair-amd,
313eec02cc6,skip,not user facing,Add hash function of std::string_view to torch/csrc/lazy/core/hash.h (#128800),torch/csrc/lazy/core/hash.h,https://github.com/pytorch/pytorch/pull/128800,cyyever,ezyang,,,
e1b426b345c,rocm,bug fixes,[ROCm] CUDA_VISIBLE_DEVICES fallback option for device_count (#129650),test/test_cuda.py torch/cuda/__init__.py,https://github.com/pytorch/pytorch/pull/129650,jataylo,hongxiayang,malfet,,
75f64e12030,skip,not user facing,Fix test `test_type_hints.py::TestTypeHints::test_doc_examples` (#129829),test/test_type_hints.py torch/_torch_docs.py torch/autograd/grad_mode.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/129829,XuehaiPan,ezyang,,,
cc9b005bf2a,Uncategorized,Untopiced,Enable torchao nightly workflow (#129779),benchmarks/dynamo/torchao_backend.py,https://github.com/pytorch/pytorch/pull/129779,xuzhao9,jerryzh168,,,
a83eaf1c3aa,skip,Untopiced,"Revert ""[halide-backend] Support manual schedules (#129321)""",test/inductor/test_halide.py torch/_inductor/codecache.py torch/_inductor/codegen/halide.py torch/_inductor/runtime/hints.py,,,,,,
e385bf8ef8f,skip,Untopiced,"Revert ""[halide-backend] Disable split reductions for Halide (#129320)""",torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/halide.py torch/_inductor/ir.py,,,,,,
ca5d13c6728,sparse_frontend,Untopiced,[1/N] Enable unused variable warnings on torch_cpu and fix some violations (#128670),aten/src/ATen/EmptyTensor.cpp aten/src/ATen/LegacyBatchedFallback.cpp aten/src/ATen/NamedTensorUtils.cpp aten/src/ATen/ParallelCommon.cpp aten/src/ATen/SparseTensorImpl.h aten/src/ATen/core/VariableFallbackKernel.cpp aten/src/ATen/core/ivalue.cpp aten/src/ATen/core/library.cpp aten/src/ATen/functorch/BatchedFallback.cpp aten/src/ATen/native/CPUBlas.cpp aten/src/ATen/native/Convolution.cpp aten/src/ATen/native/cpu/int4mm_kernel.cpp aten/src/ATen/native/sparse/SparseBlasImpl.cpp cmake/public/utils.cmake torch/csrc/jit/tensorexpr/external_functions_codegen.cpp torch/csrc/profiler/collection.cpp,https://github.com/pytorch/pytorch/pull/128670,cyyever,ezyang,,,
b6dc37bb4e8,skip,Untopiced,"Revert ""[inductor] unificate toolchain code. (#129816)""",torch/_inductor/codecache.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/cpp_builder.py torch/_inductor/cpu_vec_isa.py,,,,,,
19e17216a2c,skip,Untopiced,"Revert ""[inductor] split cpu vec isa to dedicate file (keep git history) (#129789)""",test/inductor/test_cpu_repro.py test/inductor/test_cpu_select_algorithm.py test/inductor/test_extension_backend.py torch/_inductor/codecache.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/cpp_builder.py torch/_inductor/cpu_vec_isa.py,,,,,,
433b691f982,skip,Untopiced,"Revert ""[inductor] optimize cpp builder configuration code (#129577)""",torch/_inductor/cpp_builder.py,,,,,,
c9dc9887db0,skip,Untopiced,"Revert ""Enable UFMT on test/test_public_bindings.py (#128389)""",.lintrunner.toml test/test_public_bindings.py,,,,,,
1956d87c1f8,Uncategorized,Untopiced,Increase riscv implementation in DepthwiseConvKernel (#127867),aten/src/ATen/native/Convolution.cpp aten/src/ATen/native/cpu/DepthwiseConvKernel.cpp,https://github.com/pytorch/pytorch/pull/127867,zhangfeiv0,malfet,,,
bb0f3df5627,fx,Untopiced,Fix index issues in torch.fx.interpreter (#129527),torch/fx/interpreter.py,https://github.com/pytorch/pytorch/pull/129527,haodongucsb,dulinriley,,,
b02186ffc16,skip,Untopiced,"Revert ""Allow get attributes on DDP similar to FSDP (#128620)""",torch/nn/parallel/distributed.py,,,,,,
fdd0a7f9b4f,skip,not user facing,Run test_mps_allocator_module serially (#129340),test/test_mps.py,https://github.com/pytorch/pytorch/pull/129340,huydhn,kit1980,malfet,,
f86dbae2471,releng,not user facing,Fix typo in lxml requirement (#129695),.ci/docker/requirements-ci.txt,https://github.com/pytorch/pytorch/pull/129695,jithunnair-amd,huydhn,,,
53d67165c09,dynamo,Untopiced,[dynamo] Skip FUNCTION_MATCH guards for descriptors (#129858),torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/129858,anijain2305,mlazos,,,
78cda9a8108,fx,not user facing,[symbolic-shapes] Add FloatPow in the symbolic shape guard closure (#129857),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/129857,anijain2305,ezyang,,,
750c701e495,skip,not user facing,[ROCm] Update xlogy comment detailing issue (#128151),test/functorch/test_ops.py,https://github.com/pytorch/pytorch/pull/128151,alugorey,zou3519,,,
8c2c3a03fb8,skip,Untopiced,[ROCm] std::clamp work-around for hip-clang compiler (#127812),aten/src/ATen/native/cuda/IndexKernel.cu,https://github.com/pytorch/pytorch/pull/127812,jeffdaily,hongxiayang,malfet,,
87693b534c0,releng,Untopiced,[ROCm] Use AOTriton as a dynamic library (#129094),.ci/docker/aotriton_version.txt .ci/docker/common/install_aotriton.sh cmake/External/aotriton.cmake,https://github.com/pytorch/pytorch/pull/129094,jithunnair-amd,malfet,,,
eeef68671d4,autograd_frontend,Untopiced,[autograd] Do not detach when unpacking tensors that do not require grad  (#127959),test/test_autograd.py test/test_nestedtensor.py torch/csrc/autograd/saved_variable.cpp torch/utils/checkpoint.py,https://github.com/pytorch/pytorch/pull/127959,soulitzer,YuqingJ,,,
9645eaaaeca,releng,not user facing,[BE] Improve logging for runner-determinator (#129679),.github/scripts/runner_determinator.py .github/workflows/_runner-determinator.yml,https://github.com/pytorch/pytorch/pull/129679,ZainRizvi,Skylion007,jeanschmidt,zxiiro,
7e4329c2583,quantization,not user facing,[EZ][BE] Bump min cmake version to 3.18 (#129906),aten/src/ATen/native/quantized/cpu/qnnpack/deps/clog/CMakeLists.txt,https://github.com/pytorch/pytorch/pull/129906,malfet,kit1980,,,
46366888d7d,skip,not user facing,Remove outdated CMake code (#129851),caffe2/CMakeLists.txt cmake/Dependencies.cmake torch/CMakeLists.txt,https://github.com/pytorch/pytorch/pull/129851,cyyever,ezyang,,,
83e6ec2ccdf,distributed,Untopiced,[FSDP2+TP] Disable 2D state_dict (#129519),test/distributed/_composable/fsdp/test_fully_shard_state_dict.py test/distributed/_composable/fsdp/test_fully_shard_training.py torch/distributed/_composable/fsdp/_fsdp_param_group.py,https://github.com/pytorch/pytorch/pull/129519,wz337,awgu,,,
600bf978ba1,distributed,Untopiced,[Pipelining] Add to/from CSV format and improved __repr__ (#129264),torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/129264,wconstab,H-Huang,,,
6240cfd5c75,mps,improvements,[MPS] Add support for autocast in MPS  (#99272),aten/src/ATen/autocast_mode.cpp aten/src/ATen/autocast_mode.h aten/src/ATen/core/interned_strings.h c10/core/DispatchKey.cpp c10/core/DispatchKey.h c10/core/DispatchKeySet.h test/test_autocast.py test/test_mps.py torch/amp/autocast_mode.py torch/csrc/jit/passes/autocast.cpp torch/csrc/utils/python_dispatch.cpp torch/cuda/amp/common.py,https://github.com/pytorch/pytorch/pull/99272,kulinseth,malfet,,,
c2d0b7b96d3,skip,Untopiced,"Revert ""[ROCm] std::clamp work-around for hip-clang compiler (#127812)""",aten/src/ATen/native/cuda/IndexKernel.cu,,,,,,
f6edd1f7c94,distributed,Untopiced,[BE] Make ActivationWrapper an abstract class (#129808),torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py,https://github.com/pytorch/pytorch/pull/129808,jovianjaison,Skylion007,,,
76259ebfdd8,skip,not user facing,[inductor] split cpu vec isa to dedicate file (keep git history) (#129789),test/inductor/test_cpu_repro.py test/inductor/test_cpu_select_algorithm.py test/inductor/test_extension_backend.py torch/_inductor/codecache.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/cpp_builder.py torch/_inductor/cpu_vec_isa.py,https://github.com/pytorch/pytorch/pull/129789,xuhancn,jansel,jgong5,,
6353a12e6a8,skip,Untopiced,XPUHooksInterface inherits from AcceleratorHooksInterface (#129463),aten/src/ATen/Context.h aten/src/ATen/detail/XPUHooksInterface.h aten/src/ATen/xpu/detail/XPUHooks.cpp aten/src/ATen/xpu/detail/XPUHooks.h,https://github.com/pytorch/pytorch/pull/129463,guangyey,albanD,gujinghui,,
3e2df3ca9d0,skip,Untopiced,Add xpu to getAccelerator (#129205),aten/src/ATen/DeviceAccelerator.cpp aten/src/ATen/DeviceAccelerator.h test/test_cpp_extensions_stream_and_event.py,https://github.com/pytorch/pytorch/pull/129205,guangyey,albanD,gujinghui,,
1ad683033b7,distributed,Untopiced,Implemented flexible PP schedule (#129597),docs/source/distributed.pipelining.rst test/distributed/pipelining/test_schedule.py torch/distributed/pipelining/__init__.py torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/129597,haocizhang,H-Huang,,,
5c9d5272e43,cuda,bug fixes,fixes #124582 (#128483),test/test_cuda.py torch/cuda/graphs.py,https://github.com/pytorch/pytorch/pull/128483,jeffwillette,eqy,ezyang,,
ccc4ee77939,linalg_frontend,Untopiced,check boolean alpha and beta of Fake tensor impl for Tensor.addr (#129839),test/test_linalg.py torch/_refs/__init__.py,https://github.com/pytorch/pytorch/pull/129839,awayzjj,lezcano,,,
badc638eb68,skip,not user facing,Add support for inline_asm_elementwise in Inductor lowerings (#129846),test/inductor/test_custom_lowering.py test/inductor/test_flex_attention.py torch/_inductor/codegen/triton.py torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/129846,Chillee,shunting314,,,
567dd1a3cab,skip,not user facing,[inductor] unificate toolchain code. (#129816),torch/_inductor/codecache.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/cpp_builder.py torch/_inductor/cpu_vec_isa.py,https://github.com/pytorch/pytorch/pull/129816,xuhancn,jansel,,,
ebeeb22669b,fx,not user facing,Correctly put mark_unbacked symbols in shape_env_to_source_to_symbol_cache (#129869),test/dynamo/test_unspec.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/129869,ezyang,albanD,,,
eb1ff76f230,composability,not user facing,Make are_strides_like_channels_last size oblivious (#129677),test/dynamo/test_unspec.py torch/_prims_common/__init__.py,https://github.com/pytorch/pytorch/pull/129677,ezyang,Skylion007,albanD,,
8ef8240172d,skip,not user facing,Don't mark conversion to float as is_integer = False (#129890),torch/utils/_sympy/functions.py,https://github.com/pytorch/pytorch/pull/129890,ezyang,lezcano,,,
3c6c3b94486,skip,Untopiced,Fix typo in floordiv solver code that affects flipped relation (#129888),test/dynamo/test_exc.py test/dynamo/test_repros.py torch/fx/experimental/symbolic_shapes.py torch/utils/_sympy/solve.py,https://github.com/pytorch/pytorch/pull/129888,ezyang,lezcano,,,
95a5958db49,releng,not user facing,[ROCm] Update nightly triton-rocm pin to release branch (#129361),.ci/docker/ci_commit_pins/triton-rocm.txt .github/scripts/amd/package_triton_wheel.sh,https://github.com/pytorch/pytorch/pull/129361,jataylo,peterbell10,,,
0441173ab2c,linalg_frontend,not user facing,Add slowTest marker to test_linalg_solve_triangular_large (#129903),test/test_linalg.py,https://github.com/pytorch/pytorch/pull/129903,Fuzzkatt,lezcano,,,
07450e97134,skip,Untopiced,"Revert ""[MPS] Add support for autocast in MPS  (#99272)""",aten/src/ATen/autocast_mode.cpp aten/src/ATen/autocast_mode.h aten/src/ATen/core/interned_strings.h c10/core/DispatchKey.cpp c10/core/DispatchKey.h c10/core/DispatchKeySet.h test/test_autocast.py test/test_mps.py torch/amp/autocast_mode.py torch/csrc/jit/passes/autocast.cpp torch/csrc/utils/python_dispatch.cpp torch/cuda/amp/common.py,,,,,,
86e2d16ba0a,inductor,Untopiced,[Inductor][Quant] Change the schema of QLinear Binary (#129049),aten/src/ATen/native/quantized/cpu/qlinear.cpp aten/src/ATen/native/quantized/library.cpp test/forward_backward_compatibility/check_forward_backward_compatibility.py test/quantization/core/test_quantized_op.py torch/_inductor/fx_passes/quantization.py torch/_inductor/mkldnn_ir.py torch/_inductor/mkldnn_lowerings.py,https://github.com/pytorch/pytorch/pull/129049,leslie-fang-intel,jansel,jgong5,,
a7963583306,inductor,not user facing,[Inductor][CPP] Enable Quantized Linear GEMM Template with Binary Fusion (#129103),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/mkldnn_lowerings.py,https://github.com/pytorch/pytorch/pull/129103,leslie-fang-intel,jansel,jgong5,,
72fa8640984,inductor,not user facing,[Inductor][CPP] Enable Quantized Linear with AMX MicroGEMM (#129220),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py,https://github.com/pytorch/pytorch/pull/129220,leslie-fang-intel,jgong5,,,
b6379591a96,inductor,not user facing,[Inductor][CPP] Pass weight dtype explicitly for cpp gemm template (#129221),torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/129221,leslie-fang-intel,jansel,jgong5,,
37e3c60897d,inductor,not user facing,[Inductor][CPP] Remove redundant INT8-specific logic in the INT8 GEMM template (#129470),torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/mkldnn_lowerings.py,https://github.com/pytorch/pytorch/pull/129470,leslie-fang-intel,jgong5,,,
dc75ec252a9,inductor,bug fixes,[inductor] Fix can_merge check for expr=q0*q1 (#129806),test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/_inductor/sizevars.py,https://github.com/pytorch/pytorch/pull/129806,peterbell10,lezcano,,,
6cb0ad3375c,releng,not user facing,[BE]: Update NCCL submodule to 2.21.5 (#124014),.github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml third_party/nccl/nccl,https://github.com/pytorch/pytorch/pull/124014,Skylion007,atalman,eqy,ezyang,
2926655761f,skip,not user facing,[inductor] optimize cpp builder configuration code (#129577),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/129577,xuhancn,jansel,jgong5,,
6c2a8b6b387,distributed,Untopiced,[Ez][BE]: Enable new stable ruff rules (#129825),benchmarks/distributed/ddp/benchmark.py benchmarks/distributed/ddp/diff.py benchmarks/dynamo/timm_models.py benchmarks/dynamo/torchbench.py benchmarks/fastrnns/test.py pyproject.toml test/error_messages/storage.py test/onnx/model_defs/srresnet.py test/quantization/core/experimental/quantization_util.py test/quantization/core/test_quantized_op.py test/test_autograd.py torch/_functorch/partitioners.py torch/_inductor/codegen/common.py torch/_numpy/testing/utils.py torch/distributed/benchmarks/benchmark_ddp_rpc.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/129825,Skylion007,XuehaiPan,jansel,malfet,
257b9c7936b,skip,not user facing,Fix layout for *_like() factories on NJTs (#129879),torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/129879,jbschlosser,soulitzer,,,
f1df13f023d,dataloader_frontend,not user facing,[BE][Easy] Fix `PYI001`: unprefixed-type-param in `torch/utils/data/datapipes` (#129885),torch/utils/data/dataloader.py torch/utils/data/datapipes/_typing.py torch/utils/data/datapipes/datapipe.py torch/utils/data/datapipes/datapipe.pyi.in torch/utils/data/datapipes/iter/callable.py torch/utils/data/datapipes/iter/combinatorics.py torch/utils/data/datapipes/iter/combining.py torch/utils/data/datapipes/iter/grouping.py torch/utils/data/datapipes/iter/selecting.py torch/utils/data/datapipes/map/callable.py torch/utils/data/datapipes/map/combinatorics.py torch/utils/data/datapipes/map/combining.py torch/utils/data/datapipes/map/grouping.py torch/utils/data/dataset.py torch/utils/data/distributed.py torch/utils/data/sampler.py,https://github.com/pytorch/pytorch/pull/129885,XuehaiPan,ezyang,,,
dacc33d2fa5,skip,not user facing,Make sym_min/sym_max handle Numpy scalars (#129917),test/test_dynamic_shapes.py torch/__init__.py,https://github.com/pytorch/pytorch/pull/129917,ezyang,Skylion007,,,
3fd128361e4,skip,not user facing,[traced-graph][sparse] add relay override for layout_impl (#129930),aten/src/ATen/FunctionalTensorWrapper.cpp aten/src/ATen/FunctionalTensorWrapper.h,https://github.com/pytorch/pytorch/pull/129930,aartbik,ezyang,,,
03440a1c13c,skip,Untopiced,"Revert ""Add support for inline_asm_elementwise in Inductor lowerings (#129846)""",test/inductor/test_custom_lowering.py test/inductor/test_flex_attention.py torch/_inductor/codegen/triton.py torch/_inductor/kernel/flex_attention.py,,,,,,
e3b3431c420,quantization,Untopiced,Fix for HistogramObserver (#129387),test/quantization/core/test_workflow_module.py torch/ao/quantization/observer.py,https://github.com/pytorch/pytorch/pull/129387,TiRune,jerryzh168,,,
9fb2dec7a65,composability,Untopiced,[custom ops] Add unknown arg (#129614),test/test_custom_ops.py torch/_library/custom_ops.py torch/_library/infer_schema.py,https://github.com/pytorch/pytorch/pull/129614,yushangdi,zou3519,,,
d146a62e778,mps,not user facing,[MPS][BE] Introduce `mtl_setBytes` (#129910),aten/src/ATen/native/mps/OperationUtils.h aten/src/ATen/native/mps/operations/BitwiseOps.mm aten/src/ATen/native/mps/operations/Bucketization.mm aten/src/ATen/native/mps/operations/CrossKernel.mm aten/src/ATen/native/mps/operations/Gamma.mm aten/src/ATen/native/mps/operations/HistogramKernel.mm aten/src/ATen/native/mps/operations/Indexing.mm aten/src/ATen/native/mps/operations/LinearAlgebra.mm aten/src/ATen/native/mps/operations/Quantized.mm aten/src/ATen/native/mps/operations/RenormKernel.mm aten/src/ATen/native/mps/operations/Repeat.mm,https://github.com/pytorch/pytorch/pull/129910,malfet,Skylion007,,,
75443d3daf5,Uncategorized,Untopiced,[dynamic-shapes] Dont create symbol if .item() is a nan (#129881),torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/129881,anijain2305,ezyang,zou3519,,
9105d54c6b3,dynamo,not user facing,[dynamo][sparse] Graph break on sparse tensors (#129883),test/dynamo_expected_failures/TestNN.test_overwrite_module_params_on_conversion test/dynamo_expected_failures/TestTorchTidyProfiler.test_sparse_tensors torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/129883,anijain2305,ezyang,,,
34e94c507ab,inductor,not user facing,[Inductor] Make FlexAttention block_mask argument as tuple (#129831),test/inductor/test_flex_attention.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/_flex_attention.py,https://github.com/pytorch/pytorch/pull/129831,yanboliang,Chillee,drisspg,,
4eb449f7dc1,distributed,Untopiced,[pipelining] add small logging section to docs (#129368),docs/source/distributed.pipelining.rst,https://github.com/pytorch/pytorch/pull/129368,H-Huang,wconstab,,,
ec789a3c9dd,skip,Untopiced,[pipelining] [BE] Move pipeline_order validation to schedules.py (#129369),test/distributed/pipelining/test_schedule.py torch/distributed/pipelining/_utils.py torch/distributed/pipelining/schedules.py torch/distributed/pipelining/stage.py,https://github.com/pytorch/pytorch/pull/129369,H-Huang,wconstab,,,
aa7ea6b45cf,optim,Untopiced,Add wraps back (#129933),torch/optim/lr_scheduler.py,https://github.com/pytorch/pytorch/pull/129933,mlazos,eqy,janeyx99,,
1f3e2d7877e,inductor,not user facing,[Inductor] Rename TemplatedAttention to FlexAttention (#129859),torch/_dynamo/variables/higher_order_ops.py torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/129859,yanboliang,Chillee,drisspg,,
080149cb38c,inductor,not user facing,[Inductor][FlexAttention] Add helper functions of converting score_mod to block_mask (#129909),test/inductor/test_flex_attention.py torch/nn/attention/_flex_attention.py,https://github.com/pytorch/pytorch/pull/129909,yanboliang,Chillee,drisspg,,
bf05ea2babc,releng,not user facing,Re-generate Linux build workflows after #124014 (#129976),.github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/129976,huydhn,kit1980,,,
1f6c1fcd367,distributed,not user facing,[dtensor][debug] add operation tracing to comm_mode (#129017),torch/distributed/_tensor/debug/comm_mode.py torch/distributed/_tensor/examples/comm_mode_features_example.py torch/distributed/_tensor/examples/comm_mode_features_example_argparser.py,https://github.com/pytorch/pytorch/pull/129017,sinhaanshul,XilunWu,,,
2631a96f2ae,fx,not user facing,Stop updating hints (#129893),torch/fx/experimental/sym_node.py,https://github.com/pytorch/pytorch/pull/129893,ezyang,lezcano,,,
8ec5ba960f8,mps,Untopiced,[MPS] Add tensor_lr overloads to fused adam & adamw (#129451),aten/src/ATen/native/mps/operations/FusedAdamAmsgradKernelImpl.h aten/src/ATen/native/mps/operations/FusedAdamAmsgradKernelImpl.mm aten/src/ATen/native/mps/operations/FusedAdamKernel.mm aten/src/ATen/native/mps/operations/FusedAdamKernelImpl.h aten/src/ATen/native/mps/operations/FusedAdamKernelImpl.mm aten/src/ATen/native/mps/operations/FusedAdamWAmsgradKernelImpl.h aten/src/ATen/native/mps/operations/FusedAdamWAmsgradKernelImpl.mm aten/src/ATen/native/mps/operations/FusedAdamWKernel.mm aten/src/ATen/native/mps/operations/FusedAdamWKernelImpl.h aten/src/ATen/native/mps/operations/FusedAdamWKernelImpl.mm aten/src/ATen/native/mps/operations/MultiTensorApply.h aten/src/ATen/native/native_functions.yaml torch/optim/adam.py torch/optim/adamw.py torch/testing/_internal/common_optimizers.py,https://github.com/pytorch/pytorch/pull/129451,qqaatw,janeyx99,,,
26de2c24874,jit,Untopiced,[3/N] Enable clang-tidy on torch/csrc/jit/serialization/* (#129850),.lintrunner.toml torch/csrc/jit/serialization/export_bytecode.cpp torch/csrc/jit/serialization/export_bytecode.h torch/csrc/jit/serialization/export_module.cpp torch/csrc/jit/serialization/flatbuffer_serializer.cpp torch/csrc/jit/serialization/import_source.cpp torch/csrc/jit/serialization/import_source.h torch/csrc/jit/serialization/pickler.cpp torch/csrc/jit/serialization/python_print.cpp torch/csrc/jit/serialization/unpickler.cpp,https://github.com/pytorch/pytorch/pull/129850,cyyever,ezyang,,,
deefc10dd30,releng,not user facing,[executorch hash update] update the pinned executorch hash (#129428),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/129428,pytorchupdatebot,pytorchbot,,,
1ddb1003184,distributed,Untopiced,[FSDP1][Easy] Remove Spammy Log Lin in _runtime_utils.py (#129967),torch/distributed/fsdp/_runtime_utils.py,https://github.com/pytorch/pytorch/pull/129967,wz337,Skylion007,awgu,fduwjj,
c22e66896f8,skip,Untopiced,"Revert ""Fix typo in floordiv solver code that affects flipped relation (#129888)""",test/dynamo/test_exc.py test/dynamo/test_repros.py torch/fx/experimental/symbolic_shapes.py torch/utils/_sympy/solve.py,,,,,,
39357ba06f4,fx,not user facing,[dynamo] don't constrain range on the replacement for a symbol (#129907),test/dynamo/test_misc.py test/export/test_serialize.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/129907,ColinPeppler,jingsh,,,
b6f781e433f,inductor,Untopiced,Bug fix for captuing execution trace grid function (#129832),torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/129832,shengfukevin,Skylion007,davidberard98,,
b5fdbc1a9fb,skip,Untopiced,"Revert ""[pipelining] [BE] Move pipeline_order validation to schedules.py (#129369)""",test/distributed/pipelining/test_schedule.py torch/distributed/pipelining/_utils.py torch/distributed/pipelining/schedules.py torch/distributed/pipelining/stage.py,,,,,,
29ffa20bb13,python_frontend,not user facing,[CUDA] Bump tolerances for `test_grad_pca_lowrank` (#129902),torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/129902,eqy,ezyang,,,
d7680a564b6,dynamo,not user facing,Bug fixes for disabling 0/1 specialization on plain int (#129961),test/test_dynamic_shapes.py torch/__init__.py torch/_dynamo/variables/lists.py torch/_inductor/lowering.py torch/_inductor/sizevars.py,https://github.com/pytorch/pytorch/pull/129961,ezyang,lezcano,,,
aa0352ca38a,composability,Untopiced,[custom ops] add default value support for device types (#129792),test/test_custom_ops.py torch/_library/infer_schema.py,https://github.com/pytorch/pytorch/pull/129792,yushangdi,zou3519,,,
872d972e415,Uncategorized,Untopiced,[custom_op] better error message on no returns (#129896),test/test_custom_ops.py torch/_library/infer_schema.py,https://github.com/pytorch/pytorch/pull/129896,zou3519,yushangdi,,,
fb078c20c1c,inductor,not user facing,[inductor] Separate Buffer and Operation into two concepts (#128893),test/inductor/test_cuda_repro.py test/inductor/test_debug_trace.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/wrapper.py torch/_inductor/comms.py torch/_inductor/dependencies.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/128893,peterbell10,lezcano,,,
7955cd3e83a,inductor,not user facing,[inductor] Make UserDefinedTritonKernel a multi-output operation (#129325),torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/129325,peterbell10,lezcano,,,
45844e0d4e2,inductor,not user facing,[inductor] Add FileCheck to flex attention epilogue test (#129343),test/inductor/test_flex_attention.py,https://github.com/pytorch/pytorch/pull/129343,peterbell10,lezcano,,,
b2ac8d2af34,inductor,not user facing,[inductor] Use multiple outputs for flex-attention (#129344),test/inductor/test_flex_attention.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/129344,peterbell10,lezcano,,,
921c116089e,inductor,not user facing,[inductor] Kill mark_node_as_mutating (#129346),torch/_inductor/ir.py torch/_inductor/mkldnn_ir.py,https://github.com/pytorch/pytorch/pull/129346,peterbell10,lezcano,,,
89646ebb119,skip,Untopiced,"Revert ""[export] make with_effect mark op has_effect to prevent them from DCEed. (#129680)""",test/export/test_torchbind.py torch/_higher_order_ops/effects.py torch/export/_remove_effect_tokens_pass.py,,,,,,
111f9b5d442,dynamo,not user facing,[Dynamo] Add config to skip/inline torchrec (#129912),torch/_dynamo/config.py torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/129912,yanboliang,anijain2305,,,
be2d79a16b2,fx,Untopiced,[dynamic] config to disable duck sizing (#129804),test/dynamo/test_repros.py torch/fx/experimental/_config.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/129804,xmfan,ezyang,,,
1e27af335e5,jit,Untopiced,[easy] enhance local model loading (#129897),torch/csrc/jit/python/pybind_utils.cpp,https://github.com/pytorch/pytorch/pull/129897,842974287,jingsh,,,
3b77b122c5c,skip,not user facing,[Inductor UT] update rtol for convoluton on XPU. (#129782),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/129782,etaf,atalman,,,
c686304277f,skip,not user facing,Enable UFMT on test/test_public_bindings.py (#128389),.lintrunner.toml test/test_public_bindings.py,https://github.com/pytorch/pytorch/pull/128389,dilililiwhy,malfet,,,
c77c139878a,releng,not user facing,[Intel Triton] Update Intel Triton to resolve installation issue on manylinux. (#129847),.ci/docker/ci_commit_pins/triton-xpu.txt,https://github.com/pytorch/pytorch/pull/129847,etaf,Skylion007,atalman,gujinghui,
91a8376d470,skip,not user facing,run_test: Unset cpp stacktraces after reruns (#129004),test/conftest.py test/run_test.py,https://github.com/pytorch/pytorch/pull/129004,clee2000,PaliC,,,
9ee8c18309a,distributed,Untopiced,TCPStore: add ping to verify network connectivity on connect (#129985),test/distributed/test_store.py torch/csrc/distributed/c10d/TCPStore.cpp torch/csrc/distributed/c10d/TCPStore.hpp torch/csrc/distributed/c10d/TCPStoreBackend.cpp torch/csrc/distributed/c10d/TCPStoreBackend.hpp torch/csrc/distributed/c10d/TCPStoreLibUvBackend.cpp,https://github.com/pytorch/pytorch/pull/129985,d4l3k,kurman,rsdcastro,,
31fc5b89665,skip,not user facing,Add support for inline_asm_elementwise in Inductor lowerings (#129846),test/inductor/test_custom_lowering.py test/inductor/test_flex_attention.py torch/_inductor/codegen/triton.py torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/129846,Chillee,shunting314,,,
1026b0f687a,releng,not user facing,Use setup-miniconda step from test-infra for llm retrival workflow (#129720),.github/workflows/llm_td_retrieval.yml .github/workflows/pull.yml .github/workflows/slow.yml .github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/129720,clee2000,PaliC,huydhn,malfet,
424cd1e1dfb,skip,not user facing,Enable TORCH_TRACE by default on Conda on Mast (#129988),torch/_logging/_internal.py,https://github.com/pytorch/pytorch/pull/129988,ezyang,kunalb,,,
8af58f66bb5,skip,Untopiced,Fix typo in floordiv solver code that affects flipped relation (#129888),test/dynamo/test_exc.py test/dynamo/test_repros.py torch/fx/experimental/symbolic_shapes.py torch/utils/_sympy/solve.py,https://github.com/pytorch/pytorch/pull/129888,ezyang,lezcano,,,
0abcca85b75,skip,Untopiced,[halide-backend] Support manual schedules (#129321),test/inductor/test_halide.py torch/_inductor/codecache.py torch/_inductor/codegen/halide.py torch/_inductor/runtime/hints.py,https://github.com/pytorch/pytorch/pull/129321,jansel,shunting314,,,
4fc9157e904,skip,Untopiced,[halide-backend] Disable split reductions for Halide (#129320),torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/halide.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/129320,jansel,eellison,shunting314,,
62b710782df,skip,not user facing,change LayoutLMForSequenceClassification inference accuracy tolerance (#129728),benchmarks/dynamo/huggingface.py,https://github.com/pytorch/pytorch/pull/129728,Valentine233,jansel,jgong5,,
8f70bf7a943,skip,not user facing,Skip TestSDPAPrivateUse1Only on FBCODE (#129997),test/test_transformers.py,https://github.com/pytorch/pytorch/pull/129997,huydhn,drisspg,,,
735044191f0,skip,not user facing,[Easy] Add whitespace after comma when re-rendering tuple default value in schema (#129884),torch/_torch_docs.py torchgen/api/python.py torchgen/model.py,https://github.com/pytorch/pytorch/pull/129884,XuehaiPan,ezyang,,,
64a04d22258,sparse_frontend,not user facing,Make sparse empty constructors specialize instead of fail on symbolic inputs (#129983),aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/sparse/SparseCsrTensor.cpp aten/src/ATen/native/sparse/SparseTensor.cpp c10/core/SymIntArrayRef.h,https://github.com/pytorch/pytorch/pull/129983,ezyang,anijain2305,,,
9e1e58e0528,skip,not user facing,Support allowlisted modules and op overloads in AOTAutogradCache (#128329),test/dynamo/test_aot_autograd_cache.py test/functorch/test_aotdispatch.py torch/_functorch/_aot_autograd/autograd_cache.py,https://github.com/pytorch/pytorch/pull/128329,jamesjwu,bdhirsh,,,
29c68df600e,linalg_frontend,not user facing,Stop immediately specializing common constants 0/1 for plain int (#128327),docs/source/torch.compiler_dynamo_deepdive.rst test/dynamo/test_backward_higher_order_ops.py test/dynamo/test_misc.py test/dynamo/test_modules.py test/dynamo/test_repros.py test/test_linalg.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/128327,ezyang,lezcano,,,
e2eb33b0899,inductor,not user facing,Added methods to blockmask to visualize them (#129950),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/_flex_attention.py,https://github.com/pytorch/pytorch/pull/129950,Chillee,drisspg,yanboliang,,
01e41f1814c,inductor,not user facing,Modified autotuning for flex_attention to pass in (proper) fake inputs for the block sparse entries (#129915),torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/129915,Chillee,eellison,yanboliang,,
35600bcaad7,fx,not user facing,"Print float with full precision, don't truncate (#130027)",test/test_dynamic_shapes.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/130027,ezyang,Skylion007,lezcano,,
9b902b3ee3b,inductor,Untopiced,AOTI: dont treat views of buffers as constants (#129688),test/inductor/test_aot_inductor.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/129688,bdhirsh,eellison,,,
042d764872a,skip,not user facing,[export] Update example inputs format for DB. (#129982),docs/source/scripts/exportdb/generate_example_rst.py test/export/test_db.py test/export/test_serialize.py torch/_export/db/case.py torch/_export/db/examples/assume_constant_result.py torch/_export/db/examples/autograd_function.py torch/_export/db/examples/class_method.py torch/_export/db/examples/cond_branch_class_method.py torch/_export/db/examples/cond_branch_nested_function.py torch/_export/db/examples/cond_branch_nonlocal_variables.py torch/_export/db/examples/cond_closed_over_variable.py torch/_export/db/examples/cond_operands.py torch/_export/db/examples/cond_predicate.py torch/_export/db/examples/constrain_as_size_example.py torch/_export/db/examples/constrain_as_value_example.py torch/_export/db/examples/decorator.py torch/_export/db/examples/dictionary.py torch/_export/db/examples/dynamic_shape_assert.py torch/_export/db/examples/dynamic_shape_constructor.py torch/_export/db/examples/dynamic_shape_if_guard.py torch/_export/db/examples/dynamic_shape_map.py torch/_export/db/examples/dynamic_shape_round.py torch/_export/db/examples/dynamic_shape_slicing.py torch/_export/db/examples/dynamic_shape_view.py torch/_export/db/examples/fn_with_kwargs.py torch/_export/db/examples/list_contains.py torch/_export/db/examples/list_unpack.py torch/_export/db/examples/model_attr_mutation.py torch/_export/db/examples/nested_function.py torch/_export/db/examples/null_context_manager.py torch/_export/db/examples/optional_input.py torch/_export/db/examples/pytree_flatten.py torch/_export/db/examples/scalar_output.py torch/_export/db/examples/specialized_attribute.py torch/_export/db/examples/static_for_loop.py torch/_export/db/examples/static_if.py torch/_export/db/examples/tensor_setattr.py torch/_export/db/examples/torch_sym_min.py torch/_export/db/examples/type_reflection_method.py torch/_export/db/examples/user_input_mutation.py,https://github.com/pytorch/pytorch/pull/129982,zhxchen17,angelayi,,,
a21d4363d2e,profiler,Untopiced,[Profiler] Remove all instances of TMP_USE_TSC_AS_TIMESTAMP (#129973),buckbuild.bzl cmake/Dependencies.cmake third_party/kineto,https://github.com/pytorch/pytorch/pull/129973,sraikund16,aaronenyeshi,,,
7bbd6cf9313,Uncategorized,Untopiced,[custom_ops] Mark older custom ops prototypes as deprecated (#130032),torch/_custom_op/impl.py,https://github.com/pytorch/pytorch/pull/130032,zou3519,williamwen42,yushangdi,,
a79bb8db917,autograd_frontend,not user facing,Make `_embedding_bag_backward` explicitly dispatch to CPU and CUDA. (#129691),aten/src/ATen/native/EmbeddingBag.cpp aten/src/ATen/native/EmbeddingBag.h aten/src/ATen/native/cuda/EmbeddingBag.cu aten/src/ATen/native/native_functions.yaml test/expect/HasDecompTest.test_has_decomposition.expect tools/autograd/derivatives.yaml torch/_inductor/lowering.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/129691,ysiraichi,lezcano,,,
b0d0114f5b8,releng,not user facing,Enable automigration for windows jobs (#129977),.github/workflows/_linux-build.yml .github/workflows/_win-build.yml .github/workflows/pull.yml .github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/129977,ZainRizvi,clee2000,,,
a9a744e4429,skip,Untopiced,Change numeric_debug_handle to store per-node id (#129811),test/quantization/pt2e/test_generate_numeric_debug_handle.py torch/ao/quantization/__init__.py torch/ao/quantization/fx/convert.py torch/ao/quantization/pt2e/generate_numeric_debug_handle.py torch/ao/quantization/pt2e/prepare.py,https://github.com/pytorch/pytorch/pull/129811,jerryzh168,tarun292,,,
8a9725bedbb,skip,Untopiced,"Revert ""Add xpu to getAccelerator (#129205)""",aten/src/ATen/DeviceAccelerator.cpp aten/src/ATen/DeviceAccelerator.h test/test_cpp_extensions_stream_and_event.py,,,,,,
779fc8119e0,skip,Untopiced,"Revert ""XPUHooksInterface inherits from AcceleratorHooksInterface (#129463)""",aten/src/ATen/Context.h aten/src/ATen/detail/XPUHooksInterface.h aten/src/ATen/xpu/detail/XPUHooks.cpp aten/src/ATen/xpu/detail/XPUHooks.h,,,,,,
2fe7c1fe04b,composability,Untopiced,[custom ops] Support factory function  (#129978),test/test_custom_ops.py torch/_library/custom_ops.py torch/_library/utils.py,https://github.com/pytorch/pytorch/pull/129978,yushangdi,zou3519,,,
d95a019704f,dynamo,Untopiced,[export] construct empty graph when there's no tensor computation  (#129541),test/dynamo/test_export.py test/export/test_export.py test/export/test_serialize.py torch/_dynamo/eval_frame.py,https://github.com/pytorch/pytorch/pull/129541,yushangdi,angelayi,,,
efb73eda512,linalg_frontend,Untopiced,[2/N]  Fix some violations of unused-function and unused-variable checks in torch_cpu (#129878),aten/src/ATen/MapAllocator.cpp aten/src/ATen/core/dispatch/OperatorEntry.cpp aten/src/ATen/native/DistributionTemplates.h aten/src/ATen/native/Linear.cpp aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/LossMultiLabelMargin.cpp aten/src/ATen/native/NamedTensor.cpp aten/src/ATen/native/Pool.h aten/src/ATen/native/TensorAdvancedIndexingUtils.h aten/src/ATen/native/TensorConversions.cpp aten/src/ATen/native/TensorShape.cpp aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp aten/src/ATen/native/cpu/SerialStackImpl.h aten/src/ATen/native/quantized/cpu/qconv.cpp aten/src/ATen/native/quantized/cpu/qlinear.cpp aten/src/ATen/native/sparse/SparseCsrTensor.cpp torch/csrc/autograd/custom_function.cpp torch/csrc/jit/mobile/interpreter.cpp torch/csrc/jit/runtime/interpreter.cpp torch/csrc/jit/runtime/static/impl.cpp torch/csrc/jit/serialization/import.cpp torch/csrc/profiler/unwind/unwind.cpp torch/custom_class.h,https://github.com/pytorch/pytorch/pull/129878,cyyever,ezyang,,,
cd70ac884f2,distributed,Untopiced,c10d/Utils: better error message on 0 bytes (#130056),torch/csrc/distributed/c10d/Utils.hpp,https://github.com/pytorch/pytorch/pull/130056,d4l3k,kurman,rsdcastro,,
9108b74bbc6,skip,not user facing,Updates to scaled_mm for rowwise scaling (#130059),aten/src/ATen/native/cuda/Blas.cpp test/test_matmul_cuda.py,https://github.com/pytorch/pytorch/pull/130059,drisspg,vkuzo,,,
51fa0bd436c,skip,not user facing,[pt2-bench] pass acc test if ref is NaN (#129996),test/dynamo/test_utils.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/129996,shunting314,jansel,,,
dafbd603ee6,skip,not user facing,[pt2-bench] fix accuracy failure for a few models (#129941),benchmarks/dynamo/common.py benchmarks/dynamo/huggingface.py benchmarks/dynamo/timm_models.py benchmarks/dynamo/torchbench.py benchmarks/dynamo/torchbench.yaml test/dynamo/test_utils.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/129941,shunting314,jansel,,,
0af8c8a981e,skip,not user facing,[pt2-bench] fix accuracy failure for beit_base_patch16_224 during training (#130005),benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_timm_training.csv benchmarks/dynamo/timm_models.py,https://github.com/pytorch/pytorch/pull/130005,shunting314,eellison,jansel,,
99ec7bbee79,Uncategorized,Untopiced,Force inconsistent-missing-override for torch targets (#130010),cmake/public/utils.cmake,https://github.com/pytorch/pytorch/pull/130010,cyyever,ezyang,,,
8405ba21c13,inductor,not user facing,[inductor][cpp] fix the vec convertion between float and int64 on AVX2 (#130013),aten/src/ATen/cpu/vec/vec256/vec256.h aten/src/ATen/cpu/vec/vec256/vec256_convert.h test/inductor/test_cpu_repro.py,https://github.com/pytorch/pytorch/pull/130013,jgong5,lezcano,,,
da8af685aca,dynamo,Untopiced,[dynamo] Skip ID_MATCH guard on GetSetDescriptorType (#129913),torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/129913,anijain2305,jansel,,,
a7a7363be0a,dynamo,not user facing,[dynamo] Skip side effect tracking for c wrappers/descriptors (#129914),torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/129914,anijain2305,jansel,,,
e2e624a02fc,skip,not user facing,[AOTAutograd] Micro-optimize runtime_wrapper (#128188),benchmarks/dynamo/microbenchmarks/microbench.py torch/_functorch/_aot_autograd/functional_utils.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/utils.py,https://github.com/pytorch/pytorch/pull/128188,peterbell10,bdhirsh,,,
0b9995c1cef,distributed,not user facing,[dtensor][debug] Added forward and backward differentiation for module level tracing (#129602),test/distributed/_tensor/debug/test_comm_mode_features.py torch/distributed/_tensor/debug/comm_mode.py torch/distributed/_tensor/examples/comm_mode_features_example.py,https://github.com/pytorch/pytorch/pull/129602,sinhaanshul,XilunWu,wz337,,
9c9ac670a07,distributed,not user facing,[dtensor][be] Reduced redundant LOC by creating functions to set up models used in example (#129613),torch/distributed/_tensor/examples/comm_mode_features_example.py,https://github.com/pytorch/pytorch/pull/129613,sinhaanshul,XilunWu,,,
26be691e6bf,releng,not user facing,Unify shard logic for inductor and dynamo test_config (#129508),.ci/pytorch/test.sh,https://github.com/pytorch/pytorch/pull/129508,jithunnair-amd,clee2000,huydhn,,
07b06f0f0a7,skip,not user facing,[2/N] Remove outdated CMake code (#130006),caffe2/CMakeLists.txt,https://github.com/pytorch/pytorch/pull/130006,cyyever,drisspg,,,
42f3d7e948c,mps,Untopiced,[MPS] Add mps profiler env vars to docs (#129552),docs/source/mps_environment_variables.rst,https://github.com/pytorch/pytorch/pull/129552,qqaatw,malfet,,,
d62d3511074,skip,not user facing,[Optim][BE] Change str(device) to _get_device_type(device) (#129984),test/test_optim.py torch/testing/_internal/common_optimizers.py,https://github.com/pytorch/pytorch/pull/129984,qqaatw,janeyx99,,,
bffb278700d,skip,not user facing,[ONNX] Add `artifacts_dir` to torch-onnx-patch in benchmark (#130069),benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/130069,titaiwangms,justinchuby,,,
e98587c58d3,skip,not user facing,Update torch-xpu-ops pin (ATen XPU implementation) (#129353),third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/129353,ZhiweiYan-96,EikanWang,,,
fa4e489d70d,dynamo,not user facing,[dynamo][dynamic-shapes] Graph break if out shape changes on out= variants (#130074),test/dynamo/test_misc.py test/dynamo_expected_failures/TestNamedTensor.test_addmm test/dynamo_expected_failures/TestNamedTensor.test_bmm test/dynamo_expected_failures/TestNamedTensor.test_cat test/dynamo_expected_failures/TestNamedTensor.test_logical_ops test/dynamo_expected_failures/TestNamedTensor.test_matmul test/dynamo_expected_failures/TestNamedTensor.test_mm torch/_dynamo/variables/torch.py,https://github.com/pytorch/pytorch/pull/130074,anijain2305,ezyang,,,
f3962cfd9c2,skip,Untopiced,[RELAND] XPUHooksInterface inherits from AcceleratorHooksInterface (#129463),aten/src/ATen/Context.h aten/src/ATen/detail/XPUHooksInterface.h aten/src/ATen/xpu/detail/XPUHooks.cpp aten/src/ATen/xpu/detail/XPUHooks.h,https://github.com/pytorch/pytorch/pull/129463,guangyey,albanD,gujinghui,,
551f3b92b2d,dynamo,not user facing,[Dynamo] Add assertion for tensor unpack shape mismatch (#130077),test/dynamo/test_misc.py torch/_dynamo/variables/tensor.py,https://github.com/pytorch/pytorch/pull/130077,yanboliang,Chillee,,,
57d05f26163,skip,Untopiced,[RELAND] Add xpu to getAccelerator (#129205),aten/src/ATen/DeviceAccelerator.cpp aten/src/ATen/DeviceAccelerator.h test/test_cpp_extensions_stream_and_event.py,https://github.com/pytorch/pytorch/pull/129205,guangyey,albanD,gujinghui,,
54da35a2e02,skip,Untopiced,"Revert ""[pt2-bench] fix accuracy failure for beit_base_patch16_224 during training (#130005)""",benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_timm_training.csv benchmarks/dynamo/timm_models.py,,,,,,
fa3953a2e16,skip,Untopiced,"Revert ""[pt2-bench] fix accuracy failure for a few models (#129941)""",benchmarks/dynamo/common.py benchmarks/dynamo/huggingface.py benchmarks/dynamo/timm_models.py benchmarks/dynamo/torchbench.py benchmarks/dynamo/torchbench.yaml test/dynamo/test_utils.py torch/_dynamo/utils.py,,,,,,
6dfa53ca765,skip,Untopiced,"Revert ""[pt2-bench] pass acc test if ref is NaN (#129996)""",test/dynamo/test_utils.py torch/_dynamo/utils.py,,,,,,
5b5f4b02c27,skip,Untopiced,[pipelining] [BE] Move pipeline_order validation to schedules.py (#129369),test/distributed/pipelining/test_schedule.py torch/distributed/pipelining/_utils.py torch/distributed/pipelining/schedules.py torch/distributed/pipelining/stage.py,https://github.com/pytorch/pytorch/pull/129369,H-Huang,wconstab,,,
f78b79daaa0,nn_frontend,Untopiced,Forward fix the missing torch.nn.Module.set_submodule from D59140215 (#130075),test/test_nn.py torch/nn/modules/module.py,https://github.com/pytorch/pytorch/pull/130075,huydhn,mikaylagawarecki,,,
d496145534c,releng,not user facing,[CD] Add triton xpu wheel build (#129730),.github/scripts/build_triton_wheel.py .github/workflows/build-triton-wheel.yml,https://github.com/pytorch/pytorch/pull/129730,chuanqi129,atalman,,,
21eeedb4554,skip,not user facing,[Inductor] Add aot_mode UT to new cpp_builder. (#130105),torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/130105,xuhancn,jansel,jgong5,,
e5901688658,python_frontend,new features,Enable sharing meta tensors between processes (#129520),docs/source/conf.py test/test_multiprocessing.py torch/_dynamo/variables/builder.py torch/multiprocessing/reductions.py,https://github.com/pytorch/pytorch/pull/129520,kurtamohler,ezyang,,,
7128504424c,inductor,new features,[inductor] Add Triton template for Conv3D (#129518),test/inductor/test_max_autotune.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/_inductor/kernel/conv.py,https://github.com/pytorch/pytorch/pull/129518,antiagainst,jansel,jataylo,,
c9f1db265e3,distributed,bug fixes,[NCCL] Make sure current device is correct in `torch.distributed.barrier()`'s `streamSynchronize` (#129908),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/129908,eqy,kwen2501,,,
30fc4b06f55,skip,not user facing,[audio hash update] update the pinned audio hash (#129429),.github/ci_commit_pins/audio.txt,https://github.com/pytorch/pytorch/pull/129429,pytorchupdatebot,pytorchbot,,,
3240bff56a8,skip,not user facing,[benchmarking] Add join_results.py (#129202),benchmarks/dynamo/join_results.py,https://github.com/pytorch/pytorch/pull/129202,jansel,shunting314,yanboliang,,
78a0b010eb9,skip,not user facing,Refine XPU UTs (#130138),test/test_xpu.py,https://github.com/pytorch/pytorch/pull/130138,guangyey,EikanWang,,,
8f1c2e1e28f,skip,not user facing,[pt2-bench] pass acc test if ref is NaN (#129996),test/dynamo/test_utils.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/129996,shunting314,jansel,,,
c0735a3dd36,skip,not user facing,[pt2-bench] fix accuracy failure for a few models (#129941),benchmarks/dynamo/common.py benchmarks/dynamo/huggingface.py benchmarks/dynamo/timm_models.py benchmarks/dynamo/torchbench.py benchmarks/dynamo/torchbench.yaml test/dynamo/test_utils.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/129941,shunting314,jansel,,,
8f6765f7a7e,skip,not user facing,[pt2-bench] fix accuracy failure for beit_base_patch16_224 during training (#130005),benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_timm_training.csv benchmarks/dynamo/timm_models.py,https://github.com/pytorch/pytorch/pull/130005,shunting314,eellison,jansel,,
4b05d9d2333,skip,Untopiced,"Revert ""[NCCL] Make sure current device is correct in `torch.distributed.barrier()`'s `streamSynchronize` (#129908)""",torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,,,,,,
eea4ece256d,skip,Untopiced,"Revert ""[audio hash update] update the pinned audio hash (#129429)""",.github/ci_commit_pins/audio.txt,,,,,,
e7ab7b83bc3,inductor,Untopiced,Have torch_key hash entire torch directory (#129250),torch/_functorch/_aot_autograd/autograd_cache.py torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/129250,jamesjwu,oulgen,,,
a33ee73a284,releng,not user facing,Upload perf stats to both Rockset and dynamoDB  (#129544),.github/workflows/upload-test-stats.yml .github/workflows/upload-torch-dynamo-perf-stats.yml tools/stats/upload_dynamo_perf_stats.py tools/stats/upload_stats_lib.py,https://github.com/pytorch/pytorch/pull/129544,huydhn,clee2000,,,
7192ee07350,nested tensor_frontend,bug fixes,Default to input tensor device for as_nested_tensor(t) (#130050),test/test_nestedtensor.py torch/nested/__init__.py,https://github.com/pytorch/pytorch/pull/130050,jbschlosser,YuqingJ,,,
7ea8a3c9b81,dynamo,not user facing,[dynamo] Validate check_fn (#118448),test/dynamo/test_subclasses.py test/dynamo_expected_failures/TestAOTModuleSimplified.test_aot_module_simplified_dynamic test/export/test_export.py test/inductor/test_compiled_autograd.py torch/_dynamo/guards.py,https://github.com/pytorch/pytorch/pull/118448,anijain2305,ezyang,jansel,,
7c5f3cd049d,Uncategorized,Untopiced,Add explain function to TSConverter. (#129968),torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/129968,jiashenC,angelayi,,,
889ed48a229,releng,not user facing,Fix missing id-token write in upload stats (#130153),.github/workflows/upload-test-stats.yml .github/workflows/upload-torch-dynamo-perf-stats.yml,https://github.com/pytorch/pytorch/pull/130153,huydhn,clee2000,,,
784e3b41239,skip,Untopiced,"Revert ""Change numeric_debug_handle to store per-node id (#129811)""",test/quantization/pt2e/test_generate_numeric_debug_handle.py torch/ao/quantization/__init__.py torch/ao/quantization/fx/convert.py torch/ao/quantization/pt2e/generate_numeric_debug_handle.py torch/ao/quantization/pt2e/prepare.py,,,,,,
8ff243bcf19,skip,not user facing,Change depreacate warning on dispatch_on_subclass to warn once (#130047),torch/csrc/utils/python_arg_parser.cpp,https://github.com/pytorch/pytorch/pull/130047,wz337,XilunWu,,,
3d56673b240,skip,not user facing,"[Split Build][BE] remove extraneous .py, .a, and .so files (#130053)",setup.py,https://github.com/pytorch/pytorch/pull/130053,PaliC,atalman,,,
faebaef0892,releng,not user facing,[EZ] Fix typo in upload stats OIDC rolename (#130168),.github/workflows/upload-test-stats.yml .github/workflows/upload-torch-dynamo-perf-stats.yml,https://github.com/pytorch/pytorch/pull/130168,huydhn,atalman,kit1980,malfet,
739fc01ac91,distributed,bug fixes,[NCCL] Make sure current device is correct in `torch.distributed.barrier()`'s `streamSynchronize` (#129908),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/129908,eqy,kwen2501,,,
18e75c098b2,releng,not user facing,[DCP] Adds Checkpointing Team (dcp) to merge rules (#129582),.github/merge_rules.yaml,https://github.com/pytorch/pytorch/pull/129582,LucasLLC,fegin,,,
df504522795,skip,not user facing,Pin optree==0.11.0 on windows CI (#130155),setup.py,https://github.com/pytorch/pytorch/pull/130155,clee2000,atalman,huydhn,,
6fc771d19b1,skip,Untopiced,"Revert ""Change depreacate warning on dispatch_on_subclass to warn once (#130047)""",torch/csrc/utils/python_arg_parser.cpp,,,,,,
d1d0a7080fb,nn_frontend,not user facing,[torchgen] reference generated comment to actual location of the generator and template (#130020),tools/pyi/gen_pyi.py torch/_C/_nn.pyi.in torch/_C/return_types.pyi.in torch/nn/functional.pyi.in torchgen/utils.py,https://github.com/pytorch/pytorch/pull/130020,XuehaiPan,ezyang,,,
126796d2395,distributed,not user facing,[c10d] fixing an UT after a change in eager mode new group (#130167),test/distributed/test_c10d_nccl.py,https://github.com/pytorch/pytorch/pull/130167,shuqiangzhang,wconstab,,,
e5841bb8d5a,skip,not user facing,[3/N] Enforce unused-function and unused-variable checks (#130084),cmake/public/utils.cmake,https://github.com/pytorch/pytorch/pull/130084,cyyever,ezyang,,,
0fcbca9adbe,skip,not user facing,[pt2-bench] use eval mode for vision_maskrcnn (#130163),benchmarks/dynamo/torchbench.py,https://github.com/pytorch/pytorch/pull/130163,shunting314,jansel,,,
c5ede865c4e,skip,not user facing,[pt2-bench] raise tolerance for squeezenet1_1 (#130165),benchmarks/dynamo/torchbench.yaml,https://github.com/pytorch/pytorch/pull/130165,shunting314,jansel,,,
1927c406844,skip,Untopiced,Fix the SDPA AOT export issue (#130164),torch/_subclasses/functional_tensor.py,https://github.com/pytorch/pytorch/pull/130164,sijiac,zou3519,,,
a1a2023eb86,distributed,Untopiced,"Back out ""Pass device to is_pinned call inside TensorProperties.create_from_tensor"" (#129972)",test/distributed/checkpoint/test_utils.py torch/distributed/_shard/sharded_tensor/metadata.py torch/distributed/checkpoint/metadata.py,https://github.com/pytorch/pytorch/pull/129972,daulet-askarov,LucasLLC,,,
bd0252fb98f,dynamo,not user facing,[dynamo][user-defined] Support method descriptors (#130159),test/dynamo/test_misc.py test/dynamo_expected_failures/TestDistributionShapes.test_mixture_same_family_shape test/dynamo_expected_failures/TestFunctors.test_cat_transform test/dynamo_expected_failures/TestFunctors.test_cat_transform_non_uniform torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/130159,anijain2305,jansel,,,
7c43f59a456,skip,not user facing,[audio hash update] update the pinned audio hash (#129429),.github/ci_commit_pins/audio.txt,https://github.com/pytorch/pytorch/pull/129429,pytorchupdatebot,pytorchbot,,,
0267b2ddcb5,skip,Untopiced,[runtime asserts] deduplicate runtime asserts & CSE (#128599),test/dynamo/test_export.py test/dynamo/test_higher_order_ops.py test/dynamo/test_logging.py test/dynamo/test_misc.py test/dynamo/test_repros.py test/dynamo/test_subgraphs.py test/export/test_export.py test/export/test_passes.py test/onnx/onnx_test_common.py test/onnx/test_fx_to_onnx_with_onnxruntime.py torch/_export/serde/serialize.py torch/export/_trace.py torch/export/exported_program.py torch/export/unflatten.py torch/fx/passes/runtime_assert.py torch/onnx/_internal/exporter.py torch/utils/_sympy/interp.py torch/utils/_sympy/reference.py,https://github.com/pytorch/pytorch/pull/128599,pianpwk,ezyang,,,
c5110f63887,inductor,Untopiced,[halide-backend] Use 0D scalar inputs/outputs (#130129),test/inductor/test_torchinductor.py torch/_inductor/codegen/halide.py,https://github.com/pytorch/pytorch/pull/130129,jansel,shunting314,,,
acd03ca2d92,inductor,Untopiced,[halide-backend] Support scan kernels (#129035),test/inductor/test_halide.py test/inductor/test_torchinductor.py torch/_inductor/codegen/halide.py torch/_inductor/config.py torch/_inductor/ops_handler.py,https://github.com/pytorch/pytorch/pull/129035,jansel,eellison,shunting314,,
10c831567b3,inductor,not user facing,Make sympify'ing SymInt/etc produce their sympy expression (#130166),test/test_dynamic_shapes.py torch/__init__.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/130166,ezyang,yf225,,,
bf609630aed,inductor,not user facing,Fix a bunch of stride issues with FlexAttention (#130160),test/inductor/test_flex_attention.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/130160,Chillee,yanboliang,,,
e019540c9ec,skip,Untopiced,"Revert ""Fix the SDPA AOT export issue (#130164)""",torch/_subclasses/functional_tensor.py,,,,,,
aa4899eee96,profiler,bug fixes,[CCA][Memory Snapshot] Fix race on alloc_trace vector - S430480 (#130180),c10/cuda/CUDACachingAllocator.cpp,https://github.com/pytorch/pytorch/pull/130180,aaronenyeshi,eqy,kit1980,,
963f430d13e,skip,Untopiced,"Revert ""[runtime asserts] deduplicate runtime asserts & CSE (#128599)""",test/dynamo/test_export.py test/dynamo/test_higher_order_ops.py test/dynamo/test_logging.py test/dynamo/test_misc.py test/dynamo/test_repros.py test/dynamo/test_subgraphs.py test/export/test_export.py test/export/test_passes.py test/onnx/onnx_test_common.py test/onnx/test_fx_to_onnx_with_onnxruntime.py torch/_export/serde/serialize.py torch/export/_trace.py torch/export/exported_program.py torch/export/unflatten.py torch/fx/passes/runtime_assert.py torch/onnx/_internal/exporter.py torch/utils/_sympy/interp.py torch/utils/_sympy/reference.py,,,,,,
da66e50e6e5,skip,not user facing,Added compile option to create_block_mask (#130106),benchmarks/transformer/score_mod.py test/inductor/test_flex_attention.py torch/_higher_order_ops/flex_attention.py torch/nn/attention/_flex_attention.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/130106,Chillee,yanboliang,,,
520a4642bf5,releng,not user facing,[CI] Enable build with asserts (#129924),.ci/pytorch/build.sh .ci/pytorch/test.sh aten/src/ATen/cuda/EmptyTensor.cpp aten/src/ATen/native/transformers/attention.cpp test/inductor/test_torchinductor_dynamic_shapes.py tools/autograd/gen_variable_type.py torch/CMakeLists.txt torch/csrc/autograd/autograd_not_implemented_fallback.cpp,https://github.com/pytorch/pytorch/pull/129924,malfet,albanD,atalman,,
096eca2f9a5,skip,not user facing,[2/N] Replace exceptions with static_assert(false) in some templates  (#130116),aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/native/cudnn/RNN.cpp aten/src/ATen/native/mkldnn/RNN.cpp,https://github.com/pytorch/pytorch/pull/130116,cyyever,eqy,ezyang,,
2f219f7d793,quantization,Untopiced,Enforce unused-{variable/function} checks to all torch targets (#130189),aten/src/ATen/native/quantized/cudnn/utils.h cmake/public/utils.cmake,https://github.com/pytorch/pytorch/pull/130189,cyyever,ezyang,,,
01ec03bac61,inductor,not user facing,[inductor] switch HalideCodeCache to new cpp_builder. (#130146),torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/130146,xuhancn,jansel,jgong5,,
3f50e197c47,skip,not user facing,[BE] annotate `torch.autograd.graph` (#129558),torch/_C/__init__.pyi.in torch/autograd/graph.py,https://github.com/pytorch/pytorch/pull/129558,XuehaiPan,soulitzer,,,
dfe3534134a,sparse_frontend,not user facing,[1/N] Fix NVCC warnings (#130191),aten/src/ATen/native/sparse/cuda/SparseMatMul.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/gemm/custom_mma_multistage.h aten/src/ATen/native/transformers/cuda/mem_eff_attention/gemm/custom_mma_pipelined.h,https://github.com/pytorch/pytorch/pull/130191,cyyever,Skylion007,,,
dc5f37193f8,skip,not user facing,[inductor] switch AotCodeCompiler to new cpp_builder (#130127),torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/130127,xuhancn,jansel,jgong5,,
3957b3b3497,skip,not user facing,[inductor] switch CppCodeCache to new cpp_builder. (#130132),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/130132,xuhancn,jansel,jgong5,,
3d138af9436,inductor,not user facing,[Inductor] First implementation of the B2B-GEMM pass with tests (#129995),test/inductor/test_b2b_gemm.py torch/_inductor/config.py torch/_inductor/fx_passes/b2b_gemm.py torch/_inductor/fx_passes/post_grad.py,https://github.com/pytorch/pytorch/pull/129995,sdingcn,eellison,,,
9983242c8e7,inductor,Untopiced,[inductor] support adding a new inductor backend using PrivateUse1  (#129953),torch/_inductor/codegen/common.py,https://github.com/pytorch/pytorch/pull/129953,peaceorwell,jansel,,,
a3ce9eddd69,skip,not user facing,[BE][Easy] apply autofix for ruff rule unnecessary-literal-set (C405) and unnecessary-map (C417) (#130198),test/dynamo/test_repros.py third_party/generate-xnnpack-wrappers.py,https://github.com/pytorch/pytorch/pull/130198,XuehaiPan,Skylion007,,,
98929ceae38,skip,not user facing,[Inductor][CPP] Enable Local Buffer for Outer loop fusion (#126967),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/metrics.py torch/_inductor/virtualized.py,https://github.com/pytorch/pytorch/pull/126967,leslie-fang-intel,jgong5,peterbell10,,
f794cf59bd0,skip,not user facing,[Inductor][CPP] Support more than one LocalBuffer (#129121),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cpp_utils.py,https://github.com/pytorch/pytorch/pull/129121,leslie-fang-intel,jgong5,peterbell10,,
1b57dce35fb,skip,Untopiced,"Revert ""[Inductor][CPP] Support more than one LocalBuffer (#129121)""",test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cpp_utils.py,,,,,,
e423224546a,skip,Untopiced,"Revert ""[Inductor][CPP] Enable Local Buffer for Outer loop fusion (#126967)""",test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/metrics.py torch/_inductor/virtualized.py,,,,,,
0c44684901a,skip,not user facing,[Typo] Fix typo in DispatchKeyExtractor.h (#130221),aten/src/ATen/core/dispatch/DispatchKeyExtractor.h,https://github.com/pytorch/pytorch/pull/130221,nautsimon,Skylion007,,,
940e4477ab0,skip,Untopiced,[runtime asserts] deduplicate runtime asserts & CSE (#128599),test/dynamo/test_export.py test/dynamo/test_higher_order_ops.py test/dynamo/test_logging.py test/dynamo/test_misc.py test/dynamo/test_repros.py test/dynamo/test_subgraphs.py test/export/test_export.py test/export/test_passes.py test/onnx/onnx_test_common.py test/onnx/test_fx_to_onnx_with_onnxruntime.py torch/_dynamo/backends/cudagraphs.py torch/_export/serde/serialize.py torch/export/_trace.py torch/export/exported_program.py torch/export/unflatten.py torch/fx/passes/runtime_assert.py torch/onnx/_internal/exporter.py torch/utils/_sympy/interp.py torch/utils/_sympy/reference.py,https://github.com/pytorch/pytorch/pull/128599,pianpwk,ezyang,,,
d1b832e739d,inductor,not user facing,[inductor][mkl][inline-inbuilt-nn-modules] Change assertion (#130219),torch/_inductor/mkldnn_ir.py,https://github.com/pytorch/pytorch/pull/130219,anijain2305,leslie-fang-intel,,,
d0ad13fa42f,skip,Untopiced,[ROCm] Add int4 support (#129710),aten/src/ATen/native/cuda/int4mm.cu test/test_linalg.py torch/testing/_internal/common_cuda.py torch/utils/hipify/cuda_to_hip_mappings.py,https://github.com/pytorch/pytorch/pull/129710,jerrymannil,malfet,,,
c5c9dbece10,dynamo,not user facing,[dynamo][user-defined] Simplify and improve scope of UserDefinedObject var_getattr (#130169),test/dynamo/test_export.py test/dynamo_expected_failures/TestQuantizeFx.test_prepare_custom_config_set_standalone_module_class test/dynamo_expected_failures/TestSerialization.test_serialization_dill test/dynamo_expected_failures/TestTensorProtoSummary.test_half_tensor_proto_bfloat16_proto_type_14 test/dynamo_expected_failures/TestTensorProtoSummary.test_half_tensor_proto_float16_proto_type_19 test/functorch/test_control_flow.py test/test_tensorboard.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/130169,anijain2305,jansel,,,
31bb65de195,inductor,not user facing,[Inductor] Fix conditional codegen (#129492),test/inductor/test_control_flow.py torch/_inductor/codegen/wrapper.py,https://github.com/pytorch/pytorch/pull/129492,sijiac,aakhundov,,,
f053be2a97e,dynamo,not user facing,[dynamo] Graph break on random_ op (#130222),test/dynamo/test_repros.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/130222,anijain2305,jansel,,,
f4dcf2ae93c,skip,Untopiced,[1/N] Change #include <c10/util/Optional.h> to #include <optional> (#128301),aten/src/ATen/CPUGeneratorImpl.h aten/src/ATen/InferSize.h aten/src/ATen/SavedTensorHooks.cpp aten/src/ATen/SavedTensorHooks.h aten/src/ATen/TensorIndexing.h aten/src/ATen/native/BatchLinearAlgebra.h aten/src/ATen/record_function.h c10/core/ConstantSymNodeImpl.h c10/core/ScalarTypeToTypeMeta.h c10/core/SymBool.h c10/core/SymInt.h c10/core/SymIntArrayRef.h c10/core/SymNodeImpl.h c10/core/SymbolicShapeMeta.cpp c10/core/TensorImpl.cpp c10/core/TensorImpl.h c10/core/TensorOptions.h c10/core/UndefinedTensorImpl.cpp c10/core/impl/InlineDeviceGuard.h c10/core/impl/InlineStreamGuard.h c10/core/impl/PyObjectSlot.h c10/core/impl/TorchDispatchModeTLS.cpp c10/cuda/CUDACachingAllocator.cpp c10/cuda/CUDAFunctions.cpp c10/cuda/CUDAGuard.h c10/cuda/impl/CUDAGuardImpl.h c10/test/core/DeviceGuard_test.cpp c10/test/core/SymInt_test.cpp c10/test/core/impl/InlineDeviceGuard_test.cpp c10/test/core/impl/InlineStreamGuard_test.cpp c10/test/util/optional_test.cpp c10/util/Backtrace.cpp c10/util/OptionalArrayRef.h c10/xpu/test/impl/XPUStreamTest.cpp torch/csrc/Module.cpp torch/csrc/Storage.cpp torch/csrc/api/include/torch/expanding_array.h torch/csrc/api/include/torch/fft.h torch/csrc/api/include/torch/nested.h torch/csrc/api/include/torch/nn/functional/activation.h torch/csrc/api/include/torch/nn/functional/embedding.h torch/csrc/api/include/torch/nn/functional/loss.h torch/csrc/api/include/torch/nn/functional/normalization.h torch/csrc/api/include/torch/nn/functional/pooling.h torch/csrc/api/include/torch/nn/functional/upsampling.h torch/csrc/api/include/torch/nn/modules/batchnorm.h torch/csrc/api/include/torch/nn/modules/conv.h torch/csrc/api/include/torch/nn/modules/pooling.h torch/csrc/api/include/torch/nn/modules/utils.h torch/csrc/api/include/torch/nn/options/activation.h torch/csrc/api/include/torch/nn/options/embedding.h torch/csrc/api/include/torch/nn/options/loss.h torch/csrc/api/include/torch/nn/options/normalization.h torch/csrc/api/include/torch/nn/options/pooling.h torch/csrc/api/include/torch/nn/options/upsampling.h torch/csrc/api/include/torch/nn/options/vision.h torch/csrc/api/include/torch/nn/utils/clip_grad.h torch/csrc/api/include/torch/nn/utils/convert_parameters.h torch/csrc/api/include/torch/optim/lbfgs.h torch/csrc/api/include/torch/optim/optimizer.h torch/csrc/api/include/torch/serialize/input-archive.h torch/csrc/api/include/torch/types.h torch/csrc/api/src/jit.cpp torch/csrc/api/src/nn/modules/activation.cpp torch/csrc/api/src/nn/modules/conv.cpp torch/csrc/api/src/nn/modules/embedding.cpp torch/csrc/api/src/nn/modules/pooling.cpp torch/csrc/api/src/nn/modules/upsampling.cpp torch/csrc/api/src/optim/lbfgs.cpp torch/csrc/api/src/serialize/input-archive.cpp torch/csrc/autograd/FunctionsManual.cpp torch/csrc/autograd/FunctionsManual.h torch/csrc/autograd/TraceTypeManual.cpp torch/csrc/autograd/VariableTypeManual.cpp torch/csrc/autograd/VariableTypeUtils.h torch/csrc/autograd/autograd.h torch/csrc/autograd/autograd_not_implemented_fallback.cpp torch/csrc/autograd/engine.cpp torch/csrc/autograd/function.h torch/csrc/autograd/functions/accumulate_grad.h torch/csrc/autograd/functions/comm.cpp torch/csrc/autograd/functions/comm.h torch/csrc/autograd/init.cpp torch/csrc/autograd/input_buffer.cpp torch/csrc/autograd/input_buffer.h torch/csrc/autograd/profiler_legacy.cpp torch/csrc/autograd/profiler_legacy.h torch/csrc/autograd/profiler_python.cpp torch/csrc/autograd/python_function.cpp torch/csrc/autograd/python_function.h torch/csrc/autograd/python_variable.cpp torch/csrc/autograd/python_variable_indexing.cpp torch/csrc/autograd/record_function_ops.h torch/csrc/autograd/utils/grad_layout_contract.h torch/csrc/autograd/utils/python_arg_parsing.h torch/csrc/autograd/variable.h torch/csrc/cuda/comm.cpp torch/csrc/cuda/comm.h torch/csrc/cuda/memory_snapshot.h torch/csrc/cuda/nccl.h torch/csrc/cuda/python_nccl.cpp torch/csrc/distributed/autograd/engine/dist_engine.cpp torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupGloo.cpp torch/csrc/distributed/c10d/ProcessGroupGloo.hpp torch/csrc/distributed/c10d/ProcessGroupMPI.cpp torch/csrc/distributed/c10d/ProcessGroupMPI.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/TCPStore.cpp torch/csrc/distributed/c10d/TCPStore.hpp torch/csrc/distributed/c10d/Types.hpp torch/csrc/distributed/c10d/Utils.hpp torch/csrc/distributed/c10d/Work.hpp torch/csrc/distributed/c10d/init.cpp torch/csrc/distributed/c10d/intra_node_comm.hpp torch/csrc/distributed/c10d/logger.cpp torch/csrc/distributed/c10d/reducer.cpp torch/csrc/distributed/c10d/reducer.hpp torch/csrc/distributed/c10d/reducer_cuda.cpp torch/csrc/distributed/c10d/reducer_timer.hpp torch/csrc/distributed/c10d/sequence_num.cpp torch/csrc/distributed/c10d/sequence_num.hpp torch/csrc/distributed/rpc/profiler/remote_profiler_manager.cpp torch/csrc/distributed/rpc/profiler/remote_profiler_manager.h torch/csrc/distributed/rpc/py_rref.cpp torch/csrc/distributed/rpc/python_functions.cpp torch/csrc/distributed/rpc/request_callback_no_python.cpp torch/csrc/distributed/rpc/rref_impl.h torch/csrc/distributed/rpc/script_call.h torch/csrc/distributed/rpc/tensorpipe_cuda.cpp torch/csrc/distributed/rpc/tensorpipe_utils.cpp torch/csrc/dynamo/python_compiled_autograd.cpp torch/csrc/functorch/init.cpp torch/csrc/inductor/aoti_torch/utils.h torch/csrc/jit/api/compilation_unit.h torch/csrc/jit/api/function_impl.h torch/csrc/jit/api/module.cpp torch/csrc/jit/api/module.h torch/csrc/jit/api/object.cpp torch/csrc/jit/api/object.h torch/csrc/jit/codegen/fuser/compiler.cpp torch/csrc/jit/codegen/fuser/cpu/fused_kernel.cpp torch/csrc/jit/codegen/fuser/executor.cpp torch/csrc/jit/codegen/fuser/kernel_spec.h torch/csrc/jit/codegen/onednn/graph_helper.cpp torch/csrc/jit/codegen/onednn/graph_rewriter.cpp torch/csrc/jit/codegen/onednn/prepare_binary.cpp torch/csrc/jit/cuda/cuda.h torch/csrc/jit/frontend/builtin_functions.cpp torch/csrc/jit/frontend/canonicalize_modified_loop.cpp torch/csrc/jit/frontend/concrete_module_type.cpp torch/csrc/jit/frontend/function_schema_parser.cpp torch/csrc/jit/frontend/ir_emitter.cpp torch/csrc/jit/frontend/parse_string_literal.h torch/csrc/jit/frontend/parser.cpp torch/csrc/jit/frontend/schema_matching.cpp torch/csrc/jit/frontend/schema_matching.h torch/csrc/jit/frontend/schema_type_parser.cpp torch/csrc/jit/frontend/script_type_parser.cpp torch/csrc/jit/frontend/source_range.cpp torch/csrc/jit/frontend/source_range.h torch/csrc/jit/frontend/sugared_value.cpp torch/csrc/jit/frontend/sugared_value.h torch/csrc/jit/frontend/tracer.cpp torch/csrc/jit/ir/alias_analysis.cpp torch/csrc/jit/ir/constants.cpp torch/csrc/jit/ir/constants.h torch/csrc/jit/ir/ir.cpp torch/csrc/jit/ir/ir.h torch/csrc/jit/ir/scope.h torch/csrc/jit/mobile/compatibility/backport_manager.cpp torch/csrc/jit/mobile/compatibility/runtime_compatibility.h torch/csrc/jit/mobile/flatbuffer_loader.cpp torch/csrc/jit/mobile/flatbuffer_loader.h torch/csrc/jit/mobile/frame.h torch/csrc/jit/mobile/function.cpp torch/csrc/jit/mobile/import.cpp torch/csrc/jit/mobile/import.h torch/csrc/jit/mobile/import_data.h torch/csrc/jit/mobile/model_tracer/MobileModelRunner.h torch/csrc/jit/mobile/model_tracer/TracerRunner.cpp torch/csrc/jit/mobile/module.cpp torch/csrc/jit/mobile/promoted_prim_ops.cpp torch/csrc/jit/operator_upgraders/upgraders_entry.cpp torch/csrc/jit/operator_upgraders/utils.cpp torch/csrc/jit/operator_upgraders/utils.h torch/csrc/jit/passes/autocast.cpp torch/csrc/jit/passes/canonicalize.cpp torch/csrc/jit/passes/canonicalize_graph_fuser_ops.cpp torch/csrc/jit/passes/constant_propagation.cpp torch/csrc/jit/passes/create_autodiff_subgraphs.cpp torch/csrc/jit/passes/device_type_analysis.cpp torch/csrc/jit/passes/dtype_analysis.cpp torch/csrc/jit/passes/erase_number_types.cpp torch/csrc/jit/passes/freeze_module.cpp torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp torch/csrc/jit/passes/graph_fuser.cpp torch/csrc/jit/passes/graph_rewrite_helper.cpp torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp torch/csrc/jit/passes/integer_value_refinement.cpp torch/csrc/jit/passes/onnx/constant_fold.cpp torch/csrc/jit/passes/onnx/constant_fold.h torch/csrc/jit/passes/onnx/constant_map.cpp torch/csrc/jit/passes/onnx/function_extraction.cpp torch/csrc/jit/passes/onnx/list_model_parameters.cpp torch/csrc/jit/passes/onnx/pattern_conversion/pattern_conversion.cpp torch/csrc/jit/passes/onnx/pattern_conversion/pattern_encapsulation.cpp torch/csrc/jit/passes/onnx/peephole.cpp torch/csrc/jit/passes/onnx/scalar_type_analysis.cpp torch/csrc/jit/passes/onnx/shape_type_inference.cpp torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp torch/csrc/jit/passes/peephole_dict_idioms.cpp torch/csrc/jit/passes/peephole_list_idioms.cpp torch/csrc/jit/passes/quantization/helper.cpp torch/csrc/jit/passes/quantization/helper.h torch/csrc/jit/passes/quantization/insert_observers.cpp torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp torch/csrc/jit/passes/remove_mutation.h torch/csrc/jit/passes/replacement_of_old_operators.cpp torch/csrc/jit/passes/shape_analysis.cpp torch/csrc/jit/passes/symbolic_shape_analysis.cpp torch/csrc/jit/passes/symbolic_shape_cache.cpp torch/csrc/jit/passes/symbolic_shape_runtime_fusion.cpp torch/csrc/jit/passes/tensorexpr_fuser.cpp torch/csrc/jit/passes/utils/check_alias_annotation.cpp torch/csrc/jit/passes/utils/memory_dag.h torch/csrc/jit/passes/utils/subgraph_utils.cpp torch/csrc/jit/python/init.cpp torch/csrc/jit/python/module_python.h torch/csrc/jit/python/pybind_utils.cpp torch/csrc/jit/python/pybind_utils.h torch/csrc/jit/python/python_ir.cpp torch/csrc/jit/python/python_ivalue.h torch/csrc/jit/python/python_list.h torch/csrc/jit/python/python_sugared_value.cpp torch/csrc/jit/python/python_sugared_value.h torch/csrc/jit/python/python_tree_views.cpp torch/csrc/jit/python/script_init.cpp torch/csrc/jit/runtime/autodiff.cpp torch/csrc/jit/runtime/decomposition_registry.cpp torch/csrc/jit/runtime/graph_executor.h torch/csrc/jit/runtime/graph_executor_impl.h torch/csrc/jit/runtime/interpreter.cpp torch/csrc/jit/runtime/interpreter.h torch/csrc/jit/runtime/jit_exception.h torch/csrc/jit/runtime/operator.h torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp torch/csrc/jit/runtime/register_ops_utils.h torch/csrc/jit/runtime/register_prim_ops.cpp torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp torch/csrc/jit/runtime/register_special_ops.cpp torch/csrc/jit/runtime/simple_graph_executor_impl.cpp torch/csrc/jit/runtime/static/fusion.cpp torch/csrc/jit/runtime/static/impl.cpp torch/csrc/jit/runtime/static/ops.cpp torch/csrc/jit/runtime/static/ops.h torch/csrc/jit/runtime/symbolic_script.cpp torch/csrc/jit/runtime/symbolic_script.h torch/csrc/jit/runtime/symbolic_shape_registry.cpp torch/csrc/jit/serialization/callstack_debug_info_serialization.cpp torch/csrc/jit/serialization/export.cpp torch/csrc/jit/serialization/export_bytecode.cpp torch/csrc/jit/serialization/export_module.cpp torch/csrc/jit/serialization/flatbuffer_serializer.cpp torch/csrc/jit/serialization/import.h torch/csrc/jit/serialization/import_source.cpp torch/csrc/jit/serialization/import_source.h torch/csrc/jit/serialization/pickle.cpp torch/csrc/jit/serialization/pickler.cpp torch/csrc/jit/serialization/python_print.cpp torch/csrc/jit/serialization/source_range_serialization.cpp torch/csrc/jit/tensorexpr/codegen.cpp torch/csrc/jit/tensorexpr/eval.cpp torch/csrc/jit/tensorexpr/expr.h torch/csrc/jit/tensorexpr/external_functions.cpp torch/csrc/jit/tensorexpr/external_functions.h torch/csrc/jit/tensorexpr/graph_opt.cpp torch/csrc/jit/tensorexpr/ir.h torch/csrc/jit/tensorexpr/ir_simplifier.cpp torch/csrc/jit/tensorexpr/kernel.cpp torch/csrc/jit/tensorexpr/llvm_codegen.cpp torch/csrc/jit/tensorexpr/llvm_codegen.h torch/csrc/jit/tensorexpr/llvm_jit.h torch/csrc/jit/tensorexpr/operators/conv2d.cpp torch/csrc/jit/tensorexpr/operators/misc.cpp torch/csrc/jit/tensorexpr/operators/pointwise.h torch/csrc/jit/tensorexpr/operators/quantization.cpp torch/csrc/jit/tensorexpr/operators/softmax.cpp torch/csrc/jit/tensorexpr/tensor.cpp torch/csrc/jit/tensorexpr/tensor.h torch/csrc/jit/testing/file_check.cpp torch/csrc/lazy/backend/backend_device.cpp torch/csrc/lazy/backend/backend_device.h torch/csrc/lazy/core/ir_builder.h torch/csrc/lazy/core/ir_dump_util.cpp torch/csrc/lazy/core/lazy_graph_executor.cpp torch/csrc/lazy/core/shape.cpp torch/csrc/lazy/core/shape.h torch/csrc/lazy/core/shape_inference.h torch/csrc/lazy/core/tensor.cpp torch/csrc/lazy/core/unique.h torch/csrc/lazy/core/util.h torch/csrc/lazy/python/python_util.cpp torch/csrc/lazy/python/python_util.h torch/csrc/lazy/ts_backend/ir_builder.h torch/csrc/lazy/ts_backend/ts_eager_fallback.cpp torch/csrc/lazy/ts_backend/ts_native_functions.cpp torch/csrc/profiler/collection.cpp torch/csrc/profiler/collection.h torch/csrc/profiler/python/init.cpp torch/csrc/profiler/unwind/unwind.cpp torch/csrc/profiler/unwind/unwind.h torch/csrc/profiler/unwind/unwind_error.h torch/csrc/profiler/util.h torch/csrc/tensor/python_tensor.cpp torch/csrc/utils/nested.cpp torch/csrc/utils/python_arg_parser.cpp torch/csrc/utils/python_arg_parser.h torch/csrc/utils/python_dispatch.cpp torch/csrc/utils/python_raii.h torch/csrc/utils/python_symnode.h torch/csrc/utils/schema_info.cpp torch/csrc/utils/tensor_new.cpp torch/csrc/utils/torch_dispatch_mode.h torch/custom_class_detail.h torch/library.h,https://github.com/pytorch/pytorch/pull/128301,cyyever,ezyang,r-barnes,,
010009e6421,skip,not user facing,[compiled autograd] c++ autograd function saved_data: lift tensors (#130057),test/inductor/test_compiled_autograd.py torch/csrc/dynamo/compiled_autograd.h,https://github.com/pytorch/pytorch/pull/130057,xmfan,jansel,,,
16d53cb7d58,inductor,not user facing,Only run mixed_mm heuristic if shapes are static (#130081),test/inductor/test_pattern_matcher.py torch/_inductor/kernel/mm.py,https://github.com/pytorch/pytorch/pull/130081,AlnisM,Chillee,,,
acf9e31cf89,skip,not user facing,adding MTIA to supported activities (#130052),torch/csrc/autograd/init.cpp,https://github.com/pytorch/pytorch/pull/130052,fenypatel99,aaronenyeshi,,,
c8ab2e8b637,onnx,not user facing,Set seed per sample for OpInfo tests + support for restricting to a single sample input (#128238),test/functorch/test_eager_transforms.py test/functorch/test_ops.py test/onnx/test_fx_op_consistency.py test/test_decomp.py test/test_maskedtensor.py test/test_mps.py torch/testing/_internal/common_device_type.py torch/testing/_internal/common_methods_invocations.py torch/testing/_internal/common_utils.py torch/testing/_internal/opinfo/core.py torch/testing/_internal/opinfo/definitions/_masked.py torch/testing/_internal/opinfo/definitions/fft.py torch/testing/_internal/opinfo/definitions/linalg.py,https://github.com/pytorch/pytorch/pull/128238,jbschlosser,janeyx99,justinchuby,,
d7b7f8b79f4,skip,Untopiced,"Revert ""[ROCm] Add int4 support (#129710)""",aten/src/ATen/native/cuda/int4mm.cu test/test_linalg.py torch/testing/_internal/common_cuda.py torch/utils/hipify/cuda_to_hip_mappings.py,,,,,,
f4774d64bf9,profiler,not user facing,Skip test_profile_memory on windows (#130037),test/profiler/test_cpp_thread.py,https://github.com/pytorch/pytorch/pull/130037,clee2000,aaronenyeshi,huydhn,,
63743b223c0,quantization,Untopiced,[AO] catch qparam mismatch for cat (#123769),aten/src/ATen/native/quantized/cpu/TensorShape.cpp,https://github.com/pytorch/pytorch/pull/123769,EdanSneh,jerryzh168,,,
61017eb77b7,skip,not user facing,Add missing mapping between DLDevice and ATenDevice for MAIA (#129615),aten/src/ATen/DLConvertor.cpp aten/src/ATen/dlpack.h,https://github.com/pytorch/pytorch/pull/129615,abhi-ort,albanD,,,
a18568f2934,distributed,not user facing,[dtensor][debug] Added functionality to convert log into a json file (#129994),torch/distributed/_tensor/debug/comm_mode.py torch/distributed/_tensor/examples/comm_mode_features_example.py torch/distributed/_tensor/examples/comm_mode_features_example_argparser.py,https://github.com/pytorch/pytorch/pull/129994,sinhaanshul,XilunWu,,,
d325aaef39b,inductor,Untopiced,[halide-backend] Use get_reduction_combine_fn for reduction ops (#130212),torch/_inductor/codegen/halide.py,https://github.com/pytorch/pytorch/pull/130212,jansel,eellison,,,
b428f1ad77a,skip,not user facing,"[3.12, 3.13, dynamo] simplified construction for frame f_locals/localsplus (#129185)",build_variables.bzl torch/csrc/dynamo/cpython_defs.c torch/csrc/dynamo/cpython_defs.h torch/csrc/dynamo/cpython_includes.h torch/csrc/dynamo/debug_macros.h torch/csrc/dynamo/eval_frame.c torch/csrc/dynamo/extra_state.cpp torch/csrc/dynamo/framelocals_mapping.cpp torch/csrc/dynamo/framelocals_mapping.h,https://github.com/pytorch/pytorch/pull/129185,williamwen42,jansel,,,
e16276b9bf9,rocm,Untopiced,[ROCm] Check supported archs before setting preferred blas backend to hipblasLT (#128753),aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/cuda/detail/CUDAHooks.cpp aten/src/ATen/cuda/detail/CUDAHooks.h aten/src/ATen/detail/CUDAHooksInterface.h,https://github.com/pytorch/pytorch/pull/128753,jithunnair-amd,malfet,,,
cd683212a21,inductor,Untopiced,Fix indexing twice with score_mod (#130224),test/inductor/test_flex_attention.py torch/_inductor/codegen/common.py torch/_inductor/kernel/flex_attention.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/130224,Chillee,yanboliang,,,
64139987c05,skip,not user facing,Add block mask utility support for batches and heads > 1 (#130227),aten/src/ATen/native/IndexingUtils.h aten/src/ATen/native/TensorAdvancedIndexingUtils.h aten/src/ATen/native/cuda/Indexing.cu test/inductor/test_flex_attention.py torch/nn/attention/_flex_attention.py,https://github.com/pytorch/pytorch/pull/130227,Chillee,yanboliang,,,
6dc64026cba,inductor,not user facing,Restrict fusions in foreach if there are dependencies on multiple subkernels (#130046),test/inductor/test_compiled_optimizers.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/130046,mlazos,eellison,,,
9158bb78377,skip,not user facing,Ignore functional tensor wrapper when caching (#128335),test/functorch/test_aotdispatch.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/config.py,https://github.com/pytorch/pytorch/pull/128335,jamesjwu,bdhirsh,,,
22c809aa736,distributed,not user facing,[FSDP] Runtime Error on Checkpoint Loading for optimizer state (#129110),torch/distributed/checkpoint/optimizer.py,https://github.com/pytorch/pytorch/pull/129110,jeejakp12,fegin,,,
a8985a97f96,distributed,Untopiced,elastic/store: use wait instead of get for barrier (#130148),test/distributed/elastic/utils/util_test.py torch/distributed/elastic/utils/store.py,https://github.com/pytorch/pytorch/pull/130148,d4l3k,kurman,wconstab,,
36e26087832,skip,Untopiced,[Quant][PT2E] enable qlinear post op fusion for dynamic quant & qat (#122667),test/inductor/test_mkldnn_pattern_matcher.py test/quantization/pt2e/test_x86inductor_quantizer.py torch/ao/quantization/quantizer/x86_inductor_quantizer.py,https://github.com/pytorch/pytorch/pull/122667,Xia-Weiwen,jgong5,leslie-fang-intel,,
3e53cae0fc4,skip,not user facing,Release 2.4 matrix update. Future releases dates (#130267),RELEASE.md,https://github.com/pytorch/pytorch/pull/130267,atalman,albanD,malfet,,
f059201e0dd,distributed,not user facing,[dtensor][debug] added deviceMesh for relevant operations and module parameter sharding and module fqn (#130072),test/distributed/_tensor/debug/test_comm_mode_features.py torch/distributed/_tensor/debug/comm_mode.py torch/distributed/_tensor/examples/comm_mode_features_example.py,https://github.com/pytorch/pytorch/pull/130072,sinhaanshul,XilunWu,,,
1e61cb8c871,skip,Untopiced,"Revert ""[3.12, 3.13, dynamo] simplified construction for frame f_locals/localsplus (#129185)""",build_variables.bzl torch/csrc/dynamo/cpython_defs.c torch/csrc/dynamo/cpython_defs.h torch/csrc/dynamo/cpython_includes.h torch/csrc/dynamo/debug_macros.h torch/csrc/dynamo/eval_frame.c torch/csrc/dynamo/extra_state.cpp torch/csrc/dynamo/framelocals_mapping.cpp torch/csrc/dynamo/framelocals_mapping.h,,,,,,
3fe324ffb61,skip,Untopiced,[custom ops] infer schema (#130079),docs/source/library.rst test/test_custom_ops.py torch/_library/infer_schema.py torch/library.py,https://github.com/pytorch/pytorch/pull/130079,yushangdi,zou3519,,,
856fe230c70,inductor,not user facing,[AOTI] better approach to generating runtime checks for symbolic dimensions (#130220),torch/_inductor/codegen/cpp_wrapper_cpu.py,https://github.com/pytorch/pytorch/pull/130220,chenyang78,aakhundov,,,
09d57f577b6,skip,Untopiced,"Revert ""[inductor] switch CppCodeCache to new cpp_builder. (#130132)""",torch/_inductor/codecache.py,,,,,,
5e467604c3e,skip,Untopiced,"Revert ""[inductor] switch AotCodeCompiler to new cpp_builder (#130127)""",torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,,,,,,
f9bb258892c,skip,Untopiced,"Revert ""[Inductor] Add aot_mode UT to new cpp_builder. (#130105)""",torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,,,,,,
44a773c1215,skip,Untopiced,"Revert ""[custom ops] infer schema (#130079)""",docs/source/library.rst test/test_custom_ops.py torch/_library/infer_schema.py torch/library.py,,,,,,
922d2737d5e,skip,Untopiced,Add API for open registration between operators and subclasses (and modes) (#130064),test/test_custom_ops.py torch/_library/simple_registry.py torch/csrc/utils/python_arg_parser.cpp torch/library.py,https://github.com/pytorch/pytorch/pull/130064,zou3519,albanD,,,
00335a27b4d,nested tensor_frontend,improvements,Accept min / max sequence length in nested_tensor_from_jagged() constructor (#130175),test/test_nestedtensor.py torch/nested/__init__.py,https://github.com/pytorch/pytorch/pull/130175,jbschlosser,davidberard98,soulitzer,,
17ca0d0edf4,releng,not user facing,Add linux manywheel python 3.13 binary workflows (#130030),.ci/docker/common/install_conda.sh .github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-binary-manywheel-nightly.yml requirements.txt,https://github.com/pytorch/pytorch/pull/130030,atalman,albanD,,,
b4cc25f1263,skip,not user facing,[custom_op]Fix self in mutation_args (#130179),test/dynamo/test_misc.py torch/_higher_order_ops/auto_functionalize.py torch/_ops.py,https://github.com/pytorch/pytorch/pull/130179,FindHao,zou3519,,,
60d9f3f7d91,skip,not user facing,Set the epoch timestamp when uploading data to dynamoDB (#130273),tools/stats/upload_stats_lib.py,https://github.com/pytorch/pytorch/pull/130273,huydhn,clee2000,,,
a28bb3268da,distributed,Untopiced,[Pipelining] Reorder _Action from F1_1 to 1F1 (#129786),test/distributed/pipelining/schedule_registry.py test/distributed/pipelining/test_schedule.py torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/129786,wconstab,H-Huang,,,
4c196238001,skip,Untopiced,Change numeric_debug_handle to store per-node id (#129811),test/quantization/pt2e/test_generate_numeric_debug_handle.py torch/ao/quantization/__init__.py torch/ao/quantization/fx/convert.py torch/ao/quantization/pt2e/generate_numeric_debug_handle.py torch/ao/quantization/pt2e/prepare.py,https://github.com/pytorch/pytorch/pull/129811,jerryzh168,tarun292,,,
7f08d3d9a0d,distributed,Untopiced,[C10D] Fix corrupt log due to uint_8 printing as char (#130184),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/130184,wconstab,Skylion007,eeggl,,
75fa10066d5,python_frontend,not user facing,Mark some test_decomp tests as slow on win (#130260),torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/130260,clee2000,huydhn,malfet,,
d44c30e2f90,skip,Untopiced,"Revert ""Add API for open registration between operators and subclasses (and modes) (#130064)""",test/test_custom_ops.py torch/_library/simple_registry.py torch/csrc/utils/python_arg_parser.cpp torch/library.py,,,,,,
abe81d5d05e,skip,not user facing,Fix the rest of foreach flakers (#130277),test/test_foreach.py,https://github.com/pytorch/pytorch/pull/130277,janeyx99,soulitzer,,,
f2c9f0c0dbe,dynamo,Untopiced,[HOP] improve naming for subgraph inputs (#130255),test/dynamo/test_autograd_function.py test/dynamo/test_higher_order_ops.py torch/_dynamo/variables/higher_order_ops.py,https://github.com/pytorch/pytorch/pull/130255,zou3519,ydwu4,,,
d1e0653fad1,fx,Untopiced,[fx][easy] print_readable should recursively apply options (#130268),torch/fx/graph_module.py,https://github.com/pytorch/pytorch/pull/130268,zou3519,Chillee,,,
29861779cea,sparse_frontend,not user facing,[2/N] Change #include <c10/util/Optional.h> to #include <optional> (#130236),aten/src/ATen/CPUGeneratorImpl.cpp aten/src/ATen/Context.h aten/src/ATen/DeviceGuard.h aten/src/ATen/EmptyTensor.h aten/src/ATen/FunctionalInverses.cpp aten/src/ATen/FunctionalTensorWrapper.cpp aten/src/ATen/FunctionalizeFallbackKernel.cpp aten/src/ATen/NestedTensorImpl.cpp aten/src/ATen/ScalarOps.cpp aten/src/ATen/TensorIndexing.h aten/src/ATen/TensorIterator.cpp aten/src/ATen/TensorIterator.h aten/src/ATen/TensorUtils.cpp aten/src/ATen/autocast_mode.h aten/src/ATen/core/CachingHostAllocator.h aten/src/ATen/core/CheckMemoryFormat.h aten/src/ATen/core/Dict.h aten/src/ATen/core/Dimname.cpp aten/src/ATen/core/Dimname.h aten/src/ATen/core/DistributionsHelper.h aten/src/ATen/core/GeneratorForPrivateuseone.cpp aten/src/ATen/core/List.h aten/src/ATen/core/List_test.cpp aten/src/ATen/core/NestedIntSymNodeImpl.h aten/src/ATen/core/PythonFallbackKernel.cpp aten/src/ATen/core/TensorBase.h aten/src/ATen/core/TorchDispatchUtils.h aten/src/ATen/core/boxing/KernelFunction_impl.h aten/src/ATen/core/boxing/KernelFunction_test.cpp aten/src/ATen/core/boxing/impl/kernel_function_legacy_test.cpp aten/src/ATen/core/boxing/impl/kernel_function_test.cpp aten/src/ATen/core/boxing/impl/kernel_lambda_legacy_test.cpp aten/src/ATen/core/boxing/impl/kernel_lambda_test.cpp aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor_test.cpp aten/src/ATen/core/class_type.cpp aten/src/ATen/core/class_type.h aten/src/ATen/core/dispatch/Dispatcher.cpp aten/src/ATen/core/dispatch/OperatorEntry.cpp aten/src/ATen/core/dispatch/OperatorEntry.h aten/src/ATen/core/dynamic_type.h aten/src/ATen/core/function_schema.cpp aten/src/ATen/core/function_schema.h aten/src/ATen/core/ivalue.h aten/src/ATen/core/ivalue_inl.h aten/src/ATen/core/jit_type.h aten/src/ATen/core/jit_type_base.h aten/src/ATen/core/library.cpp aten/src/ATen/core/op_registration/infer_schema.cpp aten/src/ATen/core/op_registration/op_registration.cpp aten/src/ATen/core/op_registration/op_registration.h aten/src/ATen/core/op_registration/op_registration_test.cpp aten/src/ATen/core/operator_name.h aten/src/ATen/core/tensor_type.cpp aten/src/ATen/core/type.cpp aten/src/ATen/core/union_type.cpp aten/src/ATen/cuda/CUDAGeneratorImpl.cpp aten/src/ATen/cuda/CUDAGraph.cpp aten/src/ATen/cuda/detail/CUDAHooks.h aten/src/ATen/functorch/BatchRulesFactory.cpp aten/src/ATen/functorch/DynamicLayer.h aten/src/ATen/functorch/Interpreter.h aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp aten/src/ATen/mps/MPSGeneratorImpl.h aten/src/ATen/mps/MPSGeneratorImpl.mm aten/src/ATen/mps/MPSHooks.h aten/src/ATen/native/cuda/Normalization.cuh aten/src/ATen/native/layer_norm.cpp aten/src/ATen/ops/from_blob.h aten/src/ATen/record_function.cpp aten/src/ATen/test/Dimname_test.cpp aten/src/ATen/test/NamedTensor_test.cpp aten/src/ATen/test/cpu_rng_test.cpp aten/src/ATen/test/cuda_distributions_test.cu aten/src/ATen/test/cuda_optional_test.cu aten/src/ATen/test/cuda_stream_test.cpp aten/src/ATen/test/extension_backend_test.cpp aten/src/ATen/test/rng_test.h aten/src/ATen/test/stride_properties_test.cpp aten/src/ATen/test/type_test.cpp aten/src/ATen/test/vulkan_api_test.cpp aten/src/ATen/test/vulkan_quantized_api_test.cpp aten/src/ATen/test/xnnpack_test.cpp aten/src/ATen/xpu/XPUGeneratorImpl.cpp torch/csrc/Event.cpp torch/csrc/autograd/python_variable.cpp torch/csrc/cuda/Event.cpp torch/csrc/cuda/nccl.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/socket.cpp torch/csrc/jit/codegen/fuser/kernel_cache.cpp torch/csrc/jit/codegen/fuser/kernel_cache.h torch/csrc/jit/frontend/ir_emitter.cpp torch/csrc/jit/frontend/script_type_parser.cpp torch/csrc/jit/frontend/sugared_value.h torch/csrc/jit/mobile/function.cpp torch/csrc/jit/mobile/function.h torch/csrc/jit/passes/graph_fuser.cpp torch/csrc/jit/passes/onnx/preprocess_for_onnx.cpp torch/csrc/jit/passes/shape_analysis.cpp torch/csrc/jit/python/script_init.cpp torch/csrc/jit/runtime/symbolic_shape_registry.cpp torch/csrc/lazy/ts_backend/ts_node_lowering.cpp torch/csrc/python_dimname.cpp torch/csrc/utils/python_arg_parser.cpp torch/csrc/xpu/Event.cpp,https://github.com/pytorch/pytorch/pull/130236,cyyever,ezyang,,,
e836ee19554,dynamo,not user facing,Enhancements to recompiles logs (#130043),test/dynamo/test_frame_init.py test/dynamo/test_higher_order_ops.py test/dynamo/test_logging.py test/dynamo/test_modules.py test/dynamo/test_structured_trace.py torch/_dynamo/convert_frame.py torch/_dynamo/guards.py torch/_dynamo/testing.py torch/_dynamo/types.py torch/csrc/dynamo/cache_entry.cpp torch/csrc/dynamo/cache_entry.h torch/csrc/dynamo/init.cpp,https://github.com/pytorch/pytorch/pull/130043,ezyang,anijain2305,,,
31df1d235ec,skip,not user facing,Support tensor stride (#129297),torch/csrc/profiler/standalone/execution_trace_observer.cpp,https://github.com/pytorch/pytorch/pull/129297,shengfukevin,izaitsevfb,sanrise,,
e235db98c92,skip,not user facing,[Inductor] Add aot_mode UT to new cpp_builder. (#130105),torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/130105,xuhancn,jansel,jgong5,,
ded469cfbd5,skip,not user facing,[issue scrubbing] Fix imports in test_memory_planning.py to work with pytest (#130275),test/inductor/test_memory_planning.py,https://github.com/pytorch/pytorch/pull/130275,masnesral,zou3519,,,
6c4efd4e959,profiler,not user facing,[Memory Snapshot][BE] Clean up record function callback scope (#130265),torch/csrc/cuda/memory_snapshot.cpp,https://github.com/pytorch/pytorch/pull/130265,aaronenyeshi,davidberard98,,,
312652c3258,python_frontend,Untopiced,[RFC] Add support for device extension autoloading (#127074),docs/source/miscellaneous_environment_variables.rst test/cpp_extensions/setup.py test/cpp_extensions/torch_test_cpp_extension/__init__.py test/run_test.py test/test_autoload.py tools/testing/discover_tests.py torch/__init__.py,https://github.com/pytorch/pytorch/pull/127074,shink,albanD,jgong5,,
b139b5090fe,distributed,not user facing,[pytorch] Name threads in thread pools for better debugging (#130270),c10/core/thread_pool.cpp caffe2/utils/threadpool/WorkersPool.h torch/csrc/distributed/autograd/engine/dist_engine.cpp torch/csrc/lazy/core/thread_pool.cpp,https://github.com/pytorch/pytorch/pull/130270,valentinandrei,albanD,d4l3k,,
953c6476bd7,skip,not user facing,[CMAKE] Look for `Development.Module` instead of `Development` (#129669),cmake/Dependencies.cmake,https://github.com/pytorch/pytorch/pull/129669,oraluben,malfet,,,
86fb76e8719,skip,not user facing,[SDPA] Clean up `print` in `test/test_transformers.py`  (#130302),test/test_transformers.py,https://github.com/pytorch/pytorch/pull/130302,eqy,awgu,,,
3e48d927332,skip,not user facing,Add scale kwarg to FlexAttention (and some changes that get FlexAttention numerics to be as accurate as FA2) (#130250),test/inductor/test_flex_attention.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/_flex_attention.py,https://github.com/pytorch/pytorch/pull/130250,Chillee,drisspg,,,
a5f816df18f,cuda,Untopiced,Add more dtypes to __cuda_array_interface__ (#129621),test/test_numba_integration.py torch/_tensor.py,https://github.com/pytorch/pytorch/pull/129621,milesial,lezcano,,,
71efbf701d5,cpp_frontend,Untopiced,[3/N] Change #include <c10/util/Optional.h> to #include <optional> (#130300),aten/src/ATen/FunctionalInverses.cpp aten/src/ATen/LegacyBatchingRegistrations.cpp aten/src/ATen/core/IListRef_test.cpp aten/src/ATen/core/NamedTensor.cpp aten/src/ATen/core/ivalue.h aten/src/ATen/core/ivalue_inl.h aten/src/ATen/core/jit_type_base.h aten/src/ATen/functorch/BatchRulesHelper.cpp aten/src/ATen/functorch/BatchRulesNorm.cpp aten/src/ATen/functorch/DynamicLayer.cpp aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp aten/src/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h aten/src/ATen/mps/MPSAllocator.mm aten/src/ATen/mps/MPSFallback.mm aten/src/ATen/templates/Function.h aten/src/ATen/templates/Functions.h aten/src/ATen/templates/NativeFunction.h aten/src/ATen/templates/NativeFunctions.h aten/src/ATen/templates/NativeMetaFunction.h aten/src/ATen/templates/RedispatchFunctions.h aten/src/ATen/templates/RegisterBackendSelect.cpp aten/src/ATen/templates/RegisterDispatchKey.cpp aten/src/ATen/templates/RegisterFunctionalization.cpp aten/src/ATen/templates/TensorBody.h torch/csrc/api/include/torch/data/detail/data_shuttle.h torch/csrc/autograd/engine.cpp torch/csrc/autograd/functions/tensor.h torch/csrc/distributed/c10d/ProcessGroupWrapper.cpp torch/csrc/distributed/rpc/rref_context.h torch/csrc/jit/codegen/fuser/kernel_cache.h torch/csrc/jit/frontend/error_report.h torch/csrc/jit/ir/irparser.h torch/csrc/jit/passes/onnx/eliminate_unused_items.cpp torch/csrc/jit/passes/onnx/eval_peephole.cpp torch/csrc/lazy/core/helpers.h torch/csrc/lazy/ts_backend/tensor_aten_ops.cpp torchgen/gen_lazy_tensor.py,https://github.com/pytorch/pytorch/pull/130300,cyyever,ezyang,,,
edf273edf4a,inductor,Untopiced,Revert some PRs (#130303),test/inductor/test_cuda_repro.py test/inductor/test_debug_trace.py test/inductor/test_flex_attention.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/wrapper.py torch/_inductor/comms.py torch/_inductor/dependencies.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/mkldnn_ir.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/130303,zou3519,bdhirsh,,,
cab90b0049f,composability,Untopiced,[custom ops] disable kernel temporarily  (#130190),test/test_custom_ops.py torch/_library/custom_ops.py,https://github.com/pytorch/pytorch/pull/130190,yushangdi,williamwen42,zou3519,,
e4c51d22c54,skip,not user facing,[cuDNN] Cleanup < 8.5 #ifdefs  (#130283),aten/src/ATen/cudnn/Descriptors.cpp aten/src/ATen/cudnn/Descriptors.h aten/src/ATen/cudnn/Types.cpp aten/src/ATen/native/cudnn/RNN.cpp,https://github.com/pytorch/pytorch/pull/130283,eqy,Skylion007,,,
cb4bec311ad,Uncategorized,Untopiced,Fix nodes has more than one output users after replace_set_grad_with_hop pass (#129716),test/export/test_passes.py torch/_export/passes/replace_set_grad_with_hop_pass.py torch/_export/utils.py,https://github.com/pytorch/pytorch/pull/129716,ydwu4,angelayi,,,
c6cce976b25,distributed,Untopiced,Fix an issue where ENABLE_INTRA_NODE_COMM=1 + multiple process groups leads to failure (#130269),torch/csrc/distributed/c10d/intra_node_comm.cpp,https://github.com/pytorch/pytorch/pull/130269,yifuwang,Chillee,,,
3689471ea43,inductor,not user facing,[inductor] Add FileCheck to flex attention epilogue test (#129343),test/inductor/test_flex_attention.py,https://github.com/pytorch/pytorch/pull/129343,peterbell10,lezcano,,,
3477ee38e4d,optim,Untopiced,fix the use of initial learning rate in the OneCycleLR example (#130306),torch/optim/lr_scheduler.py,https://github.com/pytorch/pytorch/pull/130306,tianyeeT,janeyx99,,,
a6345d3477b,skip,not user facing,[CMake] [3/N] Remove unused code (#130322),CMakeLists.txt caffe2/CMakeLists.txt cmake/Dependencies.cmake cmake/public/utils.cmake,https://github.com/pytorch/pytorch/pull/130322,cyyever,r-barnes,,,
adb65682aff,skip,Untopiced,[HOP] Use user directed names for variables where possible (#130271),test/dynamo/test_higher_order_ops.py test/functorch/test_control_flow.py torch/_dynamo/variables/tensor.py,https://github.com/pytorch/pytorch/pull/130271,zou3519,Chillee,ydwu4,,
42f647219a4,skip,Untopiced,[ROCm] Add int4 support (#129710),aten/src/ATen/native/cuda/int4mm.cu test/test_linalg.py torch/csrc/jit/codegen/fuser/codegen.cpp torch/csrc/jit/codegen/fuser/cuda/resource_strings.h torch/csrc/jit/tensorexpr/cuda_codegen.cpp torch/testing/_internal/common_cuda.py torch/utils/hipify/cuda_to_hip_mappings.py,https://github.com/pytorch/pytorch/pull/129710,jerrymannil,malfet,,,
721a798886e,fx,Untopiced,add bits16 to graph dtype_abbrs (#130339),torch/fx/graph.py,https://github.com/pytorch/pytorch/pull/130339,cccclai,angelayi,,,
1d93367cfac,python_frontend,Untopiced,Fix typo (#130305),torch/_torch_docs.py,https://github.com/pytorch/pytorch/pull/130305,tianyeeT,Skylion007,,,
f85bda8bdd1,distributed,Untopiced,c10d/Handlers: expose running handlers from Python (#130149),test/distributed/elastic/test_control_plane.py torch/csrc/distributed/c10d/control_plane/Handlers.cpp torch/csrc/distributed/c10d/control_plane/Handlers.hpp torch/csrc/distributed/c10d/control_plane/WorkerServer.cpp torch/csrc/distributed/c10d/init.cpp,https://github.com/pytorch/pytorch/pull/130149,d4l3k,c-p-i-o,kurman,,
9c9744c3acf,skip,Untopiced,"Revert ""[runtime asserts] deduplicate runtime asserts & CSE (#128599)""",test/dynamo/test_export.py test/dynamo/test_higher_order_ops.py test/dynamo/test_logging.py test/dynamo/test_misc.py test/dynamo/test_repros.py test/dynamo/test_subgraphs.py test/export/test_export.py test/export/test_passes.py test/onnx/onnx_test_common.py test/onnx/test_fx_to_onnx_with_onnxruntime.py torch/_dynamo/backends/cudagraphs.py torch/_export/serde/serialize.py torch/export/_trace.py torch/export/exported_program.py torch/export/unflatten.py torch/fx/passes/runtime_assert.py torch/onnx/_internal/exporter.py torch/utils/_sympy/interp.py torch/utils/_sympy/reference.py,,,,,,
c23d103afae,skip,Untopiced,Add API for open registration between operators and subclasses (and modes) (#130064),test/profiler/test_profiler_tree.py test/test_custom_ops.py torch/_library/simple_registry.py torch/csrc/utils/python_arg_parser.cpp torch/library.py,https://github.com/pytorch/pytorch/pull/130064,zou3519,albanD,,,
bb9a73f7675,skip,Untopiced,[custom_ops] expose torch.library.register_torch_dispatch (#130261),docs/source/library.rst docs/source/notes/extending.rst test/test_custom_ops.py torch/_library/custom_ops.py torch/library.py,https://github.com/pytorch/pytorch/pull/130261,zou3519,albanD,,,
df83142131f,profiler,bug fixes,[CCA][Memory Snapshot] Stop duplicating annotations to all device_traces (#130315),c10/cuda/CUDACachingAllocator.cpp,https://github.com/pytorch/pytorch/pull/130315,aaronenyeshi,zdevito,,,
fb5cb17fbe5,fx,Untopiced,[torch][fx] Add normalize_args constructor argument to FxGraphDrawer (#130348),torch/fx/passes/graph_drawer.py,https://github.com/pytorch/pytorch/pull/130348,dulinriley,mcremon-meta,,,
37d4d04309d,jit,not user facing,[torchscript] Add logging for model id. (#130118),torch/_jit_internal.py torch/_utils_internal.py torch/jit/_script.py torch/jit/_serialization.py torch/jit/_trace.py,https://github.com/pytorch/pytorch/pull/130118,zhxchen17,BoyuanFeng,,,
3be4922a9de,skip,Untopiced,"Revert ""[HOP] Use user directed names for variables where possible (#130271)""",test/dynamo/test_higher_order_ops.py test/functorch/test_control_flow.py torch/_dynamo/variables/tensor.py,,,,,,
fd43a2ba270,python_frontend,Untopiced,Forward fix for test_compare_cpu_cuda_float32 (#130360),torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/130360,jbschlosser,malfet,,,
5b5a1f52020,python_frontend,not user facing,Add on to Mark some test_decomp tests as slow on win #130260 (#130337),torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/130337,clee2000,malfet,,,
44815ed67e8,skip,Untopiced,"Revert ""Add scale kwarg to FlexAttention (and some changes that get FlexAttention numerics to be as accurate as FA2) (#130250)""",test/inductor/test_flex_attention.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/_flex_attention.py,,,,,,
fb696bf2645,skip,Untopiced,"Revert ""Add block mask utility support for batches and heads > 1 (#130227)""",aten/src/ATen/native/IndexingUtils.h aten/src/ATen/native/TensorAdvancedIndexingUtils.h aten/src/ATen/native/cuda/Indexing.cu test/inductor/test_flex_attention.py torch/nn/attention/_flex_attention.py,,,,,,
7a3ab1fe791,sparse_frontend,Untopiced,[structural binding][7/N] Replace std::tie with structural binding  (#130216),aten/src/ATen/native/Normalization.cpp aten/src/ATen/native/TensorFactories.cpp aten/src/ATen/native/Unique.cpp aten/src/ATen/native/cpu/AdaptiveAvgPoolKernel.cpp aten/src/ATen/native/cpu/AdaptiveMaxPoolKernel.cpp aten/src/ATen/native/cpu/AvgPoolKernel.cpp aten/src/ATen/native/cpu/HistogramKernel.cpp aten/src/ATen/native/cuda/TensorModeKernel.cu aten/src/ATen/native/mkldnn/Normalization.cpp aten/src/ATen/native/mkldnn/xpu/detail/Conv.cpp aten/src/ATen/native/mkldnn/xpu/detail/Deconv.cpp aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp aten/src/ATen/native/nested/NestedTensorMath.cpp aten/src/ATen/native/sparse/cuda/SoftMax.cu torchgen/gen_vmap_plumbing.py,https://github.com/pytorch/pytorch/pull/130216,cyyever,albanD,,,
85b8503621e,skip,not user facing,[Caffe2] Remove Caffe2 documentation (#130089),CMakeLists.txt cmake/Summary.cmake docs/caffe2/.Doxyfile-c docs/caffe2/.Doxyfile-python docs/caffe2/Caffe2-with-name-55-tall.png docs/caffe2/DOXYGEN.md docs/caffe2/DoxygenLayout-c.xml docs/caffe2/DoxygenLayout-python.xml docs/caffe2/README.md docs/caffe2/footer.html docs/caffe2/header.html docs/caffe2/installation.md docs/caffe2/main.css docs/caffe2/process.py docs/caffe2/stylesheet.css,https://github.com/pytorch/pytorch/pull/130089,cyyever,albanD,r-barnes,,
9912209743b,inductor,not user facing,check if the input fx graph of aot_compile return tuple (#129824),test/inductor/test_aot_inductor.py torch/_inductor/__init__.py,https://github.com/pytorch/pytorch/pull/129824,awayzjj,angelayi,yushangdi,,
007e75958fd,foreach_frontend,not user facing,[4/N] Change #include <c10/util/Optional.h> to #include <optional>  (#130329),aten/src/ATen/native/Blas.cpp aten/src/ATen/native/CPUFallback.cpp aten/src/ATen/native/Constraints.cpp aten/src/ATen/native/Convolution.cpp aten/src/ATen/native/DistributionTemplates.h aten/src/ATen/native/Distributions.cpp aten/src/ATen/native/EmbeddingBag.h aten/src/ATen/native/ForeachOpsKernels.cpp aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/LinearAlgebra.h aten/src/ATen/native/LinearAlgebraUtils.h aten/src/ATen/native/NonSymbolicBC.h aten/src/ATen/native/PadNd.cpp aten/src/ATen/native/QuantizedLinear.cpp aten/src/ATen/native/README.md aten/src/ATen/native/ReduceOps.h aten/src/ATen/native/ReduceOpsUtils.h aten/src/ATen/native/Resize.cpp aten/src/ATen/native/Resize.h aten/src/ATen/native/ScatterGatherChecks.h aten/src/ATen/native/SegmentReduce.h aten/src/ATen/native/TensorConversions.h aten/src/ATen/native/UpSample.h aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp aten/src/ATen/native/cpu/ReduceOpsKernel.cpp aten/src/ATen/native/cpu/SoftMaxKernel.cpp aten/src/ATen/native/cpu/TensorCompareKernel.cpp aten/src/ATen/native/cuda/Blas.cpp aten/src/ATen/native/cuda/CUDAScalar.cu aten/src/ATen/native/cuda/Dropout.cu aten/src/ATen/native/cuda/ForeachReduceOp.cu aten/src/ATen/native/cuda/FusedAdamKernel.cu aten/src/ATen/native/cuda/FusedAdamWKernel.cu aten/src/ATen/native/cuda/FusedSgdKernel.cu aten/src/ATen/native/cuda/Normalization.cuh aten/src/ATen/native/cuda/Resize.cpp aten/src/ATen/native/cuda/RowwiseScaledMM.cu aten/src/ATen/native/cuda/RowwiseScaledMM.h aten/src/ATen/native/cuda/Shape.cu aten/src/ATen/native/cuda/SortUtils.cuh aten/src/ATen/native/cuda/SparseBinaryOpIntersectionKernel.cu aten/src/ATen/native/cuda/TensorFactories.cu aten/src/ATen/native/cuda/TensorShapeCUDA.cpp aten/src/ATen/native/cuda/UpSample.cuh aten/src/ATen/native/cuda/jit_utils.cpp aten/src/ATen/native/group_norm.cpp aten/src/ATen/native/layer_norm.cpp aten/src/ATen/native/metal/MetalAten.mm aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.mm aten/src/ATen/native/metal/ops/MetalClamp.mm aten/src/ATen/native/metal/ops/MetalConvolution.mm aten/src/ATen/native/metal/ops/MetalReduce.mm aten/src/ATen/native/metal/ops/MetalSoftmax.mm aten/src/ATen/native/metal/ops/MetalUpsamplingNearest.mm aten/src/ATen/native/mkldnn/Conv.cpp aten/src/ATen/native/mkldnn/ConvPrepack.cpp aten/src/ATen/native/mkldnn/Linear.cpp aten/src/ATen/native/mkldnn/xpu/Blas.cpp aten/src/ATen/native/mkldnn/xpu/Conv.cpp aten/src/ATen/native/mkldnn/xpu/detail/Conv.cpp aten/src/ATen/native/mkldnn/xpu/detail/Deconv.cpp aten/src/ATen/native/mkldnn/xpu/detail/Matmul.cpp aten/src/ATen/native/quantized/QTensor.cpp aten/src/ATen/native/quantized/cpu/AdaptiveAveragePooling.cpp aten/src/ATen/native/quantized/cpu/AveragePool2d.cpp aten/src/ATen/native/quantized/cpu/AveragePool3d.cpp aten/src/ATen/native/quantized/cpu/BinaryOps.cpp aten/src/ATen/native/quantized/cpu/ChannelShuffle.cpp aten/src/ATen/native/quantized/cpu/Normalization.cpp aten/src/ATen/native/quantized/cpu/OnednnUtils.h aten/src/ATen/native/quantized/cpu/Pooling.cpp aten/src/ATen/native/quantized/cpu/TensorOperators.cpp aten/src/ATen/native/quantized/cpu/UpSampleBilinear2d.cpp aten/src/ATen/native/quantized/cpu/UpSampleNearest2d.cpp aten/src/ATen/native/quantized/cpu/UpSampleNearest3d.cpp aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp aten/src/ATen/native/quantized/cpu/qconv.cpp aten/src/ATen/native/quantized/cpu/qconv_dynamic.cpp aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp aten/src/ATen/native/quantized/cpu/qconv_unpack_impl.cpp aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp aten/src/ATen/native/quantized/cpu/qlinear.cpp aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp aten/src/ATen/native/quantized/cpu/qmatmul.cpp aten/src/ATen/native/quantized/cpu/qmul.cpp aten/src/ATen/native/quantized/cpu/qsoftmax.cpp aten/src/ATen/native/quantized/cuda/EmbeddingBag.cu aten/src/ATen/native/quantized/qconv_unpack.cpp aten/src/ATen/native/sparse/SparseBinaryOpIntersectionCommon.h aten/src/ATen/native/sparse/SparseBinaryOpIntersectionKernel.cpp aten/src/ATen/native/sparse/SparseBlas.cpp aten/src/ATen/native/sparse/SparseStubs.h aten/src/ATen/native/sparse/SparseTensor.cpp aten/src/ATen/native/sparse/SparseTensorMath.cpp aten/src/ATen/native/sparse/cuda/SparseBlas.cpp aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/attention.h aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/attention_backward.cu aten/src/ATen/native/transformers/cuda/flash_attn/flash_api.cpp aten/src/ATen/native/transformers/hip/flash_attn/flash_api.hip aten/src/ATen/native/transformers/transformer.cpp aten/src/ATen/native/vulkan/ops/Clamp.cpp aten/src/ATen/native/vulkan/ops/Convolution.h aten/src/ATen/native/xnnpack/Convolution.cpp aten/src/ATen/native/xnnpack/Linear.cpp,https://github.com/pytorch/pytorch/pull/130329,cyyever,ezyang,,,
68751799b85,skip,Untopiced,Add decompositions for copy variants of view ops (#128416),test/distributed/_tensor/test_dtensor_ops.py test/expect/HasDecompTest.test_aten_core_operators.expect test/expect/HasDecompTest.test_has_decomposition.expect test/functorch/test_ops.py test/functorch/test_vmap.py test/onnx/test_fx_op_consistency.py test/test_mps.py test/test_proxy_tensor.py tools/autograd/gen_variable_type.py torch/_decomp/__init__.py torch/_inductor/lowering.py torch/_prims/context.py torch/_prims_common/wrappers.py torch/_refs/__init__.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/128416,rec,amjames,lezcano,,
868d9a4f129,skip,not user facing,[cpu][flash attention] fix nan issue (#130014),aten/src/ATen/native/cpu/FlashAttentionKernel.cpp test/test_transformers.py,https://github.com/pytorch/pytorch/pull/130014,Valentine233,drisspg,jgong5,,
99c68f7bea8,dynamo,not user facing,Refactor TritonKernelVariable's logic so it can be shared (#130177),torch/_dynamo/variables/functions.py torch/_higher_order_ops/triton_kernel_wrap.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/130177,zou3519,oulgen,ydwu4,,
5abe7ebd416,skip,not user facing,Add new (private) capture_triton API (#130178),test/inductor/test_triton_kernels.py torch/_higher_order_ops/triton_kernel_wrap.py,https://github.com/pytorch/pytorch/pull/130178,zou3519,oulgen,,,
a17d1e53222,jit,not user facing,Fix static `py::object` dangling pointer with `py::gil_safe_call_once_and_store` (#130341),functorch/csrc/dim/dim.cpp torch/csrc/dynamo/guards.cpp torch/csrc/jit/python/module_python.h torch/csrc/jit/python/python_ivalue.h torch/csrc/utils/python_symnode.cpp,https://github.com/pytorch/pytorch/pull/130341,XuehaiPan,ezyang,malfet,,
10c7f037fe3,skip,not user facing,Simplify c10::string_view (#130009),c10/test/util/string_view_test.cpp c10/util/TypeIndex.h c10/util/string_view.h,https://github.com/pytorch/pytorch/pull/130009,cyyever,ezyang,,,
e29657efb6d,inductor,not user facing,[Inductor][CPP] Fix typo in merge rules (#130405),.github/merge_rules.yaml,https://github.com/pytorch/pytorch/pull/130405,leslie-fang-intel,jgong5,lezcano,,
6367f02a0e1,linalg_frontend,bc breaking,update the input `weight` of `_convert_weight_to_int4pack` to `[n][k / 2] uint8` (#129940),aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/cpu/int4mm_kernel.cpp aten/src/ATen/native/cuda/int4mm.cu aten/src/ATen/native/mps/operations/Quantized.mm test/test_linalg.py test/test_mps.py torch/_meta_registrations.py torch/testing/_internal/common_quantization.py,https://github.com/pytorch/pytorch/pull/129940,yanbing-j,jgong5,lezcano,mingfeima,
cb2bce98de5,mps,Untopiced,[MPS][BE] Reduce the number of parameters encoded for no momentum fused SGD (#130131),aten/src/ATen/native/mps/OperationUtils.h aten/src/ATen/native/mps/operations/FusedOptimizerOps.h aten/src/ATen/native/mps/operations/FusedSgdKernel.mm aten/src/ATen/native/mps/operations/MultiTensorApply.h,https://github.com/pytorch/pytorch/pull/130131,qqaatw,janeyx99,malfet,,
a1590e16dff,releng,not user facing,"Add single Python 3.10, single Cuda 12.1 build with dependencies included (#130349)",.circleci/scripts/binary_upload.sh .github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/130349,atalman,malfet,,,
637cc8d27f2,skip,Untopiced,"Revert ""update the input `weight` of `_convert_weight_to_int4pack` to `[n][k / 2] uint8` (#129940)""",aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/cpu/int4mm_kernel.cpp aten/src/ATen/native/cuda/int4mm.cu aten/src/ATen/native/mps/operations/Quantized.mm test/test_linalg.py test/test_mps.py torch/_meta_registrations.py torch/testing/_internal/common_quantization.py,,,,,,
6ce0bd7d3b1,skip,Untopiced,[HOP] Use user directed names for variables where possible (#130271),test/dynamo/test_higher_order_ops.py test/functorch/test_control_flow.py test/inductor/test_flex_attention.py torch/_dynamo/variables/tensor.py,https://github.com/pytorch/pytorch/pull/130271,zou3519,Chillee,ydwu4,,
99967e11199,mps,bug fixes,[MPS][TYPE_PROMOTION] Fix Clamp (#130226),aten/src/ATen/native/mps/operations/TensorCompare.mm test/test_mps.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/130226,qqaatw,malfet,,,
bd3452f431f,mps,not user facing,[5/N] Change #include <c10/util/Optional.h> to #include <optional> (#130408),aten/src/ATen/native/EmbeddingBag.cpp aten/src/ATen/native/ReduceOps.cpp aten/src/ATen/native/TensorAdvancedIndexing.cpp aten/src/ATen/native/TensorConversions.cpp aten/src/ATen/native/TensorFactories.cpp aten/src/ATen/native/TensorShape.cpp aten/src/ATen/native/mps/OperationUtils.h aten/src/ATen/native/mps/TensorFactory.cpp aten/src/ATen/native/mps/operations/Activation.mm aten/src/ATen/native/mps/operations/AdaptivePooling.mm aten/src/ATen/native/mps/operations/BinaryOps.mm aten/src/ATen/native/mps/operations/Blas.mm aten/src/ATen/native/mps/operations/Bucketization.mm aten/src/ATen/native/mps/operations/ConstantOps.mm aten/src/ATen/native/mps/operations/Convolution.mm aten/src/ATen/native/mps/operations/Copy.mm aten/src/ATen/native/mps/operations/Distributions.mm aten/src/ATen/native/mps/operations/FusedAdamAmsgradKernelImpl.h aten/src/ATen/native/mps/operations/FusedAdamAmsgradKernelImpl.mm aten/src/ATen/native/mps/operations/FusedAdamKernel.mm aten/src/ATen/native/mps/operations/FusedAdamKernelImpl.h aten/src/ATen/native/mps/operations/FusedAdamKernelImpl.mm aten/src/ATen/native/mps/operations/FusedAdamWAmsgradKernelImpl.h aten/src/ATen/native/mps/operations/FusedAdamWAmsgradKernelImpl.mm aten/src/ATen/native/mps/operations/FusedAdamWKernel.mm aten/src/ATen/native/mps/operations/FusedAdamWKernelImpl.h aten/src/ATen/native/mps/operations/FusedAdamWKernelImpl.mm aten/src/ATen/native/mps/operations/HistogramKernel.mm aten/src/ATen/native/mps/operations/Indexing.mm aten/src/ATen/native/mps/operations/Inverse.mm aten/src/ATen/native/mps/operations/Linear.mm aten/src/ATen/native/mps/operations/LinearAlgebra.mm aten/src/ATen/native/mps/operations/LossOps.mm aten/src/ATen/native/mps/operations/Normalization.mm aten/src/ATen/native/mps/operations/Pad.mm aten/src/ATen/native/mps/operations/Pooling.mm aten/src/ATen/native/mps/operations/ReduceOps.mm aten/src/ATen/native/mps/operations/Repeat.mm aten/src/ATen/native/mps/operations/RnnOps.mm aten/src/ATen/native/mps/operations/ScatterGather.mm aten/src/ATen/native/mps/operations/Sort.mm aten/src/ATen/native/mps/operations/SummaryOps.mm aten/src/ATen/native/mps/operations/TensorCompare.mm aten/src/ATen/native/mps/operations/UnaryOps.mm aten/src/ATen/native/mps/operations/Unique.mm aten/src/ATen/native/mps/operations/UpSample.mm aten/src/ATen/native/mps/operations/View.mm aten/src/ATen/native/nested/NestedTensorBackward.cpp aten/src/ATen/native/nested/NestedTensorFactories.cpp aten/src/ATen/native/nested/NestedTensorMath.cpp aten/src/ATen/native/nested/NestedTensorMath.h aten/src/ATen/native/nested/NestedTensorTransformerFunctions.cpp aten/src/ATen/native/nested/NestedTensorTransformerFunctions.h aten/src/ATen/native/nested/NestedTensorUtils.cpp aten/src/ATen/native/nested/NestedTensorUtils.h aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu,https://github.com/pytorch/pytorch/pull/130408,cyyever,malfet,,,
b38de2f9e21,composability,not user facing,[decomps] Fix aten._to_copy decomp (#130381),test/dynamo/test_higher_order_ops.py torch/_decomp/decompositions.py,https://github.com/pytorch/pytorch/pull/130381,zou3519,Chillee,,,
417c83e7cf5,skip,not user facing,[ROCm] Unskip scaled_dot_product_attention tests on ROCm (#127966),test/functorch/test_ops.py,https://github.com/pytorch/pytorch/pull/127966,alugorey,malfet,,,
81df076bfde,mps,Untopiced,Fix Apple crash when running PyTorch with Metal API validation turned on (#130377),aten/src/ATen/native/mps/operations/View.mm,https://github.com/pytorch/pytorch/pull/130377,michaeleisel,malfet,,,
81ea2986007,distributed,not user facing,Wrap the test func with try/except to always call destroy_process_group (#124961),torch/testing/_internal/distributed/_tensor/common_dtensor.py,https://github.com/pytorch/pytorch/pull/124961,fegin,wanchaol,wz337,,
d31f866b335,skip,not user facing,[BE] [CMake] Remove AT_CORE_STATIC_WINDOWS option (#130409),caffe2/CMakeLists.txt torch/CMakeLists.txt,https://github.com/pytorch/pytorch/pull/130409,cyyever,malfet,,,
c83b9411419,quantization,Untopiced,[export] add dynamic shapes argument and infer from graph nodes (#129928),test/export/test_export.py test/functorch/test_aotdispatch.py torch/_export/serde/serialize.py torch/_functorch/aot_autograd.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/129928,yushangdi,angelayi,,,
a7715e36de5,skip,not user facing,Add block mask utility support for batches and heads > 1 (#130227),aten/src/ATen/functorch/BatchRulesScatterOps.cpp test/inductor/test_flex_attention.py torch/nn/attention/_flex_attention.py,https://github.com/pytorch/pytorch/pull/130227,Chillee,yanboliang,,,
ce4d95143fd,skip,not user facing,Add scale kwarg to FlexAttention (and some changes that get FlexAttention numerics to be as accurate as FA2) (#130250),test/inductor/test_flex_attention.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/_flex_attention.py,https://github.com/pytorch/pytorch/pull/130250,Chillee,drisspg,,,
bef085bdfa6,skip,Untopiced,[reland][custom ops] infer schema (#130079),docs/source/library.rst test/test_custom_ops.py torch/_library/infer_schema.py torch/library.py,https://github.com/pytorch/pytorch/pull/130079,yushangdi,zou3519,,,
ae73489b7d8,caffe2,not user facing,[codemod] Use C++17 [[fallthrough]] in 1 file inc caffe2/aten/src/ATen/native/cuda/DistributionTemplates.h (#130433),aten/src/ATen/native/cuda/DistributionTemplates.h,https://github.com/pytorch/pytorch/pull/130433,r-barnes,malfet,,,
9d94b122f09,cuda,Untopiced,Fix usage of USE_ROCM when calling cudaFuncGetAttributes (#130441),aten/src/ATen/native/cuda/int4mm.cu,https://github.com/pytorch/pytorch/pull/130441,atalman,Skylion007,malfet,,
fe3e6878c4b,skip,not user facing,[cond] inlining into one of the branches when pred is a python constant (#128709),test/dynamo/test_export.py test/dynamo/test_higher_order_ops.py test/export/test_export.py test/export/test_verifier.py test/functorch/test_aotdispatch.py test/functorch/test_control_flow.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/cond.py,https://github.com/pytorch/pytorch/pull/128709,ydwu4,zou3519,,,
a09910d3a93,dynamo,Untopiced,add strobelight profile links to tlparse (#129703),torch/_dynamo/convert_frame.py torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/129703,laithsakka,aorenste,,,
a7aa066b099,fx,not user facing,Fix link to dynamo in torch/fx readme (#130233),torch/fx/README.md,https://github.com/pytorch/pytorch/pull/130233,aim-nara,janeyx99,,,
7d4cb210982,skip,Untopiced,Decompose expand_copy and permute_copy (#129476),test/distributed/_tensor/test_dtensor_ops.py test/expect/HasDecompTest.test_has_decomposition.expect test/functorch/test_ops.py test/functorch/test_vmap.py test/test_mps.py tools/autograd/gen_variable_type.py torch/_decomp/__init__.py torch/_inductor/decomposition.py torch/_refs/__init__.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/129476,rec,amjames,lezcano,,
6b3460ae0da,distributed,not user facing,fix discrepancy from the export of #126601 (#130296),torch/_C/_distributed_c10d.pyi,https://github.com/pytorch/pytorch/pull/130296,izaitsevfb,kit1980,malfet,seemethere,
b4b7477d3f6,profiler,not user facing,Fix CPU Annotation Overlapping with Python Events (#129599),test/profiler/test_profiler.py torch/csrc/profiler/collection.cpp torch/csrc/profiler/collection.h,https://github.com/pytorch/pytorch/pull/129599,sraikund16,aaronenyeshi,,,
0beeac35fac,skip,Untopiced,"Revert ""[cond] inlining into one of the branches when pred is a python constant (#128709)""",test/dynamo/test_export.py test/dynamo/test_higher_order_ops.py test/export/test_export.py test/export/test_verifier.py test/functorch/test_aotdispatch.py test/functorch/test_control_flow.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/cond.py,,,,,,
08d5423d339,quantization,not user facing,"[aota] Needs autograd if an input requires_grad, agnostic to enable_grad (#128890)",test/distributed/_tensor/test_dtensor_compile.py test/functorch/test_aotdispatch.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/aot_autograd.py torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/128890,IvanKobzarev,bdhirsh,,,
4b7ee51260c,mps,not user facing,[BE][MPS] Cleanup optimizers code (#130453),aten/src/ATen/native/mps/operations/FusedAdamAmsgradKernelImpl.h aten/src/ATen/native/mps/operations/FusedAdamAmsgradKernelImpl.mm aten/src/ATen/native/mps/operations/FusedAdamKernel.mm aten/src/ATen/native/mps/operations/FusedAdamKernelImpl.h aten/src/ATen/native/mps/operations/FusedAdamKernelImpl.mm aten/src/ATen/native/mps/operations/FusedAdamWAmsgradKernelImpl.h aten/src/ATen/native/mps/operations/FusedAdamWAmsgradKernelImpl.mm aten/src/ATen/native/mps/operations/FusedAdamWKernel.mm aten/src/ATen/native/mps/operations/FusedAdamWKernelImpl.h aten/src/ATen/native/mps/operations/FusedAdamWKernelImpl.mm aten/src/ATen/native/mps/operations/FusedSgdKernel.mm,https://github.com/pytorch/pytorch/pull/130453,malfet,Skylion007,,,
cf090e222ea,skip,not user facing,Update torch-xpu-ops pin (ATen XPU implementation) (#130333),test/inductor/test_control_flow.py third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/130333,fengyuan14,EikanWang,atalman,,
1352f13f782,skip,not user facing,"[pipelining] Refactor test_schedule to fix ""-k"" (#130294)",test/distributed/pipelining/test_schedule.py test/distributed/pipelining/test_schedule_multiproc.py,https://github.com/pytorch/pytorch/pull/130294,wconstab,H-Huang,,,
1b3b4c2fb90,onnx,Untopiced,[runtime asserts] deduplicate runtime asserts & CSE (#128599) (#130380),test/dynamo/test_export.py test/dynamo/test_higher_order_ops.py test/dynamo/test_logging.py test/dynamo/test_misc.py test/dynamo/test_repros.py test/dynamo/test_subgraphs.py test/export/test_export.py test/export/test_passes.py test/onnx/onnx_test_common.py test/onnx/test_fx_to_onnx_with_onnxruntime.py torch/_dynamo/backends/cudagraphs.py torch/_export/serde/serialize.py torch/export/_trace.py torch/export/exported_program.py torch/export/unflatten.py torch/fx/passes/runtime_assert.py torch/onnx/_internal/exporter.py torch/utils/_sympy/interp.py torch/utils/_sympy/reference.py,https://github.com/pytorch/pytorch/pull/130380,pianpwk,izaitsevfb,,,
b81767161ec,skip,Untopiced,"Revert ""[aota] Needs autograd if an input requires_grad, agnostic to enable_grad (#128890)""",test/distributed/_tensor/test_dtensor_compile.py test/functorch/test_aotdispatch.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/aot_autograd.py torch/_functorch/partitioners.py,,,,,,
cd2638be097,skip,Untopiced,"Revert ""[pipelining] Refactor test_schedule to fix ""-k"" (#130294)""",test/distributed/pipelining/test_schedule.py test/distributed/pipelining/test_schedule_multiproc.py,,,,,,
80a421a54d8,releng,not user facing,[TD] Pin numpy to 1.26.0 in indexer (#130442),.github/scripts/td_llm_indexer.sh,https://github.com/pytorch/pytorch/pull/130442,clee2000,atalman,malfet,,
46c52661bc5,releng,not user facing,Use a better cherry-pick strategy for stable pytorch w/ distribute changes (#129987),.github/scripts/sync_distributed_folder_prototype.sh,https://github.com/pytorch/pytorch/pull/129987,bigfootjon,seemethere,,,
e14a0f45ed8,skip,Untopiced,"Revert ""[reland][custom ops] infer schema (#130079)""",docs/source/library.rst test/test_custom_ops.py torch/_library/infer_schema.py torch/library.py,,,,,,
86bca69c5f9,skip,Untopiced,"Revert ""[custom_ops] expose torch.library.register_torch_dispatch (#130261)""",docs/source/library.rst docs/source/notes/extending.rst test/test_custom_ops.py torch/_library/custom_ops.py torch/library.py,,,,,,
83c95c48f74,distributed,Untopiced,Flight recoder data as JSON (#129505),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/init.cpp,https://github.com/pytorch/pytorch/pull/129505,c-p-i-o,d4l3k,wconstab,,
ce499eee0c6,skip,Untopiced,"Revert ""Add API for open registration between operators and subclasses (and modes) (#130064)""",test/profiler/test_profiler_tree.py test/test_custom_ops.py torch/_library/simple_registry.py torch/csrc/utils/python_arg_parser.cpp torch/library.py,,,,,,
551b3c6dcab,Uncategorized,Untopiced,Use irange to avoid -Wsign-compare errors (#130388),aten/src/ATen/Context.cpp,https://github.com/pytorch/pytorch/pull/130388,izaitsevfb,Skylion007,malfet,,
2abc7cc21b8,skip,not user facing,[inductor] switch AotCodeCompiler to new cpp_builder (#130127),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/130127,xuhancn,jansel,jgong5,,
ceedee23ec2,distributed,Untopiced,[DTensor] Included meshes in cross-mesh error msg (#130454),torch/distributed/_tensor/_dispatch.py,https://github.com/pytorch/pytorch/pull/130454,awgu,wanchaol,wz337,,
358da54be55,inductor,not user facing,[inductor] Better messaging when triton version is too old (#130403),torch/_inductor/async_compile.py torch/_inductor/scheduler.py torch/utils/_triton.py,https://github.com/pytorch/pytorch/pull/130403,masnesral,eellison,,,
6adc7251572,nn_frontend,docs,doc - fix the `max_norm` value in a note (#129687),torch/nn/modules/sparse.py,https://github.com/pytorch/pytorch/pull/129687,VladimirFokow,malfet,mikaylagawarecki,,
5bc18ec0a18,skip,not user facing,[Inductor][CPP] Support vectorization of remainder (#129849),aten/src/ATen/cpu/vec/functional_base.h aten/src/ATen/native/cpu/BinaryOpsKernel.cpp test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/129849,leslie-fang-intel,jgong5,lezcano,,
798b9652f76,cpp_frontend,not user facing,[6/N] Replace c10::optional with std::optional (#130438),aten/src/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h torch/csrc/api/include/torch/data/dataloader.h torch/csrc/api/include/torch/data/dataloader/base.h torch/csrc/api/include/torch/data/dataloader/stateful.h torch/csrc/api/include/torch/data/dataloader/stateless.h torch/csrc/api/include/torch/data/dataloader_options.h torch/csrc/api/include/torch/data/datasets/base.h torch/csrc/api/include/torch/data/datasets/chunk.h torch/csrc/api/include/torch/data/datasets/map.h torch/csrc/api/include/torch/data/datasets/mnist.h torch/csrc/api/include/torch/data/datasets/shared.h torch/csrc/api/include/torch/data/datasets/stateful.h torch/csrc/api/include/torch/data/datasets/tensor.h torch/csrc/api/include/torch/data/detail/data_shuttle.h torch/csrc/api/include/torch/data/detail/sequencers.h torch/csrc/api/include/torch/data/iterator.h torch/csrc/api/include/torch/data/samplers/base.h torch/csrc/api/include/torch/data/samplers/distributed.h torch/csrc/api/include/torch/data/samplers/random.h torch/csrc/api/include/torch/data/samplers/sequential.h torch/csrc/api/include/torch/data/samplers/stream.h torch/csrc/api/include/torch/linalg.h torch/csrc/api/include/torch/nn/cloneable.h torch/csrc/api/include/torch/nn/module.h torch/csrc/api/include/torch/nn/modules/container/any_module_holder.h torch/csrc/api/include/torch/nn/modules/container/moduledict.h torch/csrc/api/include/torch/nn/modules/container/modulelist.h torch/csrc/api/include/torch/nn/modules/container/sequential.h torch/csrc/api/include/torch/nn/parallel/data_parallel.h torch/csrc/api/src/nn/module.cpp torch/csrc/autograd/FunctionsManual.cpp torch/csrc/autograd/FunctionsManual.h torch/csrc/distributed/c10d/Store.hpp torch/csrc/distributed/rpc/init.cpp torch/csrc/distributed/rpc/tensorpipe_agent.cpp torch/csrc/distributed/rpc/tensorpipe_agent.h torch/csrc/distributed/rpc/testing/faulty_tensorpipe_agent.h torch/csrc/jit/mobile/train/random.h torch/csrc/jit/mobile/train/sequential.h torch/csrc/lazy/core/hash.h torch/csrc/profiler/unwind/debug_info.h torch/csrc/profiler/unwind/fast_symbolizer.h torch/csrc/profiler/unwind/sections.h torch/csrc/utils/tensor_new.cpp,https://github.com/pytorch/pytorch/pull/130438,cyyever,janeyx99,,,
79e34800c30,fx,not user facing,Suppress guards generated by empty_strided in ir_node_to_tensor (#130431),torch/_inductor/ir.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/130431,ezyang,IvanKobzarev,,,
a205a53c500,fx,not user facing,Make sym_node log more useful (#130436),torch/fx/experimental/sym_node.py,https://github.com/pytorch/pytorch/pull/130436,ezyang,Skylion007,,,
df9d1b44e7b,quantization,Untopiced,Preserve _numeric_debug_handle throguh deepcopy and re-export (#129287),test/quantization/pt2e/test_generate_numeric_debug_handle.py torch/ao/quantization/fx/convert.py torch/fx/proxy.py,https://github.com/pytorch/pytorch/pull/129287,jerryzh168,zhxchen17,,,
4fcfd475bea,skip,not user facing,[halide-backend] Update CI pin (#130258),.ci/docker/ci_commit_pins/halide.txt,https://github.com/pytorch/pytorch/pull/130258,jansel,eellison,,,
dfd1d1971ea,python_frontend,bug fixes,Fix warning when pickle.load torch.Storage (#130246),test/test_nn.py torch/storage.py,https://github.com/pytorch/pytorch/pull/130246,mikaylagawarecki,albanD,,,
9f401187c70,skip,not user facing,"[pipelining] Refactor test_schedule to fix ""-k"" (#130294)",test/distributed/pipelining/test_schedule.py test/distributed/pipelining/test_schedule_multiproc.py,https://github.com/pytorch/pytorch/pull/130294,wconstab,H-Huang,,,
a4576dad340,skip,Untopiced,[reland][custom ops] infer schema (#130079),docs/source/library.rst test/test_custom_ops.py torch/_library/infer_schema.py torch/library.py,https://github.com/pytorch/pytorch/pull/130079,yushangdi,zou3519,,,
5835ff1ed51,inductor,not user facing,[Easy][Inductor] Add comment for .min_order and .max_order (#130390),torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/130390,yf225,anijain2305,,,
da24823e067,distributed,not user facing,[BE][EZ] Migrate to new dcp save and load APIs (#130475),test/distributed/checkpoint/e2e/test_pipeline.py test/distributed/checkpoint/test_fsdp_optim_state.py test/distributed/checkpoint/test_fsspec.py test/distributed/checkpoint/test_tp_checkpoint.py,https://github.com/pytorch/pytorch/pull/130475,fduwjj,wz337,,,
fdc83610f27,skip,not user facing,Support for expandable segments with cuda graph trees (#128068),c10/cuda/CUDACachingAllocator.cpp test/dynamo/test_cudagraphs_expandable_segments.py test/inductor/test_cudagraph_trees_expandable_segments.py test/test_cuda.py test/test_cuda_expandable_segments.py torch/_inductor/cudagraph_trees.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/128068,bilal2vec,eellison,eqy,,
75ab027fbbe,distributed,not user facing,[dtensor] move bernolli to op strategy (#130286),torch/distributed/_tensor/ops/experimental_ops.py torch/distributed/_tensor/ops/random_ops.py,https://github.com/pytorch/pytorch/pull/130286,wanchaol,awgu,yifuwang,,
79c41bb58ae,skip,not user facing,[inductor] switch CppCodeCache to new cpp_builder. (#130132),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/130132,xuhancn,jansel,jgong5,,
b1942a1af43,skip,not user facing,"[fbgemm_gpu] Break up `fbgemm_cuda_utils.cuh`, pt 10 (#130468)",tools/amd_build/build_amd.py,https://github.com/pytorch/pytorch/pull/130468,q10,ezyang,,,
5db9bd467e6,python_frontend,Untopiced,Skip test_nnc_correctness for new op _unsafe_masked_index (#130375),torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/130375,isuruf,lezcano,,,
c5b66c3fe14,skip,not user facing,Enable -Werror=pedantic on torch targets (#130319),cmake/public/utils.cmake,https://github.com/pytorch/pytorch/pull/130319,cyyever,ezyang,,,
c9551a3f50e,skip,Untopiced,Make c10::string_view an alias of std::string_view (#130417),c10/test/util/string_view_test.cpp c10/util/TypeIndex.h c10/util/string_view.h caffe2/serialize/inline_container.cc torch/csrc/jit/mobile/debug_info.cpp torch/csrc/jit/mobile/flatbuffer_loader.cpp torch/csrc/lazy/core/hash.h,https://github.com/pytorch/pytorch/pull/130417,cyyever,ezyang,,,
2a51ccc77e2,fx,not user facing,"When translation validation is enabled, assert that hint is consistent (#130478)",torch/fx/experimental/sym_node.py,https://github.com/pytorch/pytorch/pull/130478,ezyang,lezcano,,,
f52b2ee90f2,skip,not user facing,Modularize aten parameter parser and checker (#125308),test/inductor/test_torchinductor.py torch/_C/__init__.pyi.in torch/_inductor/utils.py torch/csrc/dynamo/guards.cpp torch/csrc/dynamo/guards.h torch/csrc/inductor/aoti_eager/kernel_holder.cpp torch/csrc/inductor/aoti_eager/kernel_holder.h torch/csrc/inductor/aoti_eager/kernel_meta_info.cpp torch/csrc/inductor/aoti_eager/kernel_meta_info.h torch/csrc/utils/python_dispatch.cpp,https://github.com/pytorch/pytorch/pull/125308,EikanWang,atalman,jansel,jgong5,
9822fdc3548,linalg_frontend,not user facing,[7/N] Replace c10::optional with std::optional  (#130510),aten/src/ATen/LegacyBatchingRegistrations.cpp aten/src/ATen/TensorSubclassLikeUtils.h aten/src/ATen/VmapModeRegistrations.cpp aten/src/ATen/autocast_mode.h aten/src/ATen/core/Dict.h aten/src/ATen/core/IListRef_inl.h aten/src/ATen/core/IListRef_test.cpp aten/src/ATen/core/NamedTensor.cpp aten/src/ATen/core/NamedTensor.h aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h aten/src/ATen/core/function_schema.h aten/src/ATen/core/function_schema_inl.h aten/src/ATen/core/ivalue.h aten/src/ATen/core/ivalue_inl.h aten/src/ATen/core/jit_type.h aten/src/ATen/core/op_registration/adaption.h aten/src/ATen/core/op_registration/op_registration.h aten/src/ATen/cuda/CUDAEvent.h aten/src/ATen/functorch/ADInterpreters.cpp aten/src/ATen/functorch/BatchRulesActivation.cpp aten/src/ATen/functorch/BatchRulesBinaryOps.cpp aten/src/ATen/functorch/BatchRulesConvolution.cpp aten/src/ATen/functorch/BatchRulesFactory.cpp aten/src/ATen/functorch/BatchRulesHelper.cpp aten/src/ATen/functorch/BatchRulesHelper.h aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp aten/src/ATen/functorch/BatchRulesLoss.cpp aten/src/ATen/functorch/BatchRulesModules.cpp aten/src/ATen/functorch/BatchRulesNorm.cpp aten/src/ATen/functorch/BatchRulesPooling.cpp aten/src/ATen/functorch/BatchRulesRandomness.cpp aten/src/ATen/functorch/BatchRulesReduceOps.cpp aten/src/ATen/functorch/BatchRulesScatterOps.cpp aten/src/ATen/functorch/BatchRulesUnaryOps.cpp aten/src/ATen/functorch/BatchRulesViews.cpp aten/src/ATen/functorch/BatchingMetaprogramming.h aten/src/ATen/functorch/DynamicLayer.cpp aten/src/ATen/functorch/DynamicLayer.h aten/src/ATen/functorch/Interpreter.h aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp aten/src/ATen/functorch/LegacyVmapTransforms.cpp aten/src/ATen/functorch/PlumbingHelper.cpp aten/src/ATen/functorch/PlumbingHelper.h aten/src/ATen/functorch/TensorWrapper.h aten/src/ATen/native/Distributions.cpp aten/src/ATen/native/EmbeddingBag.cpp aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/NamedTensor.cpp aten/src/ATen/native/ReduceOps.cpp aten/src/ATen/native/SobolEngineOps.cpp aten/src/ATen/native/SoftMax.cpp aten/src/ATen/native/Sorting.cpp aten/src/ATen/native/SpectralOps.cpp aten/src/ATen/native/TensorConversions.cpp aten/src/ATen/native/TensorFactories.cpp aten/src/ATen/native/TensorShape.cpp aten/src/ATen/native/cuda/ForeachReduceOp.cu aten/src/ATen/native/cuda/Reduce.cuh aten/src/ATen/native/metal/MetalAten.mm aten/src/ATen/native/miopen/BatchNorm_miopen.cpp aten/src/ATen/native/quantized/cpu/ReduceOps.cpp aten/src/ATen/native/quantized/cpu/qclamp.cpp aten/src/ATen/native/sparse/SoftMax.cpp aten/src/ATen/native/sparse/SparseTensorMath.cpp aten/src/ATen/native/vulkan/ops/Factory.cpp aten/src/ATen/native/vulkan/ops/Factory.h aten/src/ATen/native/vulkan/ops/Mean.cpp aten/src/ATen/native/vulkan/ops/Sum.cpp aten/src/ATen/templates/DispatchKeyFunctions.h aten/src/ATen/test/cpu_rng_test.cpp,https://github.com/pytorch/pytorch/pull/130510,cyyever,janeyx99,,,
215013daad2,cuda,not user facing,[cuDNN][SDPA] Limit cuDNN SDPA head-dim to 128 (#130494),aten/src/ATen/native/transformers/cuda/sdp_utils.cpp,https://github.com/pytorch/pytorch/pull/130494,eqy,Skylion007,drisspg,,
354edb232a1,skip,not user facing,Make public binding test only consider files that are packaged in the wheels (#130497),test/test_public_bindings.py,https://github.com/pytorch/pytorch/pull/130497,albanD,aorenste,,,
f261c6ebe8c,skip,Untopiced,"Revert ""[halide-backend] Update CI pin (#130258)""",.ci/docker/ci_commit_pins/halide.txt,,,,,,
0d66ccaf239,distributed,Untopiced,[IntraNodeComm] fix an issue where input check fails when running all-reduce on sub groups (#130492),torch/csrc/distributed/c10d/intra_node_comm.cpp torch/csrc/distributed/c10d/intra_node_comm.cu torch/csrc/distributed/c10d/intra_node_comm.hpp,https://github.com/pytorch/pytorch/pull/130492,yifuwang,Chillee,,,
bac10cdd6f8,distributed,Untopiced,[DCP] Fix duplicated logging messages when enable both c10d and dcp l… (#130423),test/distributed/checkpoint/test_utils.py torch/distributed/c10d_logger.py,https://github.com/pytorch/pytorch/pull/130423,cdzhan,Skylion007,,,
9c612df5043,dynamo,not user facing,[dynamo][cpp-guards][QOL] Print NO_TENSOR_ALIASING guard once (#130285),torch/_dynamo/guards.py,https://github.com/pytorch/pytorch/pull/130285,anijain2305,jansel,,,
fed8b0055f5,dynamo,not user facing,[dynamo][bufgix] Fix the value for key manager (#130368),torch/_dynamo/guards.py,https://github.com/pytorch/pytorch/pull/130368,anijain2305,jansel,,,
f7d7b94017f,dynamo,not user facing,[dynamo][unspecialized-nn-module] Distinguish between user-defined and builtin nn module (#130416),test/export/test_export.py torch/_dynamo/guards.py torch/_dynamo/source.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/nn_module.py,https://github.com/pytorch/pytorch/pull/130416,anijain2305,jansel,,,
a833582dbbf,dynamo,not user facing,[dynamo][tuple] Optimize guard for small tuples - helps conv2d guards (#130400),torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/130400,anijain2305,jansel,yanboliang,,
c4a2b6a943a,quantization,not user facing,[2/N] Fix NVCC warnings  (#130214),aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu aten/src/ATen/native/quantized/cudnn/Pooling.cpp,https://github.com/pytorch/pytorch/pull/130214,cyyever,ezyang,,,
6f662e95756,linalg_frontend,bc breaking,update the input `weight` of `_convert_weight_to_int4pack` to `[n][k / 2] uint8` (#129940),aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/cpu/int4mm_kernel.cpp aten/src/ATen/native/cuda/int4mm.cu aten/src/ATen/native/mps/operations/Quantized.mm test/test_linalg.py test/test_mps.py torch/_meta_registrations.py torch/testing/_internal/common_quantization.py,https://github.com/pytorch/pytorch/pull/129940,yanbing-j,jgong5,lezcano,mingfeima,
be7bf20234b,inductor,not user facing,Add JK to enable fx graph cache for amd (#130463),torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/130463,oulgen,mxz297,nmacchioni,,
5ed72ff5f58,inductor,not user facing,Reduce all tensors to their metadata in AOTAutogradCache; add tests (#128583),test/dynamo/test_aot_autograd_cache.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/128583,jamesjwu,oulgen,,,
fb9bc6d74a6,composability,Untopiced,[custom op] add doc for CustomOpDef.set_kernel_enabled (#130406),docs/source/library.rst torch/_library/custom_ops.py,https://github.com/pytorch/pytorch/pull/130406,yushangdi,zou3519,,,
ca023f77bc4,releng,not user facing,[CD] Add pytorch xpu wheel build in nightly (#129560),.circleci/scripts/binary_linux_test.sh .circleci/scripts/binary_populate_env.sh .github/actionlint.yaml .github/actions/test-pytorch-binary/action.yml .github/scripts/generate_binary_build_matrix.py .github/templates/linux_binary_build_workflow.yml.j2 .github/workflows/generated-linux-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/129560,chuanqi129,atalman,,,
68ad3eb722a,dynamo,bug fixes,Do not set hints for mark_unbacked quantities (#130483),test/dynamo/test_unspec.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/130483,ezyang,lezcano,,,
9c1ba5ac103,mps,not user facing,[BE] Cleanup unused vars in MPS (#130541),aten/src/ATen/native/mps/operations/ReduceOps.mm,https://github.com/pytorch/pytorch/pull/130541,malfet,Skylion007,,,
72d9135679e,skip,not user facing,increase tensor size to force out of memory exception on the latest generations of GPUs (#130334),test/profiler/test_profiler.py,https://github.com/pytorch/pytorch/pull/130334,dnikolaev-amd,janeyx99,jeffdaily,,
3896ba3260a,skip,not user facing,[DeviceMesh] Only include the real thread_id in DeviceMesh hash under threaded backend (#130495),torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/130495,wz337,awgu,wanchaol,,
f21a21828ac,skip,not user facing,Change deprecated warning on dispatch_on_subclass to warn once (#130047),torch/csrc/utils/python_arg_parser.cpp,https://github.com/pytorch/pytorch/pull/130047,wz337,XilunWu,,,
492de213e27,skip,Untopiced,"Revert ""Change deprecated warning on dispatch_on_subclass to warn once (#130047)""",torch/csrc/utils/python_arg_parser.cpp,,,,,,
973037be6a3,releng,not user facing,[BE][Easy] apply autofix for ruff rules unnecessary-collection-call (C408): `list()` / `tuple()` / `dict()` (#130199),.github/scripts/tag_docker_images_for_release.py benchmarks/dynamo/common.py benchmarks/dynamo/huggingface.py benchmarks/dynamo/timm_models.py benchmarks/functional_autograd_benchmark/vision_models.py benchmarks/inference/process_metrics.py benchmarks/inference/server.py benchmarks/sparse/triton_ops.py pyproject.toml scripts/release_notes/common.py test/distributed/_tensor/test_pointwise_ops.py test/distributed/checkpoint/test_state_dict.py test/dynamo/test_functions.py test/dynamo/test_guard_manager.py test/dynamo/test_higher_order_ops.py test/dynamo/test_misc.py test/dynamo/test_modules.py test/dynamo/test_trace_rules.py test/export/test_passes.py test/jit/test_list_dict.py test/onnx/test_fx_to_onnx_with_onnxruntime.py test/onnx/test_pytorch_onnx_onnxruntime.py test/test_fx_experimental.py test/test_jit.py test/test_mps.py test/test_nestedtensor.py test/test_nn.py test/test_ops.py test/test_sparse_csr.py test/test_testing.py test/torch_np/numpy_tests/core/test_scalarmath.py test/torch_np/numpy_tests/lib/test_shape_base_.py tools/autograd/load_derivatives.py tools/stats/import_test_stats.py tools/test/test_codegen_model.py torch/__init__.py torch/_dynamo/backends/registry.py torch/_dynamo/bytecode_transformation.py torch/_dynamo/convert_frame.py torch/_dynamo/eval_frame.py torch/_dynamo/guards.py torch/_dynamo/output_graph.py torch/_dynamo/resume_execution.py torch/_dynamo/symbolic_convert.py torch/_dynamo/testing.py torch/_dynamo/trace_rules.py torch/_dynamo/utils.py torch/_dynamo/variables/base.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/lazy.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/optimizer.py torch/_dynamo/variables/user_defined.py torch/_export/converter.py torch/_functorch/_aot_autograd/traced_function_transforms.py torch/_functorch/partitioners.py torch/_guards.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_higher_order_ops/while_loop.py torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cuda/cutlass_epilogue_gen.py torch/_inductor/codegen/cuda/cutlass_utils.py torch/_inductor/codegen/cuda/gemm_template.py torch/_inductor/codegen/wrapper.py torch/_inductor/dependencies.py torch/_inductor/fx_passes/group_batch_fusion.py torch/_inductor/fx_passes/split_cat.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/pattern_matcher.py torch/_inductor/runtime/hints.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py torch/_inductor/sizevars.py torch/_lazy/device_context.py torch/_logging/_internal.py torch/_numpy/_funcs_impl.py torch/_refs/__init__.py torch/_refs/nn/functional/__init__.py torch/autograd/__init__.py torch/autograd/functional.py torch/cuda/_sanitizer.py torch/distributed/_composable/replicate.py torch/distributed/_spmd/graph_utils.py torch/distributed/_spmd/iter_graph_module.py torch/distributed/_state_dict_utils.py torch/distributed/checkpoint/filesystem.py torch/distributed/checkpoint/state_dict.py torch/distributed/fsdp/_init_utils.py torch/distributed/pipelining/_IR.py torch/distributed/rpc/backend_registry.py torch/fx/experimental/recording.py torch/fx/experimental/unification/multipledispatch/utils.py torch/fx/experimental/unification/utils.py torch/fx/passes/infra/partitioner.py torch/fx/passes/utils/fuser_utils.py torch/fx/passes/utils/matcher_utils.py torch/jit/_recursive.py torch/nn/modules/module.py torch/nn/parallel/_functions.py torch/nn/utils/prune.py torch/onnx/_internal/fx/onnxfunction_dispatcher.py torch/onnx/_internal/onnx_proto_utils.py torch/onnx/_internal/onnxruntime.py torch/onnx/symbolic_helper.py torch/onnx/symbolic_opset9.py torch/overrides.py torch/profiler/profiler.py torch/sparse/_triton_ops_meta.py torch/testing/_internal/common_device_type.py torch/testing/_internal/common_methods_invocations.py torch/testing/_internal/common_nn.py torch/testing/_internal/common_optimizers.py torch/testing/_internal/distributed/distributed_test.py torch/testing/_internal/opinfo/core.py torch/testing/_internal/opinfo/definitions/linalg.py torch/testing/_internal/opinfo/utils.py torch/utils/_config_module.py torch/utils/checkpoint.py torchgen/api/python.py torchgen/gen.py torchgen/model.py,https://github.com/pytorch/pytorch/pull/130199,XuehaiPan,malfet,,,
0a4fe2ff86f,distributed,not user facing,[DSD] Use no_grad() to make some operations faster and avoid possible memory leakage (#130355),torch/distributed/checkpoint/state_dict.py,https://github.com/pytorch/pytorch/pull/130355,fegin,wz337,,,
1cae60a87e5,dynamo,Untopiced,Caching attr_proxy for nn_module attribute to fix guard check failure (#130280),test/export/test_export.py torch/_dynamo/guards.py torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/130280,ydwu4,anijain2305,,,
578388bed82,skip,Untopiced,"Revert ""Support for expandable segments with cuda graph trees (#128068)""",c10/cuda/CUDACachingAllocator.cpp test/dynamo/test_cudagraphs_expandable_segments.py test/inductor/test_cudagraph_trees_expandable_segments.py test/test_cuda.py test/test_cuda_expandable_segments.py torch/_inductor/cudagraph_trees.py torch/testing/_internal/common_utils.py,,,,,,
cd9bae30de6,Uncategorized,Untopiced,Allow kwargs in _remove_effect_tokens_pass (#130491),test/export/test_passes.py torch/export/_remove_effect_tokens_pass.py torch/testing/_internal/torchbind_impls.py,https://github.com/pytorch/pytorch/pull/130491,ydwu4,angelayi,,,
bc188637137,quantization,Untopiced,Corner-case fix for upscale_histogram in the new HistogramObserver (#130316),torch/ao/quantization/observer.py,https://github.com/pytorch/pytorch/pull/130316,TiRune,jerryzh168,,,
c50b189280c,releng,not user facing,Move trunk windows builds to CUDA-12.1 (#130446),.github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/130446,malfet,atalman,,,
5c6edd29ec1,distributed,Untopiced,Turn on splitShare=1 to make the optimization of comm_split effective. (#129929),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/129929,mengph,shuqiangzhang,,,
726a2872717,skip,not user facing,[export] Expand verifier to be multiple on ExportedProgram (#130364),torch/_export/__init__.py torch/_export/serde/schema.py torch/_export/serde/schema.yaml torch/_export/serde/serialize.py torch/_export/verifier.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/130364,zhxchen17,angelayi,ydwu4,,
fc872e98f3c,composability,not user facing,Infer prim tags from equivalent aten ones (#130367),test/inductor/test_torchinductor_opinfo.py test/test_prims.py torch/_prims/__init__.py,https://github.com/pytorch/pytorch/pull/130367,eellison,ezyang,,,
a2f630a9a4f,skip,Untopiced,"Revert ""Decompose expand_copy and permute_copy (#129476)""",test/distributed/_tensor/test_dtensor_ops.py test/expect/HasDecompTest.test_has_decomposition.expect test/functorch/test_ops.py test/functorch/test_vmap.py test/test_mps.py tools/autograd/gen_variable_type.py torch/_decomp/__init__.py torch/_inductor/decomposition.py torch/_refs/__init__.py torch/testing/_internal/common_methods_invocations.py,,,,,,
d97d962082e,skip,Untopiced,"Revert ""Add decompositions for copy variants of view ops (#128416)""",test/distributed/_tensor/test_dtensor_ops.py test/expect/HasDecompTest.test_aten_core_operators.expect test/expect/HasDecompTest.test_has_decomposition.expect test/functorch/test_ops.py test/functorch/test_vmap.py test/onnx/test_fx_op_consistency.py test/test_mps.py test/test_proxy_tensor.py tools/autograd/gen_variable_type.py torch/_decomp/__init__.py torch/_inductor/lowering.py torch/_prims/context.py torch/_prims_common/wrappers.py torch/_refs/__init__.py torch/testing/_internal/common_methods_invocations.py,,,,,,
9c6c0deadc8,skip,not user facing,Add eager_compile_backwards_failure to tlparse (#130434),torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py,https://github.com/pytorch/pytorch/pull/130434,ezyang,albanD,,,
26c2b925250,skip,not user facing,[export] make with_effect mark op has_effect to prevent them from DCEed. (#129680),test/export/test_torchbind.py torch/_higher_order_ops/effects.py,https://github.com/pytorch/pytorch/pull/129680,ydwu4,angelayi,,,
18b7633bfbf,Uncategorized,Untopiced,[export] fix kwargs in run_decompositions() for training IR (#130553),test/export/test_export.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/130553,pianpwk,ydwu4,zhxchen17,,
b5b91b418da,Uncategorized,Untopiced,[Easy] Update record_function Comment (#130561),torch/autograd/profiler.py,https://github.com/pytorch/pytorch/pull/130561,sraikund16,aaronenyeshi,,,
7ce5b5767ca,skip,Untopiced,"Revert ""Make c10::string_view an alias of std::string_view (#130417)""",c10/test/util/string_view_test.cpp c10/util/TypeIndex.h c10/util/string_view.h caffe2/serialize/inline_container.cc torch/csrc/jit/mobile/debug_info.cpp torch/csrc/jit/mobile/flatbuffer_loader.cpp torch/csrc/lazy/core/hash.h,,,,,,
7f2436014e6,skip,not user facing,add MTIA as valid device type for prof averages (#130340),torch/autograd/profiler_util.py,https://github.com/pytorch/pytorch/pull/130340,fenypatel99,aaronenyeshi,,,
536b5b19b51,skip,Untopiced,"Revert ""Simplify c10::string_view (#130009)""",c10/test/util/string_view_test.cpp c10/util/TypeIndex.h c10/util/string_view.h,,,,,,
c101c4517ad,dynamo,Untopiced,Add python type for list iterators (#130511),test/dynamo/test_misc.py torch/_dynamo/variables/lists.py,https://github.com/pytorch/pytorch/pull/130511,mlazos,anijain2305,williamwen42,yanboliang,
c6a676add44,inductor,not user facing,[Traceable FSDP2][Inductor] Add GroupedSchedulerNode to contain nodes that must be scheduled together (#128568),.ci/pytorch/multigpu-test.sh test/distributed/test_compute_comm_reordering.py torch/_inductor/config.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/128568,yf225,eellison,mlazos,,
9ae40c6bc0e,distributed,not user facing,Fix and improve raise_comms and sink_waits (#129980),test/distributed/test_compute_comm_reordering.py torch/_inductor/comms.py torch/_inductor/graph.py torch/_inductor/lowering.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/129980,yifuwang,yf225,,,
c16e90fe06f,skip,not user facing,"The device_suffix in a test_name is ""privateuse1"" sometimes. (#130091)",torch/testing/_internal/common_device_type.py,https://github.com/pytorch/pytorch/pull/130091,yan-yhy,zou3519,,,
ae0edadea03,onnx,Untopiced,[SDPA] Replace `masked_fill_` with `aten::where` (#130281),aten/src/ATen/native/transformers/attention.cpp test/inductor/test_torchinductor.py test/onnx/test_fx_to_onnx_with_onnxruntime.py,https://github.com/pytorch/pytorch/pull/130281,sijiac,Skylion007,drisspg,justinchuby,
b7d287fbec0,skip,not user facing,Constant folding for dynamic shape node (#129686),benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_training.csv test/inductor/test_cpu_repro.py test/inductor/test_cudagraph_trees.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py test/inductor/test_torchinductor_dynamic_shapes.py test/test_torch.py torch/_inductor/constant_folding.py torch/_inductor/fx_passes/joint_graph.py,https://github.com/pytorch/pytorch/pull/129686,eellison,Chillee,,,
ff25dfca5a6,Uncategorized,Untopiced,Save quantization_tag in export graph serialization (#127473),test/export/test_serialize.py torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/127473,tarun292,angelayi,,,
b6215f44ef3,distributed,not user facing,DCP checkpoint_dist_client integration (#130452),torch/distributed/checkpoint/logger.py,https://github.com/pytorch/pytorch/pull/130452,harshabhvr248,LucasLLC,,,
0d8dedb01be,distributed,Untopiced,[dtensor] Add dtensor to TORCH_LOGS (#129512),test/distributed/_tensor/test_dtensor.py torch/_logging/_internal.py torch/_logging/_registrations.py torch/distributed/_tensor/_dispatch.py torch/distributed/_tensor/experimental/attention.py,https://github.com/pytorch/pytorch/pull/129512,fegin,XilunWu,wanchaol,,
e568c91a7b8,distributed,not user facing,[CP] Fix the incorrect ring schedule in the fwd and bwd (#129514),torch/distributed/_tensor/experimental/attention.py,https://github.com/pytorch/pytorch/pull/129514,fegin,d4l3k,drisspg,wanchaol,
207564bab1c,skip,not user facing,[Inductor] FlexAttention supports partial masking (#130415),test/inductor/test_flex_attention.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/flex_attention.py torch/_inductor/codegen/simd.py torch/_inductor/kernel/flex_attention.py torch/_inductor/select_algorithm.py torch/nn/attention/_flex_attention.py,https://github.com/pytorch/pytorch/pull/130415,yanboliang,Chillee,,,
da030e7addf,skip,Untopiced,"Revert ""[Inductor] FlexAttention supports partial masking (#130415)""",test/inductor/test_flex_attention.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/flex_attention.py torch/_inductor/codegen/simd.py torch/_inductor/kernel/flex_attention.py torch/_inductor/select_algorithm.py torch/nn/attention/_flex_attention.py,,,,,,
b75cc708759,distributed,not user facing,[Pipelining] add looped schedules to fsdp/ddp test (#130563),test/distributed/pipelining/test_composability.py,https://github.com/pytorch/pytorch/pull/130563,wconstab,H-Huang,,,
3100455b8ee,skip,not user facing,Make hashing a SymInt raise an error again (#130548),test/test_dynamic_shapes.py torch/__init__.py torch/_numpy/_util.py,https://github.com/pytorch/pytorch/pull/130548,ezyang,Skylion007,albanD,lezcano,
6f54e961eaf,fx,not user facing,Add trace_shape_events artifact tracing for ShapeEnv events (#130473),test/dynamo/test_logging.py torch/_logging/_registrations.py torch/fx/experimental/recording.py,https://github.com/pytorch/pytorch/pull/130473,ezyang,lezcano,,,
ae3ac9cb648,python_frontend,bug fixes,Only test _is_param if doing instance check on Parameter base (#130578),test/test_subclass.py torch/nn/parameter.py,https://github.com/pytorch/pytorch/pull/130578,ezyang,Skylion007,,,
ba941769b5c,skip,Untopiced,Add API for open registration between operators and subclasses (and modes) (#130064),test/profiler/test_profiler_tree.py test/test_custom_ops.py torch/_library/simple_registry.py torch/csrc/utils/python_arg_parser.cpp torch/library.py,https://github.com/pytorch/pytorch/pull/130064,zou3519,albanD,,,
9c69684af8b,skip,Untopiced,[custom_ops] expose torch.library.register_torch_dispatch (#130261),docs/source/library.rst docs/source/notes/extending.rst test/test_custom_ops.py torch/_library/custom_ops.py torch/library.py,https://github.com/pytorch/pytorch/pull/130261,zou3519,albanD,,,
d443fbc025a,inductor,not user facing,[inductor] Cache precompilation functions based on configs (#130350),test/inductor/test_max_autotune.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/130350,bertmaher,eellison,,,
febadda107e,mps,bug fixes,[MPS] Fix `torch.[all|any]` for 5+D tensors (#130542),aten/src/ATen/native/mps/operations/ReduceOps.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/130542,malfet,Skylion007,albanD,,
ea4b80e6d68,fx,Untopiced,"[FX][export] strict DCE pass, check schema for node impurity (#130552)",test/expect/TestFXAPIBackwardCompatibility.test_function_back_compat-fx_backcompat_function_signatures.expect test/export/test_export.py test/fx/test_dce_pass.py torch/export/_remove_effect_tokens_pass.py torch/export/_unlift.py torch/fx/graph.py,https://github.com/pytorch/pytorch/pull/130552,yushangdi,pianpwk,,,
e5657024b52,distributed,Untopiced,Fix loss_parallel with BF16 logits (#130550),torch/distributed/tensor/parallel/loss.py,https://github.com/pytorch/pytorch/pull/130550,mayank31398,tianyu-l,,,
18418a7dbb4,skip,not user facing,[ONNX] Fix torch_onnx patch accuracy bug in benchmark (#130586),benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/130586,titaiwangms,justinchuby,,,
2a1f22e57fe,skip,not user facing,Change BN to eval before QAT Convert phase (#130598),torch/testing/_internal/common_quantization.py,https://github.com/pytorch/pytorch/pull/130598,leslie-fang-intel,jgong5,yushangdi,,
2b1df24877b,skip,Untopiced,"Revert ""Make hashing a SymInt raise an error again (#130548)""",test/test_dynamic_shapes.py torch/__init__.py torch/_numpy/_util.py,,,,,,
34e57025e1f,cpp_frontend,bug fixes,Add unsigned int types to torch/types.h (#130616),torch/csrc/api/include/torch/types.h,https://github.com/pytorch/pytorch/pull/130616,ezyang,NicolasHug,albanD,,
22fd89c9045,inductor,not user facing,[TEST][Inductor] Fix scaled_mm call (#130582),test/inductor/test_aot_inductor.py test/inductor/test_fp8.py,https://github.com/pytorch/pytorch/pull/130582,Aidyn-A,drisspg,,,
c35f12c67cc,skip,not user facing,[EZ] Add formatting changes to .git-blame-ignore-revs (#130627),.git-blame-ignore-revs,https://github.com/pytorch/pytorch/pull/130627,malfet,clee2000,izaitsevfb,,
dafef3ff359,distributed,not user facing,[CP] Make CP loss curve on par with TP (#129515),torch/distributed/_tensor/experimental/attention.py,https://github.com/pytorch/pytorch/pull/129515,fegin,d4l3k,wanchaol,,
988ed4d5db7,dynamo,Untopiced,[export] clean up allow_complex_guards_as_runtime_asserts flag (#130596),test/dynamo/test_misc.py test/export/test_export.py torch/_dynamo/config.py torch/_dynamo/eval_frame.py torch/_dynamo/output_graph.py torch/_export/non_strict_utils.py torch/export/_trace.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/130596,pianpwk,angelayi,,,
55dc82bef92,skip,not user facing,[EZ] Make test_pytree_inputs actually run tests on CUDA (#130593),test/inductor/test_aot_inductor.py,https://github.com/pytorch/pytorch/pull/130593,malfet,angelayi,,,
4865c6425ca,distributed,Untopiced,Add new control plane handler (#129712),test/distributed/elastic/test_control_plane.py torch/csrc/distributed/c10d/NCCLUtils.cpp,https://github.com/pytorch/pytorch/pull/129712,c-p-i-o,wconstab,,,
b0a597fcb49,dynamo,Untopiced,Fix #121334: graph break on constant method call (#130158),test/dynamo/test_functions.py torch/__init__.py torch/_dynamo/variables/constant.py,https://github.com/pytorch/pytorch/pull/130158,rec,lezcano,,,
c3e77d144ec,skip,not user facing,"[3.12, 3.13, dynamo] simplified construction for frame f_locals/localsplus (#129185)",build_variables.bzl test/dynamo_expected_failures/TestLinalgCPU.test_lobpcg_torchscript_cpu_float64 test/dynamo_expected_failures/TestScript.test_function_overloading_isinstance test/dynamo_expected_failures/TestScript.test_function_overloads test/dynamo_expected_failures/TestScript.test_ignored_as_value test/dynamo_expected_failures/TestScript.test_namedtuple_python test/dynamo_expected_failures/TestScript.test_no_self_arg_ignore_function test/dynamo_expected_failures/TestScript.test_python_call_non_tensor_wrong test/dynamo_expected_failures/TestScript.test_python_op_builtins test/dynamo_expected_failures/TestScript.test_type_annotation_module test/dynamo_expected_failures/TestScript.test_unused_decorator test/dynamo_expected_failures/TestScript.test_wrong_return_type test/dynamo_skips/TestLinalgCPU.test_lobpcg_torchscript_cpu_float64 test/dynamo_skips/TestScript.test_function_overloading_isinstance test/dynamo_skips/TestScript.test_function_overloads test/dynamo_skips/TestScript.test_ignored_as_value test/dynamo_skips/TestScript.test_namedtuple_python test/dynamo_skips/TestScript.test_no_self_arg_ignore_function test/dynamo_skips/TestScript.test_python_call_non_tensor_wrong test/dynamo_skips/TestScript.test_python_op_builtins test/dynamo_skips/TestScript.test_type_annotation_module test/dynamo_skips/TestScript.test_unused_decorator test/dynamo_skips/TestScript.test_wrong_return_type torch/csrc/dynamo/cpython_defs.c torch/csrc/dynamo/cpython_defs.h torch/csrc/dynamo/cpython_includes.h torch/csrc/dynamo/debug_macros.h torch/csrc/dynamo/eval_frame.c torch/csrc/dynamo/extra_state.cpp torch/csrc/dynamo/framelocals_mapping.cpp torch/csrc/dynamo/framelocals_mapping.h,https://github.com/pytorch/pytorch/pull/129185,williamwen42,jansel,,,
0bf9a091ec1,dynamo,not user facing,[torchbind] add tracing_mode support (#129586),test/cpp/jit/test_custom_class_registrations.cpp test/export/test_torchbind.py torch/_dynamo/output_graph.py torch/_dynamo/variables/builder.py torch/_export/non_strict_utils.py torch/_functorch/aot_autograd.py torch/_higher_order_ops/torchbind.py torch/_library/fake_class_registry.py torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/129586,ydwu4,zou3519,,,
741c1710e81,dynamo,not user facing,[cond] inlining into one of the branches when pred is a python constant (#130493),test/dynamo/test_export.py test/dynamo/test_higher_order_ops.py test/export/test_export.py test/export/test_verifier.py test/functorch/test_aotdispatch.py test/functorch/test_control_flow.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/cond.py,https://github.com/pytorch/pytorch/pull/130493,ydwu4,BoyuanFeng,,,
2c4303c1d1f,inductor,not user facing,[ROCm] [BUGFIX] Re-enable rocm-specific tuning parameters (#130617),torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/130617,jataylo,masnesral,,,
43b98fa5218,jit,Untopiced,Add debug repr to SymNode (#129925),c10/core/SymNodeImpl.h torch/__init__.py torch/csrc/jit/python/init.cpp torch/csrc/utils/python_symnode.h torch/fx/experimental/sym_node.py,https://github.com/pytorch/pytorch/pull/129925,bt2513,aorenste,,,
60fc01d0ab1,skip,not user facing,[CUDA] Don't double-destroy CUDA graph when debug dump is used (#130401),aten/src/ATen/cuda/CUDAGraph.cpp test/test_cuda.py,https://github.com/pytorch/pytorch/pull/130401,eqy,Skylion007,eellison,,
d727e2f2d12,dynamo,not user facing,add total wall time in calculate_time_spent (#130611),torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/130611,dshi7,Yuzhen11,oulgen,,
af4da0799c5,Uncategorized,Untopiced,[PyTorch] Half: don't disable direct conversion to/from float on mobile (#130465),c10/util/Half-inl.h c10/util/Half.h,https://github.com/pytorch/pytorch/pull/130465,swolchok,ezyang,malfet,,
103b6ccab2b,skip,not user facing,Increase tolerance for tensorsolve tests (#130620),torch/testing/_internal/opinfo/definitions/linalg.py,https://github.com/pytorch/pytorch/pull/130620,albanD,atalman,lezcano,malfet,
f0d7164cb93,skip,Untopiced,"Revert ""[inductor] switch AotCodeCompiler to new cpp_builder (#130127)""",torch/_inductor/codecache.py,,,,,,
7c289c2a5c4,python_frontend,new features,Add torch.serialization.safe_globals context manager (#127939),docs/source/notes/serialization.rst test/test_serialization.py torch/_weights_only_unpickler.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/127939,mikaylagawarecki,albanD,,,
f093cd40864,export,Untopiced,Fix custom ops warning during export (#130623),test/export/test_export.py torch/_ops.py torch/_subclasses/functional_tensor.py,https://github.com/pytorch/pytorch/pull/130623,zou3519,williamwen42,,,
95046c86e35,composability,Untopiced,[HOP] add HOP x torch_dispatch interaction (#130606),test/dynamo/test_higher_order_ops.py torch/_ops.py torch/testing/_internal/two_tensor.py,https://github.com/pytorch/pytorch/pull/130606,zou3519,ydwu4,,,
619029e8929,python_frontend,docs,[easy] Small rendering fix in Tensor.module_load doc (#130489),torch/_tensor.py,https://github.com/pytorch/pytorch/pull/130489,mikaylagawarecki,janeyx99,,,
06ebf87a1ec,distributed,not user facing,Fix and improve reorder_compute_for_overlap (#130573),test/distributed/test_compute_comm_reordering.py torch/_inductor/comms.py,https://github.com/pytorch/pytorch/pull/130573,yifuwang,Chillee,,,
f422027fceb,linalg_frontend,Untopiced,fix torch.linalg.lstsq input check (#130612),aten/src/ATen/native/BatchLinearAlgebra.cpp test/test_linalg.py,https://github.com/pytorch/pytorch/pull/130612,inkcherry,lezcano,,,
ea78b0c1773,skip,Untopiced,"Revert ""Fix static `py::object` dangling pointer with `py::gil_safe_call_once_and_store` (#130341)""",functorch/csrc/dim/dim.cpp torch/csrc/dynamo/guards.cpp torch/csrc/jit/python/module_python.h torch/csrc/jit/python/python_ivalue.h torch/csrc/utils/python_symnode.cpp,,,,,,
634b62f1117,fx,not user facing,typing proxy_tensor.py (#129182),torch/_dynamo/compiled_autograd.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/_aot_autograd/subclass_utils.py torch/_higher_order_ops/flex_attention.py torch/_inductor/pattern_matcher.py torch/_subclasses/fake_tensor.py torch/_subclasses/meta_utils.py torch/_torch_docs.py torch/distributed/_shard/api.py torch/distributed/_spmd/comm_tensor.py torch/distributed/fsdp/_init_utils.py torch/fx/experimental/proxy_tensor.py torch/nn/parallel/distributed.py torch/onnx/_internal/fx/passes/decomp.py torch/onnx/_internal/fx/passes/functionalization.py torch/types.py torch/utils/_python_dispatch.py,https://github.com/pytorch/pytorch/pull/129182,aorenste,Chillee,,,
a7f54c7f8ae,dynamo,not user facing,[dynamo] add meta fn for aten.kthvalue.default (#130562),test/functorch/test_aotdispatch.py test/test_meta.py test/test_proxy_tensor.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/130562,ColinPeppler,jingsh,zou3519,,
f44739cf42e,onnx,not user facing,[ONNX] Remove beartype usage (#130484),.ci/docker/common/install_onnx.sh test/onnx/dynamo/test_exporter_api.py test/onnx/internal/test_beartype.py test/onnx/onnx_test_common.py test/onnx/test_fx_to_onnx_with_onnxruntime.py torch/onnx/README.md torch/onnx/_exporter_states.py torch/onnx/_internal/_beartype.py torch/onnx/_internal/diagnostics/_diagnostic.py torch/onnx/_internal/diagnostics/infra/decorator.py torch/onnx/_internal/diagnostics/infra/formatter.py torch/onnx/_internal/diagnostics/infra/utils.py torch/onnx/_internal/exporter.py torch/onnx/_internal/fx/_pass.py torch/onnx/_internal/fx/decomposition_table.py torch/onnx/_internal/fx/diagnostics.py torch/onnx/_internal/fx/dynamo_graph_extractor.py torch/onnx/_internal/fx/fx_onnx_interpreter.py torch/onnx/_internal/fx/fx_symbolic_graph_extractor.py torch/onnx/_internal/fx/onnxfunction_dispatcher.py torch/onnx/_internal/fx/op_validation.py torch/onnx/_internal/fx/passes/_utils.py torch/onnx/_internal/fx/passes/decomp.py torch/onnx/_internal/fx/passes/functionalization.py torch/onnx/_internal/fx/passes/modularization.py torch/onnx/_internal/fx/passes/readability.py torch/onnx/_internal/fx/passes/type_promotion.py torch/onnx/_internal/fx/passes/virtualization.py torch/onnx/_internal/fx/patcher.py torch/onnx/_internal/fx/registration.py torch/onnx/_internal/fx/serialization.py torch/onnx/_internal/fx/torch_export_graph_extractor.py torch/onnx/_internal/io_adapter.py torch/onnx/_internal/jit_utils.py torch/onnx/_internal/onnx_proto_utils.py torch/onnx/_internal/onnxruntime.py torch/onnx/_internal/registration.py torch/onnx/_type_utils.py torch/onnx/errors.py torch/onnx/symbolic_caffe2.py torch/onnx/symbolic_helper.py torch/onnx/symbolic_opset10.py torch/onnx/symbolic_opset11.py torch/onnx/symbolic_opset12.py torch/onnx/symbolic_opset13.py torch/onnx/symbolic_opset14.py torch/onnx/symbolic_opset15.py torch/onnx/symbolic_opset16.py torch/onnx/symbolic_opset17.py torch/onnx/symbolic_opset18.py torch/onnx/symbolic_opset20.py torch/onnx/symbolic_opset8.py torch/onnx/symbolic_opset9.py torch/onnx/utils.py torch/onnx/verification.py,https://github.com/pytorch/pytorch/pull/130484,justinchuby,titaiwangms,,,
3710a796226,inductor,not user facing,Flex Attention HOP: Add support for flex decoding (#129415),test/inductor/test_flex_decoding.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/nn/attention/_flex_attention.py,https://github.com/pytorch/pytorch/pull/129415,joydddd,Chillee,,,
c03e667276e,inductor,not user facing,[Inductor][PatternMatcher] Always prevent match across mutations (#130584),test/inductor/test_pattern_matcher.py torch/_inductor/fx_passes/b2b_gemm.py torch/_inductor/fx_passes/pre_grad.py torch/_inductor/fx_passes/split_cat.py torch/_inductor/pattern_matcher.py,https://github.com/pytorch/pytorch/pull/130584,yf225,jansel,,,
1ad0f38a372,inductor,not user facing,Fix IMAs in FlexAttention + autotuning (#130352),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/_flex_attention.py,https://github.com/pytorch/pytorch/pull/130352,drisspg,Chillee,Skylion007,,
567482973d7,distributed,not user facing,typing fake_tensor.py (#128041),tools/pyi/gen_pyi.py torch/_C/__init__.pyi.in torch/__init__.py torch/_dynamo/decorators.py torch/_export/non_strict_utils.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/subclass_utils.py torch/_functorch/config.py torch/_subclasses/fake_tensor.py torch/distributed/_composable/fsdp/_fsdp_init.py torch/export/_trace.py torch/fx/experimental/proxy_tensor.py torch/types.py torch/utils/_python_dispatch.py,https://github.com/pytorch/pytorch/pull/128041,aorenste,eellison,,,
0effcb70efe,skip,Untopiced,"Revert ""[ONNX] Remove beartype usage (#130484)""",.ci/docker/common/install_onnx.sh test/onnx/dynamo/test_exporter_api.py test/onnx/internal/test_beartype.py test/onnx/onnx_test_common.py test/onnx/test_fx_to_onnx_with_onnxruntime.py torch/onnx/README.md torch/onnx/_exporter_states.py torch/onnx/_internal/_beartype.py torch/onnx/_internal/diagnostics/_diagnostic.py torch/onnx/_internal/diagnostics/infra/decorator.py torch/onnx/_internal/diagnostics/infra/formatter.py torch/onnx/_internal/diagnostics/infra/utils.py torch/onnx/_internal/exporter.py torch/onnx/_internal/fx/_pass.py torch/onnx/_internal/fx/decomposition_table.py torch/onnx/_internal/fx/diagnostics.py torch/onnx/_internal/fx/dynamo_graph_extractor.py torch/onnx/_internal/fx/fx_onnx_interpreter.py torch/onnx/_internal/fx/fx_symbolic_graph_extractor.py torch/onnx/_internal/fx/onnxfunction_dispatcher.py torch/onnx/_internal/fx/op_validation.py torch/onnx/_internal/fx/passes/_utils.py torch/onnx/_internal/fx/passes/decomp.py torch/onnx/_internal/fx/passes/functionalization.py torch/onnx/_internal/fx/passes/modularization.py torch/onnx/_internal/fx/passes/readability.py torch/onnx/_internal/fx/passes/type_promotion.py torch/onnx/_internal/fx/passes/virtualization.py torch/onnx/_internal/fx/patcher.py torch/onnx/_internal/fx/registration.py torch/onnx/_internal/fx/serialization.py torch/onnx/_internal/fx/torch_export_graph_extractor.py torch/onnx/_internal/io_adapter.py torch/onnx/_internal/jit_utils.py torch/onnx/_internal/onnx_proto_utils.py torch/onnx/_internal/onnxruntime.py torch/onnx/_internal/registration.py torch/onnx/_type_utils.py torch/onnx/errors.py torch/onnx/symbolic_caffe2.py torch/onnx/symbolic_helper.py torch/onnx/symbolic_opset10.py torch/onnx/symbolic_opset11.py torch/onnx/symbolic_opset12.py torch/onnx/symbolic_opset13.py torch/onnx/symbolic_opset14.py torch/onnx/symbolic_opset15.py torch/onnx/symbolic_opset16.py torch/onnx/symbolic_opset17.py torch/onnx/symbolic_opset18.py torch/onnx/symbolic_opset20.py torch/onnx/symbolic_opset8.py torch/onnx/symbolic_opset9.py torch/onnx/utils.py torch/onnx/verification.py,,,,,,
7c83f5f7d5f,skip,not user facing,[8/N] Replace c10::optional with std::optional  (#130509),torchgen/api/python.py,https://github.com/pytorch/pytorch/pull/130509,cyyever,ezyang,,,
8714b7fc69d,dynamo,not user facing,[dynamo][cpp-guards] Use dict tags to skip guards on immutable dict getitems (#130654),test/dynamo/test_repros.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/130654,anijain2305,jansel,,,
1f162a5fce1,skip,Untopiced,"Revert ""[Inductor][CPP] Support vectorization of remainder (#129849)""",aten/src/ATen/cpu/vec/functional_base.h aten/src/ATen/native/cpu/BinaryOpsKernel.cpp test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,,,,,,
e5de25896f0,linalg_frontend,Untopiced,Fixed CUDA randint generation for large ranges. (#126066),aten/src/ATen/native/cuda/DistributionTemplates.h aten/src/ATen/native/cuda/RreluWithNoise.cu test/test_cuda.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/126066,tringwald,eqy,lezcano,,
97cfc65dbc4,distributed,Untopiced,"Back out ""[DeviceMesh] Only include the real thread_id in DeviceMesh hash under threaded backend (#130495)"" (#130676)",torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/130676,gag1jain,xunnanxu,,,
92ac9ee83cb,dynamo,not user facing,"[3.13, dynamo] swap null and pop_null in codegen (#130383)",torch/_dynamo/codegen.py,https://github.com/pytorch/pytorch/pull/130383,williamwen42,jansel,,,
87b406d7e53,dynamo,not user facing,"[3.13, dynamo] codegen TO_BOOL before conditional jump (#130384)",torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/130384,williamwen42,jansel,,,
0b5792c0ae7,dynamo,not user facing,"[3.13, dynamo] fix NULL ordering in symbolic_convert CALL (#130385)",torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/130385,williamwen42,jansel,,,
cbb7e26acdf,dynamo,not user facing,"[3.13, dynamo] fix jump target offset calculation (#130458)",torch/_dynamo/bytecode_transformation.py,https://github.com/pytorch/pytorch/pull/130458,williamwen42,jansel,,,
f9f85bfc0b5,inductor,not user facing,[Inductor] FlexAttention supports partial masking (#130415) (#130626),test/inductor/test_flex_attention.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/flex_attention.py torch/_inductor/codegen/common.py torch/_inductor/codegen/simd.py torch/_inductor/kernel/flex_attention.py torch/_inductor/select_algorithm.py torch/nn/attention/_flex_attention.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/130626,Chillee,drisspg,yanboliang,,
6f275ae4d06,profiler,Untopiced,Add kwinputs to Kineto Traces (#130373),test/profiler/test_profiler.py torch/_C/_autograd.pyi torch/autograd/profiler.py torch/autograd/profiler_util.py torch/csrc/autograd/init.cpp torch/csrc/autograd/profiler_kineto.cpp torch/csrc/autograd/profiler_kineto.h torch/csrc/profiler/collection.cpp torch/csrc/profiler/collection.h torch/csrc/profiler/util.cpp torch/csrc/profiler/util.h,https://github.com/pytorch/pytorch/pull/130373,sraikund16,davidberard98,,,
4410c44ae6f,skip,not user facing,[5/N] Change static functions in headers to inline  (#130673),aten/src/ATen/Context.h aten/src/ATen/ExpandUtils.h aten/src/ATen/TensorOperators.h aten/src/ATen/TracerMode.h aten/src/ATen/Utils.h aten/src/ATen/WrapDimUtilsMulti.h aten/src/ATen/core/Formatting.h aten/src/ATen/core/boxing/impl/boxing.h aten/src/ATen/cuda/Atomic.cuh aten/src/ATen/native/ScatterGatherChecks.h aten/src/ATen/native/cuda/GridSampler.cuh aten/src/ATen/native/cuda/SortingCommon.cuh aten/src/ATen/native/cuda/UpSample.cuh c10/cuda/CUDAMathCompat.h,https://github.com/pytorch/pytorch/pull/130673,cyyever,ezyang,,,
fa5f5727484,inductor,Untopiced,[cudagraph] fallback to eager if re-record too many times (#129349),test/inductor/test_cudagraph_trees.py torch/_inductor/config.py torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py,https://github.com/pytorch/pytorch/pull/129349,BoyuanFeng,eellison,,,
4d7bf72d937,quantization,not user facing,[BE][Easy] fix ruff rule needless-bool (SIM103) (#130206),benchmarks/dynamo/training_loss.py benchmarks/operator_benchmark/benchmark_core.py scripts/compile_tests/common.py scripts/compile_tests/passrate.py test/dynamo/test_higher_order_ops.py test/dynamo/test_structured_trace.py test/fx/test_subgraph_rewriter.py test/jit/test_data_parallel.py test/run_test.py test/test_jit.py test/test_modules.py test/test_ops.py test/test_overrides.py tools/autograd/load_derivatives.py tools/code_coverage/package/tool/parser/gcov_coverage_parser.py tools/code_coverage/package/tool/parser/llvm_coverage_parser.py torch/_dynamo/repro/after_dynamo.py torch/_dynamo/variables/builder.py torch/_functorch/_aot_autograd/input_output_analysis.py torch/_inductor/codegen/wrapper.py torch/_inductor/fx_passes/group_batch_fusion.py torch/_inductor/fx_passes/quantization.py torch/_inductor/sizevars.py torch/_prims_common/__init__.py torch/_refs/__init__.py torch/_subclasses/fake_impls.py torch/distributed/_tensor/placement_types.py torch/distributed/fsdp/_state_dict_utils.py torch/distributed/remote_device.py torch/fx/passes/infra/pass_manager.py torch/fx/passes/operator_support.py torch/fx/passes/pass_manager.py torch/onnx/verification.py torch/utils/cpp_extension.py torch/utils/data/graph_settings.py,https://github.com/pytorch/pytorch/pull/130206,XuehaiPan,malfet,,,
dcaa111dc83,dynamo,not user facing,support intersection by polyfill (#130672),test/dynamo/test_functions.py torch/_dynamo/polyfill.py torch/_dynamo/variables/dicts.py,https://github.com/pytorch/pytorch/pull/130672,awayzjj,anijain2305,,,
adaa0fea5a9,skip,not user facing,[Inductor][CPP] Enable Local Buffer for Outer loop fusion (#126967),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/metrics.py torch/_inductor/virtualized.py,https://github.com/pytorch/pytorch/pull/126967,leslie-fang-intel,jgong5,peterbell10,,
81322aee745,skip,not user facing,[Inductor][CPP] Support more than one LocalBuffer (#129121),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cpp_utils.py,https://github.com/pytorch/pytorch/pull/129121,leslie-fang-intel,jgong5,peterbell10,,
5fe9515d356,skip,not user facing,[structural binding][8/N] Replace std::tie with structural binding (#130544),aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu aten/src/ATen/native/transformers/cuda/attention_backward.cu,https://github.com/pytorch/pytorch/pull/130544,cyyever,ezyang,,,
774ca93fd2b,distributed,Untopiced,Added zb1p schedule (#130210),test/distributed/pipelining/model_registry.py test/distributed/pipelining/test_schedule.py test/distributed/pipelining/test_schedule_multiproc.py torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/130210,haocizhang,H-Huang,,,
28f6ae27186,cpp_frontend,not user facing,[9/N] Replace c10::optional with std::optional (#130674),android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp android/pytorch_android/src/main/cpp/pytorch_jni_lite.cpp aten/src/ATen/native/cuda/MultinomialKernel.cu aten/src/ATen/native/cuda/SummaryOps.cu aten/src/ATen/native/cuda/layer_norm_kernel.cu benchmarks/instruction_counts/definitions/standard.py benchmarks/static_runtime/test_static_runtime.cc c10/core/DeviceGuard.h c10/core/DispatchKey.h c10/core/DispatchKeySet.h c10/core/ScalarTypeToTypeMeta.h c10/core/StreamGuard.h c10/core/TensorOptions.h c10/core/impl/InlineDeviceGuard.h c10/core/impl/InlineStreamGuard.h c10/cuda/CUDAGuard.h c10/test/core/impl/InlineDeviceGuard_test.cpp c10/util/Optional.h c10/util/env.h test/cpp/api/autograd.cpp test/cpp/api/modules.cpp test/cpp/c10d/ProcessGroupNCCLTest.cpp test/cpp/c10d/TCPStoreTest.cpp test/cpp/jit/test_exception.cpp test/cpp/jit/test_flatbuffer.cpp test/cpp/jit/test_irparser.cpp test/cpp/jit/test_jit_type.cpp test/cpp/jit/test_lite_interpreter.cpp test/cpp/jit/test_lite_interpreter_direct.cpp test/cpp/jit/test_misc.cpp test/cpp/jit/test_save_load.cpp test/cpp/jit/test_shape_analysis.cpp test/cpp/lazy/test_lazy_ops.cpp test/cpp/lazy/test_lazy_ops_util.cpp test/cpp/lazy/test_misc.cpp test/cpp/profiler/containers.cpp test/cpp_extensions/open_registration_extension.cpp test/test_autograd.py tools/autograd/derivatives.yaml tools/autograd/gen_trace_type.py tools/autograd/gen_variable_factories.py tools/autograd/gen_variable_type.py tools/autograd/gen_view_funcs.py tools/autograd/load_derivatives.py tools/autograd/templates/Functions.h tools/autograd/templates/variable_factories.h tools/jit/gen_unboxing.py torch/_inductor/ir.py torch/testing/_internal/common_nn.py,https://github.com/pytorch/pytorch/pull/130674,cyyever,Skylion007,,,
c0897919dac,skip,Untopiced,"Revert "" [5/N] Change static functions in headers to inline  (#130673)""",aten/src/ATen/Context.h aten/src/ATen/ExpandUtils.h aten/src/ATen/TensorOperators.h aten/src/ATen/TracerMode.h aten/src/ATen/Utils.h aten/src/ATen/WrapDimUtilsMulti.h aten/src/ATen/core/Formatting.h aten/src/ATen/core/boxing/impl/boxing.h aten/src/ATen/cuda/Atomic.cuh aten/src/ATen/native/ScatterGatherChecks.h aten/src/ATen/native/cuda/GridSampler.cuh aten/src/ATen/native/cuda/SortingCommon.cuh aten/src/ATen/native/cuda/UpSample.cuh c10/cuda/CUDAMathCompat.h,,,,,,
c547b2e8713,build_frontend,bug fixes,Fix python detection in cuda.cmake (#130651),cmake/public/cuda.cmake,https://github.com/pytorch/pytorch/pull/130651,malfet,Skylion007,,,
a3c0bab502d,inductor,not user facing,[inductor] [cpp] use non-temporal tile load for A (#129455),torch/_inductor/codegen/cpp_micro_gemm.py,https://github.com/pytorch/pytorch/pull/129455,chunyuan-w,jgong5,,,
1bc390c5f5a,skip,Untopiced,Invalidate StorageImpl instances when tensor is overwritten with cudagraphs (#125264),c10/core/StorageImpl.cpp c10/core/StorageImpl.h test/inductor/test_cudagraph_trees.py torch/_C/__init__.pyi.in torch/_inductor/cudagraph_trees.py torch/csrc/cuda/Module.cpp,https://github.com/pytorch/pytorch/pull/125264,isuruf,ezyang,,,
dc7725cc164,inductor,Untopiced,[halide-backend] Random number generation (#130211),test/inductor/test_halide.py test/inductor/test_torchinductor.py torch/_inductor/codegen/halide.py torch/_inductor/runtime/halide_helpers.py,https://github.com/pytorch/pytorch/pull/130211,qqaatw,jansel,,,
1a266def4fa,dynamo,not user facing,[dynamo][unsoundness but very controlled] Skip guards on inbuilt nn module hooks (#130420),torch/_dynamo/guards.py,https://github.com/pytorch/pytorch/pull/130420,anijain2305,jansel,,,
1d983bbb289,skip,not user facing,[easy][inline-inbuilt-nn-module] Update test output (#130681),test/dynamo/test_structured_trace.py test/functorch/test_control_flow.py,https://github.com/pytorch/pytorch/pull/130681,anijain2305,jansel,zou3519,,
ac28ae18dc6,skip,not user facing,[BE][Ez]: Update pybind11 submodule to v2.13.1 (#129827),third_party/pybind11,https://github.com/pytorch/pytorch/pull/129827,Skylion007,XuehaiPan,albanD,atalman,
6beec34b1c6,skip,not user facing,[structural binding][9/N] Replace std::tie with structural binding (#130404),aten/src/ATen/native/cpu/FusedAdagradKernel.cpp aten/src/ATen/native/cpu/FusedAdamKernel.cpp aten/src/ATen/native/cpu/FusedSGDKernel.cpp aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu aten/src/ATen/native/nested/cuda/NestedTensorTransformerUtils.cpp,https://github.com/pytorch/pytorch/pull/130404,cyyever,janeyx99,,,
ee039c06146,skip,not user facing,[custom_op] triton_op API V0 (#130637),test/inductor/test_triton_kernels.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_library/__init__.py torch/_library/triton.py,https://github.com/pytorch/pytorch/pull/130637,zou3519,albanD,,,
74da2a467f1,skip,not user facing,Fix names conflict when lifting (#129817),test/dynamo/test_autograd_function.py test/dynamo/test_export.py test/dynamo/test_functions.py test/dynamo/test_higher_order_ops.py test/dynamo/test_repros.py test/dynamo/test_subclasses.py test/functorch/test_control_flow.py test/inductor/test_flex_attention.py torch/_dynamo/output_graph.py,https://github.com/pytorch/pytorch/pull/129817,RabbitWhite1,,,,
9cae2160f58,skip,not user facing,Introduce the concept of Accelerators to PyTorch doc (#129363),docs/source/torch.rst torch/_torch_docs.py,https://github.com/pytorch/pytorch/pull/129363,guangyey,EikanWang,albanD,gujinghui,
7cd48df2dae,python_frontend,Untopiced,Refine the logic of device construction when only device index is given (#129119),docs/source/tensor_attributes.rst torch/csrc/utils/python_arg_parser.h,https://github.com/pytorch/pytorch/pull/129119,guangyey,EikanWang,albanD,gujinghui,
9df4bc6a0dc,skip,Untopiced,"Revert ""Constant folding for dynamic shape node (#129686)""",benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_training.csv test/inductor/test_cpu_repro.py test/inductor/test_cudagraph_trees.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py test/inductor/test_torchinductor_dynamic_shapes.py test/test_torch.py torch/_inductor/constant_folding.py torch/_inductor/fx_passes/joint_graph.py,,,,,,
ca2d424c6e5,skip,Untopiced,Tighten torch.library.infer_schema input types (#130705),test/custom_operator/test_infer_schema_annotation.py test/test_custom_ops.py torch/_custom_op/impl.py torch/_custom_ops.py torch/_library/custom_ops.py torch/_library/infer_schema.py,https://github.com/pytorch/pytorch/pull/130705,zou3519,yushangdi,,,
0099e15b47b,dynamo,bug fixes,Also put unbacked symbols in symbol_to_node in split_module pass (#130535),test/distributed/test_dynamo_distributed.py torch/fx/passes/split_module.py,https://github.com/pytorch/pytorch/pull/130535,ezyang,malfet,,,
1e897a0ca44,skip,Untopiced,"Revert ""Fix names conflict when lifting (#129817)""",test/dynamo/test_autograd_function.py test/dynamo/test_export.py test/dynamo/test_functions.py test/dynamo/test_higher_order_ops.py test/dynamo/test_repros.py test/dynamo/test_subclasses.py test/functorch/test_control_flow.py test/inductor/test_flex_attention.py torch/_dynamo/output_graph.py,,,,,,
8fcb156e8b5,releng,not user facing,[BE] bump `optree` version to 0.12.1 (#130139),.ci/docker/requirements-ci.txt .github/requirements/pip-requirements-iOS.txt .github/requirements/pip-requirements-macOS.txt .github/workflows/_win-test.yml .lintrunner.toml requirements.txt setup.py test/functorch/test_eager_transforms.py test/profiler/test_profiler.py test/test_mps.py test/test_stateless.py test/test_testing.py torch/utils/_cxx_pytree.py,https://github.com/pytorch/pytorch/pull/130139,XuehaiPan,zou3519,,,
9e161af179a,skip,Untopiced,"Revert ""Increase tolerance for tensorsolve tests (#130620)""",torch/testing/_internal/opinfo/definitions/linalg.py,,,,,,
00d71b3e865,skip,not user facing,Tweak tolerances for test_vjp_linalg_tensorsolve_cuda_float32 to pass in Windows / debug builds (#130449),test/functorch/test_ops.py,https://github.com/pytorch/pytorch/pull/130449,jbschlosser,malfet,zou3519,,
b4b64f76e55,skip,not user facing,Ensure tensors devices match on `torch.index_put` batch rule impl (#130479),aten/src/ATen/functorch/BatchRulesScatterOps.cpp test/functorch/test_vmap.py,https://github.com/pytorch/pytorch/pull/130479,guilhermeleobas,zou3519,,,
53cf46b8c60,skip,not user facing,Fix names conflict when lifting (#129817),test/dynamo/test_autograd_function.py test/dynamo/test_export.py test/dynamo/test_functions.py test/dynamo/test_higher_order_ops.py test/dynamo/test_repros.py test/dynamo/test_subclasses.py test/functorch/test_control_flow.py test/inductor/test_flex_attention.py torch/_dynamo/output_graph.py,https://github.com/pytorch/pytorch/pull/129817,RabbitWhite1,,,,
7d82dc2c238,distributed,Untopiced,[dtensor] slice_backward to use op strategy (#130287),test/distributed/_tensor/test_tensor_ops.py torch/distributed/_tensor/ops/experimental_ops.py,https://github.com/pytorch/pytorch/pull/130287,wanchaol,awgu,,,
3342f3aa4ef,distributed,not user facing,[dtensor] simplify sdpa strategies (#130288),torch/distributed/_tensor/_op_schema.py torch/distributed/_tensor/ops/embedding_ops.py torch/distributed/_tensor/ops/math_ops.py torch/distributed/_tensor/ops/matrix_ops.py torch/distributed/_tensor/ops/tensor_ops.py torch/distributed/_tensor/ops/utils.py,https://github.com/pytorch/pytorch/pull/130288,wanchaol,tianyu-l,,,
a7cfe40c9bb,distributed,Untopiced,[dtensor] Improve from_local API with run_check (#130289),test/distributed/_tensor/test_dtensor.py torch/distributed/_tensor/_collective_utils.py torch/distributed/_tensor/api.py,https://github.com/pytorch/pytorch/pull/130289,wanchaol,awgu,wz337,,
f1456c74a09,Uncategorized,Untopiced,Fix mkl-static issue for Windows. (#130697),cmake/Modules/FindMKL.cmake,https://github.com/pytorch/pytorch/pull/130697,xuhancn,atalman,jgong5,,
074a5c0c9b9,skip,Untopiced,"Revert ""[BE] bump `optree` version to 0.12.1 (#130139)""",.ci/docker/requirements-ci.txt .github/requirements/pip-requirements-iOS.txt .github/requirements/pip-requirements-macOS.txt .github/workflows/_win-test.yml .lintrunner.toml requirements.txt setup.py test/functorch/test_eager_transforms.py test/profiler/test_profiler.py test/test_mps.py test/test_stateless.py test/test_testing.py torch/utils/_cxx_pytree.py,,,,,,
0e79e1f9584,skip,not user facing,[NJT+SDPA]Fix flash_attention output when batch_size=1 and seq_len=1 (#130652),test/test_nestedtensor.py torch/nested/_internal/sdpa.py,https://github.com/pytorch/pytorch/pull/130652,YuqingJ,Skylion007,drisspg,jbschlosser,
27322355dec,inductor,not user facing,Added some more documentation to block mask creation (#130649),torch/_inductor/kernel/flex_attention.py torch/nn/attention/_flex_attention.py,https://github.com/pytorch/pytorch/pull/130649,Chillee,drisspg,,,
ee6f0ab190a,distributed,not user facing,[DeviceMesh][Reland] Only include the real thread_id in DeviceMesh hash under threaded backend (#130495) (#130685),torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/130685,wz337,gag1jain,,,
ac3e2cb64a9,releng,not user facing,[BE] Delete unused -rg.yml workflow (#130759),.github/workflows/_linux-build-rg.yml .github/workflows/_linux-test-label.yml .github/workflows/_linux-test-rg.yml,https://github.com/pytorch/pytorch/pull/130759,malfet,ZainRizvi,,,
77fb5b0e231,distributed,Untopiced,[c10d] a new Pytorch API (split_group) to create a process group (#130507),test/distributed/test_c10d_nccl.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/130507,shuqiangzhang,wconstab,,,
db3a641b71f,distributed,not user facing,Implement operator for micro-pipelined all-gather -> _scaled_mm (#129289),test/distributed/test_symmetric_memory.py torch/distributed/_symmetric_memory/__init__.py,https://github.com/pytorch/pytorch/pull/129289,yifuwang,Chillee,drisspg,weifengpy,
78799e82b06,skip,Untopiced,"Revert ""Invalidate StorageImpl instances when tensor is overwritten with cudagraphs (#125264)""",c10/core/StorageImpl.cpp c10/core/StorageImpl.h test/inductor/test_cudagraph_trees.py torch/_C/__init__.pyi.in torch/_inductor/cudagraph_trees.py torch/csrc/cuda/Module.cpp,,,,,,
dff9d68f187,skip,Untopiced,"Revert ""Fix names conflict when lifting (#129817)""",test/dynamo/test_autograd_function.py test/dynamo/test_export.py test/dynamo/test_functions.py test/dynamo/test_higher_order_ops.py test/dynamo/test_repros.py test/dynamo/test_subclasses.py test/functorch/test_control_flow.py test/inductor/test_flex_attention.py torch/_dynamo/output_graph.py,,,,,,
6f32dc0c7b7,distributed,not user facing,Don't pass error message as `places` in `assertGreaterAlmostEqual` (#130648),torch/testing/_internal/distributed/distributed_test.py,https://github.com/pytorch/pytorch/pull/130648,eqy,awgu,,,
3928ca2ab64,dynamo,not user facing,[dynamo] update call map to allow multiple input parameters (#130748),test/dynamo/test_repros.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/130748,williamwen42,Skylion007,anijain2305,,
7d4f50de190,dynamo,not user facing,dynamo add support for `defaultdict(set)` (#130745),test/dynamo/test_functions.py torch/_dynamo/utils.py torch/_dynamo/variables/dicts.py,https://github.com/pytorch/pytorch/pull/130745,alexcdennis,Skylion007,,,
535016967ae,sparse_frontend,not user facing,Enable UFMT on all of torch/sparse (#130545),.lintrunner.toml torch/sparse/__init__.py torch/sparse/_semi_structured_conversions.py torch/sparse/_triton_ops.py torch/sparse/semi_structured.py,https://github.com/pytorch/pytorch/pull/130545,WeiChunyu-star,ezyang,,,
b893aa71ca9,quantization,not user facing,Rename generate_numeric_debug_handle to numeric_debugger (#130590),docs/source/quantization-support.rst test/quantization/pt2e/test_generate_numeric_debug_handle.py test/quantization/pt2e/test_numeric_debugger.py test/test_quantization.py torch/ao/quantization/__init__.py torch/ao/quantization/fx/convert.py torch/ao/quantization/pt2e/generate_numeric_debug_handle.py torch/ao/quantization/pt2e/numeric_debugger.py torch/ao/quantization/pt2e/prepare.py,https://github.com/pytorch/pytorch/pull/130590,jerryzh168,dulinriley,tarun292,,
3f031b96c6a,skip,not user facing,[Fix] Correctly identifying arguments for sub-blocks with renaming logic during TorchScript to ExportedProgram conversion (#128386),test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/128386,jiashenC,angelayi,,,
9a5204dc2db,inductor,not user facing,"[inductor] Remove ""spawn"" as an option for parallel compile method (#130746)",torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/130746,masnesral,Skylion007,,,
50ef099ad0e,inductor,not user facing,Learn a heuristic to decide whether to pad before mm (#128643),test/inductor/test_autoheuristic.py test/inductor/test_pattern_matcher.py torch/_inductor/autoheuristic/__init__.py torch/_inductor/autoheuristic/artifacts/_PadMMA100.py torch/_inductor/autoheuristic/artifacts/__init__.py torch/_inductor/autoheuristic/autoheuristic.py torch/_inductor/autoheuristic/autoheuristic_utils.py torch/_inductor/autoheuristic/learned_heuristic_controller.py torch/_inductor/autoheuristic/learnedheuristic_interface.py torch/_inductor/config.py torch/_inductor/fx_passes/pad_mm.py torch/testing/_internal/inductor_utils.py torchgen/autoheuristic/gen_data_pad_mm.py torchgen/autoheuristic/gen_pad_mm_a100.sh torchgen/autoheuristic/gen_pad_mm_h100.sh torchgen/autoheuristic/get_padmm_dataset.sh torchgen/autoheuristic/train.py torchgen/autoheuristic/train_pad_mm.py,https://github.com/pytorch/pytorch/pull/128643,AlnisM,Chillee,eellison,,
006020ff6e7,skip,not user facing,Fix the cudagraph capture of SDPA (#130712),aten/src/ATen/native/transformers/attention.cpp,https://github.com/pytorch/pytorch/pull/130712,sijiac,Skylion007,chenyang78,,
54a932b0acc,skip,not user facing,Support for expandable segments with cuda graph trees (#128068),c10/cuda/CUDACachingAllocator.cpp test/dynamo/test_cudagraphs_expandable_segments.py test/inductor/test_cudagraph_trees_expandable_segments.py test/test_cuda.py test/test_cuda_expandable_segments.py torch/_inductor/cudagraph_trees.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/128068,bilal2vec,eellison,eqy,,
b27695791e9,skip,not user facing,[PT-D] Relaxed `contract` to allow `Sequence[nn.Module]` (#127773),test/distributed/_composable/test_contract.py torch/distributed/_composable/contract.py torch/distributed/utils.py,https://github.com/pytorch/pytorch/pull/127773,awgu,weifengpy,,,
d3ab8cecedd,skip,Untopiced,[FSDP2] Allowed `List[nn.Module]` as arg (#127786),test/distributed/_composable/fsdp/test_fully_shard_comm.py test/distributed/_composable/fsdp/test_fully_shard_compile.py test/distributed/_composable/fsdp/test_fully_shard_init.py test/distributed/_composable/fsdp/test_fully_shard_training.py torch/distributed/_composable/fsdp/_fsdp_init.py torch/distributed/_composable/fsdp/_fsdp_param_group.py torch/distributed/_composable/fsdp/_fsdp_state.py torch/distributed/_composable/fsdp/fully_shard.py,https://github.com/pytorch/pytorch/pull/127786,awgu,weifengpy,yf225,,
ea4f310ff12,skip,not user facing,[Nested Tensor][easy] Add softmax backward support (#130602),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/130602,YuqingJ,davidberard98,jbschlosser,,
9ab8d47f9d2,skip,not user facing,Constant folding for dynamic shape node (#129686),benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_training.csv test/inductor/test_cpu_repro.py test/inductor/test_cudagraph_trees.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py test/inductor/test_torchinductor_dynamic_shapes.py test/test_torch.py torch/_inductor/constant_folding.py torch/_inductor/fx_passes/joint_graph.py,https://github.com/pytorch/pytorch/pull/129686,eellison,Chillee,,,
d8616eb66aa,dynamo,Untopiced,Mark nn_module params and buffers as static in dynamo (#130391),test/dynamo/test_modules.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/130391,mlazos,anijain2305,,,
0d0c09702ae,dynamo,Untopiced,Update mark_static_address for inlining NN modules (#130392),test/dynamo/test_decorators.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/130392,mlazos,anijain2305,,,
200d3d0a89a,inductor,Untopiced,Remove static param counting if inlining NN modules (#130503),torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/130503,mlazos,bdhirsh,,,
69a77389e2c,skip,Untopiced,Propagate buffer and parameter indices through AOT (#130393),test/dynamo/test_subclasses.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/_aot_autograd/subclass_utils.py torch/_functorch/_aot_autograd/traced_function_transforms.py torch/_functorch/aot_autograd.py torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/130393,mlazos,bdhirsh,,,
80236dca90b,skip,not user facing,Add buffer static input tests to cudagraph trees (#130402),test/inductor/test_cudagraph_trees.py,https://github.com/pytorch/pytorch/pull/130402,mlazos,eellison,,,
5e617d7ef53,python_frontend,not user facing,[CUDA] Actually bump tolerances for `test_grad_pca_lowrank` (#130770),torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/130770,eqy,Skylion007,,,
a3abfa5cb57,releng,not user facing,[BE][Easy][1/19] enforce style for empty lines in import segments (#129752),.circleci/codegen_validation/normalize_yaml_fragment.py .circleci/scripts/trigger_azure_pipeline.py android/pytorch_android/generate_test_torchscripts.py android/test_app/make_assets.py android/test_app/make_assets_custom.py aten/src/ATen/native/quantized/cpu/qnnpack/deps/clog/configure.py aten/src/ATen/native/transformers/cuda/flash_attn/kernels/generate_kernels.py aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/generate_kernels.py docs/cpp/source/conf.py docs/source/conf.py docs/source/scripts/build_activation_images.py docs/source/scripts/build_opsets.py docs/source/scripts/exportdb/generate_example_rst.py docs/source/scripts/onnx/build_onnx_torchscript_supported_aten_op_csv_table.py ios/TestApp/benchmark/coreml_backend.py ios/TestApp/benchmark/trace_model.py ios/TestApp/custom_build/custom_build.py ios/TestApp/run_on_aws_devicefarm.py scripts/analysis/format_test_csv.py scripts/compile_tests/common.py scripts/compile_tests/failures_histogram.py scripts/compile_tests/passrate.py scripts/compile_tests/update_failures.py scripts/diagnose_protobuf.py scripts/export/update_schema.py scripts/get_python_cmake_flags.py scripts/jit/log_extract.py scripts/release_notes/apply_categories.py scripts/release_notes/namespace_check.py setup.py tools/linter/adapters/ufmt_linter.py,https://github.com/pytorch/pytorch/pull/129752,XuehaiPan,ezyang,malfet,,
93a03edcf99,composability,not user facing,Update error message in meta__convert_weight_to_int4pack (#130707),torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/130707,yanbing-j,lezcano,malfet,,
39eeaac4e59,inductor,Untopiced,inductor: avoiding moving constructor to cuda when it would cause h2d sync in index_put_ fallback (#130338),test/inductor/test_torchinductor.py torch/_inductor/fx_passes/post_grad.py,https://github.com/pytorch/pytorch/pull/130338,bdhirsh,eellison,,,
3cd2ae331a5,skip,Untopiced,Use inductor TestCase for distributed tests (#129494),test/distributed/test_c10d_nccl.py torch/testing/_internal/common_distributed.py,https://github.com/pytorch/pytorch/pull/129494,masnesral,eellison,,,
ad314a2f055,distributed,Untopiced,Pass `torch.load(weights_only=)` internally to avoid FutureWarning (#130663),torch/distributed/checkpoint/default_planner.py torch/distributed/checkpoint/filesystem.py torch/distributed/checkpoint/format_utils.py torch/distributed/checkpoint/planner.py torch/distributed/optim/zero_redundancy_optimizer.py,https://github.com/pytorch/pytorch/pull/130663,awaelchli,LucasLLC,malfet,,
a29052a0bf3,skip,not user facing,[BE][Ez]: Update ruff to 0.5.2 (#130698),.lintrunner.toml,https://github.com/pytorch/pytorch/pull/130698,Skylion007,ezyang,,,
2b2671a7b11,distributed,Untopiced,[dtensor] fix foreach_norm when ord is 2 (#130753),test/distributed/_tensor/test_math_ops.py torch/distributed/_tensor/ops/math_ops.py,https://github.com/pytorch/pytorch/pull/130753,wanchaol,awgu,,,
7b2e802f314,distributed,Untopiced,[dtensor] add a few dunder methods to pointwise ops (#130754),torch/distributed/_tensor/ops/pointwise_ops.py,https://github.com/pytorch/pytorch/pull/130754,wanchaol,Skylion007,awgu,msaroufim,
95dbbf713ec,distributed,Untopiced,[Distributed] [9/N] Fix clang-tidy warnings in torch/csrc/distributed/rpc   (#130109),torch/csrc/distributed/c10d/ProcessGroupGloo.cpp torch/csrc/distributed/rpc/agent_utils.cpp torch/csrc/distributed/rpc/init.cpp torch/csrc/distributed/rpc/message.cpp torch/csrc/distributed/rpc/metrics/RpcMetricsHandler.h torch/csrc/distributed/rpc/metrics/registry.cpp torch/csrc/distributed/rpc/profiler/remote_profiler_manager.cpp torch/csrc/distributed/rpc/profiler/remote_profiler_manager.h torch/csrc/distributed/rpc/profiler/server_process_global_profiler.cpp torch/csrc/distributed/rpc/profiler/server_process_global_profiler.h torch/csrc/distributed/rpc/py_rref.cpp torch/csrc/distributed/rpc/python_call.cpp torch/csrc/distributed/rpc/python_call.h torch/csrc/distributed/rpc/python_functions.cpp torch/csrc/distributed/rpc/python_functions.h torch/csrc/distributed/rpc/python_remote_call.cpp torch/csrc/distributed/rpc/python_remote_call.h torch/csrc/distributed/rpc/python_resp.cpp torch/csrc/distributed/rpc/python_resp.h torch/csrc/distributed/rpc/python_rpc_handler.cpp torch/csrc/distributed/rpc/python_rpc_handler.h torch/csrc/distributed/rpc/request_callback.cpp torch/csrc/distributed/rpc/request_callback.h torch/csrc/distributed/rpc/request_callback_impl.cpp torch/csrc/distributed/rpc/request_callback_impl.h torch/csrc/distributed/rpc/request_callback_no_python.cpp torch/csrc/distributed/rpc/request_callback_no_python.h torch/csrc/distributed/rpc/rpc.h torch/csrc/distributed/rpc/rpc_agent.cpp torch/csrc/distributed/rpc/rpc_agent.h torch/csrc/distributed/rpc/rpc_command_base.h torch/csrc/distributed/rpc/rref_context.cpp torch/csrc/distributed/rpc/rref_context.h torch/csrc/distributed/rpc/rref_impl.cpp torch/csrc/distributed/rpc/rref_impl.h torch/csrc/distributed/rpc/rref_proto.cpp torch/csrc/distributed/rpc/rref_proto.h torch/csrc/distributed/rpc/script_call.cpp torch/csrc/distributed/rpc/script_remote_call.cpp torch/csrc/distributed/rpc/script_resp.cpp torch/csrc/distributed/rpc/tensorpipe_agent.cpp torch/csrc/distributed/rpc/tensorpipe_agent.h torch/csrc/distributed/rpc/tensorpipe_cuda.cpp torch/csrc/distributed/rpc/tensorpipe_utils.cpp torch/csrc/distributed/rpc/tensorpipe_utils.h torch/csrc/distributed/rpc/torchscript_functions.cpp torch/csrc/distributed/rpc/torchscript_functions.h torch/csrc/distributed/rpc/types.cpp torch/csrc/distributed/rpc/types.h torch/csrc/distributed/rpc/unpickled_python_call.cpp torch/csrc/distributed/rpc/unpickled_python_call.h torch/csrc/distributed/rpc/unpickled_python_remote_call.cpp torch/csrc/distributed/rpc/unpickled_python_remote_call.h torch/csrc/distributed/rpc/utils.cpp,https://github.com/pytorch/pytorch/pull/130109,cyyever,ezyang,,,
c5496296966,releng,not user facing,[CD] Fix xpu nightly wheel test failure (#130742),.circleci/scripts/binary_linux_test.sh,https://github.com/pytorch/pytorch/pull/130742,chuanqi129,atalman,,,
83eedf66b9e,distributed,not user facing,Update libfmt submodule to 11.0.1 (#130628),third_party/fmt third_party/kineto torch/csrc/distributed/c10d/socket.cpp torch/csrc/profiler/standalone/execution_trace_observer.cpp torch/csrc/profiler/util.cpp,https://github.com/pytorch/pytorch/pull/130628,Skylion007,aaronenyeshi,,,
fedae41c57b,dynamo,not user facing,[dynamo] Do not mark nn.module containers as BuiltinNNModuleVariable (#130773),torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/130773,anijain2305,mlazos,williamwen42,,
096dc444cee,skip,not user facing,Keep zero check be compatible with different sympy versions (#130729),test/test_sympy_utils.py torch/utils/_sympy/value_ranges.py,https://github.com/pytorch/pytorch/pull/130729,guangyey,EikanWang,lezcano,,
747b38c131e,releng,not user facing,[BE][Easy][2/19] enforce style for empty lines in import segments in `.ci/` and `.github/` (#129753),.ci/pytorch/create_test_cert.py .ci/pytorch/perf_test/compare_with_baseline.py .ci/pytorch/perf_test/get_stats.py .ci/pytorch/perf_test/update_commit_hash.py .ci/pytorch/print_sccache_log.py .ci/pytorch/win-test-helpers/run_python_nn_smoketests.py .github/scripts/build_triton_wheel.py .github/scripts/check_labels.py .github/scripts/cherry_pick.py .github/scripts/close_nonexistent_disable_issues.py .github/scripts/collect_ciflow_labels.py .github/scripts/convert_lintrunner_annotations_to_github.py .github/scripts/delete_old_branches.py .github/scripts/ensure_actions_will_cancel.py .github/scripts/export_pytorch_labels.py .github/scripts/filter_test_configs.py .github/scripts/generate_binary_build_matrix.py .github/scripts/generate_ci_workflows.py .github/scripts/generate_docker_release_matrix.py .github/scripts/generate_pytorch_version.py .github/scripts/get_workflow_job_id.py .github/scripts/github_utils.py .github/scripts/gitutils.py .github/scripts/label_utils.py .github/scripts/pytest_cache.py .github/scripts/pytest_caching_utils.py .github/scripts/test_trymerge.py .github/scripts/trymerge.py .github/scripts/tryrebase.py tools/linter/adapters/ufmt_linter.py,https://github.com/pytorch/pytorch/pull/129753,XuehaiPan,malfet,,,
168e41009bb,mps,not user facing,[structural binding][10/N] Replace std::tie with structural binding (#130784),aten/src/ATen/native/UpSample.h aten/src/ATen/native/cpu/GridSamplerKernel.cpp aten/src/ATen/native/cpu/UpSampleKernel.cpp aten/src/ATen/native/mps/operations/HistogramKernel.mm aten/src/ATen/native/mps/operations/LinearAlgebra.mm aten/src/ATen/native/mps/operations/Normalization.mm aten/src/ATen/native/sparse/SparseBinaryOpIntersectionCommon.h aten/src/ATen/native/sparse/cuda/SparseCsrTensorMath.cu test/cpp/jit/test_graph_executor.cpp test/cpp/rpc/test_tensorpipe_serialization.cpp,https://github.com/pytorch/pytorch/pull/130784,cyyever,malfet,,,
e57101d927c,skip,not user facing,Add testing regarding SparseAdam state_dicts (#130645),test/test_optim.py torch/testing/_internal/common_optimizers.py,https://github.com/pytorch/pytorch/pull/130645,jovianjaison,janeyx99,,,
dee0f43fde7,releng,not user facing,Add a CI job to check runner det sync (#129746),.github/workflows/_runner-determinator.yml .github/workflows/runner_determinator_script_sync.yaml,https://github.com/pytorch/pytorch/pull/129746,afrittoli,ZainRizvi,jeanschmidt,zxiiro,
68a4f2a3df3,skip,Untopiced,"Revert ""Tighten torch.library.infer_schema input types (#130705)""",test/custom_operator/test_infer_schema_annotation.py test/test_custom_ops.py torch/_custom_op/impl.py torch/_custom_ops.py torch/_library/custom_ops.py torch/_library/infer_schema.py,,,,,,
705da70f2cc,inductor,not user facing,[inductor][cpp] align dtype convert cache between vec and scalar kernels (#130677),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/130677,jgong5,leslie-fang-intel,,,
67c6941b4e4,inductor,not user facing,Update torch.cat decomp for 0-dim (#130763),test/inductor/test_torchinductor.py torch/_inductor/decomposition.py,https://github.com/pytorch/pytorch/pull/130763,eellison,Skylion007,mlazos,,
213685ba975,skip,not user facing,[torchao][pt2 benchmark runner] Run performance test non-alternately (#130136),benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/130136,xuzhao9,jerryzh168,,,
53e5b8ac5b0,fx,not user facing,[BE]: Update flake8-comprehensions and enable C420 (#130699),.lintrunner.toml test/inductor/test_max_autotune.py test/jit/test_list_dict.py torch/_dynamo/variables/nn_module.py torch/_inductor/dependencies.py torch/distributed/_tools/mem_tracker.py torch/fx/passes/infra/partitioner.py,https://github.com/pytorch/pytorch/pull/130699,Skylion007,ezyang,lezcano,,
69e99172450,skip,not user facing,[inductor] adapte windows file path (#130713),torch/_inductor/codecache.py torch/_inductor/cpp_builder.py torch/_inductor/cpu_vec_isa.py,https://github.com/pytorch/pytorch/pull/130713,xuhancn,desertfire,jansel,jgong5,
1fbfb3202df,jit,not user facing,[docs][TorchScript] document c10::AliasAnalysisKind::CONSERVATIVE (#130765),torch/csrc/jit/OVERVIEW.md,https://github.com/pytorch/pytorch/pull/130765,davidberard98,aaronenyeshi,eellison,nmacchioni,
8390843eba6,skip,Untopiced,Invalidate StorageImpl instances when tensor is overwritten with cudagraphs (#125264),c10/core/StorageImpl.cpp c10/core/StorageImpl.h test/inductor/test_cudagraph_trees.py torch/_C/__init__.pyi.in torch/_inductor/cudagraph_trees.py torch/csrc/cuda/Module.cpp,https://github.com/pytorch/pytorch/pull/125264,isuruf,ezyang,,,
ea25febfab6,skip,not user facing,typing: storage (#130669),torch/storage.py,https://github.com/pytorch/pytorch/pull/130669,aorenste,Skylion007,oulgen,,
4c3348932cb,dynamo,not user facing,typing: convert_frame (#130670),torch/_dynamo/convert_frame.py torch/_dynamo/exc.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/130670,aorenste,Skylion007,,,
e11c41035ca,inductor,not user facing,Directly use empty strided in cudagraph copy (#130777),torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/130777,eellison,BoyuanFeng,,,
aa4ad711ef3,profiler,not user facing,[CCA][Memory Snapshot] Create TraceEntryRingBuffer class for alloc_trace logic (#130741),c10/cuda/CUDACachingAllocator.cpp,https://github.com/pytorch/pytorch/pull/130741,aaronenyeshi,davidberard98,,,
c5093192107,inductor,not user facing,[inductor] Disable remote fx graph cache in test_snode_runtime (#130655),test/inductor/test_snode_runtime.py,https://github.com/pytorch/pytorch/pull/130655,masnesral,oulgen,,,
9cb23ba85b0,skip,Untopiced,"Revert ""Add buffer static input tests to cudagraph trees (#130402)""",test/inductor/test_cudagraph_trees.py,,,,,,
cbda8be537f,skip,Untopiced,"Revert ""Propagate buffer and parameter indices through AOT (#130393)""",test/dynamo/test_subclasses.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/_aot_autograd/subclass_utils.py torch/_functorch/_aot_autograd/traced_function_transforms.py torch/_functorch/aot_autograd.py torch/_inductor/compile_fx.py,,,,,,
2b43d339fe7,nn_frontend,docs,Make FlexAttention API public (#130755),docs/source/nn.attention.flex_attention.rst docs/source/nn.attention.rst test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py torch/_higher_order_ops/flex_attention.py torch/nn/attention/_flex_attention.py torch/nn/attention/flex_attention.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/130755,drisspg,Chillee,,,
f272e0ab4a5,inductor,not user facing,[inductor] support unbacked symint divisors in vars_and_sizes (#130595),test/inductor/test_unbacked_symints.py torch/_inductor/codegen/simd.py,https://github.com/pytorch/pytorch/pull/130595,ColinPeppler,aakhundov,ezyang,,
d2f44eabe73,fx,Untopiced,[Export] Support aten.full.default and aten.full_like.default (#130639),torch/fx/experimental/const_fold.py,https://github.com/pytorch/pytorch/pull/130639,sidt-meta,StellarrZ,,,
e8998d68c89,Uncategorized,Untopiced,[export] add non-strict training IR (#130062),test/export/test_export.py test/export/test_export_training_ir_to_run_decomp.py test/export/testing.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/130062,pianpwk,ydwu4,zhxchen17,,
67e22d6c617,skip,not user facing,[Fix]: Convert operator that does specialization to its symbolic counterpart (#129578),test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/129578,jiashenC,angelayi,,,
1794c359120,onnx,not user facing,[ONNX] Remove beartype usage (#130484),.ci/docker/common/install_onnx.sh test/onnx/dynamo/test_exporter_api.py test/onnx/internal/test_beartype.py test/onnx/onnx_test_common.py test/onnx/test_fx_to_onnx_with_onnxruntime.py torch/onnx/README.md torch/onnx/_internal/_beartype.py torch/onnx/_internal/diagnostics/_diagnostic.py torch/onnx/_internal/diagnostics/infra/decorator.py torch/onnx/_internal/diagnostics/infra/formatter.py torch/onnx/_internal/diagnostics/infra/utils.py torch/onnx/_internal/exporter.py torch/onnx/_internal/fx/_pass.py torch/onnx/_internal/fx/decomposition_table.py torch/onnx/_internal/fx/dynamo_graph_extractor.py torch/onnx/_internal/fx/fx_onnx_interpreter.py torch/onnx/_internal/fx/fx_symbolic_graph_extractor.py torch/onnx/_internal/fx/onnxfunction_dispatcher.py torch/onnx/_internal/fx/op_validation.py torch/onnx/_internal/fx/passes/_utils.py torch/onnx/_internal/fx/passes/decomp.py torch/onnx/_internal/fx/passes/functionalization.py torch/onnx/_internal/fx/passes/modularization.py torch/onnx/_internal/fx/passes/readability.py torch/onnx/_internal/fx/passes/type_promotion.py torch/onnx/_internal/fx/passes/virtualization.py torch/onnx/_internal/fx/patcher.py torch/onnx/_internal/fx/registration.py torch/onnx/_internal/fx/serialization.py torch/onnx/_internal/fx/torch_export_graph_extractor.py torch/onnx/_internal/io_adapter.py torch/onnx/_internal/jit_utils.py torch/onnx/_internal/onnx_proto_utils.py torch/onnx/_internal/onnxruntime.py torch/onnx/_internal/registration.py torch/onnx/_type_utils.py torch/onnx/symbolic_caffe2.py torch/onnx/symbolic_helper.py torch/onnx/symbolic_opset10.py torch/onnx/symbolic_opset11.py torch/onnx/symbolic_opset12.py torch/onnx/symbolic_opset13.py torch/onnx/symbolic_opset14.py torch/onnx/symbolic_opset15.py torch/onnx/symbolic_opset16.py torch/onnx/symbolic_opset17.py torch/onnx/symbolic_opset18.py torch/onnx/symbolic_opset20.py torch/onnx/symbolic_opset8.py torch/onnx/symbolic_opset9.py torch/onnx/utils.py torch/onnx/verification.py,https://github.com/pytorch/pytorch/pull/130484,justinchuby,titaiwangms,,,
1d8baa4df2d,Uncategorized,Untopiced,[torchbench][servicelab] Fix servicelab test failures (#130781),benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/130781,xuzhao9,desertfire,,,
408c921d960,skip,not user facing,Make hashing a SymInt raise an error again (#130548),test/test_dynamic_shapes.py torch/__init__.py torch/_dynamo/variables/builder.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_numpy/_util.py,https://github.com/pytorch/pytorch/pull/130548,ezyang,Skylion007,albanD,lezcano,
09b1b113f51,skip,not user facing,Cache min / max seq len for torch.nested.as_nested_tensor(t) (#130766),test/test_nestedtensor.py torch/nested/__init__.py,https://github.com/pytorch/pytorch/pull/130766,jbschlosser,YuqingJ,soulitzer,,
0851de5b167,skip,Untopiced,"Revert ""[ONNX] Remove beartype usage (#130484)""",.ci/docker/common/install_onnx.sh test/onnx/dynamo/test_exporter_api.py test/onnx/internal/test_beartype.py test/onnx/onnx_test_common.py test/onnx/test_fx_to_onnx_with_onnxruntime.py torch/onnx/README.md torch/onnx/_internal/_beartype.py torch/onnx/_internal/diagnostics/_diagnostic.py torch/onnx/_internal/diagnostics/infra/decorator.py torch/onnx/_internal/diagnostics/infra/formatter.py torch/onnx/_internal/diagnostics/infra/utils.py torch/onnx/_internal/exporter.py torch/onnx/_internal/fx/_pass.py torch/onnx/_internal/fx/decomposition_table.py torch/onnx/_internal/fx/dynamo_graph_extractor.py torch/onnx/_internal/fx/fx_onnx_interpreter.py torch/onnx/_internal/fx/fx_symbolic_graph_extractor.py torch/onnx/_internal/fx/onnxfunction_dispatcher.py torch/onnx/_internal/fx/op_validation.py torch/onnx/_internal/fx/passes/_utils.py torch/onnx/_internal/fx/passes/decomp.py torch/onnx/_internal/fx/passes/functionalization.py torch/onnx/_internal/fx/passes/modularization.py torch/onnx/_internal/fx/passes/readability.py torch/onnx/_internal/fx/passes/type_promotion.py torch/onnx/_internal/fx/passes/virtualization.py torch/onnx/_internal/fx/patcher.py torch/onnx/_internal/fx/registration.py torch/onnx/_internal/fx/serialization.py torch/onnx/_internal/fx/torch_export_graph_extractor.py torch/onnx/_internal/io_adapter.py torch/onnx/_internal/jit_utils.py torch/onnx/_internal/onnx_proto_utils.py torch/onnx/_internal/onnxruntime.py torch/onnx/_internal/registration.py torch/onnx/_type_utils.py torch/onnx/symbolic_caffe2.py torch/onnx/symbolic_helper.py torch/onnx/symbolic_opset10.py torch/onnx/symbolic_opset11.py torch/onnx/symbolic_opset12.py torch/onnx/symbolic_opset13.py torch/onnx/symbolic_opset14.py torch/onnx/symbolic_opset15.py torch/onnx/symbolic_opset16.py torch/onnx/symbolic_opset17.py torch/onnx/symbolic_opset18.py torch/onnx/symbolic_opset20.py torch/onnx/symbolic_opset8.py torch/onnx/symbolic_opset9.py torch/onnx/utils.py torch/onnx/verification.py,,,,,,
d548417d95e,skip,not user facing,[NJT] throw an exception if nested_tensor_from_jagged is fx-traced without being fx.wrapped (#130702),test/test_nestedtensor.py torch/nested/__init__.py,https://github.com/pytorch/pytorch/pull/130702,davidberard98,jbschlosser,soulitzer,,
156b99cfb1c,inductor,not user facing,[inductor] Handle inductor counters in fx graph cache (#130635),test/inductor/test_codecache.py torch/_inductor/codecache.py torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/130635,masnesral,eellison,,,
f2f31027ce8,inductor,Untopiced,[Traceable FSDP2][Inductor] Re-inplace all_gather_into_tensor (#129773),test/distributed/_composable/fsdp/test_fully_shard_compile.py torch/_inductor/comms.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/129773,yf225,eellison,,,
0468f2616a1,distributed,not user facing,[SymmetricMemory] make sure different subgroups with the same name use different store prefixes (#130756),torch/distributed/_symmetric_memory/__init__.py,https://github.com/pytorch/pytorch/pull/130756,yifuwang,Chillee,,,
2ceade37c59,distributed,not user facing,[SymmetricMemory] put socket files in /tmp (#130757),torch/csrc/distributed/c10d/CUDASymmetricMemory.cu,https://github.com/pytorch/pytorch/pull/130757,yifuwang,Chillee,,,
4e479568df3,dynamo,Untopiced,[PT2] Log compile ID in the signpost event (#130801),torch/_dynamo/convert_frame.py,https://github.com/pytorch/pytorch/pull/130801,atuljangra,oulgen,,,
c1e7e40f246,skip,Untopiced,"Revert ""[Traceable FSDP2][Inductor] Re-inplace all_gather_into_tensor (#129773)""",test/distributed/_composable/fsdp/test_fully_shard_compile.py torch/_inductor/comms.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/utils.py,,,,,,
2eec02523b1,autograd_frontend,Untopiced,[autograd] Support GradientEdge as output for torch.autograd.grad (#127766),test/inductor/test_compiled_autograd.py test/profiler/test_profiler_tree.py test/test_autograd.py torch/autograd/__init__.py torch/autograd/graph.py torch/csrc/autograd/init.cpp torch/csrc/autograd/python_cpp_function.cpp torch/csrc/autograd/python_cpp_function.h torch/csrc/autograd/python_engine.cpp torch/csrc/autograd/python_function.cpp,https://github.com/pytorch/pytorch/pull/127766,soulitzer,albanD,,,
5f3c356a56e,skip,Untopiced,"Revert ""[inductor] adapte windows file path (#130713)""",torch/_inductor/codecache.py torch/_inductor/cpp_builder.py torch/_inductor/cpu_vec_isa.py,,,,,,
415d5e53aec,skip,Untopiced,Propagate buffer and parameter indices through AOT (#130393),test/dynamo/test_subclasses.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/_aot_autograd/subclass_utils.py torch/_functorch/_aot_autograd/traced_function_transforms.py torch/_functorch/aot_autograd.py torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/130393,mlazos,bdhirsh,,,
7919f0b9522,skip,not user facing,Add buffer static input tests to cudagraph trees (#130402),test/inductor/test_cudagraph_trees.py,https://github.com/pytorch/pytorch/pull/130402,mlazos,eellison,,,
4f40a7078ef,skip,Untopiced,"Revert ""[FSDP2] Allowed `List[nn.Module]` as arg (#127786)""",test/distributed/_composable/fsdp/test_fully_shard_comm.py test/distributed/_composable/fsdp/test_fully_shard_compile.py test/distributed/_composable/fsdp/test_fully_shard_init.py test/distributed/_composable/fsdp/test_fully_shard_training.py torch/distributed/_composable/fsdp/_fsdp_init.py torch/distributed/_composable/fsdp/_fsdp_param_group.py torch/distributed/_composable/fsdp/_fsdp_state.py torch/distributed/_composable/fsdp/fully_shard.py,,,,,,
de177b50f89,skip,not user facing,[cuDNN][SDPA] Support `attn_bias` in cuDNN (#130482),aten/src/ATen/native/cudnn/MHA.cpp aten/src/ATen/native/cudnn/MHA.h aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/attention_backward.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.h,https://github.com/pytorch/pytorch/pull/130482,eqy,Skylion007,drisspg,malfet,
9a6d81b178f,jit,Untopiced,Fix pytorch JIT build for LLVM 18+ (#130661),torch/csrc/jit/tensorexpr/llvm_jit.cpp,https://github.com/pytorch/pytorch/pull/130661,wlei-llvm,Skylion007,,,
d7a8e8f7c5a,skip,Untopiced,"Revert ""[PT-D] Relaxed `contract` to allow `Sequence[nn.Module]` (#127773)""",test/distributed/_composable/test_contract.py torch/distributed/_composable/contract.py torch/distributed/utils.py,,,,,,
8458dc8966d,skip,Untopiced,"Revert ""Use inductor TestCase for distributed tests (#129494)""",test/distributed/test_c10d_nccl.py torch/testing/_internal/common_distributed.py,,,,,,
25cb4426d32,inductor,not user facing,[inductor] Add num_matches_for_scatter_upon_const_tensor to list of cached metrics (#130843),torch/_inductor/metrics.py,https://github.com/pytorch/pytorch/pull/130843,masnesral,oulgen,,,
ef9d9be236b,distributed,Untopiced,TCPStoreLibUvBackend: log port on error (#130797),test/distributed/test_store.py torch/csrc/distributed/c10d/TCPStoreLibUvBackend.cpp,https://github.com/pytorch/pytorch/pull/130797,d4l3k,kurman,,,
6024fea0f89,inductor,Untopiced,Compute q_num_blocks from kv_num_blocks if q_num_blocks is not passed in (#130809),test/inductor/test_flex_attention.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/130809,Chillee,drisspg,,,
1a97bcf93b2,skip,Untopiced,Renamed mask_fn to mask_mod (#130818),test/inductor/test_flex_attention.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/130818,Chillee,drisspg,,,
03c660468eb,skip,not user facing,Removed q_num_blocks from constructor (#130819),test/inductor/test_flex_attention.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/130819,Chillee,drisspg,,,
e9023d57b0d,rocm,bug fixes,[ROCm] Return correct AMDSMI socket_power metric   (#130331),torch/cuda/__init__.py,https://github.com/pytorch/pytorch/pull/130331,jataylo,eqy,jeffdaily,malfet,
aa95fb99af9,skip,not user facing,"On advice of James March, log pid instead of tid (#130679)",torch/_logging/_internal.py,https://github.com/pytorch/pytorch/pull/130679,ezyang,jmarchfb,,,
c24c50da928,xpu,Untopiced,fix tensor print behavior for XPU (#130523),torch/_tensor_str.py,https://github.com/pytorch/pytorch/pull/130523,guangyey,EikanWang,albanD,gujinghui,
d1c4e6b55f5,linalg_frontend,not user facing,[BE]: Enable a few additional ruff rules (#130700),.flake8 pyproject.toml test/jit/test_tracer.py test/test_linalg.py torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/130700,Skylion007,ezyang,justinchuby,,
fc238db62ad,skip,not user facing,Separate AOTI Eager utils as a single file (#125819),test/inductor/test_torchinductor.py torch/_inductor/aoti_eager.py torch/_inductor/utils.py torch/csrc/inductor/aoti_eager/kernel_holder.cpp,https://github.com/pytorch/pytorch/pull/125819,EikanWang,desertfire,jansel,jgong5,
c6aa03bd4e9,skip,not user facing,Add allow_xpu to enable XPU UTs (#130312),aten/src/ATen/native/mkldnn/xpu/Blas.cpp test/xpu/test_conv.py test/xpu/test_gemm.py,https://github.com/pytorch/pytorch/pull/130312,guangyey,EikanWang,albanD,gujinghui,
f2552dcc3d6,skip,not user facing,refactor cached tensor more generic (#129359),test/test_autograd.py torch/csrc/Module.cpp torch/csrc/cuda/Module.cpp,https://github.com/pytorch/pytorch/pull/129359,guangyey,EikanWang,albanD,eqy,
419b8df0b6f,inductor,not user facing,[inductor][easy] add debug logs for inlining constants (#130799),torch/_inductor/graph.py,https://github.com/pytorch/pytorch/pull/130799,ColinPeppler,chenyang78,,,
7c45476d381,Uncategorized,Untopiced,[pytorch][counters] WaitCounter cleanup (#130664),torch/csrc/monitor/instrumentation.cpp torch/csrc/monitor/instrumentation.h,https://github.com/pytorch/pytorch/pull/130664,andriigrynenko,asiab4,c-p-i-o,,
e51e971a867,skip,not user facing,[inductor] adapte windows file path (#130713),test/functorch/test_eager_transforms.py torch/_inductor/codecache.py torch/_inductor/cpp_builder.py torch/_inductor/cpu_vec_isa.py,https://github.com/pytorch/pytorch/pull/130713,xuhancn,desertfire,jansel,jgong5,
ba48cf65354,skip,not user facing,[BE][Easy][6/19] enforce style for empty lines in import segments in `test/` (#129757),test/conftest.py test/load_torchscript_model.py test/run_test.py test/simulate_nccl_errors.py test/test_ao_sparsity.py test/test_autograd.py test/test_binary_ufuncs.py test/test_bundled_images.py test/test_compile_benchmark_util.py test/test_complex.py test/test_cpp_api_parity.py test/test_cpp_extensions_aot.py test/test_cpp_extensions_jit.py test/test_cpp_extensions_open_device_registration.py test/test_cuda.py test/test_cuda_expandable_segments.py test/test_cuda_multigpu.py test/test_cuda_nvml_based_avail.py test/test_cuda_primary_ctx.py test/test_cuda_trace.py test/test_custom_ops.py test/test_decomp.py test/test_deploy.py test/test_dispatch.py test/test_dynamic_shapes.py test/test_foreach.py test/test_indexing.py test/test_jit.py test/test_jit_fuser_te.py test/test_model_exports_to_core_aten.py test/test_nestedtensor.py test/test_numba_integration.py test/test_numpy_interop.py test/test_ops.py test/test_ops_fwd_gradients.py test/test_ops_gradients.py test/test_ops_jit.py test/test_optim.py test/test_python_dispatch.py test/test_pytree.py test/test_shape_ops.py test/test_show_pickle.py test/test_sort_and_select.py test/test_throughput_benchmark.py test/test_type_hints.py test/test_type_info.py test/test_typing.py test/test_utils.py test/test_view_ops.py test/test_weak.py test/test_xnnpack_integration.py test/test_xpu.py tools/linter/adapters/ufmt_linter.py,https://github.com/pytorch/pytorch/pull/129757,XuehaiPan,ezyang,,,
f6838d521a2,autograd_frontend,not user facing,[BE][Easy][5/19] enforce style for empty lines in import segments in `tools/` and `torchgen/` (#129756),tools/amd_build/build_amd.py tools/autograd/gen_inplace_or_view_type.py tools/build_libtorch.py tools/linter/adapters/s3_init.py tools/linter/adapters/ufmt_linter.py tools/setup_helpers/gen.py tools/setup_helpers/gen_unboxing.py tools/setup_helpers/generate_code.py tools/stats/export_test_times.py tools/stats/import_test_stats.py tools/stats/upload_stats_lib.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_gen_backend_stubs.py tools/test/test_test_run.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/testing/discover_tests.py tools/testing/do_target_determination_for_s3.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/gen_artifact.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/previously_failed_in_pr.py tools/testing/test_selections.py tools/vscode_settings.py torchgen/api/types/__init__.py torchgen/autoheuristic/gen_data_pad_mm.py torchgen/autoheuristic/train.py torchgen/executorch/api/custom_ops.py torchgen/executorch/api/types/__init__.py,https://github.com/pytorch/pytorch/pull/129756,XuehaiPan,ezyang,,,
bb62e9d7c3e,skip,not user facing,Avoid autocast deprecation warning in DataParallel (#130660),torch/nn/parallel/parallel_apply.py,https://github.com/pytorch/pytorch/pull/130660,awaelchli,albanD,fegin,guangyey,
8c9a9960917,dynamo,not user facing,"[3.13, dynamo] support LOAD_FAST_LOAD_FAST and STORE_FAST_STORE_FAST (#130459)",torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/130459,williamwen42,jansel,,,
82b2e7a253a,dynamo,not user facing,"[3.13, dynamo] fix CALL_FUNCTION_EX in symbolic_convert (#130460)",torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/130460,williamwen42,jansel,,,
e2365c05d79,dynamo,not user facing,"[3.13, dynamo] fix instruction line numbers (#130461)",torch/_dynamo/bytecode_transformation.py,https://github.com/pytorch/pytorch/pull/130461,williamwen42,jansel,,,
539acf7656a,dynamo,not user facing,"[3.13, dynamo] support CALL_KW (#130564)",torch/_dynamo/codegen.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/130564,williamwen42,jansel,,,
2300bb2a880,dynamo,not user facing,"[3.13, dynamo] support TO_BOOL (#130565)",torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/130565,williamwen42,jansel,,,
2af2d26562d,skip,not user facing,[Inductor UT] Generalize device-bias code in test_triton_kernels.py and test_torchinductor.py (#130817),test/inductor/test_torchinductor.py test/inductor/test_triton_kernels.py,https://github.com/pytorch/pytorch/pull/130817,etaf,zou3519,,,
d3a11a01980,inductor,not user facing,[Inductor] Handle device_put op in constant folding. (#130824),torch/_inductor/fx_passes/joint_graph.py,https://github.com/pytorch/pytorch/pull/130824,etaf,EikanWang,eellison,,
e1b2d8f975b,skip,Untopiced,"Revert ""[cuDNN][SDPA] Support `attn_bias` in cuDNN (#130482)""",aten/src/ATen/native/cudnn/MHA.cpp aten/src/ATen/native/cudnn/MHA.h aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/attention_backward.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.h,,,,,,
4b7ff356221,skip,not user facing,Fix flex_attention import in score_mod (#130906),benchmarks/transformer/score_mod.py,https://github.com/pytorch/pytorch/pull/130906,AlnisM,Chillee,,,
d027aef8f86,skip,Untopiced,"Revert ""Removed q_num_blocks from constructor (#130819)""",test/inductor/test_flex_attention.py torch/nn/attention/flex_attention.py,,,,,,
94a910b43b5,skip,Untopiced,"Revert ""Renamed mask_fn to mask_mod (#130818)""",test/inductor/test_flex_attention.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/flex_attention.py,,,,,,
cbf274d4a75,inductor,Untopiced,[aoti] Add packaging solution (#129895),test/inductor/test_aot_inductor_package.py torch/_inductor/codecache.py torch/_inductor/config.py torch/_inductor/cpp_builder.py torch/_inductor/package/__init__.py torch/_inductor/package/build_package.py torch/_inductor/package/package.py torch/_inductor/package/pt2_archive_constants.py,https://github.com/pytorch/pytorch/pull/129895,angelayi,malfet,,,
76169cf6918,fx,not user facing,[BE][Easy][9/19] enforce style for empty lines in import segments in `test/[e-h]*/` (#129760),test/export/opinfo_schema.py test/export/test_converter.py test/export/test_experimental.py test/export/test_export.py test/export/test_export_nonstrict.py test/export/test_export_training_ir_to_run_decomp.py test/export/test_hop.py test/export/test_lift_unlift.py test/export/test_pass_infra.py test/export/test_passes.py test/export/test_retraceability.py test/export/test_schema.py test/export/test_serdes.py test/export/test_sparse.py test/export/test_tools.py test/export/test_torchbind.py test/export/test_tree_utils.py test/export/test_unflatten.py test/export/test_verifier.py test/export/testing.py test/functorch/attn_ft.py test/functorch/common_utils.py test/functorch/discover_coverage.py test/functorch/functorch_additional_op_db.py test/functorch/test_aotdispatch.py test/functorch/test_control_flow.py test/functorch/test_dims.py test/functorch/test_eager_transforms.py test/functorch/test_memory_efficient_fusion.py test/functorch/test_minifier.py test/functorch/test_ops.py test/functorch/test_parsing.py test/functorch/test_rearrange.py test/functorch/test_vmap.py test/functorch/test_vmap_registrations.py test/functorch/xfail_suggester.py test/fx/quantization.py test/fx/test_common_passes.py test/fx/test_cse_pass.py test/fx/test_dce_pass.py test/fx/test_fx_param_shape_control_flow.py test/fx/test_fx_split.py test/fx/test_fx_xform_observer.py test/fx/test_matcher_utils.py test/fx/test_partitioner_order.py test/fx/test_pass_infra.py test/fx/test_source_matcher_utils.py test/fx/test_subgraph_rewriter.py test/higher_order_ops/test_with_effects.py tools/linter/adapters/ufmt_linter.py,https://github.com/pytorch/pytorch/pull/129760,XuehaiPan,ezyang,,,
32995dec281,skip,not user facing,Add support for XPU accumulate type (#128579),aten/src/ATen/AccumulateType.h,https://github.com/pytorch/pytorch/pull/128579,xytintel,EikanWang,albanD,,
c0ed38e644a,distributed,not user facing,[BE][Easy][3/19] enforce style for empty lines in import segments in `benchmarks/` (#129754),benchmarks/compare-fastrnn-results.py benchmarks/distributed/rpc/parameter_server/data/__init__.py benchmarks/distributed/rpc/parameter_server/models/__init__.py benchmarks/distributed/rpc/parameter_server/server/__init__.py benchmarks/distributed/rpc/parameter_server/trainer/__init__.py benchmarks/distributed/rpc/parameter_server/utils.py benchmarks/distributed/rpc/rl/coordinator.py benchmarks/distributed/rpc/rl/launcher.py benchmarks/dynamo/benchmarks.py benchmarks/dynamo/ci_expected_accuracy/update_expected.py benchmarks/dynamo/combine_csv.py benchmarks/dynamo/common.py benchmarks/dynamo/dist_util.py benchmarks/dynamo/distributed.py benchmarks/dynamo/huggingface.py benchmarks/dynamo/microbenchmarks/bench_mm_fusion.py benchmarks/dynamo/microbenchmarks/fx_microbenchmarks.py benchmarks/dynamo/microbenchmarks/inductor_bmm.py benchmarks/dynamo/microbenchmarks/inductor_mm.py benchmarks/dynamo/microbenchmarks/matmul_relu.py benchmarks/dynamo/microbenchmarks/microbench.py benchmarks/dynamo/microbenchmarks/operator_inp_utils.py benchmarks/dynamo/microbenchmarks/operatorbench.py benchmarks/dynamo/parse_logs.py benchmarks/dynamo/runner.py benchmarks/dynamo/test.py benchmarks/dynamo/timm_models.py benchmarks/dynamo/torchbench.py benchmarks/dynamo/training_loss.py benchmarks/fastrnns/__init__.py benchmarks/fastrnns/conftest.py benchmarks/fastrnns/custom_lstms.py benchmarks/fastrnns/scratch.py benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py benchmarks/framework_overhead_benchmark/utils.py benchmarks/functional_autograd_benchmark/audio_text_models.py benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py benchmarks/functional_autograd_benchmark/torchaudio_models.py benchmarks/functional_autograd_benchmark/torchvision_models.py benchmarks/functional_autograd_benchmark/utils.py benchmarks/functional_autograd_benchmark/vision_models.py benchmarks/fuser/plot_speedups.py benchmarks/fuser/run_benchmarks.py benchmarks/gpt_fast/benchmark.py benchmarks/gpt_fast/generate.py benchmarks/gpt_fast/mixtral_moe_quantize.py benchmarks/inference/process_metrics.py benchmarks/inference/server.py benchmarks/instruction_counts/core/api.py benchmarks/instruction_counts/core/expand.py benchmarks/instruction_counts/execution/work.py benchmarks/operator_benchmark/benchmark_all_other_test.py benchmarks/operator_benchmark/benchmark_all_test.py benchmarks/operator_benchmark/benchmark_core.py benchmarks/operator_benchmark/benchmark_runner.py benchmarks/operator_benchmark/common/repeat_benchmark.py benchmarks/operator_benchmark/common/tests/jit_forward_test.py benchmarks/operator_benchmark/common/tests/pt_configs_list_test.py benchmarks/operator_benchmark/operator_benchmark.py benchmarks/operator_benchmark/pt/add_test.py benchmarks/operator_benchmark/pt/ao_sparsifier_test.py benchmarks/operator_benchmark/pt/batchnorm_test.py benchmarks/operator_benchmark/pt/binary_test.py benchmarks/operator_benchmark/pt/bmm_test.py benchmarks/operator_benchmark/pt/channel_shuffle_test.py benchmarks/operator_benchmark/pt/chunk_test.py benchmarks/operator_benchmark/pt/clip_ranges_test.py benchmarks/operator_benchmark/pt/configs.py benchmarks/operator_benchmark/pt/conv_test.py benchmarks/operator_benchmark/pt/diag_test.py benchmarks/operator_benchmark/pt/embeddingbag_test.py benchmarks/operator_benchmark/pt/fill_test.py benchmarks/operator_benchmark/pt/gather_test.py benchmarks/operator_benchmark/pt/gelu_test.py benchmarks/operator_benchmark/pt/groupnorm_test.py benchmarks/operator_benchmark/pt/hardsigmoid_test.py benchmarks/operator_benchmark/pt/hardswish_test.py benchmarks/operator_benchmark/pt/index_select_test.py benchmarks/operator_benchmark/pt/instancenorm_test.py benchmarks/operator_benchmark/pt/interpolate_test.py benchmarks/operator_benchmark/pt/layernorm_test.py benchmarks/operator_benchmark/pt/linear_prepack_fp16_test.py benchmarks/operator_benchmark/pt/linear_test.py benchmarks/operator_benchmark/pt/linear_unpack_fp16_test.py benchmarks/operator_benchmark/pt/matmul_test.py benchmarks/operator_benchmark/pt/matrix_mult_test.py benchmarks/operator_benchmark/pt/pool_test.py benchmarks/operator_benchmark/pt/qactivation_test.py benchmarks/operator_benchmark/pt/qarithmetic_test.py benchmarks/operator_benchmark/pt/qatembedding_ops_test.py benchmarks/operator_benchmark/pt/qbatchnorm_test.py benchmarks/operator_benchmark/pt/qcomparators_test.py benchmarks/operator_benchmark/pt/qconv_test.py benchmarks/operator_benchmark/pt/qembedding_bag_lookups_test.py benchmarks/operator_benchmark/pt/qembedding_pack_test.py benchmarks/operator_benchmark/pt/qembeddingbag_test.py benchmarks/operator_benchmark/pt/qgroupnorm_test.py benchmarks/operator_benchmark/pt/qinstancenorm_test.py benchmarks/operator_benchmark/pt/qinterpolate_test.py benchmarks/operator_benchmark/pt/qlayernorm_test.py benchmarks/operator_benchmark/pt/qlinear_test.py benchmarks/operator_benchmark/pt/qobserver_test.py benchmarks/operator_benchmark/pt/qpool_test.py benchmarks/operator_benchmark/pt/qrnn_test.py benchmarks/operator_benchmark/pt/qtensor_method_test.py benchmarks/operator_benchmark/pt/quantization_test.py benchmarks/operator_benchmark/pt/qunary_test.py benchmarks/operator_benchmark/pt/remainder_test.py benchmarks/operator_benchmark/pt/softmax_test.py benchmarks/operator_benchmark/pt/split_test.py benchmarks/operator_benchmark/pt/sum_test.py benchmarks/operator_benchmark/pt/tensor_to_test.py benchmarks/operator_benchmark/pt/unary_test.py benchmarks/operator_benchmark/pt_extension/setup.py benchmarks/overrides_benchmark/bench.py benchmarks/overrides_benchmark/common.py benchmarks/overrides_benchmark/pyspybench.py benchmarks/profiler_benchmark/profiler_bench.py benchmarks/profiler_benchmark/resnet_memory_profiler.py benchmarks/serialization/nested_annotation_str.py benchmarks/serialization/simple_measurement.py benchmarks/tensorexpr/__main__.py benchmarks/transformer/better_transformer_vs_mha_functional.py benchmarks/transformer/score_mod.py benchmarks/transformer/sdp.py tools/linter/adapters/ufmt_linter.py,https://github.com/pytorch/pytorch/pull/129754,XuehaiPan,ezyang,,,
b29b23137cc,composability,not user facing,[Easy] Fix argument name collision in dispatched functions (#129562),torch/_meta_registrations.py torch/_ops.py torch/testing/_internal/distributed/distributed_test.py,https://github.com/pytorch/pytorch/pull/129562,XuehaiPan,fegin,zou3519,,
b7d2abd766c,inductor,Untopiced,Fix vectorized ops.masked (#130130),test/inductor/test_torchinductor.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/130130,isuruf,jgong5,lezcano,,
1fb572289bf,distributed,Untopiced,[BE][c10d] Add a warning messages in the comment about cuda hang (#130844),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/130844,fduwjj,wconstab,,,
6e916f112fc,inductor,not user facing,[inductor] skip fx remote cache for 2 tests in test_metrics.py (#130853),test/inductor/test_metrics.py,https://github.com/pytorch/pytorch/pull/130853,masnesral,oulgen,,,
af0b5ee924e,python_frontend,not user facing,"Reduce number of samples in {svd,pca}_lowrank OpInfos (#127199)",test/functorch/test_ops.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/127199,lezcano,peterbell10,zou3519,,
1e13cb2f28c,inductor,not user facing,Log cache state to structured logs (#130845),test/dynamo/test_structured_trace.py test/functorch/test_aotdispatch.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/130845,oulgen,jamesjwu,,,
db3290846e4,distributed,not user facing,[BE][Easy][10/19] enforce style for empty lines in import segments in `test/d*/` (#129761),test/distributed/_composable/fsdp/test_fully_shard_autograd.py test/distributed/_composable/fsdp/test_fully_shard_comm.py test/distributed/_composable/fsdp/test_fully_shard_compile.py test/distributed/_composable/fsdp/test_fully_shard_extensions.py test/distributed/_composable/fsdp/test_fully_shard_frozen.py test/distributed/_composable/fsdp/test_fully_shard_logging.py test/distributed/_composable/fsdp/test_fully_shard_memory.py test/distributed/_composable/fsdp/test_fully_shard_mixed_precision.py test/distributed/_composable/fsdp/test_fully_shard_overlap.py test/distributed/_composable/fsdp/test_fully_shard_state_dict.py test/distributed/_composable/fsdp/test_fully_shard_training.py test/distributed/_composable/fully_shard/test_fully_shard_compile.py test/distributed/_composable/fully_shard/test_fully_shard_init.py test/distributed/_composable/fully_shard/test_fully_shard_model_checkpoint.py test/distributed/_composable/fully_shard/test_fully_shard_optim_checkpoint.py test/distributed/_composable/fully_shard/test_fully_shard_runtime.py test/distributed/_composable/fully_shard/test_fully_shard_util.py test/distributed/_composable/test_compose.py test/distributed/_shard/sharded_optim/test_sharded_optim.py test/distributed/_shard/sharded_tensor/ops/test_binary_cmp.py test/distributed/_shard/sharded_tensor/ops/test_embedding.py test/distributed/_shard/sharded_tensor/ops/test_embedding_bag.py test/distributed/_shard/sharded_tensor/ops/test_init.py test/distributed/_shard/sharded_tensor/ops/test_tensor_ops.py test/distributed/_shard/sharded_tensor/test_sharded_tensor.py test/distributed/_shard/sharded_tensor/test_sharded_tensor_reshard.py test/distributed/_shard/sharding_plan/test_sharding_plan.py test/distributed/_shard/test_sharder.py test/distributed/_spmd/test_data_parallel.py test/distributed/_tensor/debug/test_comm_mode.py test/distributed/_tensor/debug/test_op_coverage.py test/distributed/_tensor/test_common_rules.py test/distributed/_tensor/test_convolution_ops.py test/distributed/_tensor/test_dtensor.py test/distributed/_tensor/test_dtensor_compile.py test/distributed/_tensor/test_dtensor_ops.py test/distributed/_tensor/test_embedding_ops.py test/distributed/_tensor/test_experimental_ops.py test/distributed/_tensor/test_math_ops.py test/distributed/_tensor/test_op_strategy.py test/distributed/_tensor/test_optimizers.py test/distributed/_tensor/test_pointwise_ops.py test/distributed/_tensor/test_random_ops.py test/distributed/_tensor/test_redistribute.py test/distributed/_tensor/test_utils.py test/distributed/_tools/test_fsdp2_mem_tracker.py test/distributed/_tools/test_memory_tracker.py test/distributed/algorithms/ddp_comm_hooks/test_ddp_hooks.py test/distributed/algorithms/quantization/test_quantization.py test/distributed/algorithms/test_join.py test/distributed/checkpoint/e2e/test_e2e_save_and_load.py test/distributed/checkpoint/fsdp/test_fsdp_dsd.py test/distributed/checkpoint/test_checkpoint.py test/distributed/checkpoint/test_file_system_checkpoint.py test/distributed/checkpoint/test_file_system_checkpoint_cpu.py test/distributed/checkpoint/test_format_utils.py test/distributed/checkpoint/test_fsdp_model_state.py test/distributed/checkpoint/test_fsdp_optim_state.py test/distributed/checkpoint/test_fsdp_tp_checkpoint_conversion.py test/distributed/checkpoint/test_hsdp_checkpoint.py test/distributed/checkpoint/test_planner.py test/distributed/checkpoint/test_state_dict_utils.py test/distributed/checkpoint/test_tp_checkpoint.py test/distributed/checkpoint/test_traverse.py test/distributed/checkpoint/test_utils.py test/distributed/elastic/agent/server/test/api_test.py test/distributed/elastic/rendezvous/c10d_rendezvous_backend_test.py test/distributed/elastic/rendezvous/dynamic_rendezvous_test.py test/distributed/elastic/rendezvous/etcd_rendezvous_backend_test.py test/distributed/elastic/rendezvous/etcd_rendezvous_test.py test/distributed/elastic/rendezvous/etcd_server_test.py test/distributed/elastic/utils/logging_test.py test/distributed/elastic/utils/util_test.py test/distributed/fsdp/test_checkpoint_wrapper.py test/distributed/fsdp/test_distributed_checkpoint.py test/distributed/fsdp/test_fsdp_apply.py test/distributed/fsdp/test_fsdp_checkpoint.py test/distributed/fsdp/test_fsdp_clip_grad_norm.py test/distributed/fsdp/test_fsdp_comm.py test/distributed/fsdp/test_fsdp_comm_hooks.py test/distributed/fsdp/test_fsdp_core.py test/distributed/fsdp/test_fsdp_dtensor_state_dict.py test/distributed/fsdp/test_fsdp_exec_order.py test/distributed/fsdp/test_fsdp_fine_tune.py test/distributed/fsdp/test_fsdp_flatten_params.py test/distributed/fsdp/test_fsdp_freezing_weights.py test/distributed/fsdp/test_fsdp_grad_acc.py test/distributed/fsdp/test_fsdp_hybrid_shard.py test/distributed/fsdp/test_fsdp_ignored_modules.py test/distributed/fsdp/test_fsdp_input.py test/distributed/fsdp/test_fsdp_memory.py test/distributed/fsdp/test_fsdp_meta.py test/distributed/fsdp/test_fsdp_misc.py test/distributed/fsdp/test_fsdp_mixed_precision.py test/distributed/fsdp/test_fsdp_multiple_forward.py test/distributed/fsdp/test_fsdp_multiple_wrapping.py test/distributed/fsdp/test_fsdp_optim_state.py test/distributed/fsdp/test_fsdp_overlap.py test/distributed/fsdp/test_fsdp_pure_fp16.py test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py test/distributed/fsdp/test_fsdp_state_dict.py test/distributed/fsdp/test_fsdp_tp_integration.py test/distributed/fsdp/test_fsdp_traversal.py test/distributed/fsdp/test_fsdp_uneven.py test/distributed/fsdp/test_fsdp_unshard_params.py test/distributed/fsdp/test_fsdp_use_orig_params.py test/distributed/fsdp/test_hsdp_dtensor_state_dict.py test/distributed/fsdp/test_shard_utils.py test/distributed/fsdp/test_utils.py test/distributed/optim/test_apply_optimizer_in_backward.py test/distributed/optim/test_named_optimizer.py test/distributed/optim/test_zero_redundancy_optimizer.py test/distributed/pipelining/schedule_registry.py test/distributed/pipelining/test_composability.py test/distributed/pipelining/test_schedule.py test/distributed/pipelining/test_schedule_multiproc.py test/distributed/pipelining/test_stage.py test/distributed/rpc/cuda/test_tensorpipe_agent.py test/distributed/rpc/test_faulty_agent.py test/distributed/rpc/test_share_memory.py test/distributed/rpc/test_tensorpipe_agent.py test/distributed/tensor/parallel/test_ddp_2d_parallel.py test/distributed/tensor/parallel/test_fsdp_2d_parallel.py test/distributed/tensor/parallel/test_tp_random_state.py test/distributed/tensor/parallel/test_tp_style.py test/distributed/test_c10d_common.py test/distributed/test_c10d_functional_native.py test/distributed/test_c10d_gloo.py test/distributed/test_c10d_logger.py test/distributed/test_c10d_nccl.py test/distributed/test_c10d_object_collectives.py test/distributed/test_c10d_ops_nccl.py test/distributed/test_c10d_spawn.py test/distributed/test_c10d_spawn_gloo.py test/distributed/test_c10d_spawn_nccl.py test/distributed/test_c10d_spawn_ucc.py test/distributed/test_c10d_ucc.py test/distributed/test_data_parallel.py test/distributed/test_device_mesh.py test/distributed/test_distributed_spawn.py test/distributed/test_fake_pg.py test/distributed/test_functional_api.py test/distributed/test_launcher.py test/distributed/test_multi_threaded_pg.py test/distributed/test_nccl.py test/distributed/test_store.py test/distributed/test_symmetric_memory.py test/distributions/test_distributions.py tools/linter/adapters/ufmt_linter.py,https://github.com/pytorch/pytorch/pull/129761,XuehaiPan,fegin,,,
d552e5c3d55,releng,not user facing,Fix ciflow/nightly triggering commit hash update workflow (#130570),.github/workflows/nightly.yml,https://github.com/pytorch/pytorch/pull/130570,clee2000,ZainRizvi,,,
ef0511245a9,skip,Untopiced,"[ts-migration] Support RaiseException, prim::Unitialized, prim::Enter, and prim::Exit (#129416)",test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/129416,BoyuanFeng,angelayi,,,
d96c80649f3,Uncategorized,Untopiced,[export] constants & non-persistent buffers for training IR (#130864),test/export/test_export.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/130864,pianpwk,angelayi,,,
480a5bd8819,skip,Untopiced,Renamed mask_fn to mask_mod (#130818),test/inductor/test_flex_attention.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/130818,Chillee,drisspg,,,
efefea52e0b,inductor,Untopiced,renamed inductor kernel args in flexattention properly (#130869),torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/130869,Chillee,drisspg,joydddd,,
752c8178982,inductor,not user facing,[AOTI][refactor] Unify UserDefinedTritonKernel.codegen (#130796),torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cuda.py torch/_inductor/codegen/wrapper.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/130796,desertfire,oulgen,,,
bea6762c013,distributed,Untopiced,Add guards on subclass metadata (#130779),test/distributed/_tensor/test_dtensor_compile.py test/dynamo/test_subclasses.py torch/_dynamo/guards.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/130779,mlazos,anijain2305,bdhirsh,,
470f07c8401,dynamo,Untopiced,Add guard override capability for tensor subclass metadata (#130780),test/dynamo/test_subclasses.py torch/_dynamo/guards.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/130780,mlazos,anijain2305,bdhirsh,,
e4f9d01cd9a,skip,not user facing,Add test for dataclass field accesses (#130848),test/dynamo/test_repros.py,https://github.com/pytorch/pytorch/pull/130848,mlazos,anijain2305,williamwen42,,
b0387449db4,skip,Untopiced,Ensure staticmethods can be allowed in graph (#130882),test/dynamo/test_repros.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/130882,mlazos,anijain2305,williamwen42,,
1bf4a44b335,skip,Untopiced,"Revert ""[ts-migration] Support RaiseException, prim::Unitialized, prim::Enter, and prim::Exit (#129416)""",test/export/test_converter.py torch/_export/converter.py,,,,,,
41f5d5dcafb,skip,Untopiced,"Revert ""[inductor] adapte windows file path (#130713)""",test/functorch/test_eager_transforms.py torch/_inductor/codecache.py torch/_inductor/cpp_builder.py torch/_inductor/cpu_vec_isa.py,,,,,,
882fd918692,skip,Untopiced,Relax constraints for creating a `GenericContextWrappingVariable` (#129091),test/dynamo/test_ctx_manager.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/129091,guilhermeleobas,yanboliang,zou3519,,
a8bd2933d9e,skip,not user facing,wrap self.call_function(...) in try finally block to undo changes to self.kw_names (#130490),test/dynamo/test_exceptions.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/130490,guilhermeleobas,williamwen42,zou3519,,
65b4163bd22,dynamo,not user facing,[dynamo][nn-module] Make slice getitem on nn module container sourceless (#130852),torch/_dynamo/variables/builder.py torch/_dynamo/variables/nn_module.py,https://github.com/pytorch/pytorch/pull/130852,anijain2305,mlazos,,,
c49f909aab0,skip,Untopiced,"Revert ""wrap self.call_function(...) in try finally block to undo changes to self.kw_names (#130490)""",test/dynamo/test_exceptions.py torch/_dynamo/symbolic_convert.py,,,,,,
0b134c15cdf,skip,Untopiced,"Revert ""Relax constraints for creating a `GenericContextWrappingVariable` (#129091)""",test/dynamo/test_ctx_manager.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/user_defined.py,,,,,,
4112f687831,skip,not user facing,Define key in codecache (#130979),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/130979,oulgen,jamesjwu,,,
f0faecd2915,skip,Untopiced,"[ts-migration] Support RaiseException, prim::Unitialized, prim::Enter, and prim::Exit (#129416)",test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/129416,BoyuanFeng,angelayi,,,
df5919393c9,skip,Untopiced,[ROCm] std::clamp work-around for hip-clang compiler (#127812),aten/src/ATen/native/cuda/IndexKernel.cu,https://github.com/pytorch/pytorch/pull/127812,jeffdaily,hongxiayang,malfet,,
ebdfc7e37da,skip,not user facing,[BE] Rename `ISORT_WHITELIST` to `ISORT_SKIPLIST` (#130987),tools/linter/adapters/ufmt_linter.py,https://github.com/pytorch/pytorch/pull/130987,malfet,ZainRizvi,seemethere,,
0eb43ed1895,skip,Untopiced,"Revert ""[ts-migration] Support RaiseException, prim::Unitialized, prim::Enter, and prim::Exit (#129416)""",test/export/test_converter.py torch/_export/converter.py,,,,,,
43a6d208834,composability,Untopiced,"Add decomposition for reflection_pad{1,2,3}d_backward (#130299)",test/expect/HasDecompTest.test_has_decomposition.expect test/test_decomp.py torch/_decomp/__init__.py torch/_decomp/decompositions.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/130299,isuruf,lezcano,,,
874bbc53c9d,skip,Untopiced,"Revert ""Define key in codecache (#130979)""",torch/_inductor/codecache.py,,,,,,
90105a4f3e7,skip,Untopiced,"[ts-migration] Support RaiseException, prim::Unitialized, prim::Enter, and prim::Exit (#129416)",test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/129416,BoyuanFeng,angelayi,,,
ff7e021e949,distributed,Untopiced,[Reland][PT-D] Relaxed `contract` to allow `Sequence[nn.Module]` (#127773) (#130947),test/distributed/_composable/test_contract.py torch/distributed/_composable/contract.py torch/distributed/utils.py,https://github.com/pytorch/pytorch/pull/130947,awgu,atalman,weifengpy,,
31e33300407,distributed,Untopiced,[Reland][FSDP2] Allowed `List[nn.Module]` as arg (#130949),test/distributed/_composable/fsdp/test_fully_shard_comm.py test/distributed/_composable/fsdp/test_fully_shard_compile.py test/distributed/_composable/fsdp/test_fully_shard_init.py test/distributed/_composable/fsdp/test_fully_shard_training.py torch/distributed/_composable/fsdp/_fsdp_init.py torch/distributed/_composable/fsdp/_fsdp_param_group.py torch/distributed/_composable/fsdp/_fsdp_state.py torch/distributed/_composable/fsdp/fully_shard.py,https://github.com/pytorch/pytorch/pull/130949,awgu,weifengpy,,,
a0da1265c5e,skip,not user facing,Define key in codecache (#130979),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/130979,oulgen,jamesjwu,,,
442bfa7fc4d,inductor,not user facing,Fix mypy error (#130992),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/130992,oulgen,izaitsevfb,,,
fc3dbcd1c3b,inductor,Untopiced,[Traceable FSDP2][Inductor] Re-inplace all_gather_into_tensor (#129773),test/distributed/_composable/fsdp/test_fully_shard_compile.py torch/_inductor/comms.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/ir.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/129773,yf225,eellison,,,
d77af49380d,distributed,Untopiced,[Traceable FSDP2] Preserve fsdp.set_ op through lowering; Add unit test for multiple .set_ into same primal; Add unit test for FSDP2 module layer reuse (#130786),test/distributed/_composable/fsdp/test_fully_shard_compile.py test/functorch/test_aotdispatch.py torch/_functorch/_aot_autograd/functional_utils.py torch/_inductor/lowering.py torch/distributed/_composable/fsdp/_fsdp_param.py,https://github.com/pytorch/pytorch/pull/130786,yf225,bdhirsh,,,
e14d1d10ef9,inductor,not user facing,Unwrap Identity in prepare indexing (#130967),torch/_inductor/codegen/simd.py,https://github.com/pytorch/pytorch/pull/130967,eellison,Chillee,jansel,,
73d0f484b3f,mps,not user facing,[structural binding][11/N] Replace std::tie with structural binding (#130830),aten/src/ATen/native/cuda/Indexing.cu test/cpp/api/modules.cpp test/cpp/jit/test_autodiff.cpp test/cpp/jit/test_misc.cpp test/cpp/tensorexpr/test_external_calls.cpp torch/csrc/jit/passes/onnx/shape_type_inference.cpp,https://github.com/pytorch/pytorch/pull/130830,cyyever,janeyx99,,,
e22b0acc766,skip,Untopiced,"[FX][export] DCE pass, check schema for node impurity (#130395)",test/fx/test_dce_pass.py test/quantization/pt2e/test_x86inductor_quantizer.py torch/export/_remove_effect_tokens_pass.py torch/export/_unlift.py torch/fx/node.py,https://github.com/pytorch/pytorch/pull/130395,yushangdi,angelayi,jgong5,,
051971ab32a,skip,not user facing,Reorder MIOpen conditions so getCUDAHooks only called when CUDA input (#130867),aten/src/ATen/native/Convolution.cpp,https://github.com/pytorch/pytorch/pull/130867,ezyang,albanD,,,
d818c3319f9,inductor,not user facing,Autoheuristic: add config options for specifying optimizations to collect data for and use heuristics (#130245),test/inductor/test_autoheuristic.py torch/_inductor/autoheuristic/autoheuristic.py torch/_inductor/config.py torch/_inductor/fx_passes/pad_mm.py torchgen/autoheuristic/gen_data_pad_mm.py,https://github.com/pytorch/pytorch/pull/130245,AlnisM,shunting314,,,
a0ae77b25b4,cuda,not user facing,Simpilfy cub::unique_by_key code (#130907),aten/src/ATen/cuda/cub.cuh aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu,https://github.com/pytorch/pytorch/pull/130907,cyyever,ezyang,,,
9fee87e4cd9,skip,Untopiced,[export] Add print_readable to unflattener (#128617),test/dynamo/test_autograd_function.py test/dynamo/test_higher_order_ops.py test/dynamo/test_subclasses.py torch/export/unflatten.py torch/fx/graph_module.py,https://github.com/pytorch/pytorch/pull/128617,angelayi,pianpwk,zhxchen17,,
bd56bcf0aba,skip,not user facing,[TEST] Fix _scaled_mm tests (#130897),test/test_meta.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/130897,Aidyn-A,drisspg,,,
433ef4e4443,skip,Untopiced,"Revert ""[FX][export] DCE pass, check schema for node impurity (#130395)""",test/fx/test_dce_pass.py test/quantization/pt2e/test_x86inductor_quantizer.py torch/export/_remove_effect_tokens_pass.py torch/export/_unlift.py torch/fx/node.py,,,,,,
9f392f8294e,skip,Untopiced,Use inductor TestCase for test_replicate_with_compiler.py (#129494),test/distributed/_composable/test_replicate_with_compiler.py torch/testing/_internal/common_distributed.py,https://github.com/pytorch/pytorch/pull/129494,masnesral,eellison,,,
a085acd7d68,dynamo,not user facing,[dynamo] Revert back changes to UnspecializedBuiltinNNModuleVariable (#130991),torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/130991,anijain2305,mlazos,williamwen42,,
740fb229660,skip,not user facing,[BE][Easy][4/19] enforce style for empty lines in import segments in `functorch/` (#129755),functorch/__init__.py functorch/benchmarks/chrome_trace_parser.py functorch/benchmarks/cse.py functorch/benchmarks/operator_authoring.py functorch/benchmarks/per_sample_grads.py functorch/benchmarks/pointwise_scorecard.py functorch/benchmarks/process_scorecard.py functorch/dim/__init__.py functorch/dim/batch_tensor.py functorch/dim/dim.py functorch/dim/op_properties.py functorch/dim/reference.py functorch/dim/tree_map.py functorch/dim/wrap_type.py functorch/docs/source/conf.py functorch/einops/__init__.py functorch/einops/_parsing.py functorch/einops/rearrange.py functorch/examples/compilation/eager_fusion.py functorch/examples/compilation/fuse_module.py functorch/examples/compilation/linear_train.py functorch/examples/compilation/simple_function.py functorch/examples/dp_cifar10/cifar10_transforms.py functorch/examples/ensembling/parallel_train.py functorch/examples/lennard_jones/lennard_jones.py functorch/examples/maml_omniglot/maml-omniglot-higher.py functorch/examples/maml_omniglot/maml-omniglot-ptonly.py functorch/examples/maml_omniglot/maml-omniglot-transforms.py functorch/examples/maml_regression/evjang.py functorch/examples/maml_regression/evjang_transforms.py functorch/examples/maml_regression/evjang_transforms_module.py functorch/experimental/control_flow.py functorch/notebooks/_src/plot_ensembling.py functorch/notebooks/_src/plot_jacobians_and_hessians.py functorch/notebooks/_src/plot_per_sample_gradients.py tools/linter/adapters/ufmt_linter.py,https://github.com/pytorch/pytorch/pull/129755,XuehaiPan,zou3519,,,
aecc746fccc,skip,not user facing,[BE][Easy][12/19] enforce style for empty lines in import segments in `test/i*/` (#129763),test/inductor/indirect_assert_helper.py test/inductor/minifier_smoke.py test/inductor/opinfo_harness.py test/inductor/test_aot_inductor.py test/inductor/test_aot_inductor_utils.py test/inductor/test_autoheuristic.py test/inductor/test_benchmark_fusion.py test/inductor/test_binary_folding.py test/inductor/test_ck_backend.py test/inductor/test_codecache.py test/inductor/test_codegen_triton.py test/inductor/test_compile_worker.py test/inductor/test_compiled_autograd.py test/inductor/test_compiled_optimizers.py test/inductor/test_config.py test/inductor/test_control_flow.py test/inductor/test_coordinate_descent_tuner.py test/inductor/test_cpp_wrapper_hipify.py test/inductor/test_cpu_repro.py test/inductor/test_cpu_select_algorithm.py test/inductor/test_cuda_repro.py test/inductor/test_cudacodecache.py test/inductor/test_cudagraph_trees.py test/inductor/test_cudagraph_trees_expandable_segments.py test/inductor/test_custom_lowering.py test/inductor/test_custom_post_grad_passes.py test/inductor/test_cutlass_backend.py test/inductor/test_decompose_mem_bound_mm.py test/inductor/test_dependencies.py test/inductor/test_distributed_patterns.py test/inductor/test_efficient_conv_bn_eval.py test/inductor/test_extension_backend.py test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py test/inductor/test_foreach.py test/inductor/test_fp8.py test/inductor/test_fx_fusion.py test/inductor/test_graph_transform_observer.py test/inductor/test_group_batch_fusion.py test/inductor/test_halide.py test/inductor/test_indexing.py test/inductor/test_inductor_freezing.py test/inductor/test_inductor_utils.py test/inductor/test_inplacing_pass.py test/inductor/test_layout_optim.py test/inductor/test_loop_ordering.py test/inductor/test_max_autotune.py test/inductor/test_memory_planning.py test/inductor/test_metrics.py test/inductor/test_minifier_isolate.py test/inductor/test_mkldnn_pattern_matcher.py test/inductor/test_mmdecomp.py test/inductor/test_move_constructors_to_cuda.py test/inductor/test_multi_kernel.py test/inductor/test_pad_mm.py test/inductor/test_padding.py test/inductor/test_pattern_matcher.py test/inductor/test_perf.py test/inductor/test_profiler.py test/inductor/test_scatter_optimization.py test/inductor/test_select_algorithm.py test/inductor/test_smoke.py test/inductor/test_snode_runtime.py test/inductor/test_torchbind.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py test/inductor/test_torchinductor_dynamic_shapes.py test/inductor/test_torchinductor_opinfo.py test/inductor/test_triton_extension_backend.py test/inductor/test_triton_heuristics.py test/inductor/test_triton_kernels.py test/inductor/test_xpu_basic.py tools/linter/adapters/ufmt_linter.py,https://github.com/pytorch/pytorch/pull/129763,XuehaiPan,jansel,,,
6c2c8ee15bf,Uncategorized,Untopiced,[export] Remove preserved ops from decomp list (#130970),test/export/test_export.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/130970,angelayi,bdhirsh,,,
b732b52f1e4,skip,Untopiced,"Revert ""[BE][Easy][12/19] enforce style for empty lines in import segments in `test/i*/` (#129763)""",test/inductor/indirect_assert_helper.py test/inductor/minifier_smoke.py test/inductor/opinfo_harness.py test/inductor/test_aot_inductor.py test/inductor/test_aot_inductor_utils.py test/inductor/test_autoheuristic.py test/inductor/test_benchmark_fusion.py test/inductor/test_binary_folding.py test/inductor/test_ck_backend.py test/inductor/test_codecache.py test/inductor/test_codegen_triton.py test/inductor/test_compile_worker.py test/inductor/test_compiled_autograd.py test/inductor/test_compiled_optimizers.py test/inductor/test_config.py test/inductor/test_control_flow.py test/inductor/test_coordinate_descent_tuner.py test/inductor/test_cpp_wrapper_hipify.py test/inductor/test_cpu_repro.py test/inductor/test_cpu_select_algorithm.py test/inductor/test_cuda_repro.py test/inductor/test_cudacodecache.py test/inductor/test_cudagraph_trees.py test/inductor/test_cudagraph_trees_expandable_segments.py test/inductor/test_custom_lowering.py test/inductor/test_custom_post_grad_passes.py test/inductor/test_cutlass_backend.py test/inductor/test_decompose_mem_bound_mm.py test/inductor/test_dependencies.py test/inductor/test_distributed_patterns.py test/inductor/test_efficient_conv_bn_eval.py test/inductor/test_extension_backend.py test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py test/inductor/test_foreach.py test/inductor/test_fp8.py test/inductor/test_fx_fusion.py test/inductor/test_graph_transform_observer.py test/inductor/test_group_batch_fusion.py test/inductor/test_halide.py test/inductor/test_indexing.py test/inductor/test_inductor_freezing.py test/inductor/test_inductor_utils.py test/inductor/test_inplacing_pass.py test/inductor/test_layout_optim.py test/inductor/test_loop_ordering.py test/inductor/test_max_autotune.py test/inductor/test_memory_planning.py test/inductor/test_metrics.py test/inductor/test_minifier_isolate.py test/inductor/test_mkldnn_pattern_matcher.py test/inductor/test_mmdecomp.py test/inductor/test_move_constructors_to_cuda.py test/inductor/test_multi_kernel.py test/inductor/test_pad_mm.py test/inductor/test_padding.py test/inductor/test_pattern_matcher.py test/inductor/test_perf.py test/inductor/test_profiler.py test/inductor/test_scatter_optimization.py test/inductor/test_select_algorithm.py test/inductor/test_smoke.py test/inductor/test_snode_runtime.py test/inductor/test_torchbind.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py test/inductor/test_torchinductor_dynamic_shapes.py test/inductor/test_torchinductor_opinfo.py test/inductor/test_triton_extension_backend.py test/inductor/test_triton_heuristics.py test/inductor/test_triton_kernels.py test/inductor/test_xpu_basic.py tools/linter/adapters/ufmt_linter.py,,,,,,
dfc3347c4a0,Uncategorized,Untopiced,[pytorch][counters] Make WaitCounter backend pluggable (#130934),torch/csrc/monitor/instrumentation.cpp torch/csrc/monitor/instrumentation.h,https://github.com/pytorch/pytorch/pull/130934,andriigrynenko,asiab4,,,
134bc4fc34b,skip,not user facing,[BE][Easy][12/19] enforce style for empty lines in import segments in `test/i*/` (#129763),test/inductor/indirect_assert_helper.py test/inductor/minifier_smoke.py test/inductor/opinfo_harness.py test/inductor/test_aot_inductor.py test/inductor/test_aot_inductor_package.py test/inductor/test_aot_inductor_utils.py test/inductor/test_autoheuristic.py test/inductor/test_benchmark_fusion.py test/inductor/test_binary_folding.py test/inductor/test_ck_backend.py test/inductor/test_codecache.py test/inductor/test_codegen_triton.py test/inductor/test_compile_worker.py test/inductor/test_compiled_autograd.py test/inductor/test_compiled_optimizers.py test/inductor/test_config.py test/inductor/test_control_flow.py test/inductor/test_coordinate_descent_tuner.py test/inductor/test_cpp_wrapper_hipify.py test/inductor/test_cpu_repro.py test/inductor/test_cpu_select_algorithm.py test/inductor/test_cuda_repro.py test/inductor/test_cudacodecache.py test/inductor/test_cudagraph_trees.py test/inductor/test_cudagraph_trees_expandable_segments.py test/inductor/test_custom_lowering.py test/inductor/test_custom_post_grad_passes.py test/inductor/test_cutlass_backend.py test/inductor/test_decompose_mem_bound_mm.py test/inductor/test_dependencies.py test/inductor/test_distributed_patterns.py test/inductor/test_efficient_conv_bn_eval.py test/inductor/test_extension_backend.py test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py test/inductor/test_foreach.py test/inductor/test_fp8.py test/inductor/test_fx_fusion.py test/inductor/test_graph_transform_observer.py test/inductor/test_group_batch_fusion.py test/inductor/test_halide.py test/inductor/test_indexing.py test/inductor/test_inductor_freezing.py test/inductor/test_inductor_utils.py test/inductor/test_inplacing_pass.py test/inductor/test_layout_optim.py test/inductor/test_loop_ordering.py test/inductor/test_max_autotune.py test/inductor/test_memory_planning.py test/inductor/test_metrics.py test/inductor/test_minifier_isolate.py test/inductor/test_mkldnn_pattern_matcher.py test/inductor/test_mmdecomp.py test/inductor/test_move_constructors_to_cuda.py test/inductor/test_multi_kernel.py test/inductor/test_pad_mm.py test/inductor/test_padding.py test/inductor/test_pattern_matcher.py test/inductor/test_perf.py test/inductor/test_profiler.py test/inductor/test_scatter_optimization.py test/inductor/test_select_algorithm.py test/inductor/test_smoke.py test/inductor/test_snode_runtime.py test/inductor/test_torchbind.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py test/inductor/test_torchinductor_dynamic_shapes.py test/inductor/test_torchinductor_opinfo.py test/inductor/test_triton_extension_backend.py test/inductor/test_triton_heuristics.py test/inductor/test_triton_kernels.py test/inductor/test_xpu_basic.py tools/linter/adapters/ufmt_linter.py,https://github.com/pytorch/pytorch/pull/129763,XuehaiPan,jansel,,,
cf3f4285a81,dynamo,Untopiced,Add recursive metadata guard test (#131002),test/dynamo/test_subclasses.py torch/testing/_internal/two_tensor.py,https://github.com/pytorch/pytorch/pull/131002,mlazos,bdhirsh,,,
e98135d1ad2,quantization,not user facing,"[aota] Needs autograd if an input requires_grad, agnostic to enable_grad (#128890)",test/distributed/_tensor/test_dtensor_compile.py test/functorch/test_aotdispatch.py test/inductor/test_flex_attention.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/aot_autograd.py torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/128890,IvanKobzarev,bdhirsh,,,
28a74b9fa43,nested tensor_frontend,improvements,[NestedTensor] Integrate sum along the jagged dimension into NestedTensor (#130425),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/130425,jananisriram,davidberard98,,,
38b7d89aa45,skip,not user facing,Uses context pointer for deleter to enable multiple CUDAPluggableAllocator usage (#130472),aten/src/ATen/test/cuda_allocator_test.cpp build_variables.bzl torch/csrc/cuda/CUDAPluggableAllocator.cpp torch/csrc/cuda/CUDAPluggableAllocator.h torch/csrc/cuda/Module.cpp,https://github.com/pytorch/pytorch/pull/130472,syed-ahmed,eqy,ezyang,,
c986aeea2d7,skip,Untopiced,Re-implement pin_memory to be device-agnostic by leveraging the Accelerator concept (#126376),aten/src/ATen/Context.h aten/src/ATen/DeviceAccelerator.cpp aten/src/ATen/DeviceAccelerator.h aten/src/ATen/cuda/PinnedMemoryAllocator.cpp aten/src/ATen/detail/AcceleratorHooksInterface.h aten/src/ATen/detail/CUDAHooksInterface.h aten/src/ATen/detail/HIPHooksInterface.h aten/src/ATen/detail/MPSHooksInterface.h aten/src/ATen/detail/MTIAHooksInterface.h aten/src/ATen/detail/PrivateUse1HooksInterface.h aten/src/ATen/detail/XPUHooksInterface.h aten/src/ATen/mps/MPSAllocator.mm aten/src/ATen/mps/MPSAllocatorInterface.h aten/src/ATen/mps/MPSGuardImpl.h aten/src/ATen/mps/MPSHooks.h aten/src/ATen/mps/MPSHooks.mm aten/src/ATen/native/Memory.cpp aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/sparse/SparseUnaryOps.cpp aten/src/ATen/templates/RegisterBackendSelect.cpp test/cpp_extensions/open_registration_extension.cpp test/inductor/test_codecache.py test/test_cpp_extensions_open_device_registration.py test/test_cpp_extensions_stream_and_event.py test/test_torch.py torch/csrc/Stream.cpp,https://github.com/pytorch/pytorch/pull/126376,wizzniu,albanD,,,
1b07d421717,skip,not user facing,Add @syed-ahmed to CUDA `CODEOWNERS` paths (#130971),CODEOWNERS,https://github.com/pytorch/pytorch/pull/130971,eqy,soulitzer,,,
63a0a65df9a,sparse_frontend,docs,Define 'zero-preserving unary functions' in docs (#130804),docs/source/sparse.rst,https://github.com/pytorch/pytorch/pull/130804,redwrasse,soulitzer,,,
9f6db5d0e23,skip,Untopiced,"Revert ""Ensure staticmethods can be allowed in graph (#130882)""",test/dynamo/test_repros.py torch/_dynamo/variables/user_defined.py,,,,,,
dd39dca0341,nn_frontend,Untopiced,Removing some cruff and updating signatures for consistency (#130871),test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py torch/nn/attention/flex_attention.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/130871,drisspg,Chillee,joydddd,,
5a90ed35232,inductor,Untopiced,Reinplacing should ignore copy_ nodes where the mutated arg is not read (#130866),test/inductor/test_perf.py torch/_inductor/fx_passes/reinplace.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/130866,zou3519,oulgen,,,
120fdf7ee21,skip,Untopiced,"Revert ""[aota] Needs autograd if an input requires_grad, agnostic to enable_grad (#128890)""",test/distributed/_tensor/test_dtensor_compile.py test/functorch/test_aotdispatch.py test/inductor/test_flex_attention.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/aot_autograd.py torch/_functorch/partitioners.py,,,,,,
d6ae8bbf16e,skip,Untopiced,"Revert ""[export] Add print_readable to unflattener (#128617)""",test/dynamo/test_autograd_function.py test/dynamo/test_higher_order_ops.py test/dynamo/test_subclasses.py torch/export/unflatten.py torch/fx/graph_module.py,,,,,,
7c90a829700,skip,not user facing,[Reland] [5/N] Change static functions in headers to inline (#131010),aten/src/ATen/Context.h aten/src/ATen/ExpandUtils.h aten/src/ATen/TensorOperators.h aten/src/ATen/TracerMode.h aten/src/ATen/Utils.h aten/src/ATen/WrapDimUtilsMulti.h aten/src/ATen/core/Formatting.h aten/src/ATen/core/boxing/impl/boxing.h aten/src/ATen/cuda/Atomic.cuh aten/src/ATen/native/ScatterGatherChecks.h aten/src/ATen/native/cuda/GridSampler.cuh aten/src/ATen/native/cuda/SortingCommon.cuh aten/src/ATen/native/cuda/UpSample.cuh c10/cuda/CUDAMathCompat.h,https://github.com/pytorch/pytorch/pull/131010,cyyever,Skylion007,,,
8ea03372a1e,mps,bug fixes,[MPS] Store philox counter as part of the RNG state (#130662),aten/src/ATen/mps/MPSGeneratorImpl.h aten/src/ATen/mps/MPSGeneratorImpl.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/130662,qqaatw,malfet,,,
32ff04d30a0,distributed,not user facing,[dtensor][debug] adding functionality to control noisiness of the debug output (#130410),torch/distributed/_tensor/debug/comm_mode.py torch/distributed/_tensor/examples/comm_mode_features_example.py,https://github.com/pytorch/pytorch/pull/130410,sinhaanshul,XilunWu,,,
27ded035455,skip,Untopiced,"[FX][export] DCE pass, check schema for node impurity (#130395)",test/fx/test_dce_pass.py test/quantization/pt2e/test_x86inductor_quantizer.py torch/export/_remove_effect_tokens_pass.py torch/export/_trace.py torch/export/_unlift.py torch/fx/node.py,https://github.com/pytorch/pytorch/pull/130395,yushangdi,angelayi,jgong5,,
16aaff77832,inductor,not user facing,Fix mm pad regresion - more conservative estimation of plannable inputs (#128909),test/inductor/test_pad_mm.py test/inductor/test_perf.py torch/_inductor/config.py torch/_inductor/fx_passes/pad_mm.py torch/_inductor/lowering.py torch/_inductor/utils.py torch/utils/_config_module.py,https://github.com/pytorch/pytorch/pull/128909,eellison,Chillee,,,
85451b2cded,distributed,not user facing,[DTensor] Fix shard_dim_alltoall fake tensor return  (#129945),test/distributed/_tensor/test_redistribute.py torch/distributed/_tensor/_collective_utils.py,https://github.com/pytorch/pytorch/pull/129945,wz337,wanchaol,,,
5484c860215,distributed,Untopiced,[export] Fully support extension op in serialization/deserialization. (#130851),test/export/test_serialize.py torch/_export/serde/serialize.py torch/_export/verifier.py torch/distributed/_tensor/experimental/tp_transform.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/130851,zhxchen17,angelayi,,,
5979014059b,distributed,not user facing,DSD for TorchTune LoRA (#129635),test/distributed/checkpoint/test_state_dict.py torch/distributed/checkpoint/state_dict.py,https://github.com/pytorch/pytorch/pull/129635,mori360,fegin,,,
8bf0be7c78e,dynamo,not user facing,[CUDAGraph] Add operator.mul to skip list for find_input_mutations (#130986),torch/_dynamo/backends/cudagraphs.py,https://github.com/pytorch/pytorch/pull/130986,BoyuanFeng,eellison,,,
22388ffe03e,dynamo,Untopiced,Graph break on tostring for numpy remapping (#131007),torch/_dynamo/variables/tensor.py,https://github.com/pytorch/pytorch/pull/131007,mlazos,williamwen42,,,
745324e487c,fx,Untopiced,[export] turn on hybrid symints by default (#130775),test/dynamo/test_export.py test/export/test_export.py torch/_export/non_strict_utils.py torch/_export/serde/serialize.py torch/_export/utils.py torch/export/_trace.py torch/export/exported_program.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/130775,pianpwk,avikchaudhuri,,,
fff92d4f18f,skip,Untopiced,"Revert ""Use inductor TestCase for test_replicate_with_compiler.py (#129494)""",test/distributed/_composable/test_replicate_with_compiler.py torch/testing/_internal/common_distributed.py,,,,,,
6d9f74f0af5,Uncategorized,Untopiced,Add flex decoding benchmark (#130850),benchmarks/transformer/score_mod.py,https://github.com/pytorch/pytorch/pull/130788,joydddd,drisspg,,,
82242a258a7,skip,not user facing,rm duplicate index_dtype arg (#130803),test/test_sparse.py,https://github.com/pytorch/pytorch/pull/130803,redwrasse,soulitzer,,,
c015e5b9e3b,skip,not user facing,Make sure that TransformGetItemToIndex for all graph replay (#131003),test/inductor/test_flex_attention.py torch/_higher_order_ops/flex_attention.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/131003,drisspg,Chillee,,,
367213a6085,skip,Untopiced,[c10] add an option to pg_config split share (#130877),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/init.cpp torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/130877,shuqiangzhang,fduwjj,,,
e7f7c5c3f83,inductor,not user facing,[inductor] Avoid fallback case for custom scan op lowering (#130936),test/inductor/test_torchinductor.py torch/_inductor/ir.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/130936,peterbell10,lezcano,,,
726b9268d24,skip,Untopiced,"Revert ""Re-implement pin_memory to be device-agnostic by leveraging the Accelerator concept (#126376)""",aten/src/ATen/Context.h aten/src/ATen/DeviceAccelerator.cpp aten/src/ATen/DeviceAccelerator.h aten/src/ATen/cuda/PinnedMemoryAllocator.cpp aten/src/ATen/detail/AcceleratorHooksInterface.h aten/src/ATen/detail/CUDAHooksInterface.h aten/src/ATen/detail/HIPHooksInterface.h aten/src/ATen/detail/MPSHooksInterface.h aten/src/ATen/detail/MTIAHooksInterface.h aten/src/ATen/detail/PrivateUse1HooksInterface.h aten/src/ATen/detail/XPUHooksInterface.h aten/src/ATen/mps/MPSAllocator.mm aten/src/ATen/mps/MPSAllocatorInterface.h aten/src/ATen/mps/MPSGuardImpl.h aten/src/ATen/mps/MPSHooks.h aten/src/ATen/mps/MPSHooks.mm aten/src/ATen/native/Memory.cpp aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/sparse/SparseUnaryOps.cpp aten/src/ATen/templates/RegisterBackendSelect.cpp test/cpp_extensions/open_registration_extension.cpp test/inductor/test_codecache.py test/test_cpp_extensions_open_device_registration.py test/test_cpp_extensions_stream_and_event.py test/test_torch.py torch/csrc/Stream.cpp,,,,,,
793b17ebcbf,quantization,Untopiced,Add numeric_debugger top level APIs (#130643),docs/source/quantization-support.rst test/quantization/pt2e/test_numeric_debugger.py torch/ao/quantization/__init__.py torch/ao/quantization/fx/convert.py torch/ao/quantization/pt2e/_numeric_debugger.py torch/ao/quantization/pt2e/numeric_debugger.py torch/ao/quantization/pt2e/prepare.py,https://github.com/pytorch/pytorch/pull/130643,jerryzh168,dulinriley,tarun292,,
24467ba2ece,releng,not user facing,Update pin (#130896),.github/ci_commit_pins/xla.txt,https://github.com/pytorch/pytorch/pull/130896,JackCaoG,anijain2305,,,
f7058b735e5,skip,Untopiced,[Autograd] Cond Higher-Order Operation (#126911),test/functorch/test_control_flow.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/map.py torch/_higher_order_ops/utils.py torch/fx/experimental/proxy_tensor.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/126911,bohnstingl,ydwu4,,,
3e9cf1cc807,skip,not user facing,Fix potential segfault during deletion (#131036),torch/csrc/utils/python_arg_parser.cpp,https://github.com/pytorch/pytorch/pull/131036,zou3519,Skylion007,albanD,,
d8fed480ef6,Uncategorized,Untopiced,Move handle-creation logic into cudacaching allocator. (#130888),c10/cuda/CUDACachingAllocator.cpp c10/cuda/CUDACachingAllocator.h c10/cuda/CUDAMallocAsyncAllocator.cpp torch/csrc/StorageSharing.cpp torch/csrc/cuda/CUDAPluggableAllocator.cpp torch/csrc/cuda/CUDAPluggableAllocator.h,https://github.com/pytorch/pytorch/pull/130888,zdevito,dsjohns2,,,
4d9f2a6d568,skip,not user facing,Small expandable segments refactor. (#130889),c10/cuda/CUDACachingAllocator.cpp,https://github.com/pytorch/pytorch/pull/130889,zdevito,dsjohns2,,,
7f1cda15333,inductor,not user facing,Autoheuristic: Do not store choices as metadata (#130304),test/inductor/test_autoheuristic.py torch/_inductor/autoheuristic/artifacts/_PadMMA100.py torch/_inductor/autoheuristic/autoheuristic_utils.py torchgen/autoheuristic/train.py,https://github.com/pytorch/pytorch/pull/130304,AlnisM,eellison,,,
686b7f046aa,skip,not user facing,[Fix]: TSConverter handles call ops with multiple outputs (#129294),test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/129294,jiashenC,angelayi,,,
fb3674b1f48,skip,Untopiced,"Revert ""[Autograd] Cond Higher-Order Operation (#126911)""",test/functorch/test_control_flow.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/map.py torch/_higher_order_ops/utils.py torch/fx/experimental/proxy_tensor.py torch/testing/_internal/hop_db.py,,,,,,
e880cb2fe0f,onnx,not user facing,[ONNX] Remove beartype usage (#130484),test/onnx/dynamo/test_exporter_api.py test/onnx/internal/test_beartype.py test/onnx/onnx_test_common.py test/onnx/test_fx_to_onnx_with_onnxruntime.py torch/onnx/README.md torch/onnx/_internal/_beartype.py torch/onnx/_internal/diagnostics/_diagnostic.py torch/onnx/_internal/diagnostics/infra/decorator.py torch/onnx/_internal/diagnostics/infra/formatter.py torch/onnx/_internal/diagnostics/infra/utils.py torch/onnx/_internal/exporter.py torch/onnx/_internal/fx/_pass.py torch/onnx/_internal/fx/decomposition_table.py torch/onnx/_internal/fx/dynamo_graph_extractor.py torch/onnx/_internal/fx/fx_onnx_interpreter.py torch/onnx/_internal/fx/fx_symbolic_graph_extractor.py torch/onnx/_internal/fx/onnxfunction_dispatcher.py torch/onnx/_internal/fx/op_validation.py torch/onnx/_internal/fx/passes/_utils.py torch/onnx/_internal/fx/passes/decomp.py torch/onnx/_internal/fx/passes/functionalization.py torch/onnx/_internal/fx/passes/modularization.py torch/onnx/_internal/fx/passes/readability.py torch/onnx/_internal/fx/passes/type_promotion.py torch/onnx/_internal/fx/passes/virtualization.py torch/onnx/_internal/fx/patcher.py torch/onnx/_internal/fx/registration.py torch/onnx/_internal/fx/serialization.py torch/onnx/_internal/fx/torch_export_graph_extractor.py torch/onnx/_internal/io_adapter.py torch/onnx/_internal/jit_utils.py torch/onnx/_internal/onnx_proto_utils.py torch/onnx/_internal/onnxruntime.py torch/onnx/_internal/registration.py torch/onnx/_type_utils.py torch/onnx/symbolic_caffe2.py torch/onnx/symbolic_helper.py torch/onnx/symbolic_opset10.py torch/onnx/symbolic_opset11.py torch/onnx/symbolic_opset12.py torch/onnx/symbolic_opset13.py torch/onnx/symbolic_opset14.py torch/onnx/symbolic_opset15.py torch/onnx/symbolic_opset16.py torch/onnx/symbolic_opset17.py torch/onnx/symbolic_opset18.py torch/onnx/symbolic_opset20.py torch/onnx/symbolic_opset8.py torch/onnx/symbolic_opset9.py torch/onnx/utils.py torch/onnx/verification.py,https://github.com/pytorch/pytorch/pull/130484,justinchuby,titaiwangms,,,
6e7b9ee8a07,skip,not user facing,[inductor] adapte windows file path (#130713),test/functorch/test_eager_transforms.py torch/_inductor/codecache.py torch/_inductor/cpp_builder.py torch/_inductor/cpu_vec_isa.py,https://github.com/pytorch/pytorch/pull/130713,xuhancn,desertfire,jansel,jgong5,
3cc6183ce13,jit,not user facing,Fix getAugOp error (#131033),torch/csrc/jit/frontend/ir_emitter.cpp,https://github.com/pytorch/pytorch/pull/131033,cyyever,ezyang,,,
208dffa702d,distributed,not user facing,[Compiled DDP] DDP + AC unit test (#130981),test/distributed/_composable/test_replicate_with_compiler.py,https://github.com/pytorch/pytorch/pull/130981,yf225,fegin,,,
5a6a806b192,skip,not user facing,[Inductor UT] Generalize device-bias code in case TestFxGraphCache.test_inductor_counters. (#131006),test/inductor/test_codecache.py,https://github.com/pytorch/pytorch/pull/131006,etaf,masnesral,,,
39493aa9341,inductor,not user facing,[inductor][cpp][gemm] move bias add to epilogue (#130675),torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/130675,jgong5,jansel,leslie-fang-intel,,
52cb9abb1d3,nn_frontend,Untopiced,Add deterministic support in nn.functional.interpolate for XPU (#129864),torch/nn/functional.py,https://github.com/pytorch/pytorch/pull/129864,majing921201,EikanWang,albanD,dvrogozh,
b556d315868,skip,not user facing,Update torch-xpu-ops pin (ATen XPU implementation) (#131015),third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/131015,fengyuan14,EikanWang,,,
3c622fbcd3f,inductor,bug fixes,[inductor] Fix var_to_range in IndexPropagation (#130984),torch/_inductor/index_propagation.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/130984,peterbell10,lezcano,,,
00e54e74ff6,dynamo,not user facing,[dynamo][cpp-guards] Fix bug in dict tags (#131056),test/dynamo/test_repros.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/131056,anijain2305,jansel,williamwen42,,
ac76dd606fa,dynamo,not user facing,[dynamo] Alternative way to skip empty hooks guards on inbuilt nn modules (#131057),torch/_dynamo/guards.py torch/_dynamo/variables/nn_module.py,https://github.com/pytorch/pytorch/pull/131057,anijain2305,jansel,williamwen42,,
d59803fb67e,inductor,not user facing,Refactored flexattention kernel (#130904),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/select_algorithm.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/130904,Chillee,drisspg,,,
4f60a2e39c8,quantization,not user facing,Set correct output dtype for dequantize op during convert_pt2e in decomposed mode (#128953),test/quantization/pt2e/test_quantize_pt2e.py torch/ao/quantization/fx/convert.py torch/testing/_internal/common_quantization.py,https://github.com/pytorch/pytorch/pull/128953,kausikmaiti,jerryzh168,jgong5,,
fd4899bc588,onnx,not user facing,[ONNX] Run ruff pyupgrade to update type annotations (#130657),torch/onnx/_exporter_states.py torch/onnx/_internal/diagnostics/_diagnostic.py torch/onnx/_internal/diagnostics/infra/_infra.py torch/onnx/_internal/diagnostics/infra/context.py torch/onnx/_internal/diagnostics/infra/decorator.py torch/onnx/_internal/diagnostics/infra/formatter.py torch/onnx/_internal/diagnostics/infra/utils.py torch/onnx/_internal/exporter.py torch/onnx/_internal/fx/_pass.py torch/onnx/_internal/fx/analysis/unsupported_nodes.py torch/onnx/_internal/fx/decomposition_skip.py torch/onnx/_internal/fx/decomposition_table.py torch/onnx/_internal/fx/diagnostics.py torch/onnx/_internal/fx/dynamo_graph_extractor.py torch/onnx/_internal/fx/fx_onnx_interpreter.py torch/onnx/_internal/fx/fx_symbolic_graph_extractor.py torch/onnx/_internal/fx/onnxfunction_dispatcher.py torch/onnx/_internal/fx/op_validation.py torch/onnx/_internal/fx/passes/_utils.py torch/onnx/_internal/fx/passes/decomp.py torch/onnx/_internal/fx/passes/functionalization.py torch/onnx/_internal/fx/passes/modularization.py torch/onnx/_internal/fx/passes/readability.py torch/onnx/_internal/fx/passes/type_promotion.py torch/onnx/_internal/fx/passes/virtualization.py torch/onnx/_internal/fx/registration.py torch/onnx/_internal/fx/serialization.py torch/onnx/_internal/fx/torch_export_graph_extractor.py torch/onnx/_internal/fx/type_utils.py torch/onnx/_internal/io_adapter.py torch/onnx/_internal/jit_utils.py torch/onnx/_internal/onnx_proto_utils.py torch/onnx/_type_utils.py torch/onnx/errors.py torch/onnx/symbolic_helper.py torch/onnx/symbolic_opset10.py torch/onnx/symbolic_opset11.py torch/onnx/symbolic_opset12.py torch/onnx/symbolic_opset14.py torch/onnx/symbolic_opset9.py torch/onnx/utils.py torch/onnx/verification.py,https://github.com/pytorch/pytorch/pull/130657,justinchuby,titaiwangms,,,
982309b5018,distributed,not user facing,Initial commit of flight recorder trace (#130764),tools/flight_recorder/fr_trace.py,https://github.com/pytorch/pytorch/pull/130764,c-p-i-o,fduwjj,,,
eee76c86a84,skip,not user facing,Write trace_structured events to scuba (#130955),torch/_logging/_internal.py torch/_utils_internal.py,https://github.com/pytorch/pytorch/pull/130955,oulgen,ezyang,,,
feef057691b,cpp_frontend,Untopiced,[1/N] Fix Wunused-parameter warnings (#130924),aten/src/ATen/NumericUtils.h aten/src/ATen/detail/AcceleratorHooksInterface.h aten/src/ATen/detail/IPUHooksInterface.h aten/src/ATen/detail/MPSHooksInterface.h aten/src/ATen/detail/XPUHooksInterface.h aten/src/ATen/native/mkl/LinearAlgebra.cpp benchmarks/instruction_counts/definitions/standard.py c10/core/SymNodeImpl.h c10/util/Half.h c10/util/logging_is_not_google_glog.h torch/csrc/api/include/torch/enum.h,https://github.com/pytorch/pytorch/pull/130924,cyyever,ezyang,,,
d7a78ec8b93,skip,Untopiced,[ROCm] Enable ROCm support for inductor's dynamic_rblock_scaling  (#129663),torch/_inductor/runtime/hints.py torch/_inductor/runtime/triton_heuristics.py torch/csrc/cuda/Module.cpp,https://github.com/pytorch/pytorch/pull/129663,jataylo,jansel,shunting314,,
1f961ad4959,skip,not user facing,Runs aten cuda cpp tests in CI (#131061),aten/tools/run_tests.sh,https://github.com/pytorch/pytorch/pull/131061,syed-ahmed,eqy,malfet,,
1b72cf0b095,dynamo,Untopiced,Add hasattr for tensor variable (#131008),test/dynamo/test_misc.py test/dynamo_expected_failures/TestTorchDeviceTypeCPU.test_broadcast_fn_copy_cpu test/dynamo_expected_failures/TestTorchDeviceTypeCPU.test_broadcast_fn_map2_cpu test/dynamo_expected_failures/TestTorchDeviceTypeCPU.test_broadcast_fn_map_cpu torch/_dynamo/variables/tensor.py,https://github.com/pytorch/pytorch/pull/131008,mlazos,anijain2305,,,
a6a2cd62573,dynamo,not user facing,Typo fix (#131037),torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/131037,ezyang,Skylion007,albanD,,
25d8a0480b3,Uncategorized,Untopiced,[lint] Remove unnecessary BUCKRESTRICTEDSYNTAX suppressions,buckbuild.bzl pt_ops.bzl third_party/kineto.buck.bzl tools/BUCK.bzl tools/build_defs/android/build_mode_defs.bzl tools/build_defs/apple/build_mode_defs.bzl tools/build_defs/default_platform_defs.bzl tools/build_defs/fb_native_wrapper.bzl tools/build_defs/fb_xplat_cxx_library.bzl tools/build_defs/fb_xplat_cxx_test.bzl tools/build_defs/fb_xplat_genrule.bzl tools/build_defs/fbsource_utils.bzl tools/build_defs/select.bzl tools/build_defs/windows/windows_flag_map.bzl,https://github.com/pytorch/pytorch/pull/131187,zertosh,,,,
5f3d8b87886,skip,Untopiced,"Revert ""[c10] add an option to pg_config split share (#130877)""",test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/init.cpp torch/distributed/distributed_c10d.py,,,,,,
451fc029fe0,nn_frontend,docs,docs: note transposed weight initialisations (#130122),torch/nn/init.py,https://github.com/pytorch/pytorch/pull/130122,sradc,mikaylagawarecki,,,
d4a79d4a7c7,nn_frontend,docs,Fix an example: Resolve broadcasting error in attn_bias and attn_mask… (#130209),torch/nn/functional.py,https://github.com/pytorch/pytorch/pull/130209,xingyunjohn1,mikaylagawarecki,,,
31e79aae6a1,python_frontend,Untopiced,Another follow up to #130260 (#130993),torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/130993,clee2000,huydhn,,,
abb3f2822c1,inductor,Untopiced,[aotinductor] Support additional lifted constants supplied to const folding. (#130743),test/export/test_export.py torch/_inductor/compile_fx.py torch/_inductor/constant_folding.py,https://github.com/pytorch/pytorch/pull/130743,zhxchen17,SherlockNoMad,desertfire,,
9b5c70878b4,skip,not user facing,[Fix] Missing parameter happens when retracing an already jit.scripted module (#129787),test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/129787,jiashenC,angelayi,,,
042be441bad,skip,not user facing,[aoti] Unskip some aot inductor tests (#130973),test/inductor/test_aot_inductor.py,https://github.com/pytorch/pytorch/pull/130973,henrylhtsang,ColinPeppler,,,
e49c0acc396,dynamo,not user facing,[dynamo] Revert https://github.com/pytorch/pytorch/pull/130416 (#131058),torch/_dynamo/guards.py torch/_dynamo/source.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/nn_module.py,https://github.com/pytorch/pytorch/pull/131058,anijain2305,williamwen42,,,
125be005ebd,Uncategorized,docs,[Docs] Fix fake tensor doc (#131205),docs/source/torch.compiler_fake_tensor.rst,https://github.com/pytorch/pytorch/pull/131205,qqaatw,eellison,,,
5f981388ec4,skip,Untopiced,"Revert ""[ROCm] Enable ROCm support for inductor's dynamic_rblock_scaling  (#129663)""",torch/_inductor/runtime/hints.py torch/_inductor/runtime/triton_heuristics.py torch/csrc/cuda/Module.cpp,,,,,,
ceee87df2e7,skip,not user facing,[export] modify export code owners (#130894),CODEOWNERS,https://github.com/pytorch/pytorch/pull/130894,ydwu4,zhxchen17,,,
85ca88a2bb2,distributed,not user facing,[Distributed][PP export] update tracing to handle autocast inclusion (#130998),torch/distributed/pipelining/_IR.py,https://github.com/pytorch/pytorch/pull/130998,lessw2020,fduwjj,,,
c64ad2403c0,releng,not user facing,LF runners: Add new runner types for Amazon2023 AMIs (#131246),.github/lf-canary-scale-config.yml .github/lf-scale-config.yml,https://github.com/pytorch/pytorch/pull/131246,ZainRizvi,malfet,,,
0ca7b6ddd91,skip,Untopiced,[easy][pytorch][counters] Move WaitCounter in c10/util (#131021),build_variables.bzl c10/util/WaitCounter.cpp c10/util/WaitCounter.h torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/monitor/instrumentation.cpp torch/csrc/monitor/instrumentation.h,https://github.com/pytorch/pytorch/pull/131021,andriigrynenko,asiab4,,,
4aef5a1134b,skip,Untopiced,[c10] add an option to pg_config split share (#130877),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/init.cpp torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/130877,shuqiangzhang,fduwjj,,,
0e72baddf0b,skip,Untopiced,"Revert ""[easy][pytorch][counters] Move WaitCounter in c10/util (#131021)""",build_variables.bzl c10/util/WaitCounter.cpp c10/util/WaitCounter.h torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/monitor/instrumentation.cpp torch/csrc/monitor/instrumentation.h,,,,,,
6657b14a64d,inductor,not user facing,[inductor] Fix the method for checking the variable type of entry.numel (#131026),torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/131026,peaceorwell,peterbell10,,,
35bf05561cb,inductor,not user facing,[Inductor] B2B-GEMM performance tuning with test (#130778),test/inductor/test_b2b_gemm.py torch/_inductor/fx_passes/b2b_gemm.py,https://github.com/pytorch/pytorch/pull/130778,sdingcn,eellison,,,
7c299b46ca4,skip,Untopiced,"Revert ""Invalidate StorageImpl instances when tensor is overwritten with cudagraphs (#125264)""",c10/core/StorageImpl.cpp c10/core/StorageImpl.h test/inductor/test_cudagraph_trees.py torch/_C/__init__.pyi.in torch/_inductor/cudagraph_trees.py torch/csrc/cuda/Module.cpp,,,,,,
4821f72457a,skip,not user facing,[executorch hash update] update the pinned executorch hash (#130001),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/130001,pytorchupdatebot,pytorchbot,,,
207fb96155e,functorch,Untopiced,"[functorch] saved tensor hooks error should only apply to grad, vjp transforms. (#131191)",test/dynamo/test_higher_order_ops.py test/functorch/test_eager_transforms.py test/functorch/test_vmap.py torch/_functorch/eager_transforms.py torch/_functorch/vmap.py,https://github.com/pytorch/pytorch/pull/131191,zou3519,drisspg,,,
cd8bbdc71a0,jit,not user facing,[2/N] Fix Wunused-parameter warnings (#131170),aten/src/ATen/core/CachingHostAllocator.h aten/src/ATen/native/verbose_wrapper.cpp torch/csrc/StorageMethods.cpp torch/csrc/jit/runtime/logging.h torch/csrc/profiler/stubs/itt.cpp torch/csrc/profiler/unwind/line_number_program.h torch/csrc/profiler/unwind/sections.h torch/csrc/profiler/unwind/unwind.cpp torch/csrc/utils/byte_order.cpp torch/csrc/utils/byte_order.h,https://github.com/pytorch/pytorch/pull/131170,cyyever,mikaylagawarecki,,,
30d1826b2bd,skip,Untopiced,"Revert ""[executorch hash update] update the pinned executorch hash (#130001)""",.ci/docker/ci_commit_pins/executorch.txt,,,,,,
f0075c179b1,releng,Untopiced,Pin `sympy >= 1.13.0` (#130895),.circleci/scripts/binary_linux_test.sh .github/requirements/pip-requirements-macOS.txt .lintrunner.toml requirements.txt setup.py torch/_inductor/codegen/common.py torch/_inductor/scheduler.py torch/utils/_sympy/functions.py torch/utils/_sympy/numbers.py torch/utils/_sympy/value_ranges.py,https://github.com/pytorch/pytorch/pull/130895,XuehaiPan,ezyang,,,
bb4251213bd,autograd_frontend,Untopiced,Add decomposition for channel_shuffle (#118775),aten/src/ATen/functorch/BatchRulesModules.cpp test/expect/HasDecompTest.test_has_decomposition.expect tools/autograd/derivatives.yaml tools/autograd/gen_variable_type.py torch/_decomp/__init__.py torch/_meta_registrations.py torch/_refs/nn/functional/__init__.py torch/nn/modules/channelshuffle.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/118775,isuruf,peterbell10,,,
27c2a0d63b8,inductor,not user facing,[inductor] Separate Buffer and Operation into two concepts (#130831),test/inductor/test_aot_inductor.py test/inductor/test_cuda_repro.py test/inductor/test_debug_trace.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/wrapper.py torch/_inductor/comms.py torch/_inductor/dependencies.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/130831,peterbell10,lezcano,,,
deacc543f13,skip,not user facing,[inductor] Make UserDefinedTritonKernel a multi-output operation (#130832),torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/130832,peterbell10,lezcano,,,
9df8ea1cf2d,skip,not user facing,[inductor] Use multiple outputs for flex-attention (#130833),test/inductor/test_flex_attention.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/130833,peterbell10,lezcano,,,
3c43fe068f4,inductor,not user facing,[inductor] parallel compile: Create new pipes for subproc communication (#131194),torch/_inductor/compile_worker/__main__.py torch/_inductor/compile_worker/subproc_pool.py,https://github.com/pytorch/pytorch/pull/131194,masnesral,atalman,eellison,malfet,
50436d5bdb5,skip,not user facing,[export] fix zero arg export in training_ir (#130990),test/export/test_export.py torch/_export/utils.py torch/export/_trace.py torch/export/_unlift.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/130990,ydwu4,pianpwk,,,
d2bd9acabde,releng,not user facing,[BE] bump `optree` version to 0.12.1 (#130139),.ci/docker/requirements-ci.txt .github/requirements/pip-requirements-iOS.txt .github/requirements/pip-requirements-macOS.txt .github/workflows/_win-test.yml .lintrunner.toml requirements.txt setup.py test/functorch/test_eager_transforms.py test/profiler/test_profiler.py test/test_mps.py test/test_stateless.py test/test_testing.py torch/utils/_cxx_pytree.py,https://github.com/pytorch/pytorch/pull/130139,XuehaiPan,zou3519,,,
168c0e24a54,distributed,Untopiced,[IntraNodeComm] Fix some issues in two-shot all-reduce (#131244),torch/csrc/distributed/c10d/intra_node_comm.cu,https://github.com/pytorch/pytorch/pull/131244,yifuwang,Chillee,weifengpy,,
37337ef5c3a,skip,not user facing,add some description on create_block_mask and mask mods (#131209),torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/131209,drisspg,joydddd,,,
d31f2ae904b,dynamo,not user facing,Ensure invariant that all inputs have tensor dict (#131249),torch/_dynamo/output_graph.py,https://github.com/pytorch/pytorch/pull/131249,mlazos,anijain2305,,,
ebc012ace69,skip,not user facing,Add hooks for execution on intel gaudi devices - 1 (#128584),test/test_ops.py torch/testing/_internal/common_device_type.py torch/testing/_internal/common_modules.py torch/testing/_internal/opinfo/core.py,https://github.com/pytorch/pytorch/pull/128584,ankurneog,albanD,,,
0b44e1a74c7,inductor,not user facing,[inductor][cpp][gemm] optimize arbitrary N in packed gemm template (#130690),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/130690,jgong5,jansel,leslie-fang-intel,,
a8319698b3b,inductor,not user facing,[inductor] [cpp] improve cache blocking with CPU info (#129348),aten/src/ATen/cpu/Utils.cpp aten/src/ATen/cpu/Utils.h torch/_C/_cpu.pyi torch/_inductor/codegen/cpp_gemm_template.py torch/csrc/cpu/Module.cpp,https://github.com/pytorch/pytorch/pull/129348,chunyuan-w,jansel,jgong5,,
637ab85e7ff,cuda,Untopiced,fix for launching kernel invalid config error when calling embedding … (#130994),aten/src/ATen/native/cuda/Indexing.cu test/nn/test_embedding.py,https://github.com/pytorch/pytorch/pull/130994,hongxiayang,jeffdaily,xw285cornell,,
8e478d4fb18,skip,not user facing,Add Alban and Piotr into Core Maintainers (#130903),docs/source/community/persons_of_interest.rst,https://github.com/pytorch/pytorch/pull/130903,soumith,Skylion007,albanD,,
b6d477fd568,inductor,not user facing,[BE][Easy][16/19] enforce style for empty lines in import segments in `torch/_i*/` (#129768),tools/linter/adapters/ufmt_linter.py torch/_inductor/__init__.py torch/_inductor/aoti_eager.py torch/_inductor/async_compile.py torch/_inductor/autoheuristic/autoheuristic.py torch/_inductor/autoheuristic/autoheuristic_utils.py torch/_inductor/autoheuristic/learned_heuristic_controller.py torch/_inductor/autotune_process.py torch/_inductor/bounds.py torch/_inductor/codecache.py torch/_inductor/codegen/aoti_hipify_utils.py torch/_inductor/codegen/codegen_device_driver.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_template.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cuda.py torch/_inductor/codegen/cuda/cuda_cpp_scheduling.py torch/_inductor/codegen/cuda/cuda_env.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/cuda/cuda_template.py torch/_inductor/codegen/cuda/cutlass_lib_extensions/gemm_operation_extensions.py torch/_inductor/codegen/cuda/cutlass_utils.py torch/_inductor/codegen/cuda/gemm_template.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/memory_planning.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/codegen/rocm/ck_universal_gemm_template.py torch/_inductor/codegen/rocm/compile_command.py torch/_inductor/codegen/rocm/rocm_benchmark_request.py torch/_inductor/codegen/rocm/rocm_cpp_scheduling.py torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/codegen/rocm/rocm_template.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_split_scan.py torch/_inductor/codegen/wrapper.py torch/_inductor/comm_analysis.py torch/_inductor/comms.py torch/_inductor/compile_fx.py torch/_inductor/compile_worker/__main__.py torch/_inductor/compile_worker/subproc_pool.py torch/_inductor/config.py torch/_inductor/constant_folding.py torch/_inductor/cpp_builder.py torch/_inductor/cpu_vec_isa.py torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py torch/_inductor/debug.py torch/_inductor/decomposition.py torch/_inductor/dependencies.py torch/_inductor/exc.py torch/_inductor/freezing.py torch/_inductor/fx_passes/b2b_gemm.py torch/_inductor/fx_passes/binary_folding.py torch/_inductor/fx_passes/ddp_fusion.py torch/_inductor/fx_passes/decompose_mem_bound_mm.py torch/_inductor/fx_passes/efficient_conv_bn_eval.py torch/_inductor/fx_passes/freezing_patterns.py torch/_inductor/fx_passes/fuse_attention.py torch/_inductor/fx_passes/group_batch_fusion.py torch/_inductor/fx_passes/joint_graph.py torch/_inductor/fx_passes/micro_pipeline_tp.py torch/_inductor/fx_passes/misc_patterns.py torch/_inductor/fx_passes/mkldnn_fusion.py torch/_inductor/fx_passes/numeric_utils.py torch/_inductor/fx_passes/pad_mm.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/fx_passes/pre_grad.py torch/_inductor/fx_passes/quantization.py torch/_inductor/fx_passes/reinplace.py torch/_inductor/fx_passes/replace_random.py torch/_inductor/fx_passes/split_cat.py torch/_inductor/fx_utils.py torch/_inductor/graph.py torch/_inductor/hooks.py torch/_inductor/index_propagation.py torch/_inductor/inductor_prims.py torch/_inductor/ir.py torch/_inductor/jagged_lowerings.py torch/_inductor/kernel/bmm.py torch/_inductor/kernel/conv.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_common.py torch/_inductor/kernel/mm_plus_mm.py torch/_inductor/kernel/unpack_mixed_mm.py torch/_inductor/lowering.py torch/_inductor/metrics.py torch/_inductor/mkldnn_ir.py torch/_inductor/mkldnn_lowerings.py torch/_inductor/ops_handler.py torch/_inductor/optimize_indexing.py torch/_inductor/package/package.py torch/_inductor/pattern_matcher.py torch/_inductor/quantized_lowerings.py torch/_inductor/runtime/coordinate_descent_tuner.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py torch/_inductor/sizevars.py torch/_inductor/subgraph_lowering.py torch/_inductor/test_case.py torch/_inductor/test_operators.py torch/_inductor/utils.py torch/_inductor/virtualized.py torch/_inductor/wrapper_benchmark.py,https://github.com/pytorch/pytorch/pull/129768,XuehaiPan,jansel,,,
a7a951a4ae4,skip,not user facing,[executorch hash update] update the pinned executorch hash (#130001),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/130001,pytorchupdatebot,pytorchbot,,,
fccbe854759,cuda,improvements,[BE] Improve CUDA UpSample error message (#131252),aten/src/ATen/native/cuda/UpSampleBilinear2d.cu aten/src/ATen/native/cuda/UpSampleNearest1d.cu aten/src/ATen/native/cuda/UpSampleNearest2d.cu aten/src/ATen/native/cuda/UpSampleNearest3d.cu,https://github.com/pytorch/pytorch/pull/131252,malfet,albanD,,,
33f036a6f71,skip,not user facing,[inductor] Kill mark_node_as_mutating (#130834),test/inductor/test_perf.py torch/_inductor/ir.py torch/_inductor/mkldnn_ir.py,https://github.com/pytorch/pytorch/pull/130834,peterbell10,lezcano,,,
407c87a32cc,distributed,not user facing,[debug][dtensor] fixed updating current module (#130995),test/distributed/_composable/fsdp/test_fully_shard_training.py test/distributed/_tensor/debug/test_comm_mode_features.py test/distributed/_tensor/test_math_ops.py torch/distributed/_tensor/debug/comm_mode.py torch/distributed/_tensor/examples/comm_mode_features_example.py,https://github.com/pytorch/pytorch/pull/130995,sinhaanshul,XilunWu,,,
13283fb4bcf,distributed,not user facing,[distributed] test_store: remove flaky bind test (#131262),test/distributed/test_store.py,https://github.com/pytorch/pytorch/pull/131262,d4l3k,XilunWu,mori360,,
d57af32e635,skip,not user facing,Fix undefined tensor error in _copy_from_and_resize when fallback to cpu. (#130237),aten/src/ATen/native/CPUFallback.cpp aten/src/ATen/native/TensorConversions.cpp test/cpp_extensions/open_registration_extension.cpp test/test_cpp_extensions_open_device_registration.py,https://github.com/pytorch/pytorch/pull/130237,Shan19900305,ezyang,,,
1d1d074072e,skip,not user facing,[3/N] Fix Wunused-parameter warnings  (#131271),aten/src/ATen/EmptyTensor.cpp aten/src/ATen/FunctionalizeFallbackKernel.cpp aten/src/ATen/LegacyBatchingRegistrations.cpp aten/src/ATen/TensorMeta.h aten/src/ATen/core/jit_type.h aten/src/ATen/detail/MTIAHooksInterface.h aten/src/ATen/detail/PrivateUse1HooksInterface.h aten/src/ATen/native/TensorFactories.h c10/core/ConstantSymNodeImpl.h c10/core/SymBool.h c10/core/TensorImpl.h torch/library.h,https://github.com/pytorch/pytorch/pull/131271,cyyever,ezyang,,,
e506dfa6406,dynamo,Untopiced,[dynamo] Add a JK kill switch for disabling compile (#131258),test/dynamo/test_frame_init.py torch/_dynamo/eval_frame.py,https://github.com/pytorch/pytorch/pull/131258,chuanhaozhuge,c00w,,,
ddde9dd25cd,dynamo,Untopiced,[dynamo][automatic_dynamic] Trigger dynamism on stride changes (#130232),test/dynamo/test_repros.py test/test_dynamic_shapes.py torch/_dynamo/variables/builder.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/130232,anijain2305,ezyang,,,
1439bd3c9c0,skip,not user facing,[Easy][pytree] enable CXX pytree under `torch::deploy` (#130144),torch/utils/_cxx_pytree.py,https://github.com/pytorch/pytorch/pull/130144,XuehaiPan,zou3519,,,
f3562e2cdc6,skip,not user facing,backport dataclass(slots=True) (#131014),torch/utils/_backport_slots.py,https://github.com/pytorch/pytorch/pull/131014,aorenste,eellison,oulgen,,
ebce85172e6,skip,not user facing,FakeTensor cache SymInt support: flatten cache key (#129780),torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/129780,aorenste,eellison,oulgen,,
b193894b94e,fx,not user facing,FakeTensor cache SymInt support (#127596),test/dynamo/test_misc.py test/test_fake_tensor.py tools/pyi/gen_pyi.py torch/_C/__init__.pyi.in torch/_inductor/fx_passes/dedupe_symint_uses.py torch/_logging/_registrations.py torch/_prims_common/__init__.py torch/_subclasses/_fake_tensor_utils.py torch/_subclasses/fake_tensor.py torch/_subclasses/meta_utils.py torch/fx/experimental/proxy_tensor.py torch/fx/experimental/sym_node.py torch/fx/experimental/symbolic_shapes.py torch/types.py,https://github.com/pytorch/pytorch/pull/127596,aorenste,eellison,,,
f6288130664,onnx,Untopiced,"Fix out_wrapper, _make_copy_from_view to handle all signatures (#130937)",test/functorch/test_ops.py test/onnx/test_fx_op_consistency.py torch/_prims/context.py torch/_prims_common/wrappers.py torch/_refs/__init__.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/130937,rec,peterbell10,,,
500cbb5b901,autograd_frontend,not user facing,Add decomposition for view_copy (#130938),test/expect/HasDecompTest.test_aten_core_operators.expect test/expect/HasDecompTest.test_has_decomposition.expect test/test_mps.py test/test_proxy_tensor.py tools/autograd/gen_variable_type.py torch/_refs/__init__.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/130938,rec,peterbell10,,,
c2425a3b572,releng,not user facing,[BE] Use `_linux-build.yml` instead of `-linux-build-label.yml` flavor (#130762),.github/workflows/_linux-build.yml .github/workflows/periodic.yml .github/workflows/pull.yml .github/workflows/rocm.yml .github/workflows/slow.yml .github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/130762,malfet,Skylion007,atalman,jeanschmidt,
16148919463,profiler,bug fixes,[Profiler] exclude gpu_user_annotation when accumulating cuda time total (#130733),test/profiler/test_profiler.py torch/_C/_autograd.pyi torch/autograd/profiler.py torch/autograd/profiler_util.py torch/csrc/autograd/init.cpp,https://github.com/pytorch/pytorch/pull/130733,fwenguang,aaronenyeshi,,,
2820e1d9f8b,skip,not user facing,Update CPython support policy (#130989),RELEASE.md,https://github.com/pytorch/pytorch/pull/130989,albanD,seemethere,,,
8da19fec604,inductor,not user facing,[Inductor] Support store SPIR-V binary file output from Intel Triton. (#130849),torch/_inductor/codecache.py torch/_inductor/codegen/wrapper.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/130849,etaf,EikanWang,peterbell10,,
6d65a2c3f4a,skip,not user facing,[3/N] Non-Tensor: Support string parameter for aten operations (#125831),test/inductor/test_torchinductor.py torch/_inductor/aoti_eager.py torch/csrc/inductor/aoti_eager/kernel_holder.cpp torch/csrc/inductor/aoti_eager/kernel_meta_info.cpp torch/csrc/inductor/aoti_eager/kernel_meta_info.h,https://github.com/pytorch/pytorch/pull/125831,EikanWang,jansel,jgong5,,
68df24f9b67,releng,not user facing,[xla hash update] update the pinned xla hash (#126672),.github/ci_commit_pins/xla.txt,https://github.com/pytorch/pytorch/pull/126672,pytorchupdatebot,pytorchbot,,,
92bb323d36a,skip,not user facing,Added and_masks and or_masks utilities (#131073),docs/source/nn.attention.flex_attention.rst test/inductor/test_flex_attention.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/131073,Chillee,drisspg,,,
69e2590490e,skip,not user facing,Fix MKLDNN check in `test_aot_inductor.py` (#130982),test/inductor/test_aot_inductor.py,https://github.com/pytorch/pytorch/pull/130982,eqy,malfet,,,
3eb9fa5d581,releng,not user facing,Add support for using LF Canary runners (#131188),.github/scripts/runner_determinator.py .github/workflows/_runner-determinator.yml,https://github.com/pytorch/pytorch/pull/131188,zxiiro,ZainRizvi,,,
26383a6cc01,skip,Untopiced,"Revert ""Added and_masks and or_masks utilities (#131073)""",docs/source/nn.attention.flex_attention.rst test/inductor/test_flex_attention.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/flex_attention.py,,,,,,
5c78581fc91,python_frontend,docs,Fix documentation for tensor.repeat. (#131195),torch/_tensor_docs.py,https://github.com/pytorch/pytorch/pull/131195,m1guelperez,mikaylagawarecki,soulitzer,,
5b5e0698a5f,skip,not user facing,Add wrappers for synchronous GPUDirect Storage APIs (#130633),BUILD.bazel CMakeLists.txt build_variables.bzl caffe2/CMakeLists.txt cmake/Dependencies.cmake cmake/Modules/FindCUDAToolkit.cmake cmake/Summary.cmake cmake/public/cuda.cmake docs/source/cuda.rst setup.py test/test_cuda.py third_party/cuda.BUILD torch/CMakeLists.txt torch/_C/__init__.pyi.in torch/__init__.py torch/csrc/cuda/GdsFile.cpp torch/csrc/cuda/GdsFile.h torch/csrc/cuda/Module.cpp torch/cuda/__init__.py torch/cuda/gds.py,https://github.com/pytorch/pytorch/pull/130633,mikaylagawarecki,albanD,,,
0246b28510f,inductor,not user facing,[aoti] refactor aoti_torch__scaled_mm and skip aoti fp8 test for some cases (#130868),test/inductor/test_aot_inductor.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/130868,henrylhtsang,chenyang78,desertfire,drisspg,
32c2f84e349,dataloader_frontend,not user facing,Support IPC for Expandable Segments (#130890),c10/cuda/CUDACachingAllocator.cpp test/test_dataloader.py test/test_multiprocessing.py torch/csrc/StorageSharing.cpp,https://github.com/pytorch/pytorch/pull/130890,zdevito,dsjohns2,,,
b9912f31ef9,skip,Untopiced,"Revert ""[export] fix zero arg export in training_ir (#130990)""",test/export/test_export.py torch/_export/utils.py torch/export/_trace.py torch/export/_unlift.py torch/export/exported_program.py,,,,,,
d8a35d57220,skip,not user facing,"[TD] More synonyms, new heuristic for test_public_bindings (#130397)",tools/testing/target_determination/heuristics/__init__.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/public_bindings.py,https://github.com/pytorch/pytorch/pull/130397,clee2000,malfet,,,
56bb047449b,Uncategorized,Untopiced,[pt2] Increase dynamo/inductor default log level to info (#131311),torch/_logging/_internal.py,https://github.com/pytorch/pytorch/pull/131311,xw285cornell,oulgen,,,
44e689d9475,skip,Untopiced,"Revert ""[TD] More synonyms, new heuristic for test_public_bindings (#130397)""",tools/testing/target_determination/heuristics/__init__.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/public_bindings.py,,,,,,
4319147ca99,dynamo,not user facing,"[3.13, dynamo] fix closures, MAKE_FUNCTION, LOAD_CLOSURE; support SET_FUNCTION_ATTRIBUTE (#130566)",torch/_dynamo/codegen.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/130566,williamwen42,jansel,,,
1e116c7a1ec,dynamo,not user facing,"[3.13, dynamo] fix END_FOR (#130567)",torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/130567,williamwen42,jansel,,,
157f38bc4d4,dynamo,not user facing,"[3.13, dynamo] support STORE_FAST_LOAD_FAST (#130568)",torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/130568,williamwen42,jansel,,,
375a4d7e9ed,dynamo,not user facing,"[3.13, dynamo] decompose fused load/store instructions (#130569)",torch/_dynamo/bytecode_transformation.py torch/_dynamo/codegen.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/130569,williamwen42,jansel,,,
93ef2e53f88,dynamo,not user facing,"[3.13, dynamo] support FORMAT_SIMPLE/FORMAT_SPEC (#130751)",torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/130751,williamwen42,Skylion007,,,
d3556786b8d,python_frontend,improvements,Blocklist certain modules for weights_only load (#131259),test/test_serialization.py torch/_weights_only_unpickler.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/131259,mikaylagawarecki,albanD,malfet,,
23ae6e2eb3d,releng,Untopiced,[FSDP2] Removed state dict error for HSDP (#131320),.ci/pytorch/test.sh test/distributed/_composable/fsdp/test_fully_shard_state_dict.py torch/distributed/_composable/fsdp/_fsdp_param.py torch/distributed/_composable/fsdp/_fsdp_param_group.py,https://github.com/pytorch/pytorch/pull/131320,awgu,wanchaol,weifengpy,,
9e753d1f206,Uncategorized,Untopiced,[AMD] catch exception when other processes belong to other users (#131018),torch/cuda/memory.py,https://github.com/pytorch/pytorch/pull/131018,xw285cornell,clee2000,eqy,,
406f510f89d,distributed,Untopiced,[c10d] add bfloat16 support for NAN check (#131131),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/Utils.cu,https://github.com/pytorch/pytorch/pull/131131,shuqiangzhang,XilunWu,wconstab,,
781a33f5d89,releng,not user facing,Enable dynamic rollout for Linux trunk workflows (#131325),.github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/131325,zxiiro,ZainRizvi,,,
0c074352ab6,skip,not user facing,[c10d] remove non-necessary tests (#131212),test/distributed/test_c10d_common.py test/distributed/test_c10d_gloo.py test/distributed/test_c10d_nccl.py,https://github.com/pytorch/pytorch/pull/131212,wanchaol,XilunWu,,,
e3eaa221265,Uncategorized,Untopiced,[torchbench][multisect] Run accuracy check at Diff time (#131266),benchmarks/dynamo/torchbench.py,https://github.com/pytorch/pytorch/pull/131266,xuzhao9,oulgen,,,
83b355bad5b,skip,not user facing,"[aoti] forward fix of D60006838, add back test_multiple_output_alias (#131331) (#131356)",test/inductor/test_aot_inductor.py,https://github.com/pytorch/pytorch/pull/131356,clee2000,kit1980,malfet,,
0bf59db6cc0,skip,not user facing,Add sparse block to flex_decoding kernel (#130884),benchmarks/transformer/score_mod.py test/inductor/test_flex_decoding.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/130884,joydddd,drisspg,,,
a3922acc069,skip,not user facing,"[TD] More synonyms, new heuristic for test_public_bindings (#130397)",tools/testing/target_determination/heuristics/__init__.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/public_bindings.py,https://github.com/pytorch/pytorch/pull/130397,clee2000,malfet,,,
a136a7d623c,distributed,Untopiced,[Functional Collective] enable custom work registration from python (#130354),test/distributed/test_c10d_functional_native.py torch/csrc/distributed/c10d/Functional.cpp torch/csrc/distributed/c10d/Functional.hpp torch/csrc/distributed/c10d/PyProcessGroup.hpp torch/csrc/distributed/c10d/init.cpp,https://github.com/pytorch/pytorch/pull/130354,yifuwang,fegin,wanchaol,wconstab,
d33804f8b6e,skip,Untopiced,"Replace manual parsing of ""TMPDIR"", ""TMP"", ""TEMP"" and ""TEMPDIR"" with std::filesystem::temp_directory_path() (#130842)",c10/util/tempfile.cpp torch/csrc/distributed/c10d/CUDASymmetricMemory.cu,https://github.com/pytorch/pytorch/pull/130842,yifuwang,fegin,,,
f8875e8277e,skip,Untopiced,"Revert ""[inductor] Kill mark_node_as_mutating (#130834)""",test/inductor/test_perf.py torch/_inductor/ir.py torch/_inductor/mkldnn_ir.py,,,,,,
15eb10df020,skip,Untopiced,"Revert ""[inductor] Use multiple outputs for flex-attention (#130833)""",test/inductor/test_flex_attention.py torch/_inductor/ir.py,,,,,,
f8f41dcb24c,skip,Untopiced,"Revert ""[inductor] Make UserDefinedTritonKernel a multi-output operation (#130832)""",torch/_inductor/ir.py,,,,,,
c74396e8904,skip,Untopiced,"Revert ""[c10d] remove non-necessary tests (#131212)""",test/distributed/test_c10d_common.py test/distributed/test_c10d_gloo.py test/distributed/test_c10d_nccl.py,,,,,,
8ae1963a61b,skip,Untopiced,[Autograd] Cond Higher-Order Operation (#126911),test/export/test_converter.py test/functorch/test_control_flow.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/map.py torch/_higher_order_ops/utils.py torch/fx/experimental/proxy_tensor.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/126911,bohnstingl,ydwu4,,,
69b1999586e,linalg_frontend,Untopiced,TunableOp size hotfix (#130800),aten/src/ATen/cuda/tunable/GemmCommon.h test/test_linalg.py,https://github.com/pytorch/pytorch/pull/130800,jeffdaily,xw285cornell,,,
26f7dd286b7,Uncategorized,Untopiced,[export] Allow non-CIA ops to be preserved (#131075),test/export/test_export.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/131075,angelayi,bdhirsh,,,
1e5ecc4277b,Uncategorized,Untopiced,move save/load from _export to export (#131353),torch/_export/__init__.py torch/export/__init__.py,https://github.com/pytorch/pytorch/pull/131353,avikchaudhuri,angelayi,,,
074b4206419,skip,Untopiced,[custom ops] Add register_vmap for custom ops (#130589),docs/source/library.rst test/functorch/common_utils.py test/functorch/test_eager_transforms.py test/functorch/test_vmap.py test/test_custom_ops.py torch/_functorch/autograd_function.py torch/_library/custom_ops.py torch/library.py torch/testing/_internal/custom_op_db.py,https://github.com/pytorch/pytorch/pull/130589,yushangdi,zou3519,,,
8963623494d,skip,Untopiced,Re-implement pin_memory to be device-agnostic by leveraging the Accelerator concept (#126376),aten/src/ATen/Context.h aten/src/ATen/DeviceAccelerator.cpp aten/src/ATen/DeviceAccelerator.h aten/src/ATen/cuda/PinnedMemoryAllocator.cpp aten/src/ATen/detail/AcceleratorHooksInterface.h aten/src/ATen/detail/CUDAHooksInterface.h aten/src/ATen/detail/HIPHooksInterface.h aten/src/ATen/detail/MPSHooksInterface.h aten/src/ATen/detail/MTIAHooksInterface.h aten/src/ATen/detail/PrivateUse1HooksInterface.h aten/src/ATen/detail/XPUHooksInterface.h aten/src/ATen/mps/MPSAllocator.mm aten/src/ATen/mps/MPSAllocatorInterface.h aten/src/ATen/mps/MPSGuardImpl.h aten/src/ATen/mps/MPSHooks.h aten/src/ATen/mps/MPSHooks.mm aten/src/ATen/native/Memory.cpp aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/sparse/SparseUnaryOps.cpp aten/src/ATen/templates/RegisterBackendSelect.cpp test/cpp_extensions/open_registration_extension.cpp test/test_cpp_extensions_open_device_registration.py test/test_cpp_extensions_stream_and_event.py test/test_torch.py torch/csrc/Stream.cpp,https://github.com/pytorch/pytorch/pull/126376,wizzniu,albanD,,,
b435d842611,skip,Untopiced,"Revert ""[custom ops] Add register_vmap for custom ops (#130589)""",docs/source/library.rst test/functorch/common_utils.py test/functorch/test_eager_transforms.py test/functorch/test_vmap.py test/test_custom_ops.py torch/_functorch/autograd_function.py torch/_library/custom_ops.py torch/library.py torch/testing/_internal/custom_op_db.py,,,,,,
28b0ad4f461,dynamo,Untopiced,[PT2] Minor fix in signpost (#131332),torch/_dynamo/convert_frame.py,https://github.com/pytorch/pytorch/pull/131332,atuljangra,oulgen,,,
6cbb1437c1c,skip,Untopiced,"Revert ""Add sparse block to flex_decoding kernel (#130884)""",benchmarks/transformer/score_mod.py test/inductor/test_flex_decoding.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/nn/attention/flex_attention.py,,,,,,
8a591da3e71,releng,not user facing,[CI] Enable AOT inductor in cpu performance smoke test (#130097),.ci/pytorch/test.sh benchmarks/dynamo/expected_ci_speedup_inductor_torchbench_cpu.csv,https://github.com/pytorch/pytorch/pull/130097,zxd1997066,chuanqi129,desertfire,,
12434504a27,skip,not user facing,[c10d] remove non-necessary tests (#131212),test/distributed/test_c10d_common.py test/distributed/test_c10d_gloo.py test/distributed/test_c10d_nccl.py test/distributed/test_c10d_ucc.py,https://github.com/pytorch/pytorch/pull/131212,wanchaol,XilunWu,,,
35a0e0f018b,distributed,Untopiced,[tp] improve SequenceParallel and its documentation (#131346),test/distributed/tensor/parallel/test_tp_style.py torch/distributed/tensor/parallel/style.py,https://github.com/pytorch/pytorch/pull/131346,wanchaol,awgu,,,
02cd4dbcf42,skip,not user facing,[BE][CI] Get rid of duplicated code (#131406),aten/tools/run_tests.sh,https://github.com/pytorch/pytorch/pull/131406,malfet,atalman,kit1980,,
781189f25d3,python_frontend,bug fixes,Add `nvjitlink` to the list of loadable global deps (#131295),torch/__init__.py,https://github.com/pytorch/pytorch/pull/131295,nikonikolov,malfet,,,
99d9b369f45,optim,Untopiced,[Optim] Support tensor lr for all optimizers and check it is 1-element (#131065),torch/optim/adadelta.py torch/optim/adagrad.py torch/optim/adam.py torch/optim/adamax.py torch/optim/adamw.py torch/optim/asgd.py torch/optim/lbfgs.py torch/optim/nadam.py torch/optim/radam.py torch/optim/rmsprop.py torch/optim/rprop.py torch/optim/sgd.py torch/optim/sparse_adam.py torch/testing/_internal/common_optimizers.py,https://github.com/pytorch/pytorch/pull/131065,qqaatw,janeyx99,,,
b2ad16f01d6,inductor,Untopiced,avoid OpOverloadPacket.__getattr__ calls in inductor lowering (#131348),torch/_inductor/graph.py,https://github.com/pytorch/pytorch/pull/131348,bdhirsh,davidberard98,,,
29e2e2afb69,quantization,Untopiced,"Revert D59561509: Multisect successfully blamed ""D59561509: [FX][export] DCE pass, check schema for node impurity (#130395)"" for one test failure (#131341)",test/fx/test_dce_pass.py test/quantization/pt2e/test_x86inductor_quantizer.py torch/export/_remove_effect_tokens_pass.py torch/export/_trace.py torch/export/_unlift.py torch/fx/node.py,https://github.com/pytorch/pytorch/pull/131341,yushangdi,angelayi,,,
102d8e5a635,mps,Untopiced,MPS LSTM backward kernel workaround on MacOS 14.4+ (#130038),aten/src/ATen/mps/MPSDevice.h aten/src/ATen/mps/MPSDevice.mm aten/src/ATen/native/mps/operations/RnnOps.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/130038,jhavukainen,malfet,,,
16a2a1aad3e,inductor,not user facing,Annotate graph.py (#131400),torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_utils.py torch/_inductor/codegen/wrapper.py torch/_inductor/dependencies.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/metrics.py torch/_inductor/select_algorithm.py torch/_inductor/sizevars.py,https://github.com/pytorch/pytorch/pull/131400,eellison,shunting314,,,
16247987a18,distributed,not user facing,Add decomposition for t_copy (#130939),test/distributed/_tensor/test_dtensor_ops.py test/expect/HasDecompTest.test_has_decomposition.expect test/functorch/test_ops.py test/functorch/test_vmap.py test/test_mps.py tools/autograd/gen_variable_type.py torch/_decomp/__init__.py torch/_refs/__init__.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/130939,rec,peterbell10,,,
d8e2e1fe50f,skip,not user facing,[aoti] use reshape instead of view for flattening tensors for the nan checker (#131302),torch/csrc/inductor/aoti_torch/utils.h,https://github.com/pytorch/pytorch/pull/131302,chenyang78,desertfire,muchulee8,,
3f3b226ffc8,skip,not user facing,Fixes for the extension backend tests (#130933),test/inductor/extension_backends/cpp/extension_codegen_backend.py test/inductor/extension_backends/triton/device_interface.py test/inductor/extension_backends/triton/extension_codegen_backend.py,https://github.com/pytorch/pytorch/pull/130933,charlie-wt,eellison,jansel,,
9851c7313d1,skip,Untopiced,[CI][dashboard] Collect PT2 cpu perf nightly (#131369),.ci/pytorch/test.sh .github/workflows/inductor-perf-test-nightly-x86.yml .github/workflows/upload-torch-dynamo-perf-stats.yml,https://github.com/pytorch/pytorch/pull/131369,desertfire,huydhn,,,
eb146b10dba,releng,not user facing,Only depend on sympy 1.12 for conda (no 3.13 there anyways) (#131355),.circleci/scripts/binary_linux_test.sh requirements.txt,https://github.com/pytorch/pytorch/pull/131355,albanD,atalman,,,
027f35d9e6d,inductor,not user facing,[Inductor] Allow customize decompositions for fwd_only trace function (#131329),torch/_inductor/pattern_matcher.py,https://github.com/pytorch/pytorch/pull/131329,arui-meta,eellison,,,
4ac77fc6bd5,skip,not user facing,[HOP] Don't send HOPs to torch_dispatch (#131370),test/dynamo/test_higher_order_ops.py torch/_ops.py,https://github.com/pytorch/pytorch/pull/131370,zou3519,ydwu4,,,
250cdb2ac76,cuda,not user facing,Fix cuda_half_test.cu (#131416),aten/src/ATen/test/cuda_half_test.cu,https://github.com/pytorch/pytorch/pull/131416,malfet,albanD,atalman,,
a944cce5b86,dynamo,not user facing,[dynamo] Support if callable on list (#131347),test/dynamo/test_functions.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/131347,anijain2305,mlazos,williamwen42,,
0bc5e26067b,dynamo,not user facing,[dynamo] Support dict conversion of objects derived from MutableMapping (#131367),test/dynamo/test_functions.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/131367,anijain2305,williamwen42,,,
e7c5e06772a,dynamo,not user facing,[dynamo] Support __contains__ on  __dict__ on UserDefinedClassVariable (#131378),test/dynamo/test_functions.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/131378,anijain2305,mlazos,,,
6bbef2a06b5,dynamo,not user facing,[dynamo] Support set on KeysView (#131389),test/dynamo/test_functions.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/131389,anijain2305,mlazos,,,
f064dac588e,releng,not user facing,[CI] change xpu ci build runner type to reduce build time (#130922),.github/workflows/xpu.yml,https://github.com/pytorch/pytorch/pull/130922,chuanqi129,atalman,,,
1e863878719,skip,Untopiced,"Revert ""Support IPC for Expandable Segments (#130890)""",c10/cuda/CUDACachingAllocator.cpp test/test_dataloader.py test/test_multiprocessing.py torch/csrc/StorageSharing.cpp,,,,,,
b61600f6cc5,distributed,not user facing,[pytorch] fix the leak for pinned memory when using _create_cpu_state… (#131270),torch/distributed/_state_dict_utils.py,https://github.com/pytorch/pytorch/pull/131270,teja-rao,LucasLLC,,,
3aa45cae775,Uncategorized,Untopiced,[export] Removed deprecated dialect field from EP schema. [2/2] (#131344),torch/_export/serde/schema.py torch/_export/serde/schema.yaml,https://github.com/pytorch/pytorch/pull/131344,zhxchen17,SherlockNoMad,ydwu4,,
154f27455a6,skip,Untopiced,[triton_op] fix autotuning (#131363),test/inductor/test_triton_kernels.py torch/_higher_order_ops/triton_kernel_wrap.py torch/fx/node.py,https://github.com/pytorch/pytorch/pull/131363,zou3519,justinchuby,oulgen,,
4f0497c747f,inductor,not user facing,Divorce triton and pt2 remote caching (#131345),test/inductor/test_codecache.py test/inductor/test_max_autotune.py torch/_inductor/codecache.py torch/_inductor/compile_fx.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/131345,oulgen,aorenste,,,
db376fb6434,inductor,not user facing,Ensure non-contiguous indices are handled (#131430),test/inductor/test_cuda_repro.py torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/131430,mlazos,anijain2305,,,
c1ef2140463,Uncategorized,Untopiced,Print ExportedProgram without color by default (#131399),torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/131399,SherlockNoMad,angelayi,,,
a5ad02d05da,releng,not user facing,Remove MacOS M2 14 runner from MacMPS job (#131465),.github/workflows/mac-mps.yml,https://github.com/pytorch/pytorch/pull/131465,malfet,atalman,seemethere,,
ded5bdb0de0,distributed,not user facing,Use inductor TestCase for test_replicate_with_compiler.py (#131053),test/distributed/_composable/test_replicate_with_compiler.py,https://github.com/pytorch/pytorch/pull/131053,masnesral,eellison,,,
4ca8705035f,fx,Untopiced,Add mypy typing to fx node (#131434),test/expect/TestFXAPIBackwardCompatibility.test_function_back_compat-fx_backcompat_function_signatures.expect torch/_C/__init__.pyi.in torch/fx/node.py,https://github.com/pytorch/pytorch/pull/131434,oulgen,zou3519,,,
5f0b65bee72,skip,Untopiced,"Revert ""Replace manual parsing of ""TMPDIR"", ""TMP"", ""TEMP"" and ""TEMPDIR"" with std::filesystem::temp_directory_path() (#130842)""",c10/util/tempfile.cpp torch/csrc/distributed/c10d/CUDASymmetricMemory.cu,,,,,,
f7754c6dc52,releng,not user facing,Run pull jobs with new AMI (#131250),.github/workflows/pull.yml .github/workflows/slow.yml .github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/131250,ZainRizvi,atalman,malfet,,
e4b5645f831,skip,Untopiced,"Revert ""Add wrappers for synchronous GPUDirect Storage APIs (#130633)""",BUILD.bazel CMakeLists.txt build_variables.bzl caffe2/CMakeLists.txt cmake/Dependencies.cmake cmake/Modules/FindCUDAToolkit.cmake cmake/Summary.cmake cmake/public/cuda.cmake docs/source/cuda.rst setup.py test/test_cuda.py third_party/cuda.BUILD torch/CMakeLists.txt torch/_C/__init__.pyi.in torch/__init__.py torch/csrc/cuda/GdsFile.cpp torch/csrc/cuda/GdsFile.h torch/csrc/cuda/Module.cpp torch/cuda/__init__.py torch/cuda/gds.py,,,,,,
eab1595ce26,dynamo,Untopiced,[dynamo] Delete wrong assertion in bind_args (#131405),test/dynamo_expected_failures/TestPythonDispatch.test_make_subclass_with_modes torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/131405,anijain2305,williamwen42,yanboliang,,
f93a6a4d31f,skip,not user facing,Add mypy typing to torch_version.py (#131447),torch/torch_version.py,https://github.com/pytorch/pytorch/pull/131447,oulgen,angelayi,,,
979429ca892,inductor,Untopiced,[inductor]Add DtypeView to avoid memory leak and unnecessary kernel generations (#128883),test/distributed/test_c10d_functional_native.py test/inductor/test_cuda_cpp_wrapper.py test/inductor/test_torchinductor.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/wrapper.py torch/_inductor/ir.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/128883,FindHao,eellison,,,
404d640c394,skip,Untopiced,[1/2] PT2 Inductor ComboKernels - Foreach cases  (#124969),test/inductor/test_foreach.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cuda.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/codegen/triton_foreach.py torch/_inductor/codegen/wrapper.py torch/_inductor/config.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/124969,qchip,mlazos,,,
68c725a0940,skip,Untopiced,[custom ops] Add register_vmap for custom ops (#130589),docs/source/library.rst test/functorch/common_utils.py test/functorch/test_eager_transforms.py test/functorch/test_vmap.py test/test_custom_ops.py torch/_functorch/autograd_function.py torch/_library/custom_ops.py torch/library.py torch/testing/_internal/custom_op_db.py,https://github.com/pytorch/pytorch/pull/130589,yushangdi,zou3519,,,
3fe72e0c2e8,inductor,not user facing,"[4/N] Non-Tensor: Support layout, device and dtype for aten operations (#125897)",test/inductor/test_torchinductor.py torch/_inductor/aoti_eager.py torch/csrc/inductor/aoti_eager/kernel_holder.cpp torch/csrc/inductor/aoti_eager/kernel_meta_info.cpp torch/csrc/inductor/aoti_eager/kernel_meta_info.h,https://github.com/pytorch/pytorch/pull/125897,EikanWang,jansel,jgong5,,
6b8ec2b3716,skip,Untopiced,"Revert ""[triton_op] fix autotuning (#131363)""",test/inductor/test_triton_kernels.py torch/_higher_order_ops/triton_kernel_wrap.py torch/fx/node.py,,,,,,
cfb9ccab6c3,dynamo,Untopiced,"[export] Filter errors by exception type, add case name (#131327)",torch/_dynamo/exc.py torch/_dynamo/variables/builder.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/131327,yushangdi,zhxchen17,,,
22906be8f04,sparse_frontend,improvements,Do not abort on SPARSE_STATUS_INVALID_VALUE (#130382),aten/src/ATen/mkl/Exceptions.h aten/src/ATen/mkl/SparseBlas.cpp aten/src/ATen/mkl/SparseBlas.h aten/src/ATen/native/mkl/SparseBlasImpl.cpp,https://github.com/pytorch/pytorch/pull/130382,MatzeB,malfet,,,
f85c35872bc,Uncategorized,Untopiced,Remove GraphModuleOpUpgrader in _export.serde.upgrade.py (#131373),torch/_export/serde/upgrade.py,https://github.com/pytorch/pytorch/pull/131373,yushangdi,angelayi,,,
94f22eb6b24,Uncategorized,Untopiced,refactor post-trace fakification in strict (#131421),torch/_export/__init__.py torch/_export/non_strict_utils.py torch/export/_trace.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/131421,avikchaudhuri,zhxchen17,,,
aa54bcb6d25,skip,Untopiced,[CI] Relax config name matching for cpu inductor tests (#131467),.ci/pytorch/test.sh,https://github.com/pytorch/pytorch/pull/131467,desertfire,zou3519,,,
f75d7244826,dynamo,not user facing,Updating Types in torch/_dynamo/utils.py (#131001),torch/_dynamo/output_graph.py torch/_dynamo/utils.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/131001,adriaorenstein,aorenste,,,
120ca23a1fa,skip,not user facing,Fix IMAs in Flash-Attention splitkv kernel (#131277),aten/src/ATen/native/transformers/cuda/flash_attn/flash_api.cpp aten/src/ATen/native/transformers/cuda/flash_attn/flash_fwd_kernel.h,https://github.com/pytorch/pytorch/pull/131277,drisspg,malfet,,,
faddb0f30c4,nested tensor_frontend,not user facing,[NestedTensor] Integrate the mean operator along the jagged dimension into NestedTensor (#131132),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/131132,jananisriram,davidberard98,jbschlosser,,
93fdd0237dc,skip,Untopiced,Ensure staticmethods can be allowed in graph (#130882),test/dynamo/test_repros.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/130882,mlazos,anijain2305,williamwen42,,
7b82ed2d59e,releng,not user facing,Delete very old misleading info from .ci README (#131502),.ci/pytorch/README.md,https://github.com/pytorch/pytorch/pull/131502,kit1980,atalman,seemethere,,
d4e3fd613ce,skip,Untopiced,"Revert ""[CI] Relax config name matching for cpu inductor tests (#131467)""",.ci/pytorch/test.sh,,,,,,
84979307668,skip,Untopiced,"Revert ""[CI][dashboard] Collect PT2 cpu perf nightly (#131369)""",.ci/pytorch/test.sh .github/workflows/inductor-perf-test-nightly-x86.yml .github/workflows/upload-torch-dynamo-perf-stats.yml,,,,,,
4eee2e7a6df,Uncategorized,Untopiced,[operator_benchmark] Remove TARGETS from broken benchmarks (#131460),benchmarks/operator_benchmark/benchmark_pytorch.py,https://github.com/pytorch/pytorch/pull/131460,xuzhao9,vmpuri,,,
8a890b72dc3,skip,Untopiced,[BE] Get rid of missing destructor override warning (#131204),aten/src/ATen/detail/HIPHooksInterface.h,https://github.com/pytorch/pytorch/pull/131204,malfet,albanD,seemethere,,
c9e74449f31,releng,not user facing,bump executorch commit pin. (#131486),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/131486,zhxchen17,huydhn,,,
e3ca4e79e16,inductor,Untopiced,Fix mypy errors introduced by #131400 (#131522),torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/dependencies.py,https://github.com/pytorch/pytorch/pull/131522,aorenste,eellison,zou3519,,
5a0068cc692,quantization,not user facing,[BE] mypy: disallow untyped decorators (#131428),mypy.ini test/test_torch.py torch/_decomp/decompositions.py torch/_decomp/decompositions_for_jvp.py torch/_decomp/decompositions_for_rng.py torch/_dynamo/convert_frame.py torch/_dynamo/output_graph.py torch/_dynamo/variables/script_object.py torch/_export/__init__.py torch/_functorch/apis.py torch/_functorch/batch_norm_replacement.py torch/_functorch/functional_call.py torch/_higher_order_ops/auto_functionalize.py torch/_higher_order_ops/effects.py torch/_higher_order_ops/flex_attention.py torch/_higher_order_ops/out_dtype.py torch/_inductor/codecache.py torch/_inductor/compile_fx.py torch/_inductor/decomposition.py torch/_inductor/freezing.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/fx_passes/quantization.py torch/_inductor/jagged_lowerings.py torch/_inductor/kernel/conv.py torch/_inductor/lowering.py torch/_inductor/mkldnn_lowerings.py torch/_inductor/pattern_matcher.py torch/_inductor/runtime/triton_helpers.py torch/_inductor/scheduler.py torch/_library/custom_ops.py torch/_library/infer_schema.py torch/_meta_registrations.py torch/_refs/__init__.py torch/_refs/_conversions.py torch/_refs/fft.py torch/_refs/linalg/__init__.py torch/_refs/nn/functional/__init__.py torch/_refs/special/__init__.py torch/_subclasses/fake_tensor.py torch/ao/nn/quantized/dynamic/modules/rnn.py torch/ao/nn/quantized/modules/embedding_ops.py torch/ao/nn/quantized/modules/linear.py torch/ao/nn/sparse/quantized/linear.py torch/ao/ns/fx/utils.py torch/ao/quantization/experimental/adaround_fake_quantize.py torch/ao/quantization/fake_quantize.py torch/ao/quantization/fx/_decomposed.py torch/ao/quantization/observer.py torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py torch/backends/_nnapi/prepare.py torch/distributed/_composable/checkpoint_activation.py torch/distributed/_composable/fsdp/_fsdp_collectives.py torch/distributed/_composable/fsdp/_fsdp_state.py torch/distributed/_composable/fsdp/fully_shard.py torch/distributed/_composable/fully_shard.py torch/distributed/_composable/replicate.py torch/distributed/_spmd/experimental_ops.py torch/distributed/_symmetric_memory/__init__.py torch/distributed/_tensor/api.py torch/distributed/_tensor/ops/conv_ops.py torch/distributed/_tensor/ops/embedding_ops.py torch/distributed/_tensor/ops/experimental_ops.py torch/distributed/_tensor/ops/math_ops.py torch/distributed/_tensor/ops/matrix_ops.py torch/distributed/_tensor/ops/random_ops.py torch/distributed/_tensor/ops/tensor_ops.py torch/distributed/_tensor/ops/view_ops.py torch/distributed/checkpoint/state_dict_loader.py torch/distributed/checkpoint/state_dict_saver.py torch/distributed/optim/optimizer.py torch/distributed/rpc/api.py torch/export/_trace.py torch/export/exported_program.py torch/fx/_lazy_graph_module.py torch/fx/_symbolic_trace.py torch/fx/experimental/graph_gradual_typechecker.py torch/fx/experimental/migrate_gradual_types/constraint_generator.py torch/fx/experimental/proxy_tensor.py torch/fx/experimental/rewriter.py torch/fx/graph.py torch/fx/graph_module.py torch/fx/interpreter.py torch/fx/node.py torch/fx/operator_schemas.py torch/fx/passes/graph_manipulation.py torch/fx/passes/infra/pass_manager.py torch/fx/passes/operator_support.py torch/fx/passes/param_fetch.py torch/fx/passes/runtime_assert.py torch/fx/passes/split_module.py torch/fx/passes/split_utils.py torch/fx/passes/splitter_base.py torch/fx/passes/tools_common.py torch/fx/passes/utils/common.py torch/fx/passes/utils/fuser_utils.py torch/fx/passes/utils/source_matcher_utils.py torch/fx/subgraph_rewriter.py torch/fx/traceback.py torch/jit/_decompositions.py torch/masked/_ops.py torch/nn/attention/flex_attention.py torch/nn/modules/container.py torch/nn/modules/rnn.py torch/nn/utils/clip_grad.py torch/nn/utils/parametrize.py torch/onnx/_internal/onnxruntime.py torch/onnx/symbolic_caffe2.py torch/onnx/symbolic_helper.py torch/onnx/symbolic_opset10.py torch/onnx/symbolic_opset11.py torch/onnx/symbolic_opset12.py torch/onnx/symbolic_opset13.py torch/onnx/symbolic_opset14.py torch/onnx/symbolic_opset16.py torch/onnx/symbolic_opset17.py torch/onnx/symbolic_opset18.py torch/onnx/symbolic_opset20.py torch/onnx/symbolic_opset8.py torch/onnx/symbolic_opset9.py torch/optim/adadelta.py torch/optim/adam.py torch/optim/adamax.py torch/optim/adamw.py torch/optim/asgd.py torch/optim/nadam.py torch/optim/optimizer.py torch/optim/radam.py torch/optim/rmsprop.py torch/optim/rprop.py torch/signal/windows/windows.py torch/sparse/_triton_ops.py torch/testing/_internal/custom_op_db.py torch/utils/_freeze.py torch/utils/checkpoint.py torch/utils/flop_counter.py,https://github.com/pytorch/pytorch/pull/131428,aorenste,justinchuby,oulgen,,
53f1f750617,inductor,not user facing,[BE][Inductor] fix do_bench test (#131402),test/inductor/test_inductor_utils.py torch/_dynamo/test_case.py,https://github.com/pytorch/pytorch/pull/131402,shunting314,eellison,,,
980bb543616,inductor,not user facing,[BE][Inductor] fix failures in test_padding.py (#131417),test/inductor/test_padding.py,https://github.com/pytorch/pytorch/pull/131417,shunting314,eellison,,,
fc3d2b26cd5,distributed,not user facing,Use fake PG for test_compute_comm_reordering.py unit tests (#131415),test/distributed/test_compute_comm_reordering.py torch/testing/_internal/common_distributed.py,https://github.com/pytorch/pytorch/pull/131415,yf225,yifuwang,,,
1930698140f,skip,not user facing,Fix fake tensor SymInt caching when there's a SymInt storage_offset (#131500),torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/131500,aorenste,clee2000,,,
161c18ed0bc,distributed,Untopiced,"SymmetricMemory-based, low contention intra-node all-gather and reduce-scatter (#130583)",test/distributed/test_symmetric_memory.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/CUDASymmetricMemory.cu torch/distributed/_symmetric_memory/__init__.py,https://github.com/pytorch/pytorch/pull/130583,yifuwang,weifengpy,,,
7b375c3682e,distributed,not user facing,[dtensor][debug] changed which module tracker I inherited from to fix bug with activation checkpointing (#131419),torch/distributed/_tensor/debug/comm_mode.py,https://github.com/pytorch/pytorch/pull/131419,sinhaanshul,XilunWu,,,
e39f136c355,distributed,not user facing,[debug][dtensor] implemented activation checkpointing differentiation (#130996),torch/distributed/_tensor/debug/comm_mode.py torch/distributed/_tensor/examples/comm_mode_features_example.py torch/distributed/_tensor/examples/comm_mode_features_example_argparser.py,https://github.com/pytorch/pytorch/pull/130996,sinhaanshul,XilunWu,,,
e2b941a1b4c,dynamo,not user facing,[dynamo] Rename TENSOR_ALIASING to OBJECT_ALIASING. Permit OBJECT_ALIASING for dict guards (#131480),test/dynamo/test_functions.py test/dynamo/test_guard_manager.py torch/_C/_dynamo/guards.pyi torch/_dynamo/guards.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/131480,anijain2305,williamwen42,,,
6850e422662,dynamo,not user facing,[dynamo][exception] Remove older specialization for StopIteration (#131512),test/dynamo/test_exceptions.py torch/_dynamo/exc.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/131512,anijain2305,yanboliang,,,
480ae51f859,skip,not user facing,[pytree] Only import optree if it's used (#131478),test/test_pytree.py torch/utils/_cxx_pytree.py torch/utils/_pytree.py,https://github.com/pytorch/pytorch/pull/131478,zou3519,albanD,,,
2cf220956a3,inductor,Untopiced,[inductor] fix CacheBase.get_system on AMD (#131365),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/131365,nmacchioni,eellison,,,
fdc9a1404ee,Uncategorized,Untopiced,Remove _BLACK_LISTED_OPS (#131361),torch/_export/passes/replace_view_ops_with_view_copy_ops_pass.py,https://github.com/pytorch/pytorch/pull/131361,yushangdi,angelayi,,,
76f7b3e5600,inductor,not user facing,[inductor][cpp][gemm] improve thread blocking heuristics (#131024),torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_prefix.h,https://github.com/pytorch/pytorch/pull/131024,jgong5,chunyuan-w,leslie-fang-intel,,
14495ce2887,mps,improvements,[BE][MPS] Use `isOperatingSystemAtLeastVersion:` (#131513),aten/src/ATen/mps/MPSDevice.mm,https://github.com/pytorch/pytorch/pull/131513,malfet,atalman,,,
466c167b71e,fx,not user facing,Fix py codegen to delete values that don't have any  users (#131028),test/dynamo/test_autograd_function.py test/dynamo/test_comptime.py test/dynamo/test_ctx_manager.py test/dynamo/test_export.py test/dynamo/test_higher_order_ops.py test/dynamo/test_input_attr_tracking.py test/dynamo/test_misc.py test/dynamo/test_repros.py test/export/test_export.py test/export/test_passes.py test/functorch/test_aotdispatch.py test/functorch/test_control_flow.py test/functorch/test_eager_transforms.py test/higher_order_ops/test_with_effects.py test/test_functionalization.py test/test_fx.py test/test_fx_reinplace_pass.py test/test_proxy_tensor.py torch/fx/graph.py,https://github.com/pytorch/pytorch/pull/131028,YangQun1,ezyang,,,
aa1c78c7e99,distributed,Untopiced,[PTD][c10d][EZ] LOG error for nccl error rather than info (#131483),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/131483,fduwjj,fegin,,,
abb313b466c,skip,not user facing,[torch.mtia] Noop set_rng_state and get_rng_state APIs (#130873),docs/source/mtia.rst torch/mtia/__init__.py,https://github.com/pytorch/pytorch/pull/130873,egienvalue,hanzlfs,,,
0e780a7d699,dynamo,Untopiced,[BE] Remove some mypy allow-untyped-decorators that are no longer needed (#131564),torch/_dynamo/output_graph.py torch/_inductor/codecache.py torch/_inductor/freezing.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/131564,aorenste,oulgen,,,
0ceaabaf71b,skip,not user facing,[easy][inline-inbuilt-nn-modules] Update test (#131563),test/dynamo/test_structured_trace.py,https://github.com/pytorch/pytorch/pull/131563,anijain2305,mlazos,,,
31da9ee711a,Uncategorized,Untopiced,Use explain function to provide more meaningful information when conversion failed. (#131214),torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/131214,jiashenC,angelayi,,,
7124efa81b6,skip,not user facing,Include _native.h for structured_native_functions (#131208),torchgen/gen.py,https://github.com/pytorch/pytorch/pull/131208,nicholasw-gc,bdhirsh,,,
dffbd3a1e2e,inductor,not user facing,Add mypy typing to pattern_matcher (#131506),torch/_inductor/pattern_matcher.py,https://github.com/pytorch/pytorch/pull/131506,oulgen,zou3519,,,
9575b1afad8,dynamo,Untopiced,Ensure tensor dict is populated with compiled autograd (#131556),torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/131556,mlazos,anijain2305,,,
9f96d4b61b1,skip,not user facing,Disable inlining on cudagraph fallback tests (#131557),test/inductor/test_cudagraph_trees.py,https://github.com/pytorch/pytorch/pull/131557,mlazos,anijain2305,,,
5772c13f56f,inductor,not user facing,Dont wrap negative indexing in scatter reduce (#131503),test/inductor/test_cuda_repro.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/halide.py torch/_inductor/dependencies.py torch/_inductor/index_propagation.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/ops_handler.py torch/_inductor/select_algorithm.py torch/_inductor/virtualized.py,https://github.com/pytorch/pytorch/pull/131503,eellison,shunting314,,,
eafbd20f237,skip,not user facing,Annotate all InstructionTranslator (#131509),torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/distributed.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/higher_order_ops.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/optimizer.py torch/_dynamo/variables/sdpa.py torch/_dynamo/variables/tensor.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/torch_function.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/131509,oulgen,zou3519,,,
e9db1b05973,skip,not user facing,Add flag to ignore unsupported @triton.autotune args in user-written kernel compilation (#131431),test/inductor/test_triton_kernels.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_inductor/config.py torch/testing/_internal/triton_utils.py,https://github.com/pytorch/pytorch/pull/131431,aakhundov,oulgen,zou3519,,
e782918b8ee,skip,not user facing,[NestedTensor] Add example NestedTensor objects with inner dimension of size 1 to tests reducing along jagged dimension for NestedTensor (#131516),test/test_nestedtensor.py,https://github.com/pytorch/pytorch/pull/131516,jananisriram,davidberard98,,,
41189b0da4f,skip,not user facing,Simplify THPEvent_get_device (#131466),torch/csrc/Event.cpp,https://github.com/pytorch/pytorch/pull/131466,cyyever,albanD,,,
276b5238ef4,optim,Untopiced,[bug] Add is_compiling check for optimizers to avoid untracked tensor during graph tracing (#130909),torch/optim/adadelta.py torch/optim/adagrad.py torch/optim/adam.py torch/optim/adamax.py torch/optim/adamw.py torch/optim/asgd.py torch/optim/nadam.py torch/optim/radam.py torch/optim/rmsprop.py torch/optim/rprop.py,https://github.com/pytorch/pytorch/pull/130909,botbw,mlazos,,,
1e34870796c,releng,Untopiced,[CI][dashboard][reland] Collect PT2 cpu perf nightly (#131560),.ci/pytorch/test.sh .github/workflows/inductor-perf-test-nightly-x86.yml .github/workflows/upload-torch-dynamo-perf-stats.yml,https://github.com/pytorch/pytorch/pull/131560,desertfire,huydhn,,,
83d19620f60,skip,not user facing,kill tmp _is_executorch flag (#131488),torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/131488,avikchaudhuri,ydwu4,,,
62e566b3455,build_frontend,improvements,[BE] Remove suppression of inconsistent missing overrides (#131524),CMakeLists.txt aten/src/ATen/cuda/tunable/StreamTimer.h aten/src/ATen/native/quantized/cpu/fbgemm_utils.h caffe2/utils/proto_wrap.cc torch/csrc/jit/serialization/onnx.h torch/csrc/jit/tensorexpr/llvm_codegen.cpp,https://github.com/pytorch/pytorch/pull/131524,malfet,atalman,,,
a4c3f29047e,onnx,not user facing,[ONNX][BE] Remove ruff skips in torch/onnx (#131368),pyproject.toml torch/onnx/_exporter_states.py torch/onnx/_internal/exporter.py torch/onnx/_internal/fx/_pass.py torch/onnx/_internal/fx/diagnostics.py torch/onnx/_internal/fx/passes/decomp.py torch/onnx/_internal/fx/passes/functionalization.py torch/onnx/_internal/fx/passes/type_promotion.py torch/onnx/_internal/fx/passes/virtualization.py torch/onnx/_internal/fx/registration.py torch/onnx/_internal/fx/torch_export_graph_extractor.py torch/onnx/errors.py,https://github.com/pytorch/pytorch/pull/131368,justinchuby,Skylion007,titaiwangms,,
95c248751bd,skip,not user facing,[inductor] Make UserDefinedTritonKernel a multi-output operation (#130832),torch/_inductor/comms.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/130832,peterbell10,lezcano,,,
6415c45da5a,skip,not user facing,[inductor] Use multiple outputs for flex-attention (#130833),test/inductor/test_flex_attention.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/130833,peterbell10,lezcano,,,
89d5391bbf3,skip,not user facing,[inductor] Kill mark_node_as_mutating (#130834),test/inductor/test_perf.py torch/_inductor/comms.py torch/_inductor/ir.py torch/_inductor/mkldnn_ir.py,https://github.com/pytorch/pytorch/pull/130834,peterbell10,lezcano,,,
85d3ee1d67b,distributed,Untopiced,[micro_pipeline_tp] refactor all-gather and reduce-scatter pattern matchers to be more flexible and testable (#131409),test/distributed/tensor/parallel/test_micro_pipeline_tp.py torch/_inductor/fx_passes/micro_pipeline_tp.py torch/_inductor/fx_passes/post_grad.py,https://github.com/pytorch/pytorch/pull/131409,yifuwang,weifengpy,,,
0c6f1ca0648,dynamo,new features,Introduce torch._dynamo.config.enable_compiler_collectives for syncing compilation across ranks (#130935),test/distributed/test_dynamo_distributed.py torch/_dynamo/config.py torch/_dynamo/convert_frame.py torch/_dynamo/distributed.py torch/_dynamo/exc.py torch/_dynamo/output_graph.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/130935,ezyang,jansel,,,
451462dbff3,cpp_frontend,Untopiced,[1/N] Add missing constructors or assignment operators (#131077),aten/src/ATen/code_template.h aten/src/ATen/cuda/detail/DeviceThreadHandles.h c10/core/SafePyObject.h c10/core/SymbolicShapeMeta.h c10/core/TensorImpl.h c10/cuda/CUDAMallocAsyncAllocator.cpp torch/csrc/Exceptions.h torch/csrc/api/include/torch/optim/optimizer.h torch/csrc/cuda/CUDAPluggableAllocator.h torch/csrc/utils/invalid_arguments.cpp,https://github.com/pytorch/pytorch/pull/131077,cyyever,ezyang,,,
8ffd109a004,skip,Untopiced,"Revert ""Fix py codegen to delete values that don't have any  users (#131028)""",test/dynamo/test_autograd_function.py test/dynamo/test_comptime.py test/dynamo/test_ctx_manager.py test/dynamo/test_export.py test/dynamo/test_higher_order_ops.py test/dynamo/test_input_attr_tracking.py test/dynamo/test_misc.py test/dynamo/test_repros.py test/export/test_export.py test/export/test_passes.py test/functorch/test_aotdispatch.py test/functorch/test_control_flow.py test/functorch/test_eager_transforms.py test/higher_order_ops/test_with_effects.py test/test_functionalization.py test/test_fx.py test/test_fx_reinplace_pass.py test/test_proxy_tensor.py torch/fx/graph.py,,,,,,
bc938184de7,distributed,Untopiced,[FSDP2] Added `set_reduce_scatter_divide_factor` (#129286),test/distributed/_composable/fsdp/test_fully_shard_comm.py test/distributed/_composable/fsdp/test_fully_shard_mixed_precision.py torch/distributed/_composable/fsdp/_fsdp_collectives.py torch/distributed/_composable/fsdp/_fsdp_param_group.py torch/distributed/_composable/fsdp/fully_shard.py,https://github.com/pytorch/pytorch/pull/129286,awgu,weifengpy,,,
98984422eb3,skip,Untopiced,[triton_op] fix autotuning (#131363),test/inductor/test_triton_kernels.py torch/_higher_order_ops/triton_kernel_wrap.py torch/fx/node.py torch/onnx/_internal/fx/type_utils.py,https://github.com/pytorch/pytorch/pull/131363,zou3519,justinchuby,oulgen,,
4c7f22dee25,optim,not user facing,[BE] remove unnecessary _dispatch_sqrt by using ** 0.5 (#131358),torch/optim/adam.py torch/optim/adamw.py torch/optim/nadam.py torch/optim/optimizer.py torch/optim/radam.py,https://github.com/pytorch/pytorch/pull/131358,janeyx99,albanD,,,
72d17d95d79,quantization,not user facing,[inductor] Enable dynamo for Windows. RC1 (#131286),test/quantization/pt2e/test_x86inductor_quantizer.py torch/_dynamo/eval_frame.py torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/131286,xuhancn,jansel,jgong5,,
eb5883f8aab,releng,not user facing,Add new runner labels to actionlint (#131525),.github/actionlint.yaml,https://github.com/pytorch/pytorch/pull/131525,ZainRizvi,atalman,,,
0e71a88f9b2,dataloader_frontend,not user facing,Support IPC for Expandable Segments (#130890),c10/cuda/CUDACachingAllocator.cpp test/test_dataloader.py test/test_multiprocessing.py torch/csrc/StorageSharing.cpp,https://github.com/pytorch/pytorch/pull/130890,zdevito,dsjohns2,,,
abcd3293598,onnx,not user facing,[BE] typing for decorators - onnx/symbolic_helper (#131565),torch/onnx/symbolic_caffe2.py torch/onnx/symbolic_helper.py torch/onnx/symbolic_opset10.py torch/onnx/symbolic_opset11.py torch/onnx/symbolic_opset12.py torch/onnx/symbolic_opset13.py torch/onnx/symbolic_opset14.py torch/onnx/symbolic_opset16.py torch/onnx/symbolic_opset17.py torch/onnx/symbolic_opset18.py torch/onnx/symbolic_opset20.py torch/onnx/symbolic_opset8.py,https://github.com/pytorch/pytorch/pull/131565,aorenste,justinchuby,oulgen,titaiwangms,
106c6a49f56,dynamo,not user facing,[dynamo] limit number of compiles per frame (#130891),test/dynamo/test_modules.py torch/_dynamo/cache_size.py torch/_dynamo/convert_frame.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/130891,williamwen42,anijain2305,,,
5e6cfb7db59,releng,not user facing,Add an extra shard for distributed periodic jobs (#131498),.github/workflows/periodic.yml,https://github.com/pytorch/pytorch/pull/131498,pragupta,clee2000,jithunnair-amd,pruthvistony,
8fe5b93667b,skip,Untopiced,support zb1p and zb2p algorithms (#130752),docs/source/distributed.pipelining.rst test/distributed/pipelining/test_schedule.py test/distributed/pipelining/test_schedule_multiproc.py torch/distributed/pipelining/__init__.py torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/130752,haocizhang,H-Huang,,,
a86909d2518,inductor,Untopiced,[inductor] Type annotate constant_folding.py (#131364),torch/_inductor/constant_folding.py,https://github.com/pytorch/pytorch/pull/131364,zhxchen17,angelayi,,,
f0378912a06,skip,not user facing,"[3.13, dynamo] fix test/dynamo/test_bytecode_utils.py (#131206)",test/dynamo/test_bytecode_utils.py,https://github.com/pytorch/pytorch/pull/131206,williamwen42,anijain2305,jansel,,
7718024d2b1,skip,not user facing,[3.13] support 3.13 multiline traces in munge_exc (#131207),torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/131207,williamwen42,anijain2305,jansel,,
b98b3127f71,skip,Untopiced,[easy][pytorch][counters] Move WaitCounter in c10/util (#131021),build_variables.bzl c10/util/WaitCounter.cpp c10/util/WaitCounter.h torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/monitor/instrumentation.cpp torch/csrc/monitor/instrumentation.h,https://github.com/pytorch/pytorch/pull/131021,andriigrynenko,asiab4,,,
aeca9845a6a,releng,not user facing,Migrate Lint jobs to Amazon 2023 AMI (#131514),.github/workflows/lint.yml,https://github.com/pytorch/pytorch/pull/131514,ZainRizvi,clee2000,huydhn,malfet,
3ce6f614163,inductor,not user facing,[AOTI] Support fallback ops not in inductor_fallback_ops (#131247),torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/131247,desertfire,angelayi,,,
8aff6caf674,releng,Untopiced,[CI][dashboard] Rename cpu-x86 to cpu_x86 (#131658),.ci/pytorch/test.sh .github/workflows/inductor-perf-test-nightly-x86.yml tools/stats/upload_dynamo_perf_stats.py,https://github.com/pytorch/pytorch/pull/131658,desertfire,huydhn,,,
05064f28272,releng,not user facing,[CI] Move all ROCm jobs to periodic frequency (#131637),.github/pytorch-probot.yml .github/workflows/inductor-rocm.yml .github/workflows/inductor.yml .github/workflows/rocm.yml,https://github.com/pytorch/pytorch/pull/131637,jithunnair-amd,atalman,clee2000,huydhn,
05681b6838c,releng,not user facing,Migrate missed experimental jobs to Amazon2023 AMI (#131485),.github/workflows/pull.yml,https://github.com/pytorch/pytorch/pull/131485,ZainRizvi,atalman,malfet,,
e0f1bf14a4f,skip,not user facing,Fully type torch/utils/_config_module.py (#131676),torch/utils/_config_module.py,https://github.com/pytorch/pytorch/pull/131676,oulgen,zou3519,,,
074b46b7d90,releng,Untopiced,[1/3] 3D Composability - move fsdp tests (#129800),.ci/pytorch/multigpu-test.sh .ci/pytorch/test.sh test/distributed/_composable/fsdp/test_fully_shard_training.py test/distributed/_composable/test_composability/test_2d_composability.py,https://github.com/pytorch/pytorch/pull/129800,mori360,awgu,,,
65ce2bf4659,skip,not user facing,Allow setting `PYTHON_LIB_REL_PATH` via environment variable (#128419),tools/setup_helpers/cmake.py,https://github.com/pytorch/pytorch/pull/128419,andrewjcg,PaliC,d4l3k,malfet,
85fa66be04b,skip,not user facing,Add rerun_disabled_tests for inductor (#131681),.github/workflows/inductor-cu124.yml,https://github.com/pytorch/pytorch/pull/131681,clee2000,zou3519,,,
9db567f17d2,skip,not user facing,[ONNX] Set dump_exported_program to True in bench (#131670),benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/131670,justinchuby,titaiwangms,,,
a9e63562713,onnx,bc breaking,[ONNX] Update torch.onnx.export API (#131501),test/onnx/dynamo/test_exporter_api.py torch/onnx/_deprecation.py torch/onnx/utils.py,https://github.com/pytorch/pytorch/pull/131501,justinchuby,titaiwangms,,,
84cd062fb25,skip,Untopiced,[2/3] 3D Composability - move pp tests (#129801),.ci/pytorch/multigpu-test.sh test/distributed/_composable/test_composability/test_pp_composability.py test/distributed/pipelining/test_composability.py,https://github.com/pytorch/pytorch/pull/129801,mori360,atalman,wconstab,,
2b83e4f8d7c,skip,not user facing,[ROCm] Enable flex decoding unit tests (#131048),test/inductor/test_flex_decoding.py,https://github.com/pytorch/pytorch/pull/131048,jerrymannil,jeffdaily,malfet,pruthvistony,
44fdf249679,composability,Untopiced,[BE] typing for decorators - jit/_decompositions (#131566),torch/_decomp/__init__.py torch/_inductor/fx_passes/post_grad.py torch/_refs/__init__.py torch/distributed/_spmd/api.py torch/jit/_decompositions.py,https://github.com/pytorch/pytorch/pull/131566,aorenste,oulgen,zou3519,,
b90aa185691,inductor,Untopiced,[aoti] Add initial custom op support (#127034),build_variables.bzl caffe2/CMakeLists.txt test/inductor/CMakeLists.txt test/inductor/custom_ops.cpp test/inductor/test_aot_inductor.py test/inductor/test_aot_inductor_utils.py third_party/nlohmann.BUILD torch/_export/serde/aoti_schema.py torch/_inductor/codecache.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/extern_node_serializer.py torch/_inductor/graph.py torch/csrc/inductor/aoti_runner/model_container_runner.cpp torch/csrc/inductor/aoti_runner/model_container_runner.h torch/csrc/inductor/aoti_torch/oss_proxy_executor.cpp torch/csrc/inductor/aoti_torch/oss_proxy_executor.h torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/127034,angelayi,malfet,,,
7f61324268a,skip,not user facing,Add sparse block to flex_decoding kernel (#130884),benchmarks/transformer/score_mod.py test/inductor/test_flex_decoding.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/130884,joydddd,drisspg,,,
544f950d14e,releng,not user facing,[BE] Improve error message when there are internal changes (#131547),.github/scripts/trymerge.py,https://github.com/pytorch/pytorch/pull/131547,PaliC,atalman,malfet,xuzhao9,
eb54ca7abe3,skip,Untopiced,"Revert ""[BE] Get rid of missing destructor override warning (#131204)""",aten/src/ATen/detail/HIPHooksInterface.h,,,,,,
f9322c26b20,releng,Untopiced,Remove _export/exported_program.py (#131597),.ci/docker/ci_commit_pins/executorch.txt torch/_export/exported_program.py,https://github.com/pytorch/pytorch/pull/131597,yushangdi,avikchaudhuri,,,
b56939dae11,dynamo,not user facing,Annotate more InstructionTranslator (#131680),torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/higher_order_ops.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/sdpa.py torch/_dynamo/variables/tensor.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/torch_function.py,https://github.com/pytorch/pytorch/pull/131680,oulgen,zou3519,,,
a7e20ef7e46,skip,Untopiced,[BE] Get rid of missing destructor override warning (#131204),aten/src/ATen/detail/HIPHooksInterface.h,https://github.com/pytorch/pytorch/pull/131204,malfet,albanD,seemethere,,
5db58656141,skip,Untopiced,"Revert ""Annotate all InstructionTranslator (#131509)""",torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/distributed.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/higher_order_ops.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/optimizer.py torch/_dynamo/variables/sdpa.py torch/_dynamo/variables/tensor.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/torch_function.py torch/_dynamo/variables/user_defined.py,,,,,,
236e06f9f97,skip,Untopiced,"Revert ""Ensure staticmethods can be allowed in graph (#130882)""",test/dynamo/test_repros.py torch/_dynamo/variables/user_defined.py,,,,,,
29c9f8c782f,Uncategorized,Untopiced,[export] Fix `graph_break` log registration error when importing export/_trace.py (#131523),torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/131523,yushangdi,zhxchen17,,,
7535b23a25d,Uncategorized,Untopiced,[export] Fix set_grad hoo if output is empty (#131511),test/export/test_export.py torch/_export/passes/replace_set_grad_with_hop_pass.py,https://github.com/pytorch/pytorch/pull/131511,angelayi,ydwu4,,,
7a42470bcb7,skip,not user facing,Annotate all InstructionTranslator (#131509),torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/distributed.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/higher_order_ops.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/optimizer.py torch/_dynamo/variables/sdpa.py torch/_dynamo/variables/tensor.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/torch_function.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/131509,oulgen,zou3519,,,
803c5b8640d,skip,not user facing,[CMake] Fix private compile options for  CUDA code (#130546),cmake/public/utils.cmake,https://github.com/pytorch/pytorch/pull/130546,cyyever,ezyang,,,
c3fe9075a9d,linalg_frontend,Untopiced,[ROCM] Use hipblaslt version from hipblaslt runtime instead of header for tunableops validator (#131078),aten/src/ATen/cuda/tunable/TunableGemm.h test/test_linalg.py,https://github.com/pytorch/pytorch/pull/131078,zixi-qi,jeffdaily,xw285cornell,,
e20fb5e9751,distributed,Untopiced,[PTD][c10d] Include PG status into flight recorder (#131268),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/TraceUtils.h,https://github.com/pytorch/pytorch/pull/131268,fduwjj,shuqiangzhang,,,
ab609d6aa68,Uncategorized,Untopiced,[ts_convert] Update conversion for aten.tensor (#131549),test/export/test_converter.py torch/_export/converter.py torch/_export/passes/lift_constants_pass.py,https://github.com/pytorch/pytorch/pull/131549,angelayi,SherlockNoMad,jiashenC,,
8ea4c72eb27,skip,not user facing,"[1/N] Fix clang-tidy warnings in aten/src/ATen/native/*.{cpp,h} (#130798)",aten/src/ATen/native/AdaptiveMaxPooling3d.cpp aten/src/ATen/native/BatchLinearAlgebra.cpp aten/src/ATen/native/BatchLinearAlgebraKernel.cpp aten/src/ATen/native/BinaryOps.cpp aten/src/ATen/native/Normalization.cpp aten/src/ATen/native/RNN.cpp aten/src/ATen/native/ReduceOps.cpp aten/src/ATen/native/ReduceOpsUtils.h aten/src/ATen/native/Repeat.cpp aten/src/ATen/native/Repeat.h aten/src/ATen/native/Resize.cpp aten/src/ATen/native/Resize.h aten/src/ATen/native/cpu/AmpGradScalerKernels.cpp aten/src/ATen/native/cpu/AvgPoolKernel.cpp,https://github.com/pytorch/pytorch/pull/130798,cyyever,ezyang,,,
b5c006acac1,distributed,not user facing,[BE][Easy] enable UFMT for `torch/nn/` (#128865),.lintrunner.toml torch/nn/intrinsic/__init__.py torch/nn/intrinsic/modules/__init__.py torch/nn/intrinsic/modules/fused.py torch/nn/intrinsic/qat/__init__.py torch/nn/intrinsic/qat/modules/__init__.py torch/nn/intrinsic/qat/modules/conv_fused.py torch/nn/intrinsic/qat/modules/linear_fused.py torch/nn/intrinsic/qat/modules/linear_relu.py torch/nn/intrinsic/quantized/__init__.py torch/nn/intrinsic/quantized/dynamic/__init__.py torch/nn/intrinsic/quantized/dynamic/modules/__init__.py torch/nn/intrinsic/quantized/dynamic/modules/linear_relu.py torch/nn/intrinsic/quantized/modules/__init__.py torch/nn/intrinsic/quantized/modules/bn_relu.py torch/nn/intrinsic/quantized/modules/conv_relu.py torch/nn/intrinsic/quantized/modules/linear_relu.py torch/nn/parallel/__init__.py torch/nn/parallel/_functions.py torch/nn/parallel/data_parallel.py torch/nn/parallel/distributed.py torch/nn/parallel/parallel_apply.py torch/nn/parallel/replicate.py torch/nn/parallel/scatter_gather.py torch/nn/qat/__init__.py torch/nn/qat/dynamic/__init__.py torch/nn/qat/dynamic/modules/__init__.py torch/nn/qat/modules/__init__.py torch/nn/qat/modules/conv.py torch/nn/qat/modules/embedding_ops.py torch/nn/quantizable/__init__.py torch/nn/quantizable/modules/__init__.py torch/nn/quantizable/modules/rnn.py torch/nn/quantized/__init__.py torch/nn/quantized/_reference/__init__.py torch/nn/quantized/_reference/modules/__init__.py torch/nn/quantized/_reference/modules/conv.py torch/nn/quantized/_reference/modules/rnn.py torch/nn/quantized/_reference/modules/sparse.py torch/nn/quantized/_reference/modules/utils.py torch/nn/quantized/dynamic/modules/__init__.py torch/nn/quantized/dynamic/modules/conv.py torch/nn/quantized/dynamic/modules/rnn.py torch/nn/quantized/modules/__init__.py torch/nn/quantized/modules/activation.py torch/nn/quantized/modules/batchnorm.py torch/nn/quantized/modules/conv.py torch/nn/quantized/modules/dropout.py torch/nn/quantized/modules/embedding_ops.py torch/nn/quantized/modules/functional_modules.py torch/nn/quantized/modules/linear.py torch/nn/quantized/modules/normalization.py torch/nn/quantized/modules/utils.py,https://github.com/pytorch/pytorch/pull/128865,XuehaiPan,ezyang,,,
01bc2a8165b,Uncategorized,Untopiced,[inline-inbuilt-nn-modules] Skip mobilenet_v2 test for cpu inductor (#131694),benchmarks/dynamo/expected_ci_speedup_inductor_torchbench_cpu.csv,https://github.com/pytorch/pytorch/pull/131694,anijain2305,eellison,,,
c047bddbcab,dynamo,not user facing,[easy][dynamo] Update test for inline_inbuilt_n_modules (#131718),test/dynamo/test_modules.py,https://github.com/pytorch/pytorch/pull/131718,anijain2305,mlazos,williamwen42,,
973a1362b97,quantization,not user facing,[BE] enable UFMT for `torch/ao/nn/` (#128861),.lintrunner.toml torch/ao/nn/__init__.py torch/ao/nn/intrinsic/__init__.py torch/ao/nn/intrinsic/modules/__init__.py torch/ao/nn/intrinsic/modules/fused.py torch/ao/nn/intrinsic/qat/modules/__init__.py torch/ao/nn/intrinsic/qat/modules/conv_fused.py torch/ao/nn/intrinsic/qat/modules/linear_fused.py torch/ao/nn/intrinsic/qat/modules/linear_relu.py torch/ao/nn/intrinsic/quantized/__init__.py torch/ao/nn/intrinsic/quantized/dynamic/modules/__init__.py torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py torch/ao/nn/intrinsic/quantized/modules/__init__.py torch/ao/nn/intrinsic/quantized/modules/bn_relu.py torch/ao/nn/intrinsic/quantized/modules/conv_add.py torch/ao/nn/intrinsic/quantized/modules/conv_relu.py torch/ao/nn/intrinsic/quantized/modules/linear_relu.py torch/ao/nn/qat/dynamic/modules/__init__.py torch/ao/nn/qat/dynamic/modules/linear.py torch/ao/nn/qat/modules/__init__.py torch/ao/nn/qat/modules/conv.py torch/ao/nn/qat/modules/embedding_ops.py torch/ao/nn/qat/modules/linear.py torch/ao/nn/quantizable/modules/__init__.py torch/ao/nn/quantizable/modules/activation.py torch/ao/nn/quantizable/modules/rnn.py torch/ao/nn/quantized/__init__.py torch/ao/nn/quantized/dynamic/modules/__init__.py torch/ao/nn/quantized/dynamic/modules/conv.py torch/ao/nn/quantized/dynamic/modules/linear.py torch/ao/nn/quantized/dynamic/modules/rnn.py torch/ao/nn/quantized/functional.py torch/ao/nn/quantized/modules/__init__.py torch/ao/nn/quantized/modules/activation.py torch/ao/nn/quantized/modules/batchnorm.py torch/ao/nn/quantized/modules/conv.py torch/ao/nn/quantized/modules/dropout.py torch/ao/nn/quantized/modules/embedding_ops.py torch/ao/nn/quantized/modules/functional_modules.py torch/ao/nn/quantized/modules/linear.py torch/ao/nn/quantized/modules/normalization.py torch/ao/nn/quantized/modules/rnn.py torch/ao/nn/quantized/modules/utils.py torch/ao/nn/quantized/reference/__init__.py torch/ao/nn/quantized/reference/modules/__init__.py torch/ao/nn/quantized/reference/modules/conv.py torch/ao/nn/quantized/reference/modules/linear.py torch/ao/nn/quantized/reference/modules/rnn.py torch/ao/nn/quantized/reference/modules/sparse.py torch/ao/nn/quantized/reference/modules/utils.py torch/ao/nn/sparse/quantized/__init__.py torch/ao/nn/sparse/quantized/dynamic/__init__.py torch/ao/nn/sparse/quantized/dynamic/linear.py torch/ao/nn/sparse/quantized/linear.py torch/ao/nn/sparse/quantized/utils.py torch/nn/modules/rnn.py,https://github.com/pytorch/pytorch/pull/128861,XuehaiPan,ezyang,,,
03979a599ec,quantization,not user facing,[BE] enable UFMT for `torch/ao/pruning/` (#128862),.lintrunner.toml torch/ao/pruning/__init__.py torch/ao/pruning/_experimental/activation_sparsifier/activation_sparsifier.py torch/ao/pruning/_experimental/data_scheduler/__init__.py torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py torch/ao/pruning/_experimental/data_sparsifier/__init__.py torch/ao/pruning/_experimental/data_sparsifier/base_data_sparsifier.py torch/ao/pruning/_experimental/data_sparsifier/benchmarks/dlrm_utils.py torch/ao/pruning/_experimental/data_sparsifier/benchmarks/evaluate_disk_savings.py torch/ao/pruning/_experimental/data_sparsifier/benchmarks/evaluate_forward_time.py torch/ao/pruning/_experimental/data_sparsifier/benchmarks/evaluate_model_metrics.py torch/ao/pruning/_experimental/data_sparsifier/data_norm_sparsifier.py torch/ao/pruning/_experimental/data_sparsifier/lightning/callbacks/_data_sparstity_utils.py torch/ao/pruning/_experimental/data_sparsifier/lightning/callbacks/data_sparsity.py torch/ao/pruning/_experimental/data_sparsifier/lightning/tests/test_callbacks.py torch/ao/pruning/_experimental/data_sparsifier/quantization_utils.py torch/ao/pruning/_experimental/pruner/FPGM_pruner.py torch/ao/pruning/_experimental/pruner/__init__.py torch/ao/pruning/_experimental/pruner/base_structured_sparsifier.py torch/ao/pruning/_experimental/pruner/lstm_saliency_pruner.py torch/ao/pruning/_experimental/pruner/match_utils.py torch/ao/pruning/_experimental/pruner/parametrization.py torch/ao/pruning/_experimental/pruner/prune_functions.py torch/ao/pruning/_experimental/pruner/saliency_pruner.py torch/ao/pruning/_mappings.py torch/ao/pruning/scheduler/base_scheduler.py torch/ao/pruning/scheduler/cubic_scheduler.py torch/ao/pruning/scheduler/lambda_scheduler.py torch/ao/pruning/sparsifier/base_sparsifier.py torch/ao/pruning/sparsifier/nearly_diagonal_sparsifier.py torch/ao/pruning/sparsifier/utils.py torch/ao/pruning/sparsifier/weight_norm_sparsifier.py,https://github.com/pytorch/pytorch/pull/128862,XuehaiPan,ezyang,,,
46e42ae85d4,distributed,not user facing,[4/N] Fix Wunused-parameter warnings (#131291),aten/src/ATen/CPUGeneratorImpl.cpp aten/src/ATen/DLConvertor.cpp aten/src/ATen/EmptyTensor.cpp aten/src/ATen/EmptyTensor.h aten/src/ATen/core/op_registration/op_allowlist.h aten/src/ATen/cpu/vec/vec512/vec512_complex_double.h aten/src/ATen/cpu/vec/vec512/vec512_complex_float.h aten/src/ATen/cpu/vec/vec512/vec512_qint.h aten/src/ATen/native/MetaTensor.cpp torch/csrc/distributed/c10d/control_plane/WorkerServer.cpp torch/csrc/jit/tensorexpr/codegen.h torch/csrc/jit/tensorexpr/tensor.h,https://github.com/pytorch/pytorch/pull/131291,cyyever,ezyang,,,
538258bc135,jit,not user facing,[1/N] Fix clang-tidy warnings in jit (#131034),aten/src/ATen/core/class_type.cpp aten/src/ATen/core/class_type.h torch/csrc/jit/frontend/ir_emitter.cpp,https://github.com/pytorch/pytorch/pull/131034,cyyever,ezyang,,,
d98d00487d3,skip,not user facing,[2/N] Remove unused variables (#131468),aten/src/ATen/cuda/tunable/TunableGemm.h torch/csrc/Event.cpp,https://github.com/pytorch/pytorch/pull/131468,cyyever,eqy,ezyang,,
d6115439bee,mps,Untopiced,[MPS] Add SDPA implentation (#131362),aten/src/ATen/native/mps/operations/Attention.mm aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/transformers/attention.cpp test/expect/HasDecompTest.test_has_decomposition.expect test/test_mps.py,https://github.com/pytorch/pytorch/pull/131362,manuelcandales,kimishpatel,,,
2d7c1357577,skip,not user facing,Bump setuptools from 69.5.1 to 70.0.0 in /tools/build/bazel (#130893),tools/build/bazel/requirements.in tools/build/bazel/requirements.txt,https://github.com/pytorch/pytorch/pull/130893,dependabot,kit1980,,,
62704db5c34,distributed,not user facing,[Distributed] [10/N] Fix clang-tidy warnings in torch/csrc/distributed/c10d/control_plane (#131671),torch/csrc/distributed/c10d/control_plane/Handlers.cpp torch/csrc/distributed/c10d/control_plane/Handlers.hpp torch/csrc/distributed/c10d/control_plane/WorkerServer.cpp torch/csrc/distributed/c10d/control_plane/WorkerServer.hpp,https://github.com/pytorch/pytorch/pull/131671,cyyever,zou3519,,,
a2f6eb33d04,skip,not user facing,Register buffer in static input test (#131686),test/inductor/test_cudagraph_trees.py,https://github.com/pytorch/pytorch/pull/131686,mlazos,anijain2305,,,
2ce734cee93,quantization,not user facing,[BE] enable UFMT for `torch/ao/quantization/` (#128863),.lintrunner.toml torch/ao/quantization/__init__.py torch/ao/quantization/_correct_bias.py torch/ao/quantization/_equalize.py torch/ao/quantization/_learnable_fake_quantize.py torch/ao/quantization/backend_config/__init__.py torch/ao/quantization/backend_config/_common_operator_config_utils.py torch/ao/quantization/backend_config/_qnnpack_pt2e.py torch/ao/quantization/backend_config/backend_config.py torch/ao/quantization/backend_config/executorch.py torch/ao/quantization/backend_config/fbgemm.py torch/ao/quantization/backend_config/native.py torch/ao/quantization/backend_config/onednn.py torch/ao/quantization/backend_config/qnnpack.py torch/ao/quantization/backend_config/tensorrt.py torch/ao/quantization/backend_config/utils.py torch/ao/quantization/backend_config/x86.py torch/ao/quantization/experimental/APoT_tensor.py torch/ao/quantization/experimental/adaround_loss.py torch/ao/quantization/experimental/apot_utils.py torch/ao/quantization/experimental/fake_quantize.py torch/ao/quantization/experimental/fake_quantize_function.py torch/ao/quantization/experimental/linear.py torch/ao/quantization/experimental/observer.py torch/ao/quantization/experimental/qconfig.py torch/ao/quantization/experimental/quantizer.py torch/ao/quantization/fake_quantize.py torch/ao/quantization/fuse_modules.py torch/ao/quantization/fuser_method_mappings.py torch/ao/quantization/fx/__init__.py torch/ao/quantization/fx/_decomposed.py torch/ao/quantization/fx/_equalize.py torch/ao/quantization/fx/_lower_to_native_backend.py torch/ao/quantization/fx/_model_report/detector.py torch/ao/quantization/fx/_model_report/model_report.py torch/ao/quantization/fx/_model_report/model_report_observer.py torch/ao/quantization/fx/_model_report/model_report_visualizer.py torch/ao/quantization/fx/convert.py torch/ao/quantization/fx/custom_config.py torch/ao/quantization/fx/fuse.py torch/ao/quantization/fx/fuse_handler.py torch/ao/quantization/fx/graph_module.py torch/ao/quantization/fx/lower_to_fbgemm.py torch/ao/quantization/fx/lower_to_qnnpack.py torch/ao/quantization/fx/lstm_utils.py torch/ao/quantization/fx/match_utils.py torch/ao/quantization/fx/pattern_utils.py torch/ao/quantization/fx/prepare.py torch/ao/quantization/fx/qconfig_mapping_utils.py torch/ao/quantization/fx/quantize_handler.py torch/ao/quantization/fx/tracer.py torch/ao/quantization/fx/utils.py torch/ao/quantization/observer.py torch/ao/quantization/pt2e/duplicate_dq_pass.py torch/ao/quantization/pt2e/export_utils.py torch/ao/quantization/pt2e/graph_utils.py torch/ao/quantization/pt2e/port_metadata_pass.py torch/ao/quantization/pt2e/prepare.py torch/ao/quantization/pt2e/qat_utils.py torch/ao/quantization/pt2e/representation/__init__.py torch/ao/quantization/pt2e/representation/rewrite.py torch/ao/quantization/pt2e/utils.py torch/ao/quantization/qconfig.py torch/ao/quantization/qconfig_mapping.py torch/ao/quantization/quant_type.py torch/ao/quantization/quantization_mappings.py torch/ao/quantization/quantize.py torch/ao/quantization/quantize_fx.py torch/ao/quantization/quantize_jit.py torch/ao/quantization/quantize_pt2e.py torch/ao/quantization/quantizer/__init__.py torch/ao/quantization/quantizer/composable_quantizer.py torch/ao/quantization/quantizer/embedding_quantizer.py torch/ao/quantization/quantizer/quantizer.py torch/ao/quantization/quantizer/utils.py torch/ao/quantization/quantizer/x86_inductor_quantizer.py torch/ao/quantization/quantizer/xnnpack_quantizer.py torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py torch/ao/quantization/stubs.py torch/ao/quantization/utils.py,https://github.com/pytorch/pytorch/pull/128863,XuehaiPan,ezyang,,,
00e19ae97a4,autograd_frontend,Untopiced,[MTIA] Support module.mtia() (#131499),tools/autograd/templates/python_variable_methods.cpp torch/_tensor_docs.py torch/nn/modules/module.py torch/overrides.py,https://github.com/pytorch/pytorch/pull/131499,egienvalue,mikaylagawarecki,,,
5f3f14e5e4f,inductor,not user facing,[BE] Annotate subgraph_lowering (#131545),torch/_inductor/lowering.py torch/_inductor/subgraph_lowering.py,https://github.com/pytorch/pytorch/pull/131545,yanboliang,anijain2305,zou3519,,
35bb0d3638f,skip,not user facing,Fix unsigned type bug in CUDACachingAllocator.cpp (#131464),c10/cuda/CUDACachingAllocator.cpp,https://github.com/pytorch/pytorch/pull/131464,cyyever,eqy,ezyang,,
ceab3121de7,inductor,not user facing,[inductor] Fix flaky tests in test_memory_planning.py (#131703),test/inductor/test_memory_planning.py,https://github.com/pytorch/pytorch/pull/131703,aakhundov,eellison,,,
42a4df9447b,skip,not user facing,Support CUDA nightly package in `tools/nightly.py` (#131133),.devcontainer/scripts/install-dev-tools.sh CONTRIBUTING.md Makefile tools/linter/adapters/lintrunner_version_linter.py tools/nightly.py,https://github.com/pytorch/pytorch/pull/131133,XuehaiPan,ezyang,,,
ee996cd63cd,inductor,not user facing,[inductor] Fix flaky tests in test_benchmark_fusion.py (#131733),test/inductor/test_benchmark_fusion.py,https://github.com/pytorch/pytorch/pull/131733,aakhundov,oulgen,,,
59ef88ea5bf,inductor,not user facing,[inductor] Fix flaky tests in test_pad_mm (#131699),test/inductor/test_pad_mm.py,https://github.com/pytorch/pytorch/pull/131699,aakhundov,chenyang78,eellison,shunting314,
ace6decc994,jit,not user facing,Fix static `py::object` dangling pointer with `py::gil_safe_call_once_and_store` (#130341),functorch/csrc/dim/dim.cpp torch/csrc/dynamo/guards.cpp torch/csrc/jit/python/module_python.h torch/csrc/jit/python/python_ivalue.h torch/csrc/utils/python_arg_parser.cpp torch/csrc/utils/python_symnode.cpp,https://github.com/pytorch/pytorch/pull/130341,XuehaiPan,ezyang,malfet,,
059f9fb30be,inductor,not user facing,[BE][inductor] Type annotate `codecache.py` and `config.py` (#131427),torch/_inductor/codecache.py torch/_inductor/codegen/simd.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/131427,YUNQIUGUO,eellison,oulgen,,
3afdbecb232,inductor,not user facing,[inductor] Fix flaky tests in test_debug_trace.py (#131722),test/inductor/test_debug_trace.py,https://github.com/pytorch/pytorch/pull/131722,aakhundov,eellison,,,
41e9f9cb7c3,inductor,not user facing,[inductor] Fix flaky tests in test_select_algorithm.py (#131709),test/inductor/test_select_algorithm.py,https://github.com/pytorch/pytorch/pull/131709,aakhundov,eellison,,,
c4bf4005d1e,distributed,not user facing,[dtensor][debug] adding new noise level which allows users to only print operations with dtensors (#131592),torch/distributed/_tensor/debug/comm_mode.py torch/distributed/_tensor/examples/comm_mode_features_example.py,https://github.com/pytorch/pytorch/pull/131592,sinhaanshul,XilunWu,,,
054d214c504,skip,not user facing,[BE][tests] show local variables on failure in tests (#131151),test/run_test.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/131151,XuehaiPan,ezyang,,,
434f60ce334,skip,not user facing,Refactor nightly checkout tool (#131134),CONTRIBUTING.md tools/nightly.py,https://github.com/pytorch/pytorch/pull/131134,XuehaiPan,ezyang,,,
c04f70bb302,quantization,not user facing,[BE] enable UFMT for `torch/ao/` (#128864),.lintrunner.toml torch/ao/__init__.py torch/ao/ns/_numeric_suite.py torch/ao/ns/_numeric_suite_fx.py torch/ao/ns/fx/graph_matcher.py torch/ao/ns/fx/graph_passes.py torch/ao/ns/fx/mappings.py torch/ao/ns/fx/n_shadows_utils.py torch/ao/ns/fx/ns_types.py torch/ao/ns/fx/pattern_utils.py torch/ao/ns/fx/qconfig_multi_mapping.py torch/ao/ns/fx/utils.py torch/ao/ns/fx/weight_utils.py,https://github.com/pytorch/pytorch/pull/128864,XuehaiPan,ezyang,,,
2784b3f1b7e,inductor,Untopiced,[inductor] Fix split-scan interaction with multi-kernel (#131044),test/inductor/test_kernel_benchmark.py test/inductor/test_multi_kernel.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_split_scan.py,https://github.com/pytorch/pytorch/pull/131044,peterbell10,shunting314,,,
91aba7baac3,fx,not user facing,Fix py codegen to delete values that don't have any  users (#131028),test/dynamo/test_autograd_function.py test/dynamo/test_comptime.py test/dynamo/test_ctx_manager.py test/dynamo/test_export.py test/dynamo/test_higher_order_ops.py test/dynamo/test_input_attr_tracking.py test/dynamo/test_misc.py test/dynamo/test_repros.py test/export/test_export.py test/export/test_passes.py test/export/test_torchbind.py test/functorch/test_aotdispatch.py test/functorch/test_control_flow.py test/functorch/test_eager_transforms.py test/higher_order_ops/test_with_effects.py test/inductor/test_flex_attention.py test/test_functionalization.py test/test_fx.py test/test_fx_reinplace_pass.py test/test_proxy_tensor.py torch/fx/graph.py,https://github.com/pytorch/pytorch/pull/131028,YangQun1,ezyang,,,
e8956c9fe63,composability,not user facing,Allow cpu scalar to be moved to HPU in masked_fill_decomposition (#127871),torch/_refs/__init__.py,https://github.com/pytorch/pytorch/pull/127871,PatrykWilczewski,ezyang,jgong5,,
9c4cf866c27,optim,new features,Adafactor forloop basic impl (#129905),docs/source/optim.rst test/test_optim.py torch/optim/__init__.py torch/optim/_adafactor.py torch/testing/_internal/common_optimizers.py,https://github.com/pytorch/pytorch/pull/129905,janeyx99,albanD,,,
d962dba0c44,skip,Untopiced,"Revert ""[2/3] 3D Composability - move pp tests (#129801)""",.ci/pytorch/multigpu-test.sh test/distributed/_composable/test_composability/test_pp_composability.py test/distributed/pipelining/test_composability.py,,,,,,
316c0d3e6b2,inductor,not user facing,[inductor][cpp][gemm] support k slicing for static shapes (#130821),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/130821,jgong5,jansel,leslie-fang-intel,,
c88c90a8970,skip,not user facing,[TS2E] Improve logging (#131711),torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/131711,justinchuby,ydwu4,,,
fddb1bcdead,profiler,improvements,[CCA][Memory Snapshot] Move user_defined annotations to Native Caching Allocator (#130964),c10/cuda/CUDACachingAllocator.cpp c10/cuda/CUDACachingAllocator.h torch/csrc/cuda/Module.cpp torch/csrc/cuda/memory_snapshot.cpp torch/csrc/profiler/combined_traceback.cpp torch/csrc/profiler/combined_traceback.h,https://github.com/pytorch/pytorch/pull/130964,aaronenyeshi,zdevito,,,
7260eaeca05,vulkan,not user facing,Fix vulkan builds with missing overrides errors (#131760),aten/src/ATen/native/vulkan/api/Allocator.h,https://github.com/pytorch/pytorch/pull/131760,malfet,atalman,,,
4a5a87168e1,composability,not user facing,[BE] typing for decorators - _prims_common/wrappers (#131567),torch/_prims_common/wrappers.py torch/_refs/__init__.py torch/_refs/_conversions.py torch/_refs/fft.py torch/_refs/linalg/__init__.py torch/_refs/special/__init__.py,https://github.com/pytorch/pytorch/pull/131567,aorenste,oulgen,zou3519,,
d0e2ab617df,releng,not user facing,"Migrate conda, manywheel and libtorch docker builds to pytorch/pytorch (#129022)",.ci/docker/README.md .ci/docker/common/aotriton_version.txt .ci/docker/common/install_conda_docker.sh .ci/docker/common/install_cpython.sh .ci/docker/common/install_cuda.sh .ci/docker/common/install_cuda_aarch64.sh .ci/docker/common/install_libpng.sh .ci/docker/common/install_magma.sh .ci/docker/common/install_miopen.sh .ci/docker/common/install_mkl.sh .ci/docker/common/install_mnist.sh .ci/docker/common/install_openblas.sh .ci/docker/common/install_patchelf.sh .ci/docker/common/install_rocm_drm.sh .ci/docker/common/install_rocm_magma.sh .ci/docker/common/install_xpu.sh .ci/docker/conda/Dockerfile .ci/docker/conda/build.sh .ci/docker/libtorch/Dockerfile .ci/docker/libtorch/build.sh .ci/docker/manywheel/Dockerfile .ci/docker/manywheel/Dockerfile_2014 .ci/docker/manywheel/Dockerfile_2_28 .ci/docker/manywheel/Dockerfile_2_28_aarch64 .ci/docker/manywheel/Dockerfile_aarch64 .ci/docker/manywheel/Dockerfile_cuda_aarch64 .ci/docker/manywheel/Dockerfile_cxx11-abi .ci/docker/manywheel/Dockerfile_s390x .ci/docker/manywheel/build.sh .ci/docker/manywheel/build_scripts/build.sh .ci/docker/manywheel/build_scripts/build_utils.sh .ci/docker/manywheel/build_scripts/manylinux1-check.py .ci/docker/manywheel/build_scripts/ssl-check.py .github/actionlint.yaml .github/workflows/build-conda-images.yml .github/workflows/build-libtorch-images.yml .github/workflows/build-manywheel-images.yml .lintrunner.toml,https://github.com/pytorch/pytorch/pull/129022,juliagmt-google,atalman,chuanqi129,malfet,
1eedb0a9620,distributed,Untopiced,fix torchrun log message (#131652),torch/distributed/run.py,https://github.com/pytorch/pytorch/pull/131652,H-Huang,awgu,,,
745b55d14a8,releng,Untopiced,[CI][dashboard] Add a workflow to collect aarch64 perf (#131729),.ci/docker/build.sh .ci/pytorch/test.sh .github/workflows/docker-builds.yml .github/workflows/inductor-perf-test-nightly-aarch64.yml .github/workflows/upload-torch-dynamo-perf-stats.yml,https://github.com/pytorch/pytorch/pull/131729,desertfire,huydhn,,,
c3d099ddd14,optim,docs,[BE][Easy] Add hooks to doc for Optimizer base class  (#131628),docs/source/optim.rst,https://github.com/pytorch/pytorch/pull/131628,mikaylagawarecki,janeyx99,,,
a4be5cb50e1,skip,not user facing,Simplify some c++ code (#131612),aten/src/ATen/native/BatchLinearAlgebra.cpp aten/src/ATen/native/RNN.cpp aten/src/ATen/native/cpu/UpSampleKernel.cpp aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp aten/src/ATen/native/cuda/SpectralOps.cpp torch/csrc/utils/python_dispatch.cpp,https://github.com/pytorch/pytorch/pull/131612,cyyever,ezyang,,,
49a8e061b6d,skip,Untopiced,"Revert ""Support IPC for Expandable Segments (#130890)""",c10/cuda/CUDACachingAllocator.cpp test/test_dataloader.py test/test_multiprocessing.py torch/csrc/StorageSharing.cpp,,,,,,
b07ea91c4c0,jit,Untopiced,[2/N] Fix clang-tidy warnings in jit  (#131735),torch/csrc/jit/frontend/builtin_functions.h torch/csrc/jit/frontend/canonicalize_modified_loop.h torch/csrc/jit/frontend/convert_to_ssa.h torch/csrc/jit/frontend/edit_distance.h torch/csrc/jit/frontend/error_report.cpp torch/csrc/jit/frontend/error_report.h torch/csrc/jit/frontend/exit_transforms.h torch/csrc/jit/frontend/function_schema_parser.cpp torch/csrc/jit/frontend/function_schema_parser.h torch/csrc/jit/frontend/inline_loop_condition.h torch/csrc/jit/frontend/ir_emitter.h torch/csrc/jit/frontend/lexer.h torch/csrc/jit/frontend/mini_environment.h torch/csrc/jit/frontend/name_mangler.h torch/csrc/jit/frontend/parse_string_literal.h torch/csrc/jit/frontend/parser.h torch/csrc/jit/frontend/parser_constants.h torch/csrc/jit/frontend/resolver.h torch/csrc/jit/frontend/schema_matching.cpp torch/csrc/jit/frontend/schema_matching.h torch/csrc/jit/frontend/schema_type_parser.h torch/csrc/jit/frontend/script_type_parser.h torch/csrc/jit/frontend/source_ref.h torch/csrc/jit/frontend/strtod.h torch/csrc/jit/frontend/sugared_value.cpp torch/csrc/jit/frontend/sugared_value.h torch/csrc/jit/frontend/tracer.cpp torch/csrc/jit/frontend/tracer.h torch/csrc/jit/frontend/tree.h torch/csrc/jit/frontend/tree_views.h torch/csrc/jit/frontend/versioned_symbols.h torch/csrc/jit/tensorexpr/bounds_overlap.cpp,https://github.com/pytorch/pytorch/pull/131735,cyyever,ezyang,,,
96e8df6a3a2,Uncategorized,Untopiced,[ts_converter] Support prim::max and prim::if with multiple outputs (#131593),test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/131593,SherlockNoMad,ydwu4,,,
ffc6bf8149b,dynamo,not user facing,[dynamo] lazily guard and specialize on the symint when used in f-string. (#131529),test/dynamo/test_functions.py test/dynamo/test_misc.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/lazy.py,https://github.com/pytorch/pytorch/pull/131529,ydwu4,ezyang,,,
f063027d542,inductor,Untopiced,[aoti] Fix constant inputs passed to aoti (#131594),test/inductor/test_aot_inductor.py test/inductor/test_aot_inductor_utils.py torch/_export/__init__.py torch/_inductor/__init__.py torch/_inductor/graph.py,https://github.com/pytorch/pytorch/pull/131594,angelayi,desertfire,,,
6bc8db1d324,Uncategorized,Untopiced,Rename is_training flag to have more information (#131618),torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/131618,yushangdi,ydwu4,,,
75c4176b057,Uncategorized,Untopiced,[export][BE] consolidate export and export_for_training (#131496),torch/export/_trace.py torch/export/dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/131496,ydwu4,avikchaudhuri,pianpwk,,
29571c5c06f,skip,Untopiced,[2/3] 3D Composability - move pp tests (#129801),.ci/pytorch/multigpu-test.sh test/distributed/_composable/test_composability/test_pp_composability.py test/distributed/pipelining/test_composability.py,https://github.com/pytorch/pytorch/pull/129801,mori360,atalman,wconstab,,
ec3829795df,skip,Untopiced,[3/3] 3D Composability - move tp dp tests (#129802),.ci/pytorch/multigpu-test.sh .ci/pytorch/test.sh test/distributed/_composable/test_composability/test_2d_composability.py test/distributed/tensor/parallel/test_ddp_2d_parallel.py test/distributed/tensor/parallel/test_fsdp_2d_parallel.py,https://github.com/pytorch/pytorch/pull/129802,mori360,fduwjj,,,
c3679bed35d,skip,Untopiced,"Revert ""Fix py codegen to delete values that don't have any  users (#131028)""",test/dynamo/test_autograd_function.py test/dynamo/test_comptime.py test/dynamo/test_ctx_manager.py test/dynamo/test_export.py test/dynamo/test_higher_order_ops.py test/dynamo/test_input_attr_tracking.py test/dynamo/test_misc.py test/dynamo/test_repros.py test/export/test_export.py test/export/test_passes.py test/export/test_torchbind.py test/functorch/test_aotdispatch.py test/functorch/test_control_flow.py test/functorch/test_eager_transforms.py test/higher_order_ops/test_with_effects.py test/inductor/test_flex_attention.py test/test_functionalization.py test/test_fx.py test/test_fx_reinplace_pass.py test/test_proxy_tensor.py torch/fx/graph.py,,,,,,
2423d89d0c2,dynamo,not user facing,[dynamo] mirror training flag in OptimizedModule (#131546),test/dynamo/test_repros.py torch/_dynamo/eval_frame.py,https://github.com/pytorch/pytorch/pull/131546,williamwen42,anijain2305,yanboliang,,
f3df7deab89,skip,Untopiced,"Revert ""Add flag to ignore unsupported @triton.autotune args in user-written kernel compilation (#131431)""",test/inductor/test_triton_kernels.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_inductor/config.py torch/testing/_internal/triton_utils.py,,,,,,
dfc9bfc8839,skip,not user facing,[reland][inductor] switch AotCodeCompiler to new cpp_builder (#130127),test/inductor/test_torchinductor.py torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/130127,xuhancn,jansel,jgong5,,
2c1851f04eb,quantization,Untopiced,[export] fix output node's meta (#131706),test/quantization/pt2e/test_numeric_debugger.py torch/_dynamo/output_graph.py torch/export/_trace.py torch/export/_unlift.py torch/fx/interpreter.py,https://github.com/pytorch/pytorch/pull/131706,ydwu4,yushangdi,,,
bf6aae14686,skip,not user facing,Improve `torch.masked.mean` and `torch.masked._std_var` scaling (#131293),torch/masked/_ops.py,https://github.com/pytorch/pytorch/pull/131293,ringohoffman,ezyang,,,
8c4683c978f,skip,not user facing,Add device argument to the large_grid unit test (#131702),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/131702,FindHao,desertfire,,,
a1fad03fa86,skip,not user facing,[ROCm] Enable cudagraph expandable segments UTs in inductory/dynamo (#131111),test/dynamo/test_cudagraphs_expandable_segments.py test/inductor/test_cudagraph_trees_expandable_segments.py,https://github.com/pytorch/pytorch/pull/131111,pragupta,eqy,jataylo,peterbell10,
fba24252bde,dynamo,not user facing,[dynamo][frame summary] Skip frame summary for frames from inside torch/nn/modules (#131744),torch/_dynamo/output_graph.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/131744,anijain2305,ezyang,,,
6c31e029719,nn_frontend,docs,Fixes the example for `convert_conv3d_weight_memory_format` (#131742),torch/nn/utils/memory_format.py,https://github.com/pytorch/pytorch/pull/131742,ashwani-rathee,albanD,,,
58b8704f28f,fx,not user facing,[aot] Keep backward mutations in backward (#129130),test/functorch/test_aotdispatch.py torch/_functorch/_aot_autograd/traced_function_transforms.py torch/_functorch/partitioners.py torch/fx/proxy.py,https://github.com/pytorch/pytorch/pull/129130,IvanKobzarev,Chillee,,,
a6ebd56f7b6,inductor,not user facing,Factor out cudagraph post compile into its own function (#129384),test/inductor/test_cudagraph_trees.py torch/_inductor/compile_fx.py torch/_inductor/cudagraph_utils.py,https://github.com/pytorch/pytorch/pull/129384,jamesjwu,eellison,,,
61d7bb3e79b,releng,not user facing,Migrate trunk workflows to Amazon2023 ami (#131677),.github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/131677,ZainRizvi,malfet,,,
fdc4d6fe96f,inductor,not user facing,[inductor] Refactor fusion of inplace operations (#130835),test/inductor/test_compiled_optimizers.py torch/_inductor/comms.py torch/_inductor/dependencies.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/130835,peterbell10,lezcano,,,
69d63b2318c,cuda,not user facing,[CUDA][Pooling] Clean up unused `accscalar_t` in `maxpool2d` forward (#131728),aten/src/ATen/native/cuda/DilatedMaxPool2d.cu,https://github.com/pytorch/pytorch/pull/131728,eqy,mikaylagawarecki,,,
1ad4e6f228a,dynamo,not user facing,Refactor cudagraphs to use serializable placeholder info (#130252),test/inductor/test_cudagraph_trees.py torch/_dynamo/backends/cudagraphs.py torch/_inductor/compile_fx.py torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py,https://github.com/pytorch/pytorch/pull/130252,jamesjwu,eellison,masnesral,,
fe2e6f0c518,skip,Untopiced,"Revert ""[reland][inductor] switch AotCodeCompiler to new cpp_builder (#130127)""",test/inductor/test_torchinductor.py torch/_inductor/codecache.py,,,,,,
605dfd8fb46,releng,not user facing,Switch sync_distributed_folder to use non-reverse order (#131683),.github/scripts/sync_distributed_folder_prototype.sh,https://github.com/pytorch/pytorch/pull/131683,bigfootjon,seemethere,,,
1c58aacbc8a,distributed,not user facing,[dtensor] move ops to private (#131211),test/distributed/_tensor/test_common_rules.py test/distributed/_tensor/test_embedding_ops.py test/distributed/_tensor/test_op_strategy.py test/distributed/_tensor/test_tensor_ops.py test/distributed/_tensor/test_view_ops.py torch/distributed/_spmd/batch_dim_utils.py torch/distributed/_spmd/experimental_ops.py torch/distributed/_tensor/ops/__init__.py torch/distributed/_tensor/ops/_common_rules.py torch/distributed/_tensor/ops/_conv_ops.py torch/distributed/_tensor/ops/_einsum_strategy.py torch/distributed/_tensor/ops/_embedding_ops.py torch/distributed/_tensor/ops/_experimental_ops.py torch/distributed/_tensor/ops/_math_ops.py torch/distributed/_tensor/ops/_matrix_ops.py torch/distributed/_tensor/ops/_pointwise_ops.py torch/distributed/_tensor/ops/_random_ops.py torch/distributed/_tensor/ops/_tensor_ops.py torch/distributed/_tensor/ops/_view_ops.py torch/distributed/_tensor/ops/basic_strategy.py torch/distributed/_tensor/ops/common_rules.py torch/distributed/_tensor/ops/conv_ops.py torch/distributed/_tensor/ops/embedding_ops.py torch/distributed/_tensor/ops/experimental_ops.py torch/distributed/_tensor/ops/math_ops.py torch/distributed/_tensor/ops/matrix_ops.py torch/distributed/_tensor/ops/pointwise_ops.py torch/distributed/_tensor/ops/random_ops.py torch/distributed/_tensor/ops/tensor_ops.py torch/distributed/_tensor/ops/view_ops.py torch/distributed/tensor/parallel/loss.py,https://github.com/pytorch/pytorch/pull/131211,wanchaol,XilunWu,wz337,,
89bdd9c18fe,Uncategorized,Untopiced,[kineto] populate src/dst rank for p2p (#130812),third_party/kineto torch/csrc/profiler/util.cpp torch/csrc/profiler/util.h,https://github.com/pytorch/pytorch/pull/130812,shengbao-zheng,aaronenyeshi,,,
a34692c0a3e,inductor,not user facing,[Inductor] Added and_masks and or_masks utilities & make fully masked out rows 0 instead of nan (#131552),docs/source/nn.attention.flex_attention.rst test/inductor/test_flex_attention.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/131552,yanboliang,Chillee,,,
520182dbffe,skip,not user facing,[reland][inductor] switch AotCodeCompiler to new cpp_builder (#130127),test/inductor/test_torchinductor.py torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/130127,xuhancn,jansel,jgong5,,
9039131a89a,skip,Untopiced,TCPStore: fix remote address (#131773),test/distributed/test_store.py torch/csrc/distributed/c10d/TCPStore.cpp torch/csrc/distributed/c10d/TCPStore.hpp torch/csrc/distributed/c10d/init.cpp torch/csrc/distributed/c10d/socket.cpp,https://github.com/pytorch/pytorch/pull/131773,d4l3k,kurman,,,
513ce5f69a7,skip,not user facing,MTIA equivalent of torch.cuda.memory_stats (#131673),aten/src/ATen/detail/MTIAHooksInterface.h docs/source/mtia.rst torch/_C/__init__.pyi.in torch/csrc/mtia/Module.cpp torch/mtia/__init__.py,https://github.com/pytorch/pytorch/pull/131673,nautsimon,egienvalue,,,
0455344777f,skip,not user facing,[dynamo] Turn on inline_inbuilt_nn_modules (#131275),torch/_dynamo/config.py,https://github.com/pytorch/pytorch/pull/131275,anijain2305,ezyang,jansel,,
709ddf7a9dc,skip,not user facing,Add wrappers for synchronous GPUDirect Storage APIs (#130633),BUILD.bazel CMakeLists.txt build_variables.bzl caffe2/CMakeLists.txt cmake/Dependencies.cmake cmake/Modules/FindCUDAToolkit.cmake cmake/Summary.cmake cmake/public/cuda.cmake docs/source/cuda.rst setup.py test/test_cuda.py third_party/cuda.BUILD torch/CMakeLists.txt torch/_C/__init__.pyi.in torch/__init__.py torch/csrc/cuda/GdsFile.cpp torch/csrc/cuda/GdsFile.h torch/csrc/cuda/Module.cpp torch/cuda/__init__.py torch/cuda/gds.py,https://github.com/pytorch/pytorch/pull/130633,mikaylagawarecki,albanD,,,
193f62fde91,quantization,not user facing,[BE] typing for decorators - fx/_compatibility (#131568),torch/_dynamo/compiled_autograd.py torch/_dynamo/convert_frame.py torch/_dynamo/eval_frame.py torch/_dynamo/utils.py torch/_export/__init__.py torch/_export/converter.py torch/_export/pass_base.py torch/_export/passes/replace_set_grad_with_hop_pass.py torch/_functorch/partitioners.py torch/_higher_order_ops/flex_attention.py torch/_inductor/codegen/cpp.py torch/_inductor/constant_folding.py torch/_inductor/debug.py torch/_inductor/fx_passes/binary_folding.py torch/_inductor/fx_passes/efficient_conv_bn_eval.py torch/_inductor/fx_passes/freezing_patterns.py torch/_inductor/fx_passes/joint_graph.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/fx_passes/quantization.py torch/_inductor/fx_passes/reinplace.py torch/_inductor/fx_passes/split_cat.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/pattern_matcher.py torch/_inductor/subgraph_lowering.py torch/_inductor/utils.py torch/_subclasses/fake_tensor.py torch/ao/ns/_numeric_suite_fx.py torch/ao/ns/fx/graph_passes.py torch/ao/ns/fx/n_shadows_utils.py torch/ao/pruning/_experimental/pruner/base_structured_sparsifier.py torch/ao/pruning/_experimental/pruner/match_utils.py torch/ao/quantization/fx/_lower_to_native_backend.py torch/ao/quantization/fx/utils.py torch/ao/quantization/pt2e/duplicate_dq_pass.py torch/ao/quantization/pt2e/utils.py torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py torch/distributed/_spmd/api.py torch/distributed/_spmd/graph_utils.py torch/distributed/_spmd/iter_graph_module.py torch/distributed/_spmd/partial_lower.py torch/distributed/_tensor/experimental/tp_transform.py torch/distributed/fsdp/_trace_utils.py torch/distributed/pipelining/_IR.py torch/distributed/pipelining/microbatch.py torch/fx/_compatibility.py torch/fx/_lazy_graph_module.py torch/fx/_symbolic_trace.py torch/fx/experimental/meta_tracer.py torch/fx/experimental/proxy_tensor.py torch/fx/graph.py torch/fx/graph_module.py torch/fx/interpreter.py torch/fx/node.py torch/fx/operator_schemas.py torch/fx/passes/graph_drawer.py torch/fx/passes/graph_manipulation.py torch/fx/passes/infra/pass_manager.py torch/fx/passes/net_min_base.py torch/fx/passes/operator_support.py torch/fx/passes/param_fetch.py torch/fx/passes/runtime_assert.py torch/fx/passes/split_module.py torch/fx/passes/split_utils.py torch/fx/passes/splitter_base.py torch/fx/passes/tools_common.py torch/fx/passes/utils/common.py torch/fx/passes/utils/fuser_utils.py torch/fx/passes/utils/matcher_with_name_node_map_utils.py torch/fx/passes/utils/source_matcher_utils.py torch/fx/subgraph_rewriter.py torch/fx/traceback.py torch/nested/_internal/ops.py torch/onnx/_internal/fx/op_validation.py torch/onnx/_internal/fx/serialization.py torch/onnx/_internal/onnxruntime.py torch/utils/_python_dispatch.py,https://github.com/pytorch/pytorch/pull/131568,aorenste,justinchuby,oulgen,zou3519,
aa58af8b43a,skip,not user facing,[BE] typing for decorators - masked/_ops (#131569),torch/masked/_ops.py,https://github.com/pytorch/pytorch/pull/131569,aorenste,oulgen,zou3519,,
5731b486c87,quantization,not user facing,[BE] typing for decorators - library (#131570),torch/ao/quantization/fx/_decomposed.py torch/distributed/_composable/fsdp/_fsdp_collectives.py torch/distributed/_symmetric_memory/__init__.py torch/library.py,https://github.com/pytorch/pytorch/pull/131570,aorenste,oulgen,zou3519,,
4b985e6f803,skip,not user facing,[BE] typing for decorators - distributed/_tensor/ops/utils (#131571),torch/distributed/_spmd/experimental_ops.py torch/distributed/_tensor/ops/_conv_ops.py torch/distributed/_tensor/ops/_embedding_ops.py torch/distributed/_tensor/ops/_experimental_ops.py torch/distributed/_tensor/ops/_math_ops.py torch/distributed/_tensor/ops/_matrix_ops.py torch/distributed/_tensor/ops/_random_ops.py torch/distributed/_tensor/ops/_tensor_ops.py torch/distributed/_tensor/ops/_view_ops.py torch/distributed/_tensor/ops/utils.py,https://github.com/pytorch/pytorch/pull/131571,aorenste,oulgen,zou3519,,
bfe0079b72a,skip,not user facing,[BE] typing for decorators - _meta_registrations (#131572),torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/131572,aorenste,oulgen,zou3519,,
f0f20f7e977,quantization,not user facing,[BE] typing for decorators - _jit_internal (#131573),torch/_jit_internal.py torch/ao/nn/quantized/dynamic/modules/rnn.py torch/ao/nn/quantized/modules/embedding_ops.py torch/ao/nn/quantized/modules/linear.py torch/ao/nn/sparse/quantized/linear.py torch/ao/quantization/experimental/adaround_fake_quantize.py torch/ao/quantization/fake_quantize.py torch/ao/quantization/observer.py torch/backends/_nnapi/prepare.py torch/distributed/optim/optimizer.py torch/jit/_recursive.py torch/nn/modules/adaptive.py torch/nn/modules/container.py torch/nn/modules/rnn.py torch/nn/utils/parametrize.py,https://github.com/pytorch/pytorch/pull/131573,aorenste,oulgen,zou3519,,
b2cbcf710b2,skip,not user facing,[BE] typing for decorators - _inductor/lowering (#131574),torch/_inductor/jagged_lowerings.py torch/_inductor/kernel/conv.py torch/_inductor/lowering.py torch/_inductor/mkldnn_lowerings.py,https://github.com/pytorch/pytorch/pull/131574,aorenste,oulgen,zou3519,,
42dc5a47a15,skip,not user facing,[BE] typing for decorators - _inductor/fx_passes/post_grad (#131575),torch/_inductor/fx_passes/post_grad.py,https://github.com/pytorch/pytorch/pull/131575,aorenste,oulgen,zou3519,,
37d76c7d483,fx,not user facing,[BE] typing for decorators - fx/experimental/migrate_gradual_types/constraint_generator (#131576),torch/fx/experimental/migrate_gradual_types/constraint_generator.py,https://github.com/pytorch/pytorch/pull/131576,aorenste,oulgen,zou3519,,
5ee6a6dacc9,quantization,not user facing,[BE] typing for decorators - ao/quantization/quantizer/xnnpack_quantizer_utils (#131577),torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py,https://github.com/pytorch/pytorch/pull/131577,aorenste,oulgen,zou3519,,
c65b197b85a,skip,not user facing,[BE] typing for decorators - _library/custom_ops (#131578),torch/_functorch/apis.py torch/_functorch/batch_norm_replacement.py torch/_functorch/functional_call.py torch/_library/custom_ops.py torch/_library/infer_schema.py torch/_library/triton.py torch/nn/attention/flex_attention.py torch/testing/_internal/custom_op_db.py torch/utils/_exposed_in.py,https://github.com/pytorch/pytorch/pull/131578,aorenste,oulgen,zou3519,,
79f0c4dc04c,fx,not user facing,[BE] typing for decorators - fx/experimental/graph_gradual_typechecker (#131579),torch/fx/experimental/graph_gradual_typechecker.py,https://github.com/pytorch/pytorch/pull/131579,aorenste,oulgen,zou3519,,
4de85e3c309,distributed,not user facing,[DeviceMesh] Remove _parent_mesh as an attribute from DeviceMesh and remove it from DeviceMesh's hash (#131636),test/distributed/test_device_mesh.py torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/131636,wz337,wanchaol,,,
51f4f877186,dynamo,Untopiced,[Reland] Ensure staticmethods can be allowed in graph (#131789),test/dynamo/test_repros.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/131789,mlazos,anijain2305,,,
b4b62d39451,skip,not user facing,update to 2.5.8 (#131684),aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/flash_attn/flash_api.cpp aten/src/ATen/native/transformers/cuda/flash_attn/flash_api.h aten/src/ATen/native/transformers/cuda/flash_attn/flash_fwd_kernel.h aten/src/ATen/native/transformers/hip/flash_attn/flash_api.hip torch/nn/attention/__init__.py,https://github.com/pytorch/pytorch/pull/131684,drisspg,jainapurva,,,
f885a70fabe,inductor,not user facing,[inductor][autotune_at_compile_time] support Triton kernel with sympy fn str arg (#131253),test/inductor/test_aot_inductor.py torch/_inductor/codegen/wrapper.py,https://github.com/pytorch/pytorch/pull/131253,ColinPeppler,desertfire,,,
11673851d9f,dynamo,not user facing,[dynamo][exception][bugfix] Add a pop for < 3.11 version (#131795),test/dynamo/test_exceptions.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/131795,anijain2305,yanboliang,,,
2a4ca5ccc41,dynamo,not user facing,[dynamo] Pop the exception stack on handling the StopIteration natively (#131801),torch/_dynamo/exc.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lists.py,https://github.com/pytorch/pytorch/pull/131801,anijain2305,yanboliang,,,
eac83479ccf,distributed,not user facing,Enable Wunused-function and Wunused-result globally  (#131596),CMakeLists.txt functorch/csrc/dim/dim.cpp functorch/csrc/dim/minpybind.h torch/csrc/distributed/c10d/control_plane/WorkerServer.hpp torch/csrc/inductor/aoti_eager/kernel_holder.cpp,https://github.com/pytorch/pytorch/pull/131596,cyyever,zou3519,,,
e9443860e76,autograd_frontend,Untopiced,add python binding for _get_current_graph_task_keep_graph (#131038),test/test_autograd.py torch/csrc/autograd/init.cpp,https://github.com/pytorch/pytorch/pull/131038,bdhirsh,soulitzer,,,
adbe4f5ecf3,distributed,Untopiced,TCPStore: add better logging on wait timeout (#131808),test/distributed/test_c10d_common.py test/distributed/test_store.py torch/csrc/distributed/c10d/TCPStore.cpp torch/csrc/distributed/c10d/socket.h,https://github.com/pytorch/pytorch/pull/131808,d4l3k,kurman,,,
14920c149b9,skip,Untopiced,"Revert ""[dynamo] Turn on inline_inbuilt_nn_modules (#131275)""",torch/_dynamo/config.py,,,,,,
9ae288f4bee,inductor,Untopiced,[inductor] Simplify multi-kernel codegen by unifying kernel args (#127724),test/inductor/test_multi_kernel.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/codegen/simd.py,https://github.com/pytorch/pytorch/pull/127724,peterbell10,shunting314,,,
c92f2a19a4d,skip,not user facing,[BE] Use assertEqual in MultiKernel tests (#127725),test/inductor/test_multi_kernel.py,https://github.com/pytorch/pytorch/pull/127725,peterbell10,lezcano,,,
246e32055a1,skip,not user facing,[benchmark] Add hf_T5_generate to inline_inbuilt_nn_modules (#131804),benchmarks/dynamo/torchbench.py,https://github.com/pytorch/pytorch/pull/131804,anijain2305,shunting314,yanboliang,,
b893a57f96d,dynamo,not user facing,[Dynamo] Fix guard_on_nn_modules unit tests discrepancy between OSS and fbcode (#131810),test/dynamo/test_modules.py,https://github.com/pytorch/pytorch/pull/131810,yanboliang,zou3519,,,
b343644f3ac,skip,Untopiced,"Revert ""MTIA equivalent of torch.cuda.memory_stats (#131673)""",aten/src/ATen/detail/MTIAHooksInterface.h docs/source/mtia.rst torch/_C/__init__.pyi.in torch/csrc/mtia/Module.cpp torch/mtia/__init__.py,,,,,,
2ff98bc57fd,inductor,not user facing,[inductor][autotune_at_compile_time] fix some codegen-ing for standalone autotuning file (#131726),test/inductor/test_aot_inductor.py torch/_inductor/codegen/wrapper.py,https://github.com/pytorch/pytorch/pull/131726,ColinPeppler,desertfire,,,
16699c7d848,inductor,not user facing,[CUDAGraph] Type annotation for cudagraph_trees.py (#131621),torch/_C/__init__.pyi.in torch/_inductor/compile_fx.py torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/131621,BoyuanFeng,eellison,,,
03f49c9523d,skip,Untopiced,"Revert ""[CUDAGraph] Type annotation for cudagraph_trees.py (#131621)""",torch/_C/__init__.pyi.in torch/_inductor/compile_fx.py torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py torch/_inductor/utils.py,,,,,,
236d0553303,distributed,not user facing,[Traceable FSDP2] Add partial-graph (graph-break) unit tests (#131747),test/distributed/_composable/fsdp/test_fully_shard_compile.py torch/distributed/_composable/fsdp/_fsdp_param.py,https://github.com/pytorch/pytorch/pull/131747,yf225,bdhirsh,,,
e0d3e4a498b,skip,not user facing,remove unused code for XPU (#131856),torch/csrc/xpu/Stream.cpp,https://github.com/pytorch/pytorch/pull/131856,guangyey,EikanWang,,,
c2f3266c8e1,distributed,Untopiced,Not remove collective ops in dce since they have side-effect (#131023),test/fx/test_dce_pass.py torch/distributed/_functional_collectives.py,https://github.com/pytorch/pytorch/pull/131023,YangQun1,yf225,,,
6dbf343936d,inductor,Untopiced,Fix aten implementation for low memory max_pool2d (#131717),test/inductor/test_torchinductor.py torch/_inductor/inductor_prims.py,https://github.com/pytorch/pytorch/pull/131717,isuruf,peterbell10,,,
3d7c424a750,inductor,not user facing,[inductor] update users to buffers instead of scheduler nodes (#131796),torch/_inductor/debug.py,https://github.com/pytorch/pytorch/pull/131796,xuanzhang816,yf225,,,
a617919541b,dynamo,Untopiced,[dynamo] Do not guard on keys for _forward_hooks and _forward_pre_hooks (#131682),test/dynamo/test_hooks.py test/dynamo_expected_failures/TestPruningNN.test_pruning_id_consistency torch/_dynamo/variables/nn_module.py,https://github.com/pytorch/pytorch/pull/131682,anijain2305,bdhirsh,,,
5b05ad9697f,Uncategorized,Untopiced,fix non-persistent buffers (#131756),test/export/test_export.py torch/_export/__init__.py torch/export/_trace.py torch/export/exported_program.py torch/export/graph_signature.py,https://github.com/pytorch/pytorch/pull/131756,avikchaudhuri,ydwu4,zhxchen17,,
33069630ce6,inductor,not user facing,[inductor] Add type hints to functions in decompositions.py (#131780),torch/_inductor/decomposition.py,https://github.com/pytorch/pytorch/pull/131780,aakhundov,eellison,,,
81c26ba5ae1,skip,not user facing,[BE] typing for decorators - utils/flop_counter (#131580),torch/utils/flop_counter.py,https://github.com/pytorch/pytorch/pull/131580,aorenste,oulgen,zou3519,,
dbf7c318b2d,skip,not user facing,[BE] typing for decorators - _refs/nn/functional (#131581),torch/_refs/nn/functional/__init__.py,https://github.com/pytorch/pytorch/pull/131581,aorenste,oulgen,zou3519,,
8689d377f9b,skip,not user facing,[BE] typing for decorators - signal/windows/windows (#131582),torch/signal/windows/windows.py,https://github.com/pytorch/pytorch/pull/131582,aorenste,oulgen,zou3519,,
a1dad77dfa4,optim,not user facing,[BE] typing for decorators - optim/optimizer (#131583),torch/optim/adadelta.py torch/optim/adam.py torch/optim/adamax.py torch/optim/adamw.py torch/optim/asgd.py torch/optim/nadam.py torch/optim/optimizer.py torch/optim/radam.py torch/optim/rmsprop.py torch/optim/rprop.py,https://github.com/pytorch/pytorch/pull/131583,aorenste,janeyx99,,,
baa93e160f8,mps,Untopiced,[MPS] Add native implementation for shift ops (#131813),aten/src/ATen/mps/MPSFallback.mm aten/src/ATen/native/mps/operations/BitwiseOps.mm aten/src/ATen/native/native_functions.yaml,https://github.com/pytorch/pytorch/pull/131813,malfet,manuelcandales,,,
dfba85c26bf,skip,not user facing,Update torch-xpu-ops pin (ATen XPU implementation) (#131643),third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/131643,guangyey,EikanWang,,,
16d7cb50497,inductor,not user facing,[CUDAGraph] Type annotation for cudagraph_trees.py (#131621),torch/_C/__init__.pyi.in torch/_inductor/compile_fx.py torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/131621,BoyuanFeng,eellison,,,
aebfd3d4de5,inductor,Untopiced,[CUDAGraph] skip cudagraph if too many distinct sizes (#131387),test/inductor/test_cudagraph_trees.py torch/_inductor/config.py torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py,https://github.com/pytorch/pytorch/pull/131387,BoyuanFeng,eellison,,,
63374dda699,skip,not user facing,[BE][Easy] explicitly define global constants in `torch.testing._internal.common_utils` (#129826),torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/129826,XuehaiPan,Skylion007,,,
13e806a5917,skip,not user facing,[NestedTensor] Add support for transposed NestedTensors where ragged_idx > 1 for sum and mean operators (#131517),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/131517,jananisriram,davidberard98,,,
fb3ddafbcfe,skip,not user facing,[inductor] Add type hints to functions in mkldnn_fusion.py (#131820),torch/_inductor/fx_passes/freezing_patterns.py torch/_inductor/fx_passes/mkldnn_fusion.py torch/_inductor/fx_passes/post_grad.py,https://github.com/pytorch/pytorch/pull/131820,aakhundov,eellison,,,
2bf649f5ae5,fx,Untopiced,suggested fix for data-dependent error (#125378),test/export/test_export.py torch/export/_trace.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/125378,avikchaudhuri,ezyang,,,
bceb91222cd,linalg_frontend,Untopiced,Fix meta error in _convert_weight_to_int4pack (#130915),test/test_linalg.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/130915,yanbing-j,jerryzh168,,,
6fd28fc228f,skip,not user facing,immutable accessors in graph signature (#131807),torch/export/graph_signature.py,https://github.com/pytorch/pytorch/pull/131807,avikchaudhuri,ydwu4,,,
1e24f7875e2,releng,not user facing,[AOTI] Fix ABI-compatible mode link issue for CPU (#131791),.ci/pytorch/test.sh torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/131791,desertfire,chenyang78,zou3519,,
115994fea28,distributed,Untopiced,[aotd] Align partitioner graph output type to tuple (#131759),test/distributed/_tensor/test_dtensor_compile.py test/functorch/test_aotdispatch.py test/inductor/test_flex_attention.py torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/131759,IvanKobzarev,bdhirsh,ezyang,,
7d282d87550,dynamo,not user facing,[dynamo] add lazy IteratorVariable implementations for map and zip (#131413),test/dynamo/test_functions.py test/dynamo/test_repros.py torch/_dynamo/symbolic_convert.py torch/_dynamo/utils.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lists.py,https://github.com/pytorch/pytorch/pull/131413,williamwen42,anijain2305,,,
945946e8173,releng,not user facing,[AOTI] Fix another ABI-compatible CPU issue (#131798),.ci/pytorch/test.sh torch/csrc/inductor/aoti_runtime/utils.h,https://github.com/pytorch/pytorch/pull/131798,desertfire,chenyang78,zou3519,,
608057afe23,inductor,not user facing,[inductor] Fix duplicated range tree codegen in split scan (#131669),torch/_inductor/codegen/triton_split_scan.py,https://github.com/pytorch/pytorch/pull/131669,peterbell10,Chillee,,,
e73fa28ec81,releng,not user facing,[CI] Fix arm64 docker build arch (#131869),.github/actionlint.yaml .github/workflows/docker-builds.yml,https://github.com/pytorch/pytorch/pull/131869,huydhn,desertfire,,,
d3e932dc104,releng,not user facing,[CI] Add inductor cpu accuracy test running on AVX2 runners (#128682),.ci/pytorch/test.sh .github/workflows/inductor.yml benchmarks/dynamo/check_accuracy.py test/test_ops.py,https://github.com/pytorch/pytorch/pull/128682,zxd1997066,desertfire,jgong5,,
7ee6831ae85,skip,Untopiced,"Revert ""Fix vulkan builds with missing overrides errors (#131760)""",aten/src/ATen/native/vulkan/api/Allocator.h,,,,,,
c9888c27394,skip,Untopiced,"Revert ""[BE] typing for decorators - optim/optimizer (#131583)""",torch/optim/adadelta.py torch/optim/adam.py torch/optim/adamax.py torch/optim/adamw.py torch/optim/asgd.py torch/optim/nadam.py torch/optim/optimizer.py torch/optim/radam.py torch/optim/rmsprop.py torch/optim/rprop.py,,,,,,
e76e566cfb4,dynamo,not user facing,[Dynamo] Support zip_longest (#131497),test/dynamo/test_functions.py torch/_dynamo/polyfill.py torch/_dynamo/variables/iter.py,https://github.com/pytorch/pytorch/pull/131497,yanboliang,jansel,mlazos,zou3519,
7339c8ab28e,skip,Untopiced,"Revert ""immutable accessors in graph signature (#131807)""",torch/export/graph_signature.py,,,,,,
8bb9aa93a74,dynamo,Untopiced,dynamo: mutations on .data should be invisible to autograd (#131403),test/dynamo/test_input_attr_tracking.py test/dynamo/test_repros.py test/dynamo_expected_failures/TestVmapAPI.test_data_attribute torch/_dynamo/variables/tensor.py torch/csrc/autograd/init.cpp,https://github.com/pytorch/pytorch/pull/131403,bdhirsh,anijain2305,zou3519,,
5570a0da0af,composability,Untopiced,dont dispatch aten.conj(scalar_tensor) back to python (#131482),test/dynamo/test_repros.py torch/csrc/utils/python_arg_parser.cpp,https://github.com/pytorch/pytorch/pull/131482,bdhirsh,ezyang,zou3519,,
e4ace1a3962,composability,Untopiced,AOTDispatcher: properly bump version counter on input mutations in inference graphs (#131665),test/dynamo/test_repros.py test/inductor/test_torchinductor.py torch/_functorch/_aot_autograd/runtime_wrappers.py,https://github.com/pytorch/pytorch/pull/131665,bdhirsh,ezyang,zou3519,,
535c17efb33,Uncategorized,Untopiced,[torch] Implement c10::BFloat16 ctor from __hip_bfloat16 (#131359),c10/util/BFloat16-inl.h c10/util/BFloat16.h,https://github.com/pytorch/pytorch/pull/131359,danzimm,houseroad,,,
9440a4824de,devx,not user facing,[CI][dashboard] Add a workflow to collect A10g perf (#131816),.ci/pytorch/test.sh .github/workflows/inductor-perf-test-nightly-a10g.yml .github/workflows/upload-torch-dynamo-perf-stats.yml,https://github.com/pytorch/pytorch/pull/131816,desertfire,huydhn,,,
eba2ffd278a,skip,Untopiced,[pt2e][quant] Ensure BN node is erased after convert (#131651),test/quantization/pt2e/test_quantize_pt2e_qat.py torch/ao/quantization/pt2e/utils.py,https://github.com/pytorch/pytorch/pull/131651,andrewor14,leslie-fang-intel,yushangdi,,
5612408735a,jit,not user facing,_get_operation_overload: dont raise exception when overload does not exist (#131554),torch/_ops.py torch/csrc/jit/python/init.cpp,https://github.com/pytorch/pytorch/pull/131554,bdhirsh,ezyang,zou3519,,
2988d33c805,jit,not user facing,[3/N] Fix clang-tidy warnings in jit  (#131830),torch/csrc/jit/python/init.cpp torch/csrc/jit/python/pybind_utils.cpp torch/csrc/jit/python/pybind_utils.h torch/csrc/jit/python/python_custom_class.cpp torch/csrc/jit/python/python_custom_class.h torch/csrc/jit/python/python_ir.cpp torch/csrc/jit/python/python_sugared_value.cpp torch/csrc/jit/python/python_tracer.cpp torch/csrc/jit/python/python_tracer.h torch/csrc/jit/python/python_tree_views.cpp torch/csrc/jit/python/script_init.cpp torch/csrc/utils/python_arg_parser.cpp torch/csrc/utils/python_arg_parser.h,https://github.com/pytorch/pytorch/pull/131830,cyyever,ezyang,,,
546df5daf81,skip,Untopiced,"Revert ""[3/3] 3D Composability - move tp dp tests (#129802)""",.ci/pytorch/multigpu-test.sh .ci/pytorch/test.sh test/distributed/_composable/test_composability/test_2d_composability.py test/distributed/tensor/parallel/test_ddp_2d_parallel.py test/distributed/tensor/parallel/test_fsdp_2d_parallel.py,,,,,,
7feaa730576,Uncategorized,Untopiced,[export] Remove deprecated fields from ExportedProgram ctor. (#131697),torch/export/_trace.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/131697,zhxchen17,ydwu4,,,
13ab92b72dc,dynamo,not user facing,[dynamo][recompile-logs] Suggest force_parameter_static_shapes on the recompile log for parameter-related recomps (#131825),test/dynamo/test_misc.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/131825,anijain2305,ezyang,,,
a38890a53f8,skip,Untopiced,"Revert ""[2/3] 3D Composability - move pp tests (#129801)""",.ci/pytorch/multigpu-test.sh test/distributed/_composable/test_composability/test_pp_composability.py test/distributed/pipelining/test_composability.py,,,,,,
d57de73fe01,inductor,not user facing,AutoHeuristic: Add support for kernel choice selection (#131610),test/inductor/test_autoheuristic.py torch/_inductor/autoheuristic/autoheuristic.py torch/_inductor/autoheuristic/autoheuristic_utils.py torch/_inductor/fx_passes/pad_mm.py torch/_inductor/ir.py torch/_inductor/kernel/mm.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/131610,AlnisM,eellison,,,
bb64702eb34,skip,Untopiced,"Revert ""[reland][inductor] switch AotCodeCompiler to new cpp_builder (#130127)""",test/inductor/test_torchinductor.py torch/_inductor/codecache.py,,,,,,
404a8ae8f67,Uncategorized,Untopiced,[export] fix set_grad x tensor constant. (#131787),test/export/test_export.py torch/_export/passes/replace_set_grad_with_hop_pass.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/131787,ydwu4,yushangdi,,,
696e83a1da7,skip,Untopiced,"Revert ""TCPStore: fix remote address (#131773)""",test/distributed/test_store.py torch/csrc/distributed/c10d/TCPStore.cpp torch/csrc/distributed/c10d/TCPStore.hpp torch/csrc/distributed/c10d/init.cpp torch/csrc/distributed/c10d/socket.cpp,,,,,,
1a2edf6dca0,inductor,not user facing,[AOTI] Fix _mm_plus_mm codegen (#131689),test/inductor/test_cuda_cpp_wrapper.py test/inductor/test_select_algorithm.py torch/_inductor/ir.py torch/_inductor/kernel/mm_plus_mm.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp torch/csrc/inductor/inductor_ops.cpp torch/csrc/inductor/inductor_ops.h,https://github.com/pytorch/pytorch/pull/131689,desertfire,chenyang78,,,
c382fc3fea6,vulkan,not user facing,[Reland] Fix vulkan builds with missing overrides errors  (#131760),aten/src/ATen/native/vulkan/api/Allocator.h,https://github.com/pytorch/pytorch/pull/131760,malfet,atalman,,,
161bb671168,skip,Untopiced,"Revert ""Fix static `py::object` dangling pointer with `py::gil_safe_call_once_and_store` (#130341)""",functorch/csrc/dim/dim.cpp torch/csrc/dynamo/guards.cpp torch/csrc/jit/python/module_python.h torch/csrc/jit/python/python_ivalue.h torch/csrc/utils/python_arg_parser.cpp torch/csrc/utils/python_symnode.cpp,,,,,,
9589d986faa,skip,not user facing,[UT] Relax atol for test_non_contiguous_input_* (3 tests) (#131822),test/inductor/test_max_autotune.py,https://github.com/pytorch/pytorch/pull/131822,yangsiyu007,shunting314,,,
40cc5c06972,functorch,Untopiced,[AOT Autograd] Donated Buffer (#130580),test/dynamo/test_aot_autograd.py torch/_C/_autograd.pyi torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/config.py,https://github.com/pytorch/pytorch/pull/130580,BoyuanFeng,bdhirsh,,,
35b4de32faf,dynamo,not user facing,[dynamo] add itertools repeat/count bytecode reconstruction (#131716),test/dynamo/test_functions.py torch/_dynamo/variables/iter.py,https://github.com/pytorch/pytorch/pull/131716,williamwen42,anijain2305,,,
2576dbbc35d,dynamo,not user facing,[dynamo] implement IteratorVariable and polyfill fallbacks for enumerate (#131725),test/dynamo/test_functions.py torch/_dynamo/polyfill.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/iter.py,https://github.com/pytorch/pytorch/pull/131725,williamwen42,anijain2305,,,
e4db5dc1c48,skip,Untopiced,"Revert ""[BE] remove unnecessary _dispatch_sqrt by using ** 0.5 (#131358)""",torch/optim/adam.py torch/optim/adamw.py torch/optim/nadam.py torch/optim/optimizer.py torch/optim/radam.py,,,,,,
e191b834623,skip,Untopiced,"Revert ""Add wrappers for synchronous GPUDirect Storage APIs (#130633)""",BUILD.bazel CMakeLists.txt build_variables.bzl caffe2/CMakeLists.txt cmake/Dependencies.cmake cmake/Modules/FindCUDAToolkit.cmake cmake/Summary.cmake cmake/public/cuda.cmake docs/source/cuda.rst setup.py test/test_cuda.py third_party/cuda.BUILD torch/CMakeLists.txt torch/_C/__init__.pyi.in torch/__init__.py torch/csrc/cuda/GdsFile.cpp torch/csrc/cuda/GdsFile.h torch/csrc/cuda/Module.cpp torch/cuda/__init__.py torch/cuda/gds.py,,,,,,
8158cf2f59c,distributed,Untopiced,[c10d] Fix split_group usage when there is a single rank (#131824),torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/131824,shuqiangzhang,fduwjj,pavanbalaji,,
1dd10ac8029,nn_frontend,improvements,[BE] [Reland] Make nn.Module state_dict load_state_dict pre-hook and state_dict post-hook public (#131690),test/nn/test_load_state_dict.py test/nn/test_module_hooks.py torch/ao/quantization/fx/_lower_to_native_backend.py torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py torch/nn/modules/module.py,https://github.com/pytorch/pytorch/pull/131690,mikaylagawarecki,albanD,,,
5f2c80d16d6,skip,not user facing,Add inductor OrderedSet (#130003),test/inductor/test_ordered_set.py torch/utils/_ordered_set.py,https://github.com/pytorch/pytorch/pull/130003,eellison,aorenste,,,
cd53698df06,dynamo,not user facing,Add hpu backend support for dynamo torchVariable _in_graph_classes() function (#129948),torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/129948,VRSinghHabana,yanboliang,,,
a3cdbd8189a,composability,Untopiced,[FlopCounterMode] Fix register_flop_formula (#131777),test/test_flop_counter.py torch/utils/flop_counter.py,https://github.com/pytorch/pytorch/pull/131777,zou3519,Chillee,,,
0f9bf208ec0,skip,Untopiced,"Revert ""[BE][tests] show local variables on failure in tests (#131151)""",test/run_test.py torch/testing/_internal/common_utils.py,,,,,,
782efd8e5b7,skip,Untopiced,"Revert ""Add rerun_disabled_tests for inductor (#131681)""",.github/workflows/inductor-cu124.yml,,,,,,
6c95f79645d,devx,not user facing,[CI] Increase the timeout for aarch64 docker build (#131926),.github/workflows/docker-builds.yml,https://github.com/pytorch/pytorch/pull/131926,desertfire,avikchaudhuri,,,
da1a1fa55ff,Uncategorized,Untopiced,Move load_yaml_file to common (#131924),benchmarks/dynamo/common.py benchmarks/dynamo/torchbench.py,https://github.com/pytorch/pytorch/pull/131924,kit1980,huydhn,shunting314,,
2ec8312a284,skip,not user facing,Add rerun_disabled_tests for inductor (#131681),.github/workflows/inductor-cu124.yml .github/workflows/inductor.yml,https://github.com/pytorch/pytorch/pull/131681,clee2000,zou3519,,,
071ac381410,fx,Untopiced,fast-path FakeTensor detach (#131899),torch/_subclasses/fake_impls.py torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/131899,bdhirsh,albanD,ezyang,zou3519,
0e6df1e0fbd,skip,not user facing,Disable remote cache on test (#131908),test/inductor/test_cudagraph_trees.py,https://github.com/pytorch/pytorch/pull/131908,jamesjwu,clee2000,,,
1bda3a31356,releng,not user facing,Migrate nightly.yml workflow & docs to Amazon 2023 (#131821),.github/workflows/_docs.yml .github/workflows/nightly.yml .github/workflows/pull.yml,https://github.com/pytorch/pytorch/pull/131821,ZainRizvi,atalman,seemethere,,
fde577702d7,skip,not user facing,[TD] More synonyms for filepath (#131838),tools/testing/target_determination/heuristics/filepath.py,https://github.com/pytorch/pytorch/pull/131838,clee2000,PaliC,ZainRizvi,,
c8626a4e1f0,skip,not user facing,[BE] add a list of inductor test files to skip resetting dynamo (#131551),torch/testing/_internal/common_utils.py torch/testing/_internal/dynamo_test_failures.py,https://github.com/pytorch/pytorch/pull/131551,shunting314,zou3519,,,
fb98cd33f1c,skip,not user facing,[inline_inbuilt_nn_modules][inductor-cpu] Skip test_quantized_linear_amx (#131928),test/inductor/test_cpu_select_algorithm.py,https://github.com/pytorch/pytorch/pull/131928,anijain2305,eellison,,,
d73b55d64b0,skip,not user facing,Support meta tensors as inputs to the triton_kernel_wrapper HOPs (#131896),test/inductor/test_triton_kernels.py torch/_higher_order_ops/triton_kernel_wrap.py,https://github.com/pytorch/pytorch/pull/131896,zou3519,oulgen,,,
b90bc66766c,skip,not user facing,Enable FlashAttention on Windows (#131906),CMakeLists.txt torch/testing/_internal/common_cuda.py,https://github.com/pytorch/pytorch/pull/131906,lw,drisspg,eqy,,
16cd1aaa1dc,inductor,Untopiced,[inductor] Improve sort kernel perf (#131719),torch/_inductor/codegen/triton.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/runtime/triton_helpers.py,https://github.com/pytorch/pytorch/pull/131719,peterbell10,eellison,,,
5489ff8e949,quantization,not user facing,Use Mermaid for the diagram in torch/ao/quantization/fx/README.md (#131412),torch/ao/quantization/fx/README.md,https://github.com/pytorch/pytorch/pull/131412,kit1980,jerryzh168,,,
0272934238e,inductor,not user facing,[Inductor][CPU] Fix an InvalidVecISA issue on CI (#131812),.ci/pytorch/test.sh benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_freezing_inference.csv torch/_inductor/cpu_vec_isa.py,https://github.com/pytorch/pytorch/pull/131812,desertfire,eellison,malfet,,
02b922900b6,inductor,not user facing,[aoti] Fix float16 and bfloat16 for generated GPU code (#131437),test/inductor/test_cuda_cpp_wrapper.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cuda.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/131437,yushangdi,desertfire,,,
91fcfd87600,releng,not user facing,Fix public API tests (#131386),test/test_public_bindings.py torch/ao/quantization/__init__.py torch/distributed/pipelining/schedules.py torch/fx/experimental/graph_gradual_typechecker.py torch/fx/experimental/migrate_gradual_types/constraint_generator.py torch/library.py torch/nn/attention/flex_attention.py torch/onnx/symbolic_helper.py torch/optim/__init__.py torch/storage.py,https://github.com/pytorch/pytorch/pull/131386,jbschlosser,albanD,,,
1e00f055a4b,releng,not user facing,Move distributed experimental jobs back to the amazon2 for now (#131963),.github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/131963,ZainRizvi,clee2000,,,
8b04edcac1f,releng,not user facing,Delete unused yml files (#131298),.github/actions/linux-build/action.yml .github/workflows/_linux-build-label.yml,https://github.com/pytorch/pytorch/pull/131298,malfet,ZainRizvi,,,
ef8d118c677,releng,not user facing,Sync with changes to test-infra's scale-config.yml (#131955),.github/lf-canary-scale-config.yml .github/lf-scale-config.yml,https://github.com/pytorch/pytorch/pull/131955,ZainRizvi,malfet,,,
8458980bbf7,skip,not user facing,Move benchmarks/dynamo/huggingface configuration to YAML  (#131724),benchmarks/dynamo/huggingface.py benchmarks/dynamo/huggingface.yaml,https://github.com/pytorch/pytorch/pull/131724,kit1980,shunting314,,,
fdf1451bfa5,optim,Untopiced,Add `__all__` to torch.optim to define public interface (#131959),torch/optim/__init__.py,https://github.com/pytorch/pytorch/pull/131959,ringohoffman,ezyang,,,
9606d61e0c9,skip,not user facing,[reland][inductor] switch AotCodeCompiler to new cpp_builder (#130127),test/inductor/test_torchinductor.py torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/130127,xuhancn,jansel,jgong5,,
3768faec2f8,fx,Untopiced,carry cond in data-dependent error (#131932),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/131932,avikchaudhuri,zhxchen17,,,
a90b8b967a5,inductor,not user facing,[inductor] enable windows inductor UTs (#131767),test/inductor/test_torchinductor.py torch/_inductor/utils.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/131767,xuhancn,jansel,jgong5,,
28fd2e905da,inductor,not user facing,[inductor] enhance cpp_builder lint check. (#131752),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/131752,xuhancn,jansel,jgong5,,
bcdba9f91da,distributed,not user facing,Added hpu backend support in fsdp utils  (#127757),torch/distributed/fsdp/_init_utils.py,https://github.com/pytorch/pytorch/pull/127757,VRSinghHabana,awgu,jgong5,wconstab,
1bfe7eb7e64,skip,not user facing,Update how we do sdpa testing (#131743),test/test_transformers.py,https://github.com/pytorch/pytorch/pull/131743,drisspg,jainapurva,jbschlosser,,
96c1862e0b1,dynamo,not user facing,Remove mypy ignore from torch/_dynamo/variables/__init__.py (#131784),torch/_dynamo/variables/__init__.py,https://github.com/pytorch/pytorch/pull/131784,oulgen,Skylion007,aorenste,zou3519,
c49e857d323,releng,Untopiced,[pt] immutable accessors in graph signature (#131940),.ci/docker/ci_commit_pins/executorch.txt torch/export/graph_signature.py,https://github.com/pytorch/pytorch/pull/131940,avikchaudhuri,angelayi,zhxchen17,,
bcf5c68c18c,skip,not user facing,[NestedTensor] Integrate the softmax operator along the jagged dimension into NestedTensor (#131518),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/131518,jananisriram,davidberard98,,,
f862f457304,skip,not user facing,[NestedTensor] Integrate the layer normalization operator along the jagged dimension into NestedTensor (#131519),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/131519,jananisriram,davidberard98,,,
99e13e68e94,jit,not user facing,[4/N] Fix clang-tidy warnings in jit  (#131903),torch/csrc/jit/api/module.cpp torch/csrc/jit/api/module.h torch/csrc/jit/frontend/source_range.cpp torch/csrc/jit/frontend/source_range.h torch/csrc/jit/python/pybind.h torch/csrc/jit/python/pybind_utils.h torch/csrc/jit/python/python_dict.h torch/csrc/jit/python/python_interpreter.cpp torch/csrc/jit/python/python_ir.cpp torch/csrc/jit/python/python_list.cpp torch/csrc/jit/python/python_list.h torch/csrc/jit/python/python_sugared_value.cpp torch/csrc/jit/python/python_sugared_value.h torch/csrc/jit/python/python_tree_views.cpp torch/csrc/jit/python/script_init.cpp,https://github.com/pytorch/pytorch/pull/131903,cyyever,Skylion007,,,
9e065727048,inductor,not user facing,[Traceable FSDP2][Inductor] Create grouped nodes for FSDP2 all-gather code block and reduce-scatter code block (after Buffer/Operation split) (#131510),test/distributed/_composable/fsdp/test_fully_shard_compile.py test/distributed/test_c10d_nccl.py test/distributed/test_compute_comm_reordering.py torch/_inductor/comms.py torch/_inductor/ir.py torch/_inductor/scheduler.py torch/_inductor/utils.py torch/testing/_internal/common_distributed.py torch/testing/_internal/common_fsdp.py torch/testing/_internal/distributed/distributed_test.py,https://github.com/pytorch/pytorch/pull/131510,yf225,yifuwang,,,
aee6bcdba40,inductor,not user facing,[Traceable FSDP2][Inductor] Apply compute/comm reordering passes to achieve overlap (#131614),test/distributed/_composable/fsdp/test_fully_shard_compile.py torch/_inductor/comms.py torch/_inductor/scheduler.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/131614,yf225,yifuwang,,,
36d24925c66,skip,not user facing,[inline_inbuilt_nn_modules][inductor-cpu] More skips for dynamic shapes when inlining enabled (#131948),test/inductor/test_cpu_select_algorithm.py,https://github.com/pytorch/pytorch/pull/131948,anijain2305,eellison,leslie-fang-intel,,
12cd040eddb,distributed,not user facing,[micro_pipeline_tp] exclude simple overlappable collectives as micro-pipeline TP candidates when reorder_for_compute_comm_overlap is enabled (#131410),test/distributed/tensor/parallel/test_micro_pipeline_tp.py torch/_inductor/fx_passes/micro_pipeline_tp.py,https://github.com/pytorch/pytorch/pull/131410,yifuwang,weifengpy,,,
93a46717466,distributed,not user facing,Add out_dtypes to fused_all_gather_scaled_matmul's args (#131831),test/distributed/test_symmetric_memory.py torch/distributed/_symmetric_memory/__init__.py,https://github.com/pytorch/pytorch/pull/131831,yifuwang,weifengpy,,,
c82441e07a3,skip,not user facing,Fix std::optional checking bug (#131874),aten/src/ATen/functorch/BatchRulesRandomness.cpp,https://github.com/pytorch/pytorch/pull/131874,cyyever,Skylion007,,,
f83ef69b84b,skip,not user facing,Fix typo in assignment operators (#131890),aten/src/ATen/code_template.h c10/core/SymbolicShapeMeta.h c10/core/TensorImpl.h torch/csrc/Exceptions.h torch/csrc/cuda/CUDAPluggableAllocator.h torch/csrc/utils/invalid_arguments.cpp,https://github.com/pytorch/pytorch/pull/131890,cyyever,Skylion007,,,
07389163f07,distributed,Untopiced,[C10][BE] Use range loop (#131922),torch/csrc/distributed/c10d/NCCLUtils.hpp,https://github.com/pytorch/pytorch/pull/131922,malfet,XilunWu,,,
8cdfdb41bc2,skip,Untopiced,"Revert ""[NestedTensor] Integrate the layer normalization operator along the jagged dimension into NestedTensor (#131519)""",test/test_nestedtensor.py torch/nested/_internal/ops.py,,,,,,
ae9f17a821a,Uncategorized,Untopiced,[aoti] Rename OSS DynamicArg and OpKernel (#131862),torch/csrc/inductor/aoti_torch/oss_proxy_executor.cpp torch/csrc/inductor/aoti_torch/oss_proxy_executor.h,https://github.com/pytorch/pytorch/pull/131862,angelayi,desertfire,,,
918ece4f4d8,fx,not user facing,[BE][Easy][11/19] enforce style for empty lines in import segments in `test/dy*/` (#129762),test/dynamo/test_activation_checkpointing.py test/dynamo/test_after_aot.py test/dynamo/test_aot_autograd.py test/dynamo/test_aot_autograd_cache.py test/dynamo/test_autograd_function.py test/dynamo/test_backends.py test/dynamo/test_backward_higher_order_ops.py test/dynamo/test_base_output.py test/dynamo/test_comptime.py test/dynamo/test_config.py test/dynamo/test_ctx_manager.py test/dynamo/test_cudagraphs.py test/dynamo/test_cudagraphs_expandable_segments.py test/dynamo/test_debug_utils.py test/dynamo/test_decorators.py test/dynamo/test_deviceguard.py test/dynamo/test_dynamic_shapes.py test/dynamo/test_exceptions.py test/dynamo/test_export.py test/dynamo/test_frame_init.py test/dynamo/test_functions.py test/dynamo/test_fx_passes_pre_grad.py test/dynamo/test_global.py test/dynamo/test_guard_manager.py test/dynamo/test_higher_order_ops.py test/dynamo/test_hooks.py test/dynamo/test_inline_inbuilt_nn_modules.py test/dynamo/test_interop.py test/dynamo/test_logging.py test/dynamo/test_minifier.py test/dynamo/test_misc.py test/dynamo/test_model_output.py test/dynamo/test_modules.py test/dynamo/test_nops.py test/dynamo/test_optimizers.py test/dynamo/test_pre_dispatch.py test/dynamo/test_profiler.py test/dynamo/test_python_autograd.py test/dynamo/test_recompile_ux.py test/dynamo/test_recompiles.py test/dynamo/test_repros.py test/dynamo/test_skip_non_tensor.py test/dynamo/test_structured_trace.py test/dynamo/test_subclasses.py test/dynamo/test_subgraphs.py test/dynamo/test_torchrec.py test/dynamo/test_trace_rules.py test/dynamo/test_unspec.py test/dynamo/test_verify_correctness.py test/dynamo/test_view.py test/dynamo/utils.py tools/linter/adapters/ufmt_linter.py,https://github.com/pytorch/pytorch/pull/129762,XuehaiPan,anijain2305,,,
8e5a3673111,jit,not user facing,[5/N] Fix clang-tidy warnings in jit  (#131969),torch/csrc/jit/frontend/concrete_module_type.h torch/csrc/jit/frontend/exit_transforms.cpp torch/csrc/jit/frontend/ir_emitter.cpp torch/csrc/jit/frontend/lexer.cpp torch/csrc/jit/frontend/lexer.h torch/csrc/jit/frontend/parse_string_literal.h torch/csrc/jit/frontend/parser.cpp torch/csrc/jit/frontend/schema_matching.cpp torch/csrc/jit/frontend/sugared_value.h torch/csrc/jit/frontend/tree_views.h,https://github.com/pytorch/pytorch/pull/131969,cyyever,ezyang,,,
466ea8ce547,python_frontend,improvements,Add fallback() to torch.library (#131707),c10/core/impl/PyInterpreter.cpp c10/core/impl/PyInterpreter.h test/dynamo_skips/TestPythonRegistration.test_fallback test/dynamo_skips/TestPythonRegistration.test_fallback_keyset test/test_python_dispatch.py torch/csrc/PyInterpreter.cpp torch/csrc/PyInterpreter.h torch/csrc/utils/python_dispatch.cpp torch/csrc/utils/python_dispatch.h torch/library.py,https://github.com/pytorch/pytorch/pull/131707,albanD,zou3519,,,
14158d892a2,skip,not user facing,[BE][tests] show local variables on failure in tests (#131151),pytest.ini test/run_test.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/131151,XuehaiPan,ezyang,,,
d07a125af2b,inductor,not user facing,[Inductor] supporting pointwise intermediate nodes in B2B-GEMM (#131685),test/inductor/test_b2b_gemm.py torch/_inductor/fx_passes/b2b_gemm.py,https://github.com/pytorch/pytorch/pull/131685,sdingcn,eellison,,,
3e0ccb3a9f0,Uncategorized,Untopiced,Fixing fake tensor SymInt caching (#131966),torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/131966,aorenste,oulgen,,,
7be0ce51b6c,skip,not user facing,Fix handle serialization error (#131871),torch/csrc/Event.cpp,https://github.com/pytorch/pytorch/pull/131871,cyyever,Skylion007,,,
8f5cf464054,skip,Untopiced,"Revert ""Fix public API tests (#131386)""",test/test_public_bindings.py torch/ao/quantization/__init__.py torch/distributed/pipelining/schedules.py torch/fx/experimental/graph_gradual_typechecker.py torch/fx/experimental/migrate_gradual_types/constraint_generator.py torch/library.py torch/nn/attention/flex_attention.py torch/onnx/symbolic_helper.py torch/optim/__init__.py torch/storage.py,,,,,,
d90f6b45c0c,skip,Untopiced,"Revert ""[inductor] Add type hints to functions in mkldnn_fusion.py (#131820)""",torch/_inductor/fx_passes/freezing_patterns.py torch/_inductor/fx_passes/mkldnn_fusion.py torch/_inductor/fx_passes/post_grad.py,,,,,,
e448f32944d,skip,Untopiced,"Revert ""[BE] typing for decorators - signal/windows/windows (#131582)""",torch/signal/windows/windows.py,,,,,,
2c4023d65f6,skip,Untopiced,"Revert ""[BE] typing for decorators - _refs/nn/functional (#131581)""",torch/_refs/nn/functional/__init__.py,,,,,,
5ced63a005a,skip,Untopiced,"Revert ""[BE] typing for decorators - utils/flop_counter (#131580)""",torch/utils/flop_counter.py,,,,,,
065d0fe5708,skip,Untopiced,"Revert ""[BE] typing for decorators - fx/experimental/graph_gradual_typechecker (#131579)""",torch/fx/experimental/graph_gradual_typechecker.py,,,,,,
d3c17fea907,skip,Untopiced,"Revert ""[BE] typing for decorators - _library/custom_ops (#131578)""",torch/_functorch/apis.py torch/_functorch/batch_norm_replacement.py torch/_functorch/functional_call.py torch/_library/custom_ops.py torch/_library/infer_schema.py torch/_library/triton.py torch/nn/attention/flex_attention.py torch/testing/_internal/custom_op_db.py torch/utils/_exposed_in.py,,,,,,
b1d640a2b7a,skip,Untopiced,"Revert ""[BE] typing for decorators - ao/quantization/quantizer/xnnpack_quantizer_utils (#131577)""",torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py,,,,,,
6a0c3bae219,skip,Untopiced,"Revert ""[BE] typing for decorators - fx/experimental/migrate_gradual_types/constraint_generator (#131576)""",torch/fx/experimental/migrate_gradual_types/constraint_generator.py,,,,,,
07b7f518776,skip,Untopiced,"Revert ""[BE] typing for decorators - _inductor/fx_passes/post_grad (#131575)""",torch/_inductor/fx_passes/post_grad.py,,,,,,
4684b8e9d76,skip,Untopiced,"Revert ""[BE] typing for decorators - _inductor/lowering (#131574)""",torch/_inductor/jagged_lowerings.py torch/_inductor/kernel/conv.py torch/_inductor/lowering.py torch/_inductor/mkldnn_lowerings.py,,,,,,
609447a626f,skip,Untopiced,"Revert ""[BE] typing for decorators - _jit_internal (#131573)""",torch/_jit_internal.py torch/ao/nn/quantized/dynamic/modules/rnn.py torch/ao/nn/quantized/modules/embedding_ops.py torch/ao/nn/quantized/modules/linear.py torch/ao/nn/sparse/quantized/linear.py torch/ao/quantization/experimental/adaround_fake_quantize.py torch/ao/quantization/fake_quantize.py torch/ao/quantization/observer.py torch/backends/_nnapi/prepare.py torch/distributed/optim/optimizer.py torch/jit/_recursive.py torch/nn/modules/adaptive.py torch/nn/modules/container.py torch/nn/modules/rnn.py torch/nn/utils/parametrize.py,,,,,,
fd5b7d4bf95,skip,Untopiced,"Revert ""[BE] typing for decorators - _meta_registrations (#131572)""",torch/_meta_registrations.py,,,,,,
492e9a48867,distributed,not user facing,[micro_pipeline_tp] add support for type-erased all-gather pattern observed in DTensor + float8_experimental (#131832),test/distributed/tensor/parallel/test_micro_pipeline_tp.py torch/_inductor/fx_passes/micro_pipeline_tp.py,https://github.com/pytorch/pytorch/pull/131832,yifuwang,weifengpy,,,
0538a69a8d8,distributed,not user facing,[micro_pipeline_tp] support all-gather -> _scaled_mm (#131833),test/distributed/tensor/parallel/test_micro_pipeline_tp.py torch/_inductor/fx_passes/micro_pipeline_tp.py torch/distributed/_symmetric_memory/__init__.py,https://github.com/pytorch/pytorch/pull/131833,yifuwang,weifengpy,,,
a8a9882899c,distributed,not user facing,Implement fused_scaled_matmul_reduce_scatter for async-TP (#131950),test/distributed/test_symmetric_memory.py torch/distributed/_symmetric_memory/__init__.py,https://github.com/pytorch/pytorch/pull/131950,yifuwang,weifengpy,,,
a0abb77007e,skip,Untopiced,"Revert ""[BE] typing for decorators - distributed/_tensor/ops/utils (#131571)""",torch/distributed/_spmd/experimental_ops.py torch/distributed/_tensor/ops/_conv_ops.py torch/distributed/_tensor/ops/_embedding_ops.py torch/distributed/_tensor/ops/_experimental_ops.py torch/distributed/_tensor/ops/_math_ops.py torch/distributed/_tensor/ops/_matrix_ops.py torch/distributed/_tensor/ops/_random_ops.py torch/distributed/_tensor/ops/_tensor_ops.py torch/distributed/_tensor/ops/_view_ops.py torch/distributed/_tensor/ops/utils.py,,,,,,
a3ba4058713,skip,Untopiced,"Revert ""[BE] typing for decorators - library (#131570)""",torch/ao/quantization/fx/_decomposed.py torch/distributed/_composable/fsdp/_fsdp_collectives.py torch/distributed/_symmetric_memory/__init__.py torch/library.py,,,,,,
b002ec61b66,skip,Untopiced,"Revert ""[BE] typing for decorators - masked/_ops (#131569)""",torch/masked/_ops.py,,,,,,
945bf788943,skip,Untopiced,"Revert ""[BE] typing for decorators - fx/_compatibility (#131568)""",torch/_dynamo/compiled_autograd.py torch/_dynamo/convert_frame.py torch/_dynamo/eval_frame.py torch/_dynamo/utils.py torch/_export/__init__.py torch/_export/converter.py torch/_export/pass_base.py torch/_export/passes/replace_set_grad_with_hop_pass.py torch/_functorch/partitioners.py torch/_higher_order_ops/flex_attention.py torch/_inductor/codegen/cpp.py torch/_inductor/constant_folding.py torch/_inductor/debug.py torch/_inductor/fx_passes/binary_folding.py torch/_inductor/fx_passes/efficient_conv_bn_eval.py torch/_inductor/fx_passes/freezing_patterns.py torch/_inductor/fx_passes/joint_graph.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/fx_passes/quantization.py torch/_inductor/fx_passes/reinplace.py torch/_inductor/fx_passes/split_cat.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/pattern_matcher.py torch/_inductor/subgraph_lowering.py torch/_inductor/utils.py torch/_subclasses/fake_tensor.py torch/ao/ns/_numeric_suite_fx.py torch/ao/ns/fx/graph_passes.py torch/ao/ns/fx/n_shadows_utils.py torch/ao/pruning/_experimental/pruner/base_structured_sparsifier.py torch/ao/pruning/_experimental/pruner/match_utils.py torch/ao/quantization/fx/_lower_to_native_backend.py torch/ao/quantization/fx/utils.py torch/ao/quantization/pt2e/duplicate_dq_pass.py torch/ao/quantization/pt2e/utils.py torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py torch/distributed/_spmd/api.py torch/distributed/_spmd/graph_utils.py torch/distributed/_spmd/iter_graph_module.py torch/distributed/_spmd/partial_lower.py torch/distributed/_tensor/experimental/tp_transform.py torch/distributed/fsdp/_trace_utils.py torch/distributed/pipelining/_IR.py torch/distributed/pipelining/microbatch.py torch/fx/_compatibility.py torch/fx/_lazy_graph_module.py torch/fx/_symbolic_trace.py torch/fx/experimental/meta_tracer.py torch/fx/experimental/proxy_tensor.py torch/fx/graph.py torch/fx/graph_module.py torch/fx/interpreter.py torch/fx/node.py torch/fx/operator_schemas.py torch/fx/passes/graph_drawer.py torch/fx/passes/graph_manipulation.py torch/fx/passes/infra/pass_manager.py torch/fx/passes/net_min_base.py torch/fx/passes/operator_support.py torch/fx/passes/param_fetch.py torch/fx/passes/runtime_assert.py torch/fx/passes/split_module.py torch/fx/passes/split_utils.py torch/fx/passes/splitter_base.py torch/fx/passes/tools_common.py torch/fx/passes/utils/common.py torch/fx/passes/utils/fuser_utils.py torch/fx/passes/utils/matcher_with_name_node_map_utils.py torch/fx/passes/utils/source_matcher_utils.py torch/fx/subgraph_rewriter.py torch/fx/traceback.py torch/nested/_internal/ops.py torch/onnx/_internal/fx/op_validation.py torch/onnx/_internal/fx/serialization.py torch/onnx/_internal/onnxruntime.py torch/utils/_python_dispatch.py,,,,,,
500aea8d503,releng,not user facing,Build PT aarch64 on arm runner (#131964),.github/workflows/inductor-perf-test-nightly-aarch64.yml test/dynamo/test_model_output.py,https://github.com/pytorch/pytorch/pull/131964,huydhn,malfet,,,
8927fc209f4,inductor,not user facing,[inductor] Add type hints to functions in debug.py (#131836),torch/_inductor/debug.py,https://github.com/pytorch/pytorch/pull/131836,aakhundov,eellison,,,
6de65d5dd42,skip,not user facing,[dynamo] Turn on inline_inbuilt_nn_modules (#131275),torch/_dynamo/config.py,https://github.com/pytorch/pytorch/pull/131275,anijain2305,ezyang,jansel,,
cc512ea0f69,inductor,not user facing,[inductor] Fix flaky tests in test_aot_inductor.py (#131994),test/inductor/test_aot_inductor_utils.py,https://github.com/pytorch/pytorch/pull/131994,aakhundov,chenyang78,,,
2e4807575c1,dynamo,not user facing,Remove mypy ignore from torch/_dynamo/polyfill.py (#131786),torch/_dynamo/polyfill.py,https://github.com/pytorch/pytorch/pull/131786,oulgen,aorenste,zou3519,,
7c29665f779,distributed,not user facing,Remove mypy ignore from torch/testing/_internal/distributed/ (#131870),torch/testing/_internal/distributed/_shard/__init__.py torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py torch/testing/_internal/distributed/_shard/sharded_tensor/_test_ops_common.py torch/testing/_internal/distributed/_shard/sharded_tensor/_test_st_common.py torch/testing/_internal/distributed/_shard/test_common.py torch/testing/_internal/distributed/_tensor/common_dtensor.py torch/testing/_internal/distributed/checkpoint_utils.py torch/testing/_internal/distributed/common_state_dict.py torch/testing/_internal/distributed/ddp_under_dist_autograd_test.py torch/testing/_internal/distributed/distributed_test.py torch/testing/_internal/distributed/distributed_utils.py torch/testing/_internal/distributed/fake_pg.py torch/testing/_internal/distributed/multi_threaded_pg.py torch/testing/_internal/distributed/nn/api/remote_module_test.py torch/testing/_internal/distributed/rpc/dist_autograd_test.py torch/testing/_internal/distributed/rpc/dist_optimizer_test.py torch/testing/_internal/distributed/rpc/examples/parameter_server_test.py torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py torch/testing/_internal/distributed/rpc/jit/dist_autograd_test.py torch/testing/_internal/distributed/rpc/jit/rpc_test.py torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py torch/testing/_internal/distributed/rpc/rpc_agent_test_fixture.py torch/testing/_internal/distributed/rpc/rpc_test.py torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py torch/testing/_internal/distributed/rpc_utils.py,https://github.com/pytorch/pytorch/pull/131870,oulgen,aakhundov,,,
75c8d59ea12,dynamo,not user facing,Remove mypy ignore from torch/_dynamo/variables/lazy.py (#131785),torch/_dynamo/variables/base.py torch/_dynamo/variables/lazy.py,https://github.com/pytorch/pytorch/pull/131785,oulgen,aorenste,zou3519,,
f901b020666,distributed,Untopiced,[Distributed] Do not expose `nlohmann/json.hpp` in public headers (#131925),torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp,https://github.com/pytorch/pytorch/pull/131925,malfet,albanD,c-p-i-o,d4l3k,
7c1fbc7fe9c,quantization,Untopiced,[5/N] Remove unused parameter (#131998),aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,https://github.com/pytorch/pytorch/pull/131998,cyyever,ezyang,,,
0ab6551bcb2,inductor,not user facing,[inductor] Handle NoneLayout in count_numel (#131645),test/inductor/test_perf.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/131645,peterbell10,jansel,,,
7b0e10f0e5b,distributed,Untopiced,fix _MaskPartial when multiple embeddings coexist (#131264),test/distributed/_tensor/test_embedding_ops.py torch/distributed/_tensor/ops/_embedding_ops.py torch/distributed/_tensor/ops/_tensor_ops.py torch/distributed/tensor/parallel/loss.py,https://github.com/pytorch/pytorch/pull/131264,tianyu-l,wanchaol,,,
ddd539ba6c4,jit,not user facing,[6/N] Fix clang-tidy warnings in jit  (#131986),torch/csrc/jit/frontend/lexer.cpp torch/csrc/jit/frontend/schema_type_parser.cpp torch/csrc/jit/python/pybind.h torch/csrc/jit/python/python_custom_class.cpp torch/csrc/jit/python/python_custom_class.h torch/csrc/jit/python/python_ir.cpp torch/csrc/jit/python/python_ivalue.h torch/csrc/jit/python/python_sugared_value.cpp torch/csrc/jit/python/python_tree_views.cpp torch/csrc/jit/python/script_init.cpp torch/csrc/jit/runtime/static/ProcessedNodeInputs.h torch/csrc/jit/runtime/static/fusion.cpp torch/csrc/jit/runtime/static/generated_ops.cpp torch/csrc/jit/runtime/static/impl.cpp torch/csrc/jit/runtime/static/impl.h torch/csrc/jit/runtime/static/init.cpp torch/csrc/jit/runtime/static/memory_planner.cpp torch/csrc/jit/runtime/static/native_ops.cpp torch/csrc/jit/runtime/static/te_wrapper.cpp,https://github.com/pytorch/pytorch/pull/131986,cyyever,ezyang,,,
5b3b2b9cc79,jit,not user facing,[7/N] Fix clang-tidy warnings in jit  (#131996),torch/csrc/jit/ir/alias_analysis.cpp torch/csrc/jit/ir/alias_analysis.h torch/csrc/jit/ir/attributes.h torch/csrc/jit/ir/constants.h torch/csrc/jit/ir/graph_node_list.h torch/csrc/jit/ir/graph_utils.cpp torch/csrc/jit/ir/graph_utils.h torch/csrc/jit/ir/ir.cpp torch/csrc/jit/ir/ir.h torch/csrc/jit/ir/ir_views.h torch/csrc/jit/ir/irparser.cpp torch/csrc/jit/ir/irparser.h torch/csrc/jit/ir/named_value.h torch/csrc/jit/ir/node_hashing.h torch/csrc/jit/ir/scope.h torch/csrc/jit/ir/subgraph_matcher.h torch/csrc/jit/ir/type_hashing.h torch/csrc/jit/mobile/import_data.cpp torch/csrc/jit/passes/utils/check_alias_annotation.cpp torch/csrc/jit/python/script_init.cpp torch/csrc/jit/runtime/graph_executor.cpp torch/csrc/jit/serialization/unpickler.h,https://github.com/pytorch/pytorch/pull/131996,cyyever,ezyang,,,
2a02b5cd228,skip,not user facing,[Intel GPU] Dispatch Stub support (#130019),CMakeLists.txt aten/src/ATen/native/DispatchStub.cpp aten/src/ATen/native/DispatchStub.h test/test_cpp_extensions_open_device_registration.py test/test_transformers.py,https://github.com/pytorch/pytorch/pull/130019,ZhiweiYan-96,EikanWang,albanD,gujinghui,
03760be2714,inductor,bug fixes,[inductor] Fix unsoundness with negative-valued indexing expressions (#131761),test/inductor/test_indexing.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/_inductor/codegen/triton.py torch/_inductor/runtime/triton_helpers.py torch/_inductor/sizevars.py,https://github.com/pytorch/pytorch/pull/131761,peterbell10,ezyang,,,
30e7fc0fe1a,inductor,not user facing,Cpp wrapper: set args to CppWrapperKernelArgs in cpp template kernel (#129557),test/inductor/test_cpu_cpp_wrapper.py torch/_inductor/codegen/cpp_template_kernel.py,https://github.com/pytorch/pytorch/pull/129557,chunyuan-w,leslie-fang-intel,,,
f151f25c0b8,skip,not user facing,BE: reset dynamo before each test in test_torch.py (#131388),torch/testing/_internal/dynamo_test_failures.py,https://github.com/pytorch/pytorch/pull/131388,shunting314,zou3519,,,
dfa18bf3f39,skip,not user facing,[CI] add new test config label `ci-test-showlocals` to control test log verbosity (#131981),.github/actions/filter-test-configs/action.yml .github/actions/linux-test/action.yml .github/scripts/filter_test_configs.py .github/scripts/test_filter_test_configs.py .github/workflows/_linux-test.yml .github/workflows/_mac-test-mps.yml .github/workflows/_mac-test.yml .github/workflows/_rocm-test.yml .github/workflows/_win-test.yml .github/workflows/_xpu-test.yml test/run_test.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/131981,XuehaiPan,malfet,,,
d47c470f474,dynamo,not user facing,[dynamo] implement `var_getattr` in UserFunctionVariable (#130413),test/dynamo/test_functions.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/130413,yaochengji,yanboliang,,,
b67811abda6,skip,not user facing,[1/N] Fix clang-tidy warnings in inductor (#131979),torch/csrc/inductor/aoti_eager/kernel_holder.cpp torch/csrc/inductor/aoti_eager/kernel_meta_info.cpp torch/csrc/inductor/aoti_eager/kernel_meta_info.h torch/csrc/inductor/aoti_runner/model_container_runner.cpp torch/csrc/inductor/aoti_runner/model_container_runner_cpu.cpp torch/csrc/inductor/aoti_runner/model_container_runner_cuda.cpp torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/mkldnn_tensor.cpp torch/csrc/inductor/aoti_torch/oss_proxy_executor.cpp torch/csrc/inductor/aoti_torch/oss_proxy_executor.h torch/csrc/inductor/aoti_torch/shim_common.cpp torch/csrc/inductor/aoti_torch/tensor_converter.cpp torch/csrc/inductor/inductor_ops.cpp torch/csrc/inductor/resize_storage_bytes.cpp,https://github.com/pytorch/pytorch/pull/131979,cyyever,Skylion007,,,
9d497887b87,skip,not user facing,Changes to support clang-19 (#131905),c10/util/intrusive_ptr.h,https://github.com/pytorch/pytorch/pull/131905,amd-jmacaran,Skylion007,jeffdaily,,
eb9409511e1,skip,Untopiced,"Revert ""support zb1p and zb2p algorithms (#130752)""",docs/source/distributed.pipelining.rst test/distributed/pipelining/test_schedule.py test/distributed/pipelining/test_schedule_multiproc.py torch/distributed/pipelining/__init__.py torch/distributed/pipelining/schedules.py,,,,,,
efca51e171e,jit,not user facing,[8/N] Fix clang-tidy warnings in jit (#131997),torch/csrc/jit/cuda/cuda.h torch/csrc/jit/jit_log.cpp torch/csrc/jit/jit_log.h torch/csrc/jit/jit_opt_limit.cpp torch/csrc/jit/jit_opt_limit.h torch/csrc/jit/resource_guard.h,https://github.com/pytorch/pytorch/pull/131997,cyyever,Skylion007,,,
7ef927da155,skip,Untopiced,"Revert ""[dynamo] Turn on inline_inbuilt_nn_modules (#131275)""",torch/_dynamo/config.py,,,,,,
06fe99a097a,skip,Untopiced,"Revert ""[CI] add new test config label `ci-test-showlocals` to control test log verbosity (#131981)""",.github/actions/filter-test-configs/action.yml .github/actions/linux-test/action.yml .github/scripts/filter_test_configs.py .github/scripts/test_filter_test_configs.py .github/workflows/_linux-test.yml .github/workflows/_mac-test-mps.yml .github/workflows/_mac-test.yml .github/workflows/_rocm-test.yml .github/workflows/_win-test.yml .github/workflows/_xpu-test.yml test/run_test.py torch/testing/_internal/common_utils.py,,,,,,
c35f21e5fcb,skip,Untopiced,"Revert ""[BE][tests] show local variables on failure in tests (#131151)""",pytest.ini test/run_test.py torch/testing/_internal/common_utils.py,,,,,,
be3eba382f6,releng,Untopiced,[CI] Run perf test for perf_cpu_aarch64 (#132038),.ci/pytorch/test.sh,https://github.com/pytorch/pytorch/pull/132038,desertfire,malfet,,,
14108c1677e,skip,not user facing,Fix error handling in _triton.py (#132006),torch/utils/_triton.py,https://github.com/pytorch/pytorch/pull/132006,Ruichensun,oulgen,,,
6cbad37bee3,inductor,Untopiced,make `_inductor.config.rocm.supported_arch` set order deterministic for caching (#131921),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/131921,bdhirsh,jamesjwu,oulgen,,
1e9cdf7d918,skip,Untopiced,Relax constraints for creating a `GenericContextWrappingVariable` (#129091),test/dynamo/test_ctx_manager.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/129091,guilhermeleobas,yanboliang,zou3519,,
e0e4e84ef97,skip,not user facing,wrap self.call_function(...) in try finally block to undo changes to self.kw_names (#130490),test/dynamo/test_exceptions.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/130490,guilhermeleobas,williamwen42,zou3519,,
5aab1acc84f,skip,Untopiced,Let dynamo inline functional_call (#128646),test/dynamo/test_higher_order_ops.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/higher_order_ops.py torch/_functorch/functional_call.py torch/nn/utils/_named_member_accessor.py torch/nn/utils/stateless.py,https://github.com/pytorch/pytorch/pull/128646,guilhermeleobas,zou3519,,,
ca254d145f5,skip,not user facing,[BE][Ez]: Update fmtlib submodule to 11.0.2 (#132036),third_party/fmt,https://github.com/pytorch/pytorch/pull/132036,Skylion007,malfet,,,
957a89f56cc,skip,Untopiced,"Revert ""[inductor] Fix unsoundness with negative-valued indexing expressions (#131761)""",test/inductor/test_indexing.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/_inductor/codegen/triton.py torch/_inductor/runtime/triton_helpers.py torch/_inductor/sizevars.py,,,,,,
e393c7fa055,skip,Untopiced,Tighten torch.library.infer_schema input types (#130705),test/custom_operator/test_infer_schema_annotation.py test/test_custom_ops.py torch/_custom_op/impl.py torch/_custom_ops.py torch/_library/custom_ops.py torch/_library/infer_schema.py,https://github.com/pytorch/pytorch/pull/130705,zou3519,yushangdi,,,
962f2484377,distributed,not user facing,Add decomposition for expand_copy (#130940),test/distributed/_tensor/test_dtensor_ops.py test/expect/HasDecompTest.test_has_decomposition.expect test/test_mps.py tools/autograd/gen_variable_type.py torch/_decomp/__init__.py torch/_inductor/decomposition.py torch/_refs/__init__.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/130940,rec,peterbell10,,,
f72266eceaf,skip,Untopiced,"Revert ""Let dynamo inline functional_call (#128646)""",test/dynamo/test_higher_order_ops.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/higher_order_ops.py torch/_functorch/functional_call.py torch/nn/utils/_named_member_accessor.py torch/nn/utils/stateless.py,,,,,,
e73a4cb21fa,skip,Untopiced,"Revert ""[pt2e][quant] Ensure BN node is erased after convert (#131651)""",test/quantization/pt2e/test_quantize_pt2e_qat.py torch/ao/quantization/pt2e/utils.py,,,,,,
3d4de8e96d0,skip,not user facing,Add config option to skip autotuning conv (#131839),test/inductor/test_max_autotune.py torch/_inductor/config.py torch/_inductor/kernel/conv.py torch/_inductor/select_algorithm.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/131839,eellison,oulgen,,,
6cf493158ef,skip,Untopiced,"Revert ""Enable FlashAttention on Windows (#131906)""",CMakeLists.txt torch/testing/_internal/common_cuda.py,,,,,,
bd1a29b1580,skip,not user facing,[BE][Ez]: Update ruff to 0.5.5. Bugfixes and better LSP support (#132037),.lintrunner.toml,https://github.com/pytorch/pytorch/pull/132037,Skylion007,malfet,,,
527901f054a,skip,not user facing,BE: reset dynamo before each test in test_module.py (#131372),torch/testing/_internal/dynamo_test_failures.py,https://github.com/pytorch/pytorch/pull/131372,shunting314,zou3519,,,
ca8153ae675,skip,not user facing,BE: reset dynamo before each test in test_ops_gradients.py (#131397),torch/testing/_internal/dynamo_test_failures.py,https://github.com/pytorch/pytorch/pull/131397,shunting314,zou3519,,,
025242d065e,skip,not user facing,[cpu-test] enable test_cpu_repro in fbcode (#132022),test/inductor/test_cpu_repro.py,https://github.com/pytorch/pytorch/pull/132022,chenyang78,desertfire,,,
6c6fbb4691a,distributed,docs,Fix pyi annotation for ProcessGroupNCCL.Options (#130957),torch/_C/_distributed_c10d.pyi,https://github.com/pytorch/pytorch/pull/130957,ezyang,fduwjj,wconstab,,
f389bca2e97,dynamo,not user facing,[dynamo][inline_inbuilt_nn_modules] Skip test_dpp_graphs for now (#132053),test/dynamo/test_structured_trace.py,https://github.com/pytorch/pytorch/pull/132053,anijain2305,laithsakka,,,
c764ef6d53e,jit,not user facing,[9/N] Fix clang-tidy warnings in jit (#132010),torch/csrc/jit/passes/add_if_then_else.cpp torch/csrc/jit/passes/add_if_then_else.h torch/csrc/jit/passes/annotate_warns.cpp torch/csrc/jit/passes/annotate_warns.h torch/csrc/jit/passes/autocast.cpp torch/csrc/jit/passes/autocast.h torch/csrc/jit/passes/bailout_graph.cpp torch/csrc/jit/passes/bailout_graph.h torch/csrc/jit/passes/batch_mm.cpp torch/csrc/jit/passes/batch_mm.h torch/csrc/jit/passes/canonicalize.cpp torch/csrc/jit/passes/canonicalize.h torch/csrc/jit/passes/canonicalize_graph_fuser_ops.cpp torch/csrc/jit/passes/canonicalize_graph_fuser_ops.h torch/csrc/jit/passes/check_strict_fusion.cpp torch/csrc/jit/passes/check_strict_fusion.h torch/csrc/jit/passes/clear_profiling.cpp torch/csrc/jit/passes/clear_profiling.h torch/csrc/jit/passes/clear_undefinedness.cpp torch/csrc/jit/passes/clear_undefinedness.h torch/csrc/jit/passes/common_subexpression_elimination.cpp torch/csrc/jit/passes/common_subexpression_elimination.h torch/csrc/jit/passes/concat_opt.cpp torch/csrc/jit/passes/concat_opt.h torch/csrc/jit/passes/constant_pooling.cpp torch/csrc/jit/passes/constant_pooling.h torch/csrc/jit/passes/constant_propagation.cpp torch/csrc/jit/passes/constant_propagation.h torch/csrc/jit/passes/create_autodiff_subgraphs.cpp torch/csrc/jit/passes/create_autodiff_subgraphs.h torch/csrc/jit/passes/create_functional_graphs.cpp torch/csrc/jit/passes/create_functional_graphs.h torch/csrc/jit/passes/dead_code_elimination.cpp torch/csrc/jit/passes/dead_code_elimination.h torch/csrc/jit/passes/decompose_ops.cpp torch/csrc/jit/passes/decompose_ops.h torch/csrc/jit/passes/device_type_analysis.cpp torch/csrc/jit/passes/device_type_analysis.h torch/csrc/jit/passes/dtype_analysis.cpp torch/csrc/jit/passes/dtype_analysis.h torch/csrc/jit/passes/shape_analysis.cpp torch/csrc/jit/passes/shape_analysis.h torch/csrc/jit/passes/specialize_autogradzero.cpp torch/csrc/jit/passes/specialize_autogradzero.h torch/csrc/jit/passes/subgraph_rewrite.cpp torch/csrc/jit/passes/subgraph_rewrite.h torch/csrc/jit/passes/symbolic_shape_analysis.cpp torch/csrc/jit/passes/symbolic_shape_analysis.h torch/csrc/jit/passes/symbolic_shape_cache.cpp torch/csrc/jit/passes/symbolic_shape_cache.h torch/csrc/jit/passes/symbolic_shape_runtime_fusion.cpp torch/csrc/jit/passes/symbolic_shape_runtime_fusion.h torch/csrc/jit/passes/tensorexpr_fuser.cpp torch/csrc/jit/passes/tensorexpr_fuser.h,https://github.com/pytorch/pytorch/pull/132010,cyyever,Skylion007,,,
ab912b7fef2,skip,not user facing,[2/N] Fix clang-tidy warnings in inductor  (#132040),torch/csrc/inductor/aoti_runtime/arrayref_tensor.h torch/csrc/inductor/aoti_runtime/device_utils.h torch/csrc/inductor/aoti_runtime/scalar_to_tensor.h torch/csrc/inductor/aoti_runtime/thread_local.h torch/csrc/inductor/aoti_runtime/utils.h torch/csrc/inductor/aoti_runtime/utils_cuda.h torch/csrc/inductor/aoti_torch/mkldnn_tensor.h torch/csrc/inductor/aoti_torch/oss_proxy_executor.h torch/csrc/inductor/aoti_torch/proxy_executor.h torch/csrc/inductor/aoti_torch/tensor_converter.h torch/csrc/inductor/inductor_ops.h torch/csrc/inductor/resize_storage_bytes.cpp,https://github.com/pytorch/pytorch/pull/132040,cyyever,Skylion007,,,
4694ee1ad2f,skip,not user facing,[BE][tests] show local variables on failure in tests (#131151),pytest.ini test/run_test.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/131151,XuehaiPan,ezyang,,,
5cc34f61d1c,skip,not user facing,[CI] add new test config label `ci-test-showlocals` to control test log verbosity (#131981),.github/actions/filter-test-configs/action.yml .github/actions/linux-test/action.yml .github/scripts/filter_test_configs.py .github/scripts/test_filter_test_configs.py .github/workflows/_linux-test.yml .github/workflows/_mac-test-mps.yml .github/workflows/_mac-test.yml .github/workflows/_rocm-test.yml .github/workflows/_win-test.yml .github/workflows/_xpu-test.yml test/run_test.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/131981,XuehaiPan,malfet,,,
301ec32ae81,skip,not user facing,[EASY][TEST][CUDA] Fix typo in test_graph_make_graphed_callables_same_pool (#132059),test/test_cuda.py,https://github.com/pytorch/pytorch/pull/132059,Aidyn-A,Skylion007,,,
484852c02b9,skip,not user facing,[Doc] update guide install mkl-static from conda to pip (#130026),README.md,https://github.com/pytorch/pytorch/pull/130026,xuhancn,atalman,jgong5,,
4c2bcf92cbe,skip,not user facing,[inductor] Enable FX graph caching in OSS by default (#125863),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/125863,masnesral,eellison,oulgen,,
f44446e8512,skip,not user facing,[dynamo] Turn on inline_inbuilt_nn_modules (#131275),torch/_dynamo/config.py,https://github.com/pytorch/pytorch/pull/131275,anijain2305,ezyang,jansel,,
082d0b80cab,mps,bug fixes,Min and max NaN propagation fix in MPS backend (#130445),aten/src/ATen/native/mps/operations/ReduceOps.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/130445,jhavukainen,malfet,,,
dcb03106b70,skip,not user facing,[Land Internally] MTIA equivalent of torch.cuda.memory_stats (#132007),aten/src/ATen/detail/MTIAHooksInterface.h docs/source/mtia.rst torch/_C/__init__.pyi.in torch/csrc/mtia/Module.cpp torch/mtia/__init__.py,https://github.com/pytorch/pytorch/pull/132007,nautsimon,egienvalue,hanzlfs,,
3c1562158e1,skip,not user facing,[BE] Fix torch.compile docstring formatting issues (#131837),torch/__init__.py,https://github.com/pytorch/pytorch/pull/131837,yanboliang,williamwen42,,,
bdf5a6dca93,onnx,not user facing,Add decomposition for unsqueeze_copy (#130942),test/expect/HasDecompTest.test_has_decomposition.expect test/functorch/test_ops.py test/functorch/test_vmap.py test/onnx/test_fx_op_consistency.py test/test_mps.py tools/autograd/gen_variable_type.py torch/_decomp/__init__.py torch/_refs/__init__.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/130942,rec,peterbell10,,,
a4723b566fe,skip,Untopiced,"Revert ""BE: reset dynamo before each test in test_ops_gradients.py (#131397)""",torch/testing/_internal/dynamo_test_failures.py,,,,,,
d5e9fbb0123,skip,Untopiced,"Revert ""BE: reset dynamo before each test in test_module.py (#131372)""",torch/testing/_internal/dynamo_test_failures.py,,,,,,
d8358a2d86d,autograd_frontend,Untopiced,Made `register_multi_grad_hook` return type `RemovableHandle` (#132074),torch/autograd/graph.py,https://github.com/pytorch/pytorch/pull/132074,awgu,soulitzer,,,
05a8540041c,inductor,not user facing,[cpp-wrapper] create null pointer for zero-size array (#132023),torch/_inductor/codegen/cpp_wrapper_cpu.py,https://github.com/pytorch/pytorch/pull/132023,chenyang78,desertfire,,,
d039b14207f,skip,Untopiced,Grouped Query Attention (#128898),aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/attention.h aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.h test/cpp_extensions/open_registration_extension.cpp test/dynamo/test_sdpa.py test/test_transformers.py tools/pyi/gen_pyi.py torch/_C/__init__.pyi.in torch/_dynamo/variables/sdpa.py torch/csrc/Module.cpp torch/nested/_internal/sdpa.py torch/nn/attention/bias.py torch/nn/functional.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/128898,jainapurva,drisspg,,,
8fe2bf212dc,skip,not user facing,[NestedTensor] Integrate the layer normalization operator along the jagged dimension into NestedTensor (#131519),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/131519,jananisriram,davidberard98,,,
62b2e7a5530,skip,Untopiced,"Revert ""Add config option to skip autotuning conv (#131839)""",test/inductor/test_max_autotune.py torch/_inductor/config.py torch/_inductor/kernel/conv.py torch/_inductor/select_algorithm.py torch/_inductor/utils.py,,,,,,
e55e9d81267,dynamo,bug fixes,Clear speculation log when restarting due to compiler collective (#131983),test/distributed/test_dynamo_distributed.py torch/_dynamo/output_graph.py,https://github.com/pytorch/pytorch/pull/131983,ezyang,anijain2305,mlazos,,
884eadcd19b,autograd_frontend,bug fixes,Fix multi grad hooks thread safety (#132055),torch/autograd/graph.py,https://github.com/pytorch/pytorch/pull/132055,soulitzer,Skylion007,albanD,awgu,
8b507a922af,inductor,Untopiced,Mode to emulate amp numerics (#131595),test/inductor/test_cuda_repro.py test/inductor/test_torchinductor.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/triton.py torch/_inductor/config.py torch/_inductor/lowering.py torch/_inductor/ops_handler.py torch/_inductor/utils.py torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/131595,eellison,bdhirsh,jansel,shunting314,
5298acb5c76,inductor,Untopiced,"Back out ""[1/2] PT2 Inductor ComboKernels - Foreach cases (#124969)"" (#132065)",test/inductor/test_foreach.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cuda.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/codegen/triton_foreach.py torch/_inductor/codegen/wrapper.py torch/_inductor/config.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/132065,Yuzhen11,qchip,zw2326,,
e3dc20c94bb,nested tensor_frontend,Untopiced,[NJT] support cat backward (#132076),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/132076,YuqingJ,davidberard98,,,
b1ccd0c407d,releng,Untopiced,[CI] Update environment varible setting for aarch64 (#132046),.ci/pytorch/test.sh,https://github.com/pytorch/pytorch/pull/132046,desertfire,huydhn,,,
be5e44192df,skip,Untopiced,"Revert ""[NestedTensor] Integrate the layer normalization operator along the jagged dimension into NestedTensor (#131519)""",test/test_nestedtensor.py torch/nested/_internal/ops.py,,,,,,
2a4d9aa5483,Uncategorized,Untopiced,Disable expandable segments checkpointing internally (#132048),c10/cuda/CUDACachingAllocator.cpp,https://github.com/pytorch/pytorch/pull/132048,eellison,eqy,ezyang,,
ab9791c0e34,skip,Untopiced,[export] Add print_readable to unflattener (#128617),test/dynamo/test_autograd_function.py test/dynamo/test_higher_order_ops.py test/dynamo/test_subclasses.py test/inductor/test_flex_attention.py torch/export/unflatten.py torch/fx/graph_module.py,https://github.com/pytorch/pytorch/pull/128617,angelayi,pianpwk,zhxchen17,,
7a7dd8c29e3,skip,Untopiced,"Revert ""[NestedTensor] Integrate the softmax operator along the jagged dimension into NestedTensor (#131518)""",test/test_nestedtensor.py torch/nested/_internal/ops.py,,,,,,
3716934b1a3,inductor,Untopiced,[Inductor] Refactor autotuning utils to compute max block sizes (#131730),torch/_inductor/runtime/coordinate_descent_tuner.py,https://github.com/pytorch/pytorch/pull/131730,blaine-rister,eellison,,,
a147fa577b3,mps,Untopiced,[MPS] Fix masked_fill_ in non_contiguous cases (#131957),aten/src/ATen/native/mps/operations/Indexing.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/131957,qqaatw,DenisVieriu97,,,
5a2620302bd,inductor,not user facing,[inductor] Replace self_cuda_time_total function calls with self_dev… (#131029),torch/_inductor/utils.py torch/_inductor/wrapper_benchmark.py,https://github.com/pytorch/pytorch/pull/131029,peaceorwell,shunting314,,,
9598c58618e,skip,not user facing,Add config option to skip autotuning conv (#131839),test/inductor/test_max_autotune.py torch/_inductor/config.py torch/_inductor/kernel/conv.py torch/_inductor/select_algorithm.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/131839,eellison,oulgen,,,
8721b21b389,Uncategorized,Untopiced,Fix fake_tensor w/ non-view tensor (#132050),torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/132050,aorenste,oulgen,,,
b6c1490cc02,dynamo,not user facing,[dynamo] make more unpack_var_sequence calls forced (#132069),test/dynamo/test_functions.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/132069,williamwen42,anijain2305,,,
f8e40604846,inductor,not user facing,[Inductor][CPP] Enhance cppcsevar data type deduce (#130827),test/inductor/test_torchinductor.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_utils.py,https://github.com/pytorch/pytorch/pull/130827,leslie-fang-intel,jansel,jgong5,,
baa4c9ca466,inductor,not user facing,Optimize aten.cat calls of a repeated element (#132081),test/inductor/test_mkldnn_pattern_matcher.py test/test_decomp.py torch/_inductor/decomposition.py,https://github.com/pytorch/pytorch/pull/132081,eellison,shunting314,,,
bdc42e3fb8e,inductor,not user facing,[inductor] validate_can_generate_cpp_wrapper add win32 support. (#131978),torch/_inductor/graph.py,https://github.com/pytorch/pytorch/pull/131978,xuhancn,jansel,jgong5,,
475da800c7b,inductor,not user facing,[inductor] optimize cflags for Windows. (#131980),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/131980,xuhancn,jansel,jgong5,,
aa1488fe02a,inductor,not user facing,[inductor] turn on enable_kernel_profile on Windows. (#132025),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template.py,https://github.com/pytorch/pytorch/pull/132025,xuhancn,jansel,jgong5,,
40f8db57419,releng,not user facing,[audio hash update] update the pinned audio hash (#132105),.github/ci_commit_pins/audio.txt,https://github.com/pytorch/pytorch/pull/132105,pytorchupdatebot,pytorchbot,,,
fc6066b80f9,skip,not user facing,improve mkldnn_linear_pointwise_binary performance for contiguous tensor with non default contiguous strides (#132019),aten/src/ATen/native/mkldnn/Linear.cpp test/test_mkldnn_fusion.py,https://github.com/pytorch/pytorch/pull/132019,yanbing-j,jgong5,leslie-fang-intel,,
13457d1da01,dynamo,not user facing,[dynamo][log] Suggest to use pytree when graph-break on optree (#131827),test/dynamo/test_misc.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/131827,anijain2305,mlazos,zou3519,,
f8061286191,dynamo,not user facing,[dynamo] Skip <frozen abc> to skip __isisintance__ check on abc objects (#131956),test/test_nestedtensor.py torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/131956,anijain2305,mlazos,williamwen42,,
03e058189e4,dynamo,not user facing,[dynamo] Support dict unpack of MutableMapping objects (#131961),test/dynamo/test_functions.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/dicts.py,https://github.com/pytorch/pytorch/pull/131961,anijain2305,mlazos,williamwen42,yanboliang,
54d4f6bbcac,inductor,not user facing,[Inductor][FlexAttention] Correct partial/full blocks naming (#131993),torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/131993,yanboliang,drisspg,,,
36e8289129b,inductor,Untopiced,[PT2][Optimus] Optimize cat node inputs pattern (#131866),test/inductor/test_split_cat_fx_passes.py torch/_inductor/fx_passes/split_cat.py,https://github.com/pytorch/pytorch/pull/131866,mengluy0125,jackiexu1992,,,
83db609ee5a,inductor,not user facing,[inductor] fix the cudagraph tree test (#132043),test/inductor/test_cudagraph_trees.py,https://github.com/pytorch/pytorch/pull/132043,sijiac,BoyuanFeng,,,
eccbd408e5f,jit,not user facing,[10/N] Fix clang-tidy warnings in jit  (#132122),torch/csrc/jit/tensorexpr/analysis.h torch/csrc/jit/tensorexpr/block_codegen.cpp torch/csrc/jit/tensorexpr/block_codegen.h torch/csrc/jit/tensorexpr/bounds_inference.cpp torch/csrc/jit/tensorexpr/bounds_inference.h torch/csrc/jit/tensorexpr/bounds_overlap.cpp torch/csrc/jit/tensorexpr/bounds_overlap.h torch/csrc/jit/tensorexpr/codegen.cpp torch/csrc/jit/tensorexpr/codegen.h torch/csrc/jit/tensorexpr/cuda_codegen.h torch/csrc/jit/tensorexpr/eval.cpp torch/csrc/jit/tensorexpr/eval.h torch/csrc/jit/tensorexpr/expr.cpp torch/csrc/jit/tensorexpr/expr.h torch/csrc/jit/tensorexpr/ir.cpp torch/csrc/jit/tensorexpr/ir.h torch/csrc/jit/tensorexpr/stmt.h,https://github.com/pytorch/pytorch/pull/132122,cyyever,Skylion007,,,
bdf57da6a68,Uncategorized,Untopiced,[3/N] Enable clang-tidy on torch/csrc/inductor (#132101),.lintrunner.toml torch/csrc/inductor/aoti_runtime/arrayref_tensor.h torch/csrc/inductor/aoti_runtime/model.h torch/csrc/inductor/aoti_runtime/model_container.h torch/csrc/inductor/aoti_torch/shim_common.cpp torch/csrc/inductor/inductor_ops.cpp,https://github.com/pytorch/pytorch/pull/132101,cyyever,Skylion007,,,
499ead96ffe,skip,Untopiced,"Revert ""Grouped Query Attention (#128898)""",aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/attention.h aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.h test/cpp_extensions/open_registration_extension.cpp test/dynamo/test_sdpa.py test/test_transformers.py tools/pyi/gen_pyi.py torch/_C/__init__.pyi.in torch/_dynamo/variables/sdpa.py torch/csrc/Module.cpp torch/nested/_internal/sdpa.py torch/nn/attention/bias.py torch/nn/functional.py torch/testing/_internal/common_methods_invocations.py,,,,,,
12b67bd998f,distributed,Untopiced,Fix pyi annotation for `ProcessGroupGloo.Options`  (#132080),torch/_C/_distributed_c10d.pyi,https://github.com/pytorch/pytorch/pull/132080,ishon19,Skylion007,,,
a8431785292,skip,Untopiced,Let dynamo inline functional_call (#128646),test/dynamo/test_higher_order_ops.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/higher_order_ops.py torch/_functorch/functional_call.py torch/nn/utils/_named_member_accessor.py torch/nn/utils/stateless.py,https://github.com/pytorch/pytorch/pull/128646,guilhermeleobas,zou3519,,,
16e0868a3d2,distributed,Untopiced,[FSDP] Add hpu device to _get_remote_device_str (#132120),torch/distributed/fsdp/_shard_utils.py,https://github.com/pytorch/pytorch/pull/132120,jeejakp12,awgu,,,
33ce9cf7f9d,distributed (fsdp2),not user facing,[FSDP2] Relaxed overlap timing check to avoid flakiness (#132116),test/distributed/_composable/fsdp/test_fully_shard_overlap.py,https://github.com/pytorch/pytorch/pull/132116,awgu,Skylion007,,,
32c57e78edc,skip,not user facing,Specialize sym node when used as device kwarg (#131811),test/dynamo/test_misc.py torch/csrc/utils/python_arg_parser.cpp torch/csrc/utils/python_arg_parser.h,https://github.com/pytorch/pytorch/pull/131811,ydwu4,albanD,jansel,yanboliang,
3864a2d834e,profiler,not user facing,[profiler ut] Update event name in test_profiler.py (#131757),test/profiler/test_profiler.py,https://github.com/pytorch/pytorch/pull/131757,fwenguang,aaronenyeshi,,,
9027db1ab8c,distributed,Untopiced,TCPStore: fix remote address (#131773) (#131913),test/distributed/test_store.py torch/csrc/distributed/c10d/TCPStore.cpp torch/csrc/distributed/c10d/TCPStore.hpp torch/csrc/distributed/c10d/init.cpp torch/csrc/distributed/c10d/socket.cpp,https://github.com/pytorch/pytorch/pull/131913,d4l3k,Skylion007,kurman,rsdcastro,
239d4d24890,skip,Untopiced,"Revert ""[reland][inductor] switch AotCodeCompiler to new cpp_builder (#130127)""",test/inductor/test_torchinductor.py torch/_inductor/codecache.py,,,,,,
9f6d7df3d93,python_frontend,Untopiced,docs(multinomial): Add reference to `Multinomial` class (#131904),torch/_torch_docs.py,https://github.com/pytorch/pytorch/pull/131904,aos,jbschlosser,,,
3816f6420a2,optim,not user facing,[BE] remove unnecessary _dispatch_sqrt by using ** 0.5 (#131358),torch/optim/adam.py torch/optim/adamw.py torch/optim/nadam.py torch/optim/optimizer.py torch/optim/radam.py,https://github.com/pytorch/pytorch/pull/131358,janeyx99,albanD,,,
f217b470cc7,skip,not user facing,[CMAKE] Avoid double setting of LDFLAGS (#130370),CMakeLists.txt,https://github.com/pytorch/pytorch/pull/130370,Aidyn-A,atalman,malfet,tinglvv,
e6cddc9271f,releng,not user facing,Fix public API tests (#131386),test/test_public_bindings.py torch/ao/quantization/__init__.py torch/library.py torch/nn/attention/flex_attention.py torch/onnx/symbolic_helper.py torch/optim/__init__.py torch/storage.py,https://github.com/pytorch/pytorch/pull/131386,jbschlosser,albanD,,,
f2dedc910e8,dynamo,not user facing,Improve SpeculationLog error message (#131982),torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/131982,ezyang,anijain2305,jansel,mlazos,
964f97539f0,mps,bug fixes,[MPS] Correct nonzero warning and fix the test (#132127),aten/src/ATen/native/mps/operations/Indexing.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/132127,qqaatw,malfet,,,
58f76bc3017,dynamo,Untopiced,Revise skip torchrec logic (#130783),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/130783,Microve,yanboliang,,,
d53b11bb6e6,skip,not user facing,Strict shape checking for NJTs with TestCase.assertEqual() (#131898),test/test_nestedtensor.py torch/nested/_internal/ops.py torch/testing/_comparison.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/131898,jbschlosser,soulitzer,,,
1118c74b5fa,inductor,Untopiced,[PT2] Port fuse_chunk_reshape_unsqueeze_concat_pass to PT2 pre_grad passes (#131902) (#132078),torch/_inductor/fx_passes/pre_grad.py,https://github.com/pytorch/pytorch/pull/132078,huxintong,frank-wei,,,
19db4f60144,dynamo,not user facing,[capture_triton] fix special kwargs path (#132143),test/inductor/test_triton_kernels.py torch/_dynamo/variables/functions.py torch/_higher_order_ops/triton_kernel_wrap.py,https://github.com/pytorch/pytorch/pull/132143,zou3519,oulgen,,,
495d413519b,dynamo,not user facing,Include code object of frame being compiled in stack (#132161),torch/_dynamo/convert_frame.py torch/_logging/structured.py,https://github.com/pytorch/pytorch/pull/132161,ezyang,oulgen,,,
ff377e16abe,Uncategorized,Untopiced,Improve logging in the TSConverter (#132082),test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/132082,jiashenC,ydwu4,,,
69c34f6e4cb,distributed,not user facing,Corrects Error Codes from cudaHostRegister (#132089),torch/distributed/_state_dict_utils.py,https://github.com/pytorch/pytorch/pull/132089,LucasLLC,Skylion007,,,
3e142d766a0,releng,not user facing,[EZ] Make consistent with scale-config.yml (#132164),.github/lf-canary-scale-config.yml .github/lf-scale-config.yml,https://github.com/pytorch/pytorch/pull/132164,ZainRizvi,clee2000,malfet,zxiiro,
2b43fab555d,distributed,Untopiced,[DTensor] Added naive support for `nn.init.orthogonal_` (#132104),test/distributed/_tensor/test_dtensor_ops.py torch/distributed/_tensor/ops/_math_ops.py torch/nn/init.py,https://github.com/pytorch/pytorch/pull/132104,awgu,albanD,wanchaol,,
a141334c887,composability,Untopiced,migitate wrong tensor.dim_order() (#131366),test/test_torch.py torch/_prims_common/__init__.py,https://github.com/pytorch/pytorch/pull/131366,Gasoonjia,ezyang,,,
53a5e0f1a8e,distributed,not user facing,[BE] delete spmd module (#132072),test/distributed/_spmd/test_data_parallel.py test/distributed/_spmd/test_graph_utils.py test/distributed/_spmd/test_tracing.py test/distributed/_spmd/test_transformation.py torch/distributed/_spmd/__init__.py torch/distributed/_spmd/api.py torch/distributed/_spmd/batch_dim_utils.py torch/distributed/_spmd/comm_tensor.py torch/distributed/_spmd/config.py torch/distributed/_spmd/data_parallel.py torch/distributed/_spmd/distribute.py torch/distributed/_spmd/experimental_ops.py torch/distributed/_spmd/gm_transformation.py torch/distributed/_spmd/graph_optimization.py torch/distributed/_spmd/graph_utils.py torch/distributed/_spmd/iter_graph_module.py torch/distributed/_spmd/log_utils.py torch/distributed/_spmd/parallel_mode.py torch/distributed/_spmd/partial_lower.py,https://github.com/pytorch/pytorch/pull/132072,wanchaol,XilunWu,albanD,fegin,
93facac02c8,Uncategorized,Untopiced,[NeuralNetInference] Bring up iOS builds (#131917),c10/ovrsource_defs.bzl,https://github.com/pytorch/pytorch/pull/131917,rbergerjr,cccclai,,,
524aac413c3,skip,not user facing,Initial OpInfo-based testing for NJTs (#131704),test/test_nestedtensor.py torch/nested/_internal/ops.py torch/testing/_internal/opinfo/core.py torch/testing/_internal/opinfo/definitions/nested.py,https://github.com/pytorch/pytorch/pull/131704,jbschlosser,soulitzer,,,
5a33657b318,releng,not user facing,[micro_pipeline_tp] implement the pass for fused_scaled_matmul_reduce_scatter (#131951),.ci/pytorch/test.sh test/distributed/tensor/parallel/test_micro_pipeline_tp.py test/distributed/test_symmetric_memory.py torch/_inductor/fx_passes/micro_pipeline_tp.py torch/distributed/_symmetric_memory/__init__.py,https://github.com/pytorch/pytorch/pull/131951,yifuwang,weifengpy,,,
afb04d78c82,inductor,not user facing,Don't try hard to compute alignment of unbacked expressions (#131649),test/inductor/test_torchinductor_dynamic_shapes.py torch/_inductor/sizevars.py,https://github.com/pytorch/pytorch/pull/131649,ezyang,bdhirsh,,,
fdcd2f0dd16,inductor,Untopiced,[PT2][Optimus] Add unbind cat to view pass (#132152),test/inductor/test_split_cat_fx_passes.py torch/_inductor/fx_passes/split_cat.py,https://github.com/pytorch/pytorch/pull/132152,mengluy0125,jackiexu1992,,,
882d80fd924,inductor,not user facing,Add lowering for updated _scaled_mm (fixing submodules) (#130422),test/inductor/test_fp8.py torch/_inductor/kernel/mm_common.py torch/_inductor/kernel/mm_scaled.py torch/_inductor/lowering.py torch/_inductor/utils.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/130422,yangsiyu007,ipiszy,,,
31205d51989,inductor,not user facing,[Inductor][CPP] Fix Local Buffer issue with inplace result line (#132018),test/inductor/test_cpu_repro.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/132018,leslie-fang-intel,jgong5,peterbell10,,
f85feef1279,distributed,Untopiced,[DTensor] add support for custom op registration (#131108),test/distributed/_tensor/experimental/test_register_sharding.py torch/distributed/_tensor/experimental/__init__.py torch/distributed/_tensor/experimental/register_sharding.py,https://github.com/pytorch/pytorch/pull/131108,tianyu-l,wanchaol,,,
05317cd8f7b,distributed,not user facing,[dtensor][be] improving readability and reducing repeating code (#132070),torch/distributed/_tensor/debug/comm_mode.py,https://github.com/pytorch/pytorch/pull/132070,sinhaanshul,XilunWu,,,
914577569d2,python_frontend,bc breaking,Remove python 3.8 nightly builds (#132138),.github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-conda-nightly.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml .github/workflows/generated-macos-arm64-binary-conda-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-conda-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/132138,atalman,albanD,huydhn,malfet,
fbe6f42dcf1,mobile,not user facing,[BE][Easy][8/19] enforce style for empty lines in import segments in `test/[k-p]*/` (#129759),test/lazy/test_debug_util.py test/lazy/test_extract_compiled_graph.py test/lazy/test_functionalization.py test/lazy/test_generator.py test/lazy/test_meta_kernel.py test/lazy/test_reuse_ir.py test/lazy/test_step_closures.py test/mobile/custom_build/prepare_model.py test/mobile/lightweight_dispatch/tests_setup.py test/mobile/model_test/gen_test_model.py test/mobile/model_test/update_production_ops.py test/mobile/test_bytecode.py test/mobile/test_lite_script_module.py test/mobile/test_lite_script_type.py test/mobile/test_upgrader_codegen.py test/mobile/test_upgraders.py test/nn/test_convolution.py test/nn/test_embedding.py test/nn/test_init.py test/nn/test_load_state_dict.py test/nn/test_module_hooks.py test/nn/test_multihead_attention.py test/nn/test_parametrization.py test/nn/test_pooling.py test/nn/test_pruning.py test/onnx/dynamo/test_dynamo_with_onnxruntime_backend.py test/onnx/dynamo/test_exporter_api.py test/onnx/dynamo/test_registry_dispatcher.py test/onnx/error_reproduction.py test/onnx/internal/test_diagnostics.py test/onnx/model_defs/dcgan.py test/onnx/onnx_test_common.py test/onnx/pytorch_test_common.py test/onnx/test_export_modes.py test/onnx/test_fx_op_consistency.py test/onnx/test_fx_passes.py test/onnx/test_fx_to_onnx.py test/onnx/test_fx_to_onnx_decomp_skip.py test/onnx/test_fx_to_onnx_with_onnxruntime.py test/onnx/test_models.py test/onnx/test_onnx_opset.py test/onnx/test_onnxscript_no_runtime.py test/onnx/test_op_consistency.py test/onnx/test_operators.py test/onnx/test_pytorch_onnx_no_runtime.py test/onnx/test_pytorch_onnx_onnxruntime.py test/onnx/test_pytorch_onnx_onnxruntime_cuda.py test/onnx/test_pytorch_onnx_shape_inference.py test/onnx/test_utility_funs.py test/onnx/test_verification.py test/onnx/torch_export/test_torch_export_with_onnxruntime.py test/onnx/verify.py test/optim/test_lrscheduler.py test/optim/test_swa_utils.py test/package/generate_bc_packages.py test/package/package_a/test_module.py test/package/package_c/test_module.py test/package/test_analyze.py test/package/test_dependency_api.py test/package/test_dependency_hooks.py test/package/test_digraph.py test/package/test_directory_reader.py test/package/test_glob_group.py test/package/test_importer.py test/package/test_load_bc_packages.py test/package/test_mangling.py test/package/test_misc.py test/package/test_model.py test/package/test_package_fx.py test/package/test_package_script.py test/package/test_repackage.py test/package/test_resources.py test/package/test_save_load.py test/profiler/test_execution_trace.py test/profiler/test_profiler_tree.py test/profiler/test_record_function.py test/profiler/test_torch_tidy.py tools/linter/adapters/ufmt_linter.py,https://github.com/pytorch/pytorch/pull/129759,XuehaiPan,ezyang,justinchuby,,
c07aa1c9c98,skip,not user facing,[Easy] reorder functions in `torch._jit_internal` (#130531),torch/_jit_internal.py,https://github.com/pytorch/pytorch/pull/130531,XuehaiPan,EikanWang,ezyang,,
df0494bbba6,skip,not user facing,Clean redundant link libraries for XPU (#131322),caffe2/CMakeLists.txt,https://github.com/pytorch/pytorch/pull/131322,CuiYifeng,cyyever,ezyang,,
78020ea55d1,skip,not user facing,Add functions from `torch.masked._ops` to `__all__` for `torch.masked` (#131288),torch/masked/__init__.py,https://github.com/pytorch/pytorch/pull/131288,ringohoffman,ezyang,,,
deb788f6cc0,skip,not user facing,Merge `torch.nn.utils.rnn` type stubs (#131872),torch/_C/_nn.pyi.in torch/nn/utils/rnn.py torch/nn/utils/rnn.pyi,https://github.com/pytorch/pytorch/pull/131872,ringohoffman,Skylion007,ezyang,,
52c3af62d6f,skip,not user facing,Add fx graph runnable to tl parse (#130976),test/dynamo/test_structured_trace.py torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/130976,eellison,ezyang,,,
27c9262d298,distributed,Untopiced,Fix stdout / stderr typing in SubprocessHandler (#132071),torch/distributed/elastic/multiprocessing/subprocess_handler/subprocess_handler.py,https://github.com/pytorch/pytorch/pull/132071,cniii,Skylion007,,,
91299c95ece,skip,Untopiced,"Revert ""Add functions from `torch.masked._ops` to `__all__` for `torch.masked` (#131288)""",torch/masked/__init__.py,,,,,,
89da94594e0,jit,not user facing,[11/N] Fix clang-tidy warnings in jit  (#132131),test/cpp/tensorexpr/test_base.h test/cpp/tensorexpr/test_loopnest.cpp torch/csrc/jit/tensorexpr/analysis.h torch/csrc/jit/tensorexpr/block_codegen.cpp torch/csrc/jit/tensorexpr/block_codegen.h torch/csrc/jit/tensorexpr/cpp_codegen.cpp torch/csrc/jit/tensorexpr/cpp_codegen.h torch/csrc/jit/tensorexpr/cuda_codegen.cpp torch/csrc/jit/tensorexpr/cuda_codegen.h torch/csrc/jit/tensorexpr/eval.cpp torch/csrc/jit/tensorexpr/exceptions.h torch/csrc/jit/tensorexpr/half_support.h torch/csrc/jit/tensorexpr/hash_provider.cpp torch/csrc/jit/tensorexpr/hash_provider.h torch/csrc/jit/tensorexpr/ir_cloner.cpp torch/csrc/jit/tensorexpr/ir_printer.cpp torch/csrc/jit/tensorexpr/ir_printer.h torch/csrc/jit/tensorexpr/ir_verifier.cpp torch/csrc/jit/tensorexpr/ir_verifier.h torch/csrc/jit/tensorexpr/ir_visitor.cpp torch/csrc/jit/tensorexpr/ir_visitor.h torch/csrc/jit/tensorexpr/llvm_codegen.cpp torch/csrc/jit/tensorexpr/loopnest.cpp torch/csrc/jit/tensorexpr/mem_dependency_checker.cpp torch/csrc/jit/tensorexpr/mem_dependency_checker.h torch/csrc/jit/tensorexpr/registerizer.cpp torch/csrc/jit/tensorexpr/registerizer.h,https://github.com/pytorch/pytorch/pull/132131,cyyever,Skylion007,,,
ae708e97919,onnx,deprecation,[ONNX] Remove the deprecated SymbolicContext (#132184),test/onnx/test_custom_ops.py test/onnx/test_pytorch_onnx_no_runtime.py test/onnx/test_pytorch_onnx_onnxruntime.py torch/onnx/__init__.py torch/onnx/_exporter_states.py torch/onnx/utils.py,https://github.com/pytorch/pytorch/pull/132184,justinchuby,titaiwangms,,,
e9d1c262756,dynamo,not user facing,fix uniform op in dynamo (#132160),test/dynamo/test_repros.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/132160,yiming0416,anijain2305,,,
a94e507c39d,quantization,not user facing,"[aota] Needs autograd if an input requires_grad, agnostic to enable_grad (#128890)",test/functorch/test_aotdispatch.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/aot_autograd.py,https://github.com/pytorch/pytorch/pull/128890,IvanKobzarev,bdhirsh,,,
9e473fd8688,nn_frontend,new features,Make adding Buffers more like adding Parameters (#125971),docs/source/nn.rst test/distributed/_composable/fsdp/test_fully_shard_init.py test/distributed/fsdp/test_fsdp_flatten_params.py test/distributed/fsdp/test_fsdp_meta.py test/distributed/fsdp/test_fsdp_misc.py test/distributed/fsdp/test_fsdp_state_dict.py test/distributed/fsdp/test_fsdp_tp_integration.py test/distributed/fsdp/test_fsdp_unshard_params.py test/distributed/optim/test_zero_redundancy_optimizer.py test/distributed/pipelining/model_registry.py test/distributed/test_data_parallel.py test/distributed/test_dynamo_distributed.py test/dynamo/test_export.py test/dynamo/test_higher_order_ops.py test/dynamo/test_minifier.py test/dynamo/test_misc.py test/dynamo/test_modules.py test/dynamo/test_repros.py test/export/test_converter.py test/export/test_experimental.py test/export/test_export.py test/export/test_pass_infra.py test/export/test_passes.py test/export/test_unflatten.py test/export/test_verifier.py test/functorch/test_aotdispatch.py test/functorch/test_control_flow.py test/functorch/test_eager_transforms.py test/higher_order_ops/test_with_effects.py test/inductor/test_aot_inductor.py test/inductor/test_cpu_repro.py test/inductor/test_cuda_repro.py test/inductor/test_cudagraph_trees.py test/inductor/test_torchinductor.py test/jit/test_save_load.py test/nn/test_lazy_modules.py test/nn/test_load_state_dict.py test/nn/test_parametrization.py test/onnx/test_fx_to_onnx.py test/onnx/test_pytorch_onnx_onnxruntime.py test/onnx/test_utility_funs.py test/onnx/torch_export/test_torch_export_with_onnxruntime.py test/quantization/core/test_quantized_tensor.py test/quantization/eager/test_quantize_eager_qat.py test/test_fx.py test/test_fx_experimental.py test/test_jit.py test/test_mps.py test/test_nn.py test/test_stateless.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builder.py torch/nn/__init__.py torch/nn/modules/module.py torch/nn/parameter.py torch/nn/parameter.pyi torch/testing/_internal/common_nn.py,https://github.com/pytorch/pytorch/pull/125971,ekamiti,albanD,anijain2305,mlazos,
e7eeee473c6,quantization,not user facing,[BE][Easy][14/19] enforce style for empty lines in import segments in `torch/_[a-c]*/` and `torch/_[e-h]*/` and `torch/_[j-z]*/` (#129765),tools/linter/adapters/ufmt_linter.py torch/__init__.py torch/_appdirs.py torch/_custom_ops.py torch/_export/converter.py torch/_export/db/examples/model_attr_mutation.py torch/_export/db/examples/optional_input.py torch/_export/db/examples/torch_sym_min.py torch/_export/non_strict_utils.py torch/_export/passes/collect_tracepoints_pass.py torch/_export/passes/constant_folding.py torch/_export/passes/lift_constants_pass.py torch/_export/tools.py torch/_export/utils.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py torch/_functorch/_aot_autograd/functional_utils.py torch/_functorch/_aot_autograd/input_output_analysis.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py torch/_functorch/_aot_autograd/logging_utils.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/_aot_autograd/subclass_utils.py torch/_functorch/_aot_autograd/utils.py torch/_functorch/aot_autograd.py torch/_functorch/apis.py torch/_functorch/compile_utils.py torch/_functorch/config.py torch/_functorch/eager_transforms.py torch/_functorch/fx_minifier.py torch/_functorch/make_functional.py torch/_functorch/partitioners.py torch/_functorch/pyfunctorch.py torch/_functorch/python_key.py torch/_functorch/top_operators_github_usage.py torch/_functorch/utils.py torch/_functorch/vmap.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/map.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_higher_order_ops/wrap.py torch/_inductor/codecache.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/cudagraph_utils.py torch/_inductor/extern_node_serializer.py torch/_inductor/fx_passes/b2b_gemm.py torch/_inductor/fx_passes/micro_pipeline_tp.py torch/_inductor/graph.py torch/_inductor/kernel/mm_scaled.py torch/_inductor/utils.py torch/_lazy/extract_compiled_graph.py torch/_lazy/tensor_factory_functions.py torch/_library/__init__.py torch/_library/autograd.py torch/_library/fake_class_registry.py torch/_library/infer_schema.py torch/_library/simple_registry.py torch/_logging/__init__.py torch/_logging/_internal.py torch/_logging/_registrations.py torch/_namedtensor_internals.py torch/_numpy/_binary_ufuncs_impl.py torch/_numpy/_casting_dicts.py torch/_numpy/_dtypes_impl.py torch/_numpy/_funcs.py torch/_numpy/_funcs_impl.py torch/_numpy/_ndarray.py torch/_numpy/_normalizations.py torch/_numpy/_reductions_impl.py torch/_numpy/_unary_ufuncs_impl.py torch/_numpy/testing/__init__.py torch/_numpy/testing/utils.py torch/_prims/__init__.py torch/_prims/context.py torch/_prims/debug_prims.py torch/_prims/executor.py torch/_prims/rng_prims.py torch/_prims_common/__init__.py torch/_prims_common/wrappers.py torch/_refs/__init__.py torch/_refs/_conversions.py torch/_refs/fft.py torch/_refs/linalg/__init__.py torch/_refs/nn/__init__.py torch/_refs/nn/functional/__init__.py torch/_refs/special/__init__.py torch/_strobelight/compile_time_profiler.py torch/_strobelight/examples/cli_function_profiler_example.py torch/_strobelight/examples/compile_time_profile_example.py torch/_subclasses/__init__.py torch/_subclasses/_fake_tensor_utils.py torch/_subclasses/fake_impls.py torch/_subclasses/fake_tensor.py torch/_subclasses/functional_tensor.py torch/_subclasses/meta_utils.py torch/_subclasses/schema_check_mode.py,https://github.com/pytorch/pytorch/pull/129765,XuehaiPan,ezyang,,,
89053e382a6,skip,not user facing,[NestedTensor] Integrate the softmax operator along the jagged dimension into NestedTensor (#132170),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/132170,jananisriram,davidberard98,,,
46994e753bf,skip,not user facing,[NestedTensor] Integrate the layer normalization operator along the jagged dimension into NestedTensor (#132172),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/132172,jananisriram,davidberard98,,,
548c460bf1f,quantization,not user facing,[BE][Easy][7/19] enforce style for empty lines in import segments in `test/[a-c]*/` and `test/[q-z]*/` (#129758),test/ao/sparsity/test_activation_sparsifier.py test/ao/sparsity/test_composability.py test/ao/sparsity/test_data_scheduler.py test/ao/sparsity/test_data_sparsifier.py test/ao/sparsity/test_kernels.py test/ao/sparsity/test_parametrization.py test/ao/sparsity/test_scheduler.py test/ao/sparsity/test_sparsifier.py test/ao/sparsity/test_sparsity_utils.py test/ao/sparsity/test_structured_sparsifier.py test/autograd/test_complex.py test/autograd/test_functional.py test/bottleneck_test/test.py test/bottleneck_test/test_args.py test/cpp/aoti_inference/compile_model.py test/cpp/aoti_inference/test.py test/cpp/api/init_baseline.py test/cpp_api_parity/functional_impl_check.py test/cpp_api_parity/module_impl_check.py test/cpp_api_parity/parity_table_parser.py test/cpp_api_parity/sample_functional.py test/cpp_api_parity/sample_module.py test/cpp_api_parity/utils.py test/cpp_extensions/no_python_abi_suffix_test/setup.py test/cpp_extensions/setup.py test/custom_operator/my_custom_ops.py test/custom_operator/my_custom_ops2.py test/custom_operator/pointwise.py test/custom_operator/test_custom_ops.py test/quantization/bc/test_backward_compatibility.py test/quantization/core/experimental/test_adaround_eager.py test/quantization/core/experimental/test_float8.py test/quantization/jit/test_ondevice_quantization.py test/quantization/jit/test_quantize_jit.py test/quantization/pt2e/test_duplicate_dq.py test/quantization/pt2e/test_graph_utils.py test/quantization/pt2e/test_metadata_porting.py test/quantization/pt2e/test_quantize_pt2e.py test/quantization/pt2e/test_x86inductor_quantizer.py test/quantization/pt2e/test_xnnpack_quantizer.py test/scripts/run_cuda_memcheck.py test/torch_np/numpy_tests/core/test_dlpack.py test/torch_np/numpy_tests/core/test_dtype.py test/torch_np/numpy_tests/core/test_einsum.py test/torch_np/numpy_tests/core/test_getlimits.py test/torch_np/numpy_tests/core/test_indexing.py test/torch_np/numpy_tests/core/test_multiarray.py test/torch_np/numpy_tests/core/test_numeric.py test/torch_np/numpy_tests/core/test_numerictypes.py test/torch_np/numpy_tests/core/test_scalar_ctors.py test/torch_np/numpy_tests/core/test_scalar_methods.py test/torch_np/numpy_tests/core/test_scalarinherit.py test/torch_np/numpy_tests/core/test_scalarmath.py test/torch_np/numpy_tests/core/test_shape_base.py test/torch_np/numpy_tests/fft/test_helper.py test/torch_np/numpy_tests/fft/test_pocketfft.py test/torch_np/numpy_tests/lib/test_arraysetops.py test/torch_np/numpy_tests/lib/test_function_base.py test/torch_np/numpy_tests/lib/test_histograms.py test/torch_np/numpy_tests/lib/test_index_tricks.py test/torch_np/numpy_tests/lib/test_shape_base_.py test/torch_np/numpy_tests/lib/test_twodim_base.py test/torch_np/numpy_tests/lib/test_type_check.py test/torch_np/numpy_tests/linalg/test_linalg.py test/torch_np/test_basic.py test/torch_np/test_dtype.py test/torch_np/test_function_base.py test/torch_np/test_ndarray_methods.py test/torch_np/test_nep50_examples.py test/torch_np/test_random.py test/torch_np/test_reductions.py test/torch_np/test_ufuncs_basic.py test/torch_np/test_unary_ufuncs.py test/typing/fail/creation_ops.py test/typing/fail/disabled_bitwise_ops.py test/typing/fail/random.py test/typing/pass/disabled_jit.py test/typing/pass/math_ops.py test/typing/reveal/opt_size.py test/typing/reveal/size.py test/typing/reveal/tensor_constructors.py test/typing/reveal/tensor_sampling.py test/xpu/test_conv.py tools/linter/adapters/ufmt_linter.py,https://github.com/pytorch/pytorch/pull/129758,XuehaiPan,ezyang,,,
f4f7aba75d6,dynamo,not user facing,Expose function to probe whether PyTorch was built with FlashAttention (#131894),aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/cuda/sdp_utils.h docs/source/backends.rst torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/_dynamo/variables/builder.py torch/backends/cuda/__init__.py torch/csrc/Module.cpp torch/nn/attention/bias.py,https://github.com/pytorch/pytorch/pull/131894,lw,drisspg,eqy,,
45e6a364ee1,dynamo,not user facing,Avoid autocast deprecation warning (#132207),torch/_dynamo/repro/after_dynamo.py torch/testing/_internal/common_fsdp.py,https://github.com/pytorch/pytorch/pull/132207,guangyey,awgu,,,
dad125a64bf,skip,not user facing,Address clang-tidy nits in BFloat16 (#132203),c10/util/BFloat16.h,https://github.com/pytorch/pytorch/pull/132203,danzimm,houseroad,,,
3d7f541597e,distributed,not user facing,[BE][TP] Check module has bias before access (#132137),torch/distributed/tensor/parallel/style.py,https://github.com/pytorch/pytorch/pull/132137,kwen2501,wanchaol,,,
5406e46b00d,skip,Untopiced,"Revert ""Add fx graph runnable to tl parse (#130976)""",test/dynamo/test_structured_trace.py torch/_inductor/compile_fx.py,,,,,,
9c52013559f,skip,not user facing,[subclasses] Fix nested subclasses flattened tensors ordering (#132096),torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/132096,IvanKobzarev,bdhirsh,,,
b2118573d65,distributed,not user facing,[BE] Unify PG assignments (#132230),torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/132230,kwen2501,Skylion007,wconstab,,
39a3c98aa65,inductor,not user facing,[inductor] fix scalar miss constuctor for long type. (#132117),c10/core/Scalar.h,https://github.com/pytorch/pytorch/pull/132117,xuhancn,desertfire,jgong5,,
2c7bd61afa4,skip,Untopiced,[pytorch][counters] Pybind for WaitCounter (#132167),c10/util/WaitCounter.h test/test_monitor.py torch/csrc/monitor/python_init.cpp,https://github.com/pytorch/pytorch/pull/132167,andriigrynenko,asiab4,,,
13d744464f1,skip,not user facing,"Migrate Inductor scheduler, dependencies, ir, and codegen/common to use OrderedSet (#130004)",torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_split_scan.py torch/_inductor/dependencies.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/mkldnn_ir.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/130004,eellison,oulgen,,,
c85088b1f9e,cuda,Untopiced,[ROCm] performance optimization for index select  (#131713),aten/src/ATen/native/cuda/Indexing.cu,https://github.com/pytorch/pytorch/pull/131713,hongxiayang,jeffdaily,malfet,syed-ahmed,
aec8bc5e4cb,fx,not user facing,[easy] fix type annotation on constraint_violations variable (#127064),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/127064,davidberard98,jananisriram,,,
fe4f8e97cd7,skip,not user facing,[Intel GPU] xpu-ops codegen via backend whitelist (#130082),caffe2/CMakeLists.txt torchgen/dest/register_dispatch_key.py torchgen/model.py,https://github.com/pytorch/pytorch/pull/130082,ZhiweiYan-96,EikanWang,atalman,gujinghui,
bdd7a0322d8,dynamo,Untopiced,[Dynamo] Fix - `str` handler for UserDefinedObjectVariable (#130506),test/dynamo/test_functions.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/130506,datagero,anijain2305,oulgen,,
9826c542f0b,inductor,not user facing,[inductor] skip remote fx caching in failing pattern matcher tests (#132206),test/inductor/test_pattern_matcher.py,https://github.com/pytorch/pytorch/pull/132206,masnesral,oulgen,,,
784a6ec5a30,skip,Untopiced,"Revert ""Migrate Inductor scheduler, dependencies, ir, and codegen/common to use OrderedSet (#130004)""",torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_split_scan.py torch/_inductor/dependencies.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/mkldnn_ir.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py,,,,,,
144639797a4,dynamo,not user facing,Improve side effects error message (#132223),torch/_dynamo/side_effects.py,https://github.com/pytorch/pytorch/pull/132223,mlazos,anijain2305,,,
6214b5388b5,inductor,not user facing,typing ir.py - part 1 (#131845),torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/dependencies.py torch/_inductor/ir.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/131845,aorenste,Skylion007,eellison,,
c3a31d90e7d,skip,Untopiced,Fix inlining module-scoped store global (#132224),test/dynamo/mock_store_global_crossfile_inline.py test/dynamo/test_global.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/132224,mlazos,anijain2305,,,
7eb2a99585b,skip,not user facing,Fix to support unary pointwise ops when an NJT is not the first arg (#131937),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/131937,jbschlosser,soulitzer,,,
b40249b4627,skip,not user facing,propagate XLA's metadata after functional sync (#131076),aten/src/ATen/FunctionalTensorWrapper.cpp aten/src/ATen/FunctionalTensorWrapper.h torchgen/gen_functionalization_type.py,https://github.com/pytorch/pytorch/pull/131076,JackCaoG,bdhirsh,,,
f9e4d05c15e,inductor,not user facing,Save and run post compilation steps within FXGraphCache (#130572),test/inductor/test_cudagraph_trees.py test/inductor/test_torchinductor.py torch/_inductor/codecache.py torch/_inductor/compile_fx.py torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py torch/_inductor/ir.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/130572,jamesjwu,eellison,,,
898a431a46f,dynamo,not user facing,Dump files that look like FX graphs to structured log (#132100),test/dynamo/test_structured_trace.py torch/_dynamo/symbolic_convert.py torch/_logging/structured.py,https://github.com/pytorch/pytorch/pull/132100,ezyang,oulgen,,,
cfe61e84ac0,inductor,docs,Add a 'to' method for moving to and from device for BlockMask (#132087),test/inductor/test_flex_attention.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/132087,drisspg,yanboliang,,,
2138a710eb8,skip,not user facing,enable test_max_pool2d6 after resolving empty array (#132219),test/inductor/test_cpu_cpp_wrapper.py,https://github.com/pytorch/pytorch/pull/132219,exclamaforte,desertfire,,,
6955bc170dd,releng,not user facing,Some updates to merge rules (#132296),.github/merge_rules.yaml,https://github.com/pytorch/pytorch/pull/132296,ezyang,albanD,malfet,,
dc38646c588,skip,Untopiced,"Revert ""[pytorch][counters] Pybind for WaitCounter (#132167)""",c10/util/WaitCounter.h test/test_monitor.py torch/csrc/monitor/python_init.cpp,,,,,,
cb4c107d707,Uncategorized,Untopiced,[pytorch][counters] DynamicCounter (#132166),c10/util/DynamicCounter.cpp c10/util/DynamicCounter.h,https://github.com/pytorch/pytorch/pull/132166,andriigrynenko,asiab4,,,
aeb78c9849a,skip,not user facing,[TD] More files for test_public_bindings (#132284),tools/testing/target_determination/heuristics/public_bindings.py,https://github.com/pytorch/pytorch/pull/132284,clee2000,ZainRizvi,,,
d72e863b3ec,inductor,not user facing,Fix lint after PR #130572 (#132316),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/132316,kit1980,Skylion007,ZainRizvi,malfet,
f8b6e918403,releng,not user facing,Add sequoia runner to mac-mps (#132190),.github/workflows/mac-mps.yml aten/src/ATen/mps/MPSDevice.h aten/src/ATen/mps/MPSDevice.mm aten/src/ATen/native/mps/operations/Attention.mm,https://github.com/pytorch/pytorch/pull/132190,skotapati,malfet,,,
d3cefc9e3a7,skip,not user facing,AutoHeuristic: Collect data for mixed_mm (#131611),torchgen/_autoheuristic/benchmark_runner.py torchgen/_autoheuristic/benchmark_utils.py torchgen/_autoheuristic/mixed_mm/gen_data_mixed_mm.py torchgen/_autoheuristic/pad_mm/gen_data_pad_mm.py torchgen/_autoheuristic/pad_mm/gen_pad_mm_a100.sh torchgen/_autoheuristic/pad_mm/gen_pad_mm_h100.sh torchgen/_autoheuristic/pad_mm/get_padmm_dataset.sh torchgen/_autoheuristic/pad_mm/train_pad_mm.py torchgen/_autoheuristic/train.py torchgen/autoheuristic/gen_data_pad_mm.py torchgen/autoheuristic/gen_pad_mm_a100.sh torchgen/autoheuristic/gen_pad_mm_h100.sh torchgen/autoheuristic/get_padmm_dataset.sh torchgen/autoheuristic/train.py torchgen/autoheuristic/train_pad_mm.py,https://github.com/pytorch/pytorch/pull/131611,AlnisM,eellison,,,
ad9826208c4,Uncategorized,Untopiced,Remove string length limit in ET (#132169),torch/csrc/profiler/standalone/execution_trace_observer.cpp,https://github.com/pytorch/pytorch/pull/132169,shengfukevin,sanrise,sraikund16,,
e74ba1b34a4,onnx,not user facing,[BE][Easy][15/19] enforce style for empty lines in import segments in `torch/_d*/` (#129767),tools/linter/adapters/ufmt_linter.py torch/_decomp/__init__.py torch/_decomp/decompositions.py torch/_decomp/decompositions_for_jvp.py torch/_decomp/decompositions_for_rng.py torch/_dispatch/python.py torch/_dynamo/__init__.py torch/_dynamo/_trace_wrapped_higher_order_op.py torch/_dynamo/backends/common.py torch/_dynamo/backends/cudagraphs.py torch/_dynamo/backends/debugging.py torch/_dynamo/backends/distributed.py torch/_dynamo/backends/onnxrt.py torch/_dynamo/backends/torchxla.py torch/_dynamo/backends/tvm.py torch/_dynamo/bytecode_analysis.py torch/_dynamo/cache_size.py torch/_dynamo/codegen.py torch/_dynamo/compiled_autograd.py torch/_dynamo/config.py torch/_dynamo/convert_frame.py torch/_dynamo/create_parameter_op.py torch/_dynamo/current_scope_id.py torch/_dynamo/debug_utils.py torch/_dynamo/decorators.py torch/_dynamo/device_interface.py torch/_dynamo/distributed.py torch/_dynamo/eval_frame.py torch/_dynamo/exc.py torch/_dynamo/external_utils.py torch/_dynamo/funcname_cache.py torch/_dynamo/guards.py torch/_dynamo/hooks.py torch/_dynamo/logging.py torch/_dynamo/mutation_guard.py torch/_dynamo/output_graph.py torch/_dynamo/replay_record.py torch/_dynamo/repro/after_aot.py torch/_dynamo/repro/after_dynamo.py torch/_dynamo/resume_execution.py torch/_dynamo/source.py torch/_dynamo/symbolic_convert.py torch/_dynamo/tensor_version_op.py torch/_dynamo/test_case.py torch/_dynamo/testing.py torch/_dynamo/trace_rules.py torch/_dynamo/types.py torch/_dynamo/utils.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/distributed.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/higher_order_ops.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/optimizer.py torch/_dynamo/variables/script_object.py torch/_dynamo/variables/sdpa.py torch/_dynamo/variables/tensor.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/torch_function.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/129767,XuehaiPan,anijain2305,,,
260c991e205,inductor,bug fixes,[inductor] Fix unsoundness with negative-valued indexing expressions (#131761),test/inductor/test_indexing.py test/inductor/test_mkldnn_pattern_matcher.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/_inductor/codegen/triton.py torch/_inductor/runtime/triton_helpers.py torch/_inductor/sizevars.py,https://github.com/pytorch/pytorch/pull/131761,peterbell10,ezyang,,,
b0e06d9d6ad,inductor,Untopiced,Make config.autotune_remote_cache be a three-way option (#132285),torch/_inductor/config.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/132285,oulgen,aorenste,,,
645c1052a6e,inductor,not user facing,Refactor local autotune remote cache to make the code less error prone (#132289),torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/132289,oulgen,aorenste,,,
f0da167ce55,skip,not user facing,Add fx graph runnable to tl parse (#130976),test/dynamo/test_structured_trace.py torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/130976,eellison,ezyang,,,
6b28af1b79e,skip,Untopiced,Grouped Query Attention (#128898),aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/attention.h aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.h test/cpp_extensions/open_registration_extension.cpp test/distributed/_tensor/test_matrix_ops.py test/dynamo/test_sdpa.py test/test_transformers.py tools/pyi/gen_pyi.py torch/_C/__init__.pyi.in torch/_dynamo/variables/sdpa.py torch/csrc/Module.cpp torch/nested/_internal/sdpa.py torch/nn/attention/bias.py torch/nn/functional.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/128898,jainapurva,drisspg,,,
a488113062b,skip,not user facing,[AOTI] Fix bfloat16 in CPU (#132150),test/inductor/test_aot_inductor.py torch/_inductor/codegen/aoti_runtime/implementation.cpp torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cuda.py,https://github.com/pytorch/pytorch/pull/132150,yushangdi,chenyang78,desertfire,,
4e966e8a1c2,autograd_frontend,docs,Update inference_mode doc (#132321),torch/autograd/grad_mode.py,https://github.com/pytorch/pytorch/pull/132321,albanD,awgu,soulitzer,,
7c89ec0f7c8,skip,not user facing,Implements torch.cuda.MemPool() API (#131152),aten/src/ATen/cuda/CUDAGraph.cpp aten/src/ATen/cuda/CUDAGraph.h build_variables.bzl c10/cuda/CUDACachingAllocator.cpp c10/cuda/CUDACachingAllocator.h docs/source/conf.py docs/source/cuda.rst test/test_cuda.py torch/_C/__init__.pyi.in torch/csrc/Module.cpp torch/csrc/cuda/MemPool.cpp torch/cuda/__init__.py torch/cuda/memory.py,https://github.com/pytorch/pytorch/pull/131152,syed-ahmed,eqy,ezyang,,
2276d9045a0,skip,not user facing,[cpu] add more VecConvert for 8bits (#131876),aten/src/ATen/cpu/vec/vec256/vec256_convert.h aten/src/ATen/cpu/vec/vec512/vec512_convert.h,https://github.com/pytorch/pytorch/pull/131876,Valentine233,jgong5,leslie-fang-intel,,
344c15a0bb6,skip,not user facing,AutoHeuristic: mixed_mm heuristic for A100 (#131613),test/inductor/test_autoheuristic.py test/inductor/test_pattern_matcher.py torch/_inductor/autoheuristic/artifacts/_MixedMMA100.py torch/_inductor/autoheuristic/artifacts/_PadMMA100.py torch/_inductor/autoheuristic/autoheuristic.py torch/_inductor/autoheuristic/autoheuristic_utils.py torch/_inductor/autoheuristic/learnedheuristic_interface.py torch/_inductor/config.py torch/_inductor/kernel/mm.py torchgen/_autoheuristic/mixed_mm/gen_mixedmm_heuristic_a100.sh torchgen/_autoheuristic/mixed_mm/get_mixedmm_dataset.sh torchgen/_autoheuristic/mixed_mm/train_decision_mixedmm.py torchgen/_autoheuristic/pad_mm/gen_pad_mm_a100.sh torchgen/_autoheuristic/pad_mm/gen_pad_mm_h100.sh torchgen/_autoheuristic/pad_mm/train_decision_pad_mm.py torchgen/_autoheuristic/pad_mm/train_pad_mm.py torchgen/_autoheuristic/pad_mm/train_regression_pad_mm.py torchgen/_autoheuristic/train.py torchgen/_autoheuristic/train_decision.py torchgen/_autoheuristic/train_regression.py,https://github.com/pytorch/pytorch/pull/131613,AlnisM,eellison,,,
718c13cd39d,inductor,Untopiced,[inductor] Reinplacing should not allow an op to mutate the same input multiple times (#132238),test/inductor/test_triton_kernels.py torch/_inductor/fx_passes/reinplace.py,https://github.com/pytorch/pytorch/pull/132238,zou3519,eellison,oulgen,,
589aef4bb0d,fx,not user facing,Fix py codegen to delete values that don't have any  users (#131028),test/dynamo/test_autograd_function.py test/dynamo/test_comptime.py test/dynamo/test_ctx_manager.py test/dynamo/test_export.py test/dynamo/test_higher_order_ops.py test/dynamo/test_input_attr_tracking.py test/dynamo/test_misc.py test/dynamo/test_repros.py test/export/test_export.py test/export/test_passes.py test/export/test_torchbind.py test/functorch/test_aotdispatch.py test/functorch/test_control_flow.py test/functorch/test_eager_transforms.py test/higher_order_ops/test_with_effects.py test/inductor/test_flex_attention.py test/inductor/test_triton_kernels.py test/test_functionalization.py test/test_fx.py test/test_fx_reinplace_pass.py test/test_proxy_tensor.py torch/fx/graph.py,https://github.com/pytorch/pytorch/pull/131028,YangQun1,ezyang,,,
a28cda11ef6,skip,Untopiced,"Revert ""AutoHeuristic: mixed_mm heuristic for A100 (#131613)""",test/inductor/test_autoheuristic.py test/inductor/test_pattern_matcher.py torch/_inductor/autoheuristic/artifacts/_MixedMMA100.py torch/_inductor/autoheuristic/artifacts/_PadMMA100.py torch/_inductor/autoheuristic/autoheuristic.py torch/_inductor/autoheuristic/autoheuristic_utils.py torch/_inductor/autoheuristic/learnedheuristic_interface.py torch/_inductor/config.py torch/_inductor/kernel/mm.py torchgen/_autoheuristic/mixed_mm/gen_mixedmm_heuristic_a100.sh torchgen/_autoheuristic/mixed_mm/get_mixedmm_dataset.sh torchgen/_autoheuristic/mixed_mm/train_decision_mixedmm.py torchgen/_autoheuristic/pad_mm/gen_pad_mm_a100.sh torchgen/_autoheuristic/pad_mm/gen_pad_mm_h100.sh torchgen/_autoheuristic/pad_mm/train_decision_pad_mm.py torchgen/_autoheuristic/pad_mm/train_pad_mm.py torchgen/_autoheuristic/pad_mm/train_regression_pad_mm.py torchgen/_autoheuristic/train.py torchgen/_autoheuristic/train_decision.py torchgen/_autoheuristic/train_regression.py,,,,,,
10344d76bd4,skip,Untopiced,"Revert ""[AOTI] Fix bfloat16 in CPU (#132150)""",test/inductor/test_aot_inductor.py torch/_inductor/codegen/aoti_runtime/implementation.cpp torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cuda.py,,,,,,
81db69278d4,fx,Untopiced,unsupported sympy functions in export solver (#132325),test/export/test_export.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/132325,avikchaudhuri,ezyang,,,
073430ebea2,Uncategorized,Untopiced,Don't check for autograd state when lowering to inference IR (#131988),test/export/test_export.py torch/export/_trace.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/131988,tugsbayasgalan,angelayi,,,
46ed33b207e,fx,Untopiced,add decomposition_table as an arg to get_isolated_graphmodule (#130886),torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/130886,tianyu-l,wanchaol,,,
bc7ed1fbdcb,distributed,Untopiced,[FSDP2] add __repr__ to FSDPParamGroup and FSDPParam (#132350),torch/distributed/_composable/fsdp/_fsdp_param.py torch/distributed/_composable/fsdp/_fsdp_param_group.py,https://github.com/pytorch/pytorch/pull/132350,weifengpy,awgu,,,
b25ef91bf15,distributed,not user facing,[BE][Easy][18/19] enforce style for empty lines in import segments in `torch/d*/` (#129770),tools/linter/adapters/ufmt_linter.py torch/distributed/_composable/fsdp/_fsdp_state.py torch/distributed/_state_dict_utils.py torch/distributed/_symmetric_memory/__init__.py torch/distributed/_tensor/_shards_wrapper.py torch/distributed/_tensor/debug/comm_mode.py torch/distributed/_tensor/examples/comm_mode_features_example.py torch/distributed/_tensor/examples/comm_mode_features_example_argparser.py torch/distributed/_tools/fsdp2_mem_tracker.py torch/distributed/_tools/mem_tracker.py torch/distributed/checkpoint/logger.py torch/distributed/fsdp/_shard_utils.py torch/distributed/pipelining/_utils.py torch/distributions/__init__.py torch/distributions/bernoulli.py torch/distributions/beta.py torch/distributions/binomial.py torch/distributions/categorical.py torch/distributions/cauchy.py torch/distributions/chi2.py torch/distributions/constraint_registry.py torch/distributions/constraints.py torch/distributions/continuous_bernoulli.py torch/distributions/dirichlet.py torch/distributions/distribution.py torch/distributions/exp_family.py torch/distributions/exponential.py torch/distributions/fishersnedecor.py torch/distributions/gamma.py torch/distributions/geometric.py torch/distributions/gumbel.py torch/distributions/half_cauchy.py torch/distributions/half_normal.py torch/distributions/independent.py torch/distributions/kl.py torch/distributions/kumaraswamy.py torch/distributions/laplace.py torch/distributions/lkj_cholesky.py torch/distributions/log_normal.py torch/distributions/logistic_normal.py torch/distributions/lowrank_multivariate_normal.py torch/distributions/mixture_same_family.py torch/distributions/multinomial.py torch/distributions/multivariate_normal.py torch/distributions/negative_binomial.py torch/distributions/normal.py torch/distributions/one_hot_categorical.py torch/distributions/pareto.py torch/distributions/poisson.py torch/distributions/relaxed_bernoulli.py torch/distributions/relaxed_categorical.py torch/distributions/studentT.py torch/distributions/transformed_distribution.py torch/distributions/transforms.py torch/distributions/uniform.py torch/distributions/utils.py torch/distributions/von_mises.py torch/distributions/weibull.py,https://github.com/pytorch/pytorch/pull/129770,XuehaiPan,wconstab,,,
7911b7bfb77,inductor,not user facing,[inductor][cpp] stabilize do_bench_cpu (#131873),torch/_inductor/runtime/runtime_utils.py,https://github.com/pytorch/pytorch/pull/131873,jgong5,chunyuan-w,eellison,leslie-fang-intel,
90fa64bd7ef,Uncategorized,Untopiced,[torch][take2] Implement BFloat16 __hip_bfloat16 overloads (#132234),c10/util/BFloat16-inl.h c10/util/BFloat16.h,https://github.com/pytorch/pytorch/pull/132234,danzimm,houseroad,,,
e772547d705,dynamo,not user facing,[dynamo][rename/refactor] Rename guard_source NN_MODULE to SPECIALIZED_NN_MODULE (#132302),torch/_dynamo/guards.py torch/_dynamo/source.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/_guards.py,https://github.com/pytorch/pytorch/pull/132302,anijain2305,yanboliang,,,
bcd1d2e832b,dynamo,not user facing,[dynamo] Introduce UnspecializedNNModule guard source (#132304),test/distributed/test_dynamo_distributed.py torch/_dynamo/source.py torch/_guards.py,https://github.com/pytorch/pytorch/pull/132304,anijain2305,yanboliang,,,
f32ab3b9e37,skip,not user facing,"Migrate Inductor scheduler, dependencies, ir, and codegen/common to use OrderedSet (#130004)",torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_split_scan.py torch/_inductor/dependencies.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/mkldnn_ir.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/130004,eellison,oulgen,,,
928adb7cc2c,Uncategorized,Untopiced,Fix empty fake mode problem (#131995),test/export/test_export.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/131995,tugsbayasgalan,angelayi,,,
7a779b5257f,skip,not user facing,Add functions from `torch.masked._ops` to `__all__` for `torch.masked` (#131288),test/allowlist_for_publicAPI.json torch/masked/__init__.py,https://github.com/pytorch/pytorch/pull/131288,ringohoffman,ezyang,,,
4c29c1a96ad,skip,not user facing,[EZ] adjust test to accept training IR input (#131999),test/export/test_export.py,https://github.com/pytorch/pytorch/pull/131999,tugsbayasgalan,bdhirsh,,,
612ea353956,dynamo,not user facing,[dynamo] Introduce UnspecializedBuiltinNNModuleSource (#132312),torch/_dynamo/guards.py torch/_dynamo/source.py torch/_guards.py,https://github.com/pytorch/pytorch/pull/132312,anijain2305,yanboliang,,,
aa0ed2496f5,skip,not user facing,[dynamo] Wrap unspecialized nn module getattr with UnspecializedNNModuleSource (#132308),torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/132308,anijain2305,yanboliang,,,
d6a82ce39bd,skip,not user facing,[dynamo] Track builtin nn modules with UnspecializedBuiltinNNModuleVariable (#132314),torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/nn_module.py,https://github.com/pytorch/pytorch/pull/132314,anijain2305,yanboliang,,,
043e41f4f47,linalg_frontend,not user facing,[10/N] Use std::nullopt and std::make_optional (#132364),aten/src/ATen/core/ivalue.h aten/src/ATen/functorch/BatchRulesBinaryOps.cpp aten/src/ATen/functorch/BatchRulesConvolution.cpp aten/src/ATen/functorch/BatchRulesScatterOps.cpp aten/src/ATen/functorch/DynamicLayer.h aten/src/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/TensorAdvancedIndexing.cpp aten/src/ATen/native/cuda/Reduce.cuh aten/src/ATen/templates/Functions.cpp c10/test/core/impl/InlineDeviceGuard_test.cpp c10/test/core/impl/InlineStreamGuard_test.cpp torch/csrc/api/include/torch/data/detail/data_shuttle.h torch/csrc/api/include/torch/data/detail/queue.h torch/csrc/api/include/torch/data/samplers/distributed.h torch/csrc/api/include/torch/data/samplers/random.h torch/csrc/api/include/torch/data/samplers/sequential.h torch/csrc/api/include/torch/data/samplers/stream.h torch/csrc/api/include/torch/nn/cloneable.h torch/csrc/api/include/torch/nn/module.h torch/csrc/api/include/torch/nn/modules/container/any.h torch/csrc/api/include/torch/nn/modules/container/moduledict.h torch/csrc/api/include/torch/nn/modules/container/modulelist.h torch/csrc/api/include/torch/nn/modules/container/sequential.h torch/csrc/api/include/torch/nn/parallel/data_parallel.h torch/csrc/jit/mobile/train/random.h torch/csrc/jit/mobile/train/sequential.h torch/csrc/jit/runtime/static/impl.h,https://github.com/pytorch/pytorch/pull/132364,cyyever,ezyang,,,
bdd83c4c7f1,inductor,not user facing,Add Full block support to flex_decoding (#131404),benchmarks/transformer/score_mod.py test/inductor/test_flex_decoding.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/131404,joydddd,yanboliang,,,
d95756f6a5a,quantization,Untopiced,[Quantizer][Add] Fix add annotation with constant (#132092),torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py,https://github.com/pytorch/pytorch/pull/132092,mcr229,jerryzh168,,,
672ce4610e4,skip,not user facing,Populate submodules of `torch._C` to `sys.modules` recursively (#132216),test/dynamo/test_frame_init.py test/xpu/test_conv.py torch/_C/__init__.pyi.in torch/__init__.py torch/_dynamo/convert_frame.py torch/_dynamo/eval_frame.py torch/_dynamo/guards.py torch/_dynamo/types.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/132216,XuehaiPan,ezyang,,,
6ff1e43a416,skip,not user facing,[BE][Easy][13/19] enforce style for empty lines in import segments in `test/j*/` (#129764),test/jit/_imported_class_test/bar.py test/jit/_imported_class_test/foo.py test/jit/_imported_class_test/very/very/nested.py test/jit/fixtures_srcs/generate_models.py test/jit/fixtures_srcs/test_upgrader_models_generation.py test/jit/test_async.py test/jit/test_autodiff.py test/jit/test_autodiff_subgraph_slicing.py test/jit/test_backend_nnapi.py test/jit/test_backends.py test/jit/test_batch_mm.py test/jit/test_builtins.py test/jit/test_class_type.py test/jit/test_complex.py test/jit/test_complexity.py test/jit/test_convert_activation.py test/jit/test_cuda.py test/jit/test_custom_operators.py test/jit/test_data_parallel.py test/jit/test_decorator.py test/jit/test_device_analysis.py test/jit/test_dtype_analysis.py test/jit/test_enum.py test/jit/test_exception.py test/jit/test_freezing.py test/jit/test_functional_blocks.py test/jit/test_hash.py test/jit/test_hooks.py test/jit/test_ignorable_args.py test/jit/test_ignore_context_manager.py test/jit/test_isinstance.py test/jit/test_jit_utils.py test/jit/test_list_dict.py test/jit/test_logging.py test/jit/test_misc.py test/jit/test_models.py test/jit/test_module_apis.py test/jit/test_module_containers.py test/jit/test_module_interface.py test/jit/test_modules.py test/jit/test_op_decompositions.py test/jit/test_parametrization.py test/jit/test_pdt.py test/jit/test_peephole.py test/jit/test_profiler.py test/jit/test_python_bindings.py test/jit/test_python_builtins.py test/jit/test_python_ir.py test/jit/test_recursive_script.py test/jit/test_remove_mutation.py test/jit/test_save_load_for_op_version.py test/jit/test_script_profile.py test/jit/test_scriptmod_ann.py test/jit/test_slice.py test/jit/test_string_formatting.py test/jit/test_symbolic_shape_analysis.py test/jit/test_tensor_creation_ops.py test/jit/test_tensor_methods.py test/jit/test_torchbind.py test/jit/test_tracer.py test/jit/test_type_sharing.py test/jit/test_types.py test/jit/test_typing.py test/jit/test_union.py test/jit/test_union_pep604.py test/jit/test_unsupported_ops.py test/jit/test_upgraders.py test/jit/test_warn.py test/jit/test_with.py test/jit/xnnpack/test_xnnpack_delegate.py test/jit_hooks/model.py tools/linter/adapters/ufmt_linter.py,https://github.com/pytorch/pytorch/pull/129764,XuehaiPan,ezyang,,,
6c1f1563e1b,inductor,not user facing,[inductor] fix UndefinedTensorImpl singleton can't export on Windows. (#132326),c10/core/UndefinedTensorImpl.cpp c10/core/UndefinedTensorImpl.h,https://github.com/pytorch/pytorch/pull/132326,xuhancn,desertfire,jgong5,,
a4013e8b726,inductor,not user facing,[inductor] cpp codegen alignas for all OSs. (#132387),torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/132387,xuhancn,desertfire,jgong5,,
d7d61904936,skip,not user facing,[11/N] Use std::nullopt and std::optional (#132396),aten/src/ATen/core/IListRef_test.cpp aten/src/ATen/core/function_schema_inl.h aten/src/ATen/core/op_registration/op_registration.h aten/src/ATen/functorch/ADInterpreters.cpp aten/src/ATen/functorch/BatchRulesConvolution.cpp aten/src/ATen/functorch/BatchRulesHelper.cpp aten/src/ATen/functorch/BatchRulesLoss.cpp aten/src/ATen/functorch/BatchRulesModules.cpp aten/src/ATen/functorch/BatchRulesNorm.cpp aten/src/ATen/functorch/BatchRulesReduceOps.cpp aten/src/ATen/functorch/BatchRulesScatterOps.cpp aten/src/ATen/functorch/BatchRulesViews.cpp aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp aten/src/ATen/functorch/PlumbingHelper.cpp aten/src/ATen/functorch/PlumbingHelper.h aten/src/ATen/native/Bucketization.cpp aten/src/ATen/native/NamedTensor.cpp aten/src/ATen/native/TensorAdvancedIndexing.cpp aten/src/ATen/native/TensorConversions.cpp aten/src/ATen/native/cuda/Bucketization.cu aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/templates/LazyIr.h aten/src/ATen/test/ivalue_test.cpp c10/cuda/CUDAGuard.h c10/util/DynamicCounter.cpp functorch/csrc/dim/arena.h functorch/csrc/dim/dim.cpp test/edge/Evalue.h torch/csrc/functorch/init.cpp,https://github.com/pytorch/pytorch/pull/132396,cyyever,ezyang,,,
0d88dd0f779,skip,not user facing,[TS2E] Remove reference to torch.onnx internals (#132186),torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/132186,justinchuby,angelayi,,,
c99adce9a15,jit,Untopiced,[12/N] Fix clang-tidy warnings in jit (#132209),torch/csrc/jit/tensorexpr/codegen.cpp torch/csrc/jit/tensorexpr/codegen.h torch/csrc/jit/tensorexpr/cuda_codegen.cpp torch/csrc/jit/tensorexpr/cuda_codegen.h torch/csrc/jit/tensorexpr/eval.h torch/csrc/jit/tensorexpr/expr.cpp torch/csrc/jit/tensorexpr/expr.h torch/csrc/jit/tensorexpr/graph_opt.cpp torch/csrc/jit/tensorexpr/half_support.h torch/csrc/jit/tensorexpr/ir_cloner.cpp torch/csrc/jit/tensorexpr/ir_cloner.h torch/csrc/jit/tensorexpr/ir_mutator.cpp torch/csrc/jit/tensorexpr/ir_mutator.h torch/csrc/jit/tensorexpr/ir_printer.cpp torch/csrc/jit/tensorexpr/ir_printer.h torch/csrc/jit/tensorexpr/ir_simplifier.cpp torch/csrc/jit/tensorexpr/ir_simplifier.h torch/csrc/jit/tensorexpr/ir_verifier.cpp torch/csrc/jit/tensorexpr/ir_verifier.h torch/csrc/jit/tensorexpr/llvm_codegen.cpp torch/csrc/jit/tensorexpr/loopnest.cpp torch/csrc/jit/tensorexpr/reduction.cpp torch/csrc/jit/tensorexpr/reduction.h torch/csrc/jit/tensorexpr/registerizer.cpp torch/csrc/jit/tensorexpr/registerizer.h torch/csrc/jit/tensorexpr/stmt.h torch/csrc/jit/tensorexpr/tensor.cpp torch/csrc/jit/tensorexpr/tensor.h torch/csrc/jit/tensorexpr/tensorexpr_init.cpp torch/csrc/jit/tensorexpr/unique_name_manager.cpp torch/csrc/jit/tensorexpr/unique_name_manager.h torch/csrc/jit/tensorexpr/var_substitutor.h,https://github.com/pytorch/pytorch/pull/132209,cyyever,Skylion007,,,
30d7f0b15ab,releng,not user facing,Remove wget call to builder install_cuda.sh (#132410),.ci/docker/linter-cuda/Dockerfile .ci/docker/ubuntu/Dockerfile,https://github.com/pytorch/pytorch/pull/132410,atalman,Skylion007,,,
72d2dba992e,quantization,not user facing,Add None return type to init (#132335),test/dynamo/test_minifier.py test/inductor/test_minifier.py torch/_classes.py torch/_decomp/decompositions_for_rng.py torch/_dynamo/backends/distributed.py torch/_dynamo/code_context.py torch/_dynamo/debug_utils.py torch/_dynamo/eval_frame.py torch/_dynamo/exc.py torch/_dynamo/profiler.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builder.py torch/_export/db/examples/class_method.py torch/_export/db/examples/cond_branch_class_method.py torch/_export/db/examples/model_attr_mutation.py torch/_export/db/examples/scalar_output.py torch/_export/db/examples/specialized_attribute.py torch/_export/passes/lift_constants_pass.py torch/_export/serde/serialize.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/aot_autograd.py torch/_functorch/autograd_function.py torch/_guards.py torch/_higher_order_ops/auto_functionalize.py torch/_higher_order_ops/effects.py torch/_higher_order_ops/flex_attention.py torch/_higher_order_ops/out_dtype.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_higher_order_ops/while_loop.py torch/_higher_order_ops/wrap.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp_wrapper_cuda.py torch/_inductor/codegen/triton.py torch/_inductor/dependencies.py torch/_inductor/exc.py torch/_inductor/fx_passes/misc_patterns.py torch/_inductor/fx_passes/mkldnn_fusion.py torch/_inductor/metrics.py torch/_inductor/utils.py torch/_lazy/closure.py torch/_library/fake_class_registry.py torch/_python_dispatcher.py torch/_subclasses/schema_check_mode.py torch/ao/nn/quantized/modules/functional_modules.py torch/ao/ns/fx/qconfig_multi_mapping.py torch/ao/pruning/_experimental/pruner/README.md torch/ao/quantization/fake_quantize.py torch/ao/quantization/fx/README.md torch/ao/quantization/fx/_model_report/detector.py torch/ao/quantization/fx/custom_config.py torch/ao/quantization/observer.py torch/ao/quantization/qconfig_mapping.py torch/ao/quantization/quantize_fx.py torch/ao/quantization/quantize_pt2e.py torch/ao/quantization/quantizer/embedding_quantizer.py torch/ao/quantization/quantizer/x86_inductor_quantizer.py torch/ao/quantization/quantizer/xnnpack_quantizer.py torch/ao/quantization/utils.py torch/autograd/profiler_util.py torch/backends/xeon/run_cpu.py torch/csrc/jit/backends/backend_debug_handler.h torch/csrc/jit/docs/serialization.md torch/csrc/jit/operator_upgraders/README.md torch/csrc/jit/passes/onnx/function_extraction.h torch/csrc/lazy/test_mnist.py torch/csrc/lazy/tutorial.md torch/cuda/_sanitizer.py torch/distributed/_composable/checkpoint_activation.py torch/distributed/_composable/contract.py torch/distributed/_composable/fsdp/_fsdp_state.py torch/distributed/_shard/sharding_plan/api.py torch/distributed/_tensor/README.md torch/distributed/_tensor/examples/checkpoint_example.py torch/distributed/algorithms/join.py torch/distributed/checkpoint/examples/async_checkpointing_example.py torch/distributed/checkpoint/examples/stateful_example.py torch/distributed/distributed_c10d.py torch/distributed/fsdp/_common_utils.py torch/distributed/nn/api/remote_module.py torch/export/graph_signature.py torch/fx/README.md torch/fx/__init__.py torch/fx/_symbolic_trace.py torch/fx/experimental/migrate_gradual_types/constraint.py torch/fx/passes/graph_drawer.py torch/fx/passes/split_module.py torch/fx/passes/split_utils.py torch/fx/proxy.py torch/fx/subgraph_rewriter.py torch/fx/tensor_type.py torch/jit/__init__.py torch/jit/_async.py torch/jit/_check.py torch/jit/_freeze.py torch/jit/_monkeytype_config.py torch/jit/_recursive.py torch/jit/_script.py torch/jit/_state.py torch/jit/_trace.py torch/multiprocessing/reductions.py torch/nn/modules/container.py torch/nn/modules/lazy.py torch/nn/modules/module.py torch/onnx/_globals.py torch/onnx/_internal/exporter.py torch/onnx/_internal/fx/dynamo_graph_extractor.py torch/onnx/_internal/fx/passes/modularization.py torch/onnx/_internal/fx/patcher.py torch/onnx/_internal/onnxruntime.py torch/onnx/_internal/registration.py torch/onnx/verification.py torch/overrides.py torch/package/_mangling.py torch/profiler/profiler.py torch/testing/_internal/common_fsdp.py torch/testing/_internal/common_methods_invocations.py torch/testing/_internal/common_nn.py torch/testing/_internal/common_pruning.py torch/testing/_internal/common_quantization.py torch/testing/_internal/common_utils.py torch/testing/_internal/data/network1.py torch/testing/_internal/data/network2.py torch/testing/_internal/distributed/distributed_test.py torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py torch/testing/_internal/distributed/rpc/rpc_test.py torch/testing/_internal/jit_metaprogramming_utils.py torch/testing/_internal/jit_utils.py torch/utils/_sympy/value_ranges.py torch/utils/data/_utils/worker.py torch/utils/module_tracker.py,https://github.com/pytorch/pytorch/pull/132335,oulgen,albanD,,,
a6985c09cbc,skip,not user facing,Add None return type to init -- functorch and torchgen (#132351),functorch/dim/reference.py functorch/examples/compilation/fuse_module.py functorch/examples/maml_regression/evjang_transforms_module.py torchgen/_autoheuristic/pad_mm/train_pad_mm.py torchgen/_autoheuristic/train.py,https://github.com/pytorch/pytorch/pull/132351,oulgen,jamesjwu,,,
221350e3a49,quantization,not user facing,Add None return type to init -- tests (#132352),test/ao/sparsity/test_activation_sparsifier.py test/ao/sparsity/test_data_sparsifier.py test/ao/sparsity/test_structured_sparsifier.py test/bottleneck_test/test_cuda.py test/cpp/aoti_inference/compile_model.py test/cpp/aoti_inference/test.py test/cpp/jit/test_lite_interpreter.cpp test/cpp/jit/tests_setup.py test/cpp/lite_interpreter_runtime/test_lite_interpreter_runtime.cpp test/create_dummy_torchscript_model.py test/custom_operator/model.py test/export/opinfo_schema.py test/export/test_converter.py test/export/test_experimental.py test/export/test_export.py test/export/test_lift_unlift.py test/export/test_pass_infra.py test/export/test_passes.py test/export/test_serialize.py test/export/test_sparse.py test/export/test_tools.py test/export/test_torchbind.py test/export/test_unflatten.py test/export/test_verifier.py test/functorch/test_aotdispatch.py test/functorch/test_control_flow.py test/functorch/test_eager_transforms.py test/functorch/test_minifier.py test/fx/test_dce_pass.py test/fx/test_fx_const_fold.py test/fx/test_fx_split.py test/fx/test_gradual_type.py test/fx/test_matcher_utils.py test/fx/test_shape_inference.py test/fx/test_source_matcher_utils.py test/fx/test_subgraph_rewriter.py test/fx/test_z3_gradual_types.py test/higher_order_ops/test_with_effects.py test/lazy/test_extract_compiled_graph.py test/lazy/test_functionalization.py test/mkldnn_verbose.py test/mobile/lightweight_dispatch/test_codegen_unboxing.cpp test/mobile/lightweight_dispatch/tests_setup.py test/mobile/model_test/nn_ops.py test/mobile/model_test/quantization_ops.py test/mobile/test_lite_script_module.py test/mobile/test_lite_script_type.py test/mobile/test_quantize_fx_lite_script_module.py test/nn/test_lazy_modules.py test/nn/test_load_state_dict.py test/nn/test_module_hooks.py test/nn/test_parametrization.py test/onnx/dynamo/test_dynamo_with_onnxruntime_backend.py test/onnx/dynamo/test_exporter_api.py test/onnx/model_defs/mnist.py test/onnx/model_defs/op_test.py test/onnx/test_custom_ops.py test/onnx/test_export_modes.py test/onnx/test_fx_passes.py test/onnx/test_fx_to_onnx.py test/onnx/test_fx_to_onnx_decomp_skip.py test/onnx/test_fx_to_onnx_with_onnxruntime.py test/onnx/test_models_onnxruntime.py test/onnx/test_onnx_opset.py test/onnx/test_onnxscript_no_runtime.py test/onnx/test_operators.py test/onnx/test_pytorch_onnx_no_runtime.py test/onnx/test_pytorch_onnx_onnxruntime.py test/onnx/test_pytorch_onnx_onnxruntime_cuda.py test/onnx/test_utility_funs.py test/onnx/test_verification.py test/onnx/torch_export/test_torch_export_with_onnxruntime.py test/optim/test_lrscheduler.py test/package/package_a/fake_interface.py test/package/package_a/fake_script_class.py test/package/package_c/test_module.py test/package/test_dependency_api.py test/package/test_package_fx.py test/package/test_package_script.py test/profiler/test_memory_profiler.py test/profiler/test_profiler.py test/profiler/test_profiler_tree.py test/profiler/test_record_function.py test/profiler/test_torch_tidy.py test/quantization/bc/test_backward_compatibility.py test/quantization/core/experimental/test_adaround_eager.py test/quantization/core/test_quantized_tensor.py test/quantization/core/test_utils.py test/quantization/core/test_workflow_module.py test/quantization/core/test_workflow_ops.py test/quantization/eager/test_bias_correction_eager.py test/quantization/eager/test_equalize_eager.py test/quantization/eager/test_numeric_suite_eager.py test/quantization/eager/test_quantize_eager_ptq.py test/quantization/eager/test_quantize_eager_qat.py test/quantization/fx/test_equalize_fx.py test/quantization/fx/test_model_report_fx.py test/quantization/fx/test_numeric_suite_fx.py test/quantization/fx/test_quantize_fx.py test/quantization/fx/test_subgraph_rewriter.py test/quantization/jit/test_deprecated_jit_quant.py test/quantization/jit/test_ondevice_quantization.py test/quantization/jit/test_quantize_jit.py test/quantization/pt2e/test_duplicate_dq.py test/quantization/pt2e/test_graph_utils.py test/quantization/pt2e/test_metadata_porting.py test/quantization/pt2e/test_quantize_pt2e.py test/quantization/pt2e/test_quantize_pt2e_qat.py test/quantization/pt2e/test_representation.py test/quantization/pt2e/test_x86inductor_quantizer.py test/quantization/pt2e/test_xnnpack_quantizer.py test/test_autograd.py test/test_compile_benchmark_util.py test/test_cpp_extensions_jit.py test/test_cuda.py test/test_custom_ops.py test/test_dataloader.py test/test_datapipe.py test/test_decomp.py test/test_deploy.py test/test_expanded_weights.py test/test_fake_tensor.py test/test_flop_counter.py test/test_functional_optim.py test/test_fx.py test/test_fx_experimental.py test/test_fx_passes.py test/test_jit.py test/test_jit_autocast.py test/test_jit_fuser.py test/test_jit_fuser_te.py test/test_jit_llga_fuser.py test/test_metal.py test/test_mobile_optimizer.py test/test_model_dump.py test/test_module_tracker.py test/test_mps.py test/test_nestedtensor.py test/test_nn.py test/test_nnapi.py test/test_proxy_tensor.py test/test_python_dispatch.py test/test_serialization.py test/test_sparse.py test/test_sparse_semi_structured.py test/test_spectral_ops.py test/test_stateless.py test/test_static_runtime.py test/test_subclass.py test/test_tensorboard.py test/test_tensorexpr.py test/test_testing.py test/test_utils.py test/test_vulkan.py test/test_weak.py test/test_xnnpack_integration.py test/torch_np/numpy_tests/core/test_multiarray.py,https://github.com/pytorch/pytorch/pull/132352,oulgen,ezyang,,,
920f0426ae5,distributed,not user facing,Add None return type to init -- tests rest (#132376),test/distributed/_composable/fsdp/test_fully_shard_comm.py test/distributed/_composable/fsdp/test_fully_shard_training.py test/distributed/_composable/test_checkpoint.py test/distributed/_composable/test_contract.py test/distributed/_composable/test_replicate.py test/distributed/_shard/test_sharder.py test/distributed/_tensor/debug/test_op_coverage.py test/distributed/_tensor/experimental/test_tp_transform.py test/distributed/_tensor/test_dtensor_compile.py test/distributed/_tensor/test_xla_integration.py test/distributed/_tools/test_mod_tracker.py test/distributed/algorithms/ddp_comm_hooks/test_ddp_hooks.py test/distributed/checkpoint/e2e/test_e2e_save_and_load.py test/distributed/checkpoint/e2e/test_fsdp_ep.py test/distributed/checkpoint/test_format_utils.py test/distributed/checkpoint/test_fsdp_optim_state.py test/distributed/checkpoint/test_fsspec.py test/distributed/checkpoint/test_hsdp_checkpoint.py test/distributed/checkpoint/test_save_load_api.py test/distributed/elastic/metrics/api_test.py test/distributed/elastic/rendezvous/dynamic_rendezvous_test.py test/distributed/elastic/utils/util_test.py test/distributed/fsdp/test_checkpoint_wrapper.py test/distributed/fsdp/test_fsdp_dtensor_state_dict.py test/distributed/fsdp/test_fsdp_fine_tune.py test/distributed/fsdp/test_fsdp_hybrid_shard.py test/distributed/fsdp/test_fsdp_input.py test/distributed/fsdp/test_fsdp_misc.py test/distributed/fsdp/test_fsdp_mixed_precision.py test/distributed/fsdp/test_fsdp_multiple_wrapping.py test/distributed/fsdp/test_fsdp_optim_state.py test/distributed/fsdp/test_fsdp_overlap.py test/distributed/fsdp/test_fsdp_state_dict.py test/distributed/fsdp/test_fsdp_tp_integration.py test/distributed/fsdp/test_fsdp_unshard_params.py test/distributed/fsdp/test_fsdp_use_orig_params.py test/distributed/fsdp/test_hsdp_dtensor_state_dict.py test/distributed/fsdp/test_wrap.py test/distributed/optim/test_apply_optimizer_in_backward.py test/distributed/optim/test_named_optimizer.py test/distributed/optim/test_zero_redundancy_optimizer.py test/distributed/pipelining/test_pipe.py test/distributed/pipelining/test_stage.py test/distributed/tensor/parallel/test_fsdp_2d_parallel.py test/distributed/tensor/parallel/test_parallelize_api.py test/distributed/tensor/parallel/test_tp_examples.py test/distributed/tensor/parallel/test_tp_style.py test/distributed/test_c10d_common.py test/distributed/test_c10d_gloo.py test/distributed/test_c10d_nccl.py test/distributed/test_c10d_ucc.py test/distributed/test_data_parallel.py test/distributed/test_dynamo_distributed.py test/distributed/test_store.py test/dynamo/test_activation_checkpointing.py test/dynamo/test_aot_autograd.py test/dynamo/test_aot_autograd_cache.py test/dynamo/test_autograd_function.py test/dynamo/test_backends.py test/dynamo/test_backward_higher_order_ops.py test/dynamo/test_compile.py test/dynamo/test_decorators.py test/dynamo/test_exceptions.py test/dynamo/test_export.py test/dynamo/test_export_mutations.py test/dynamo/test_functions.py test/dynamo/test_guard_manager.py test/dynamo/test_higher_order_ops.py test/dynamo/test_hooks.py test/dynamo/test_logging.py test/dynamo/test_misc.py test/dynamo/test_model_output.py test/dynamo/test_modules.py test/dynamo/test_recompiles.py test/dynamo/test_repros.py test/dynamo/test_sources.py test/dynamo/test_structured_trace.py test/dynamo/test_subclasses.py test/dynamo/test_verify_correctness.py test/inductor/test_aot_inductor.py test/inductor/test_aot_inductor_package.py test/inductor/test_codecache.py test/inductor/test_compiled_autograd.py test/inductor/test_cpu_repro.py test/inductor/test_cuda_repro.py test/inductor/test_cudagraph_trees.py test/inductor/test_custom_post_grad_passes.py test/inductor/test_debug_trace.py test/inductor/test_decompose_mem_bound_mm.py test/inductor/test_group_batch_fusion.py test/inductor/test_inductor_freezing.py test/inductor/test_layout_optim.py test/inductor/test_max_autotune.py test/inductor/test_mkldnn_pattern_matcher.py test/inductor/test_multi_kernel.py test/inductor/test_pad_mm.py test/inductor/test_padding.py test/inductor/test_pattern_matcher.py test/inductor/test_smoke.py test/inductor/test_standalone_compile.py test/inductor/test_torchbind.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_opinfo.py test/jit/test_async.py test/jit/test_attr.py test/jit/test_backends.py test/jit/test_builtins.py test/jit/test_class_type.py test/jit/test_complex.py test/jit/test_data_parallel.py test/jit/test_dce.py test/jit/test_exception.py test/jit/test_freezing.py test/jit/test_generator.py test/jit/test_list_dict.py test/jit/test_misc.py test/jit/test_models.py test/jit/test_module_apis.py test/jit/test_module_containers.py test/jit/test_module_interface.py test/jit/test_modules.py test/jit/test_pdt.py test/jit/test_peephole.py test/jit/test_recursive_script.py test/jit/test_save_load.py test/jit/test_script_profile.py test/jit/test_scriptmod_ann.py test/jit/test_slice.py test/jit/test_sparse.py test/jit/test_symbolic_shape_analysis.py test/jit/test_torchbind.py test/jit/test_tracer.py test/jit/test_type_sharing.py test/jit/test_typing.py test/jit/test_with.py test/jit/xnnpack/test_xnnpack_delegate.py,https://github.com/pytorch/pytorch/pull/132376,oulgen,jamesjwu,,,
35fcd59fd81,inductor,not user facing,[inductor] make restrict_keyword cross OSs. (#132394),torch/_inductor/codegen/cpp_micro_gemm.py,https://github.com/pytorch/pytorch/pull/132394,xuhancn,desertfire,,,
ee09d066d35,dynamo,not user facing,[dynamo] Add line number to _warn_capture_scalar_outputs() (#132333),torch/_dynamo/variables/tensor.py,https://github.com/pytorch/pytorch/pull/132333,yiming0416,anijain2305,,,
c59f3fff52d,distributed,Untopiced,[PP] Forward only schedule (#132177),test/distributed/pipelining/test_schedule_multiproc.py torch/distributed/pipelining/__init__.py torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/132177,H-Huang,lessw2020,,,
30293319a8f,quantization,not user facing,[BE][Easy][19/19] enforce style for empty lines in import segments in `torch/[o-z]*/` (#129771),tools/linter/adapters/ufmt_linter.py torch/onnx/__init__.py torch/onnx/_deprecation.py torch/onnx/_internal/diagnostics/__init__.py torch/onnx/_internal/diagnostics/_diagnostic.py torch/onnx/_internal/diagnostics/_rules.py torch/onnx/_internal/diagnostics/infra/__init__.py torch/onnx/_internal/diagnostics/infra/context.py torch/onnx/_internal/diagnostics/infra/formatter.py torch/onnx/_internal/diagnostics/infra/sarif/__init__.py torch/onnx/_internal/diagnostics/infra/sarif/version.py torch/onnx/_internal/diagnostics/infra/utils.py torch/onnx/_internal/exporter.py torch/onnx/_internal/fx/_pass.py torch/onnx/_internal/fx/analysis/__init__.py torch/onnx/_internal/fx/decomposition_skip.py torch/onnx/_internal/fx/decomposition_table.py torch/onnx/_internal/fx/diagnostics.py torch/onnx/_internal/fx/fx_onnx_interpreter.py torch/onnx/_internal/fx/fx_symbolic_graph_extractor.py torch/onnx/_internal/fx/onnxfunction_dispatcher.py torch/onnx/_internal/fx/op_validation.py torch/onnx/_internal/fx/passes/__init__.py torch/onnx/_internal/fx/passes/_utils.py torch/onnx/_internal/fx/passes/decomp.py torch/onnx/_internal/fx/passes/functionalization.py torch/onnx/_internal/fx/passes/modularization.py torch/onnx/_internal/fx/passes/readability.py torch/onnx/_internal/fx/passes/type_promotion.py torch/onnx/_internal/fx/passes/virtualization.py torch/onnx/_internal/fx/patcher.py torch/onnx/_internal/fx/torch_export_graph_extractor.py torch/onnx/_internal/fx/type_utils.py torch/onnx/_internal/io_adapter.py torch/onnx/_internal/jit_utils.py torch/onnx/_internal/onnxruntime.py torch/onnx/_type_utils.py torch/onnx/errors.py torch/onnx/symbolic_helper.py torch/onnx/symbolic_opset10.py torch/onnx/symbolic_opset11.py torch/onnx/symbolic_opset14.py torch/onnx/symbolic_opset15.py torch/onnx/symbolic_opset16.py torch/onnx/symbolic_opset17.py torch/onnx/symbolic_opset18.py torch/onnx/symbolic_opset19.py torch/onnx/symbolic_opset20.py torch/onnx/symbolic_opset8.py torch/onnx/symbolic_opset9.py torch/onnx/verification.py torch/optim/__init__.py torch/optim/_adafactor.py torch/optim/adadelta.py torch/optim/adagrad.py torch/optim/adam.py torch/optim/adamax.py torch/optim/adamw.py torch/optim/asgd.py torch/optim/lbfgs.py torch/optim/lr_scheduler.py torch/optim/nadam.py torch/optim/optimizer.py torch/optim/radam.py torch/optim/rmsprop.py torch/optim/rprop.py torch/optim/sgd.py torch/optim/sparse_adam.py torch/optim/swa_utils.py torch/package/_directory_reader.py torch/package/_importlib.py torch/package/_mangling.py torch/package/analyze/find_first_use_of_broken_modules.py torch/package/analyze/trace_dependencies.py torch/package/file_structure_representation.py torch/package/glob_group.py torch/package/importer.py torch/package/package_exporter.py torch/package/package_importer.py torch/profiler/__init__.py torch/profiler/_memory_profiler.py torch/profiler/_utils.py torch/profiler/itt.py torch/quantization/__init__.py torch/quantization/fx/pattern_utils.py torch/quantization/fx/quantization_patterns.py torch/sparse/__init__.py torch/sparse/_semi_structured_ops.py torch/sparse/_triton_ops.py torch/sparse/semi_structured.py torch/testing/__init__.py torch/testing/_creation.py torch/testing/_internal/distributed/common_state_dict.py torch/testing/_internal/dynamo_test_failures.py torch/testing/_internal/opinfo/core.py torch/testing/_internal/opinfo/definitions/__init__.py torch/testing/_internal/opinfo/definitions/fft.py torch/testing/_internal/opinfo/definitions/linalg.py torch/testing/_internal/opinfo/definitions/signal.py torch/testing/_internal/opinfo/refs.py torch/testing/_internal/optests/generate_tests.py torch/testing/_internal/triton_utils.py torch/testing/_utils.py torch/utils/_backport_slots.py torch/utils/_config_module.py torch/utils/_content_store.py torch/utils/_cxx_pytree.py torch/utils/_ordered_set.py torch/utils/_strobelight/examples/cli_function_profiler_example.py torch/utils/_sympy/functions.py torch/utils/_sympy/interp.py torch/utils/_sympy/reference.py torch/utils/_sympy/singleton_int.py torch/utils/_sympy/solve.py torch/utils/_sympy/value_ranges.py torch/utils/_typing_utils.py torch/utils/data/datapipes/_typing.py torch/utils/module_tracker.py torch/xpu/__init__.py torch/xpu/random.py,https://github.com/pytorch/pytorch/pull/129771,XuehaiPan,janeyx99,justinchuby,,
69cbf055293,skip,not user facing,Fix recent build error on ppc64le  (#129736),aten/src/ATen/cpu/vec/vec256/vsx/vec256_complex_double_vsx.h aten/src/ATen/cpu/vec/vec256/vsx/vec256_complex_float_vsx.h aten/src/ATen/cpu/vec/vec256/vsx/vec256_double_vsx.h aten/src/ATen/cpu/vec/vec256/vsx/vec256_float_vsx.h aten/src/ATen/cpu/vec/vec256/vsx/vec256_int16_vsx.h aten/src/ATen/cpu/vec/vec256/vsx/vec256_int32_vsx.h aten/src/ATen/cpu/vec/vec256/vsx/vec256_int64_vsx.h aten/src/ATen/cpu/vec/vec256/vsx/vec256_qint32_vsx.h aten/src/ATen/cpu/vec/vec256/vsx/vec256_qint8_vsx.h aten/src/ATen/cpu/vec/vec256/vsx/vec256_quint8_vsx.h,https://github.com/pytorch/pytorch/pull/129736,pratiklp00,albanD,malfet,,
7b816d7d6d5,skip,not user facing,[dynamo] Treat attr of unspecialized buiitin nn modules as static (#132318),torch/_dynamo/utils.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/132318,anijain2305,ezyang,mlazos,yanboliang,
ff4ca0d02a1,skip,not user facing,[Easy] Fix argument name collision in `HigherOrderOperator` dispatched functions (#132377),torch/_higher_order_ops/auto_functionalize.py torch/_ops.py,https://github.com/pytorch/pytorch/pull/132377,XuehaiPan,zou3519,,,
010fc7858ad,Uncategorized,Untopiced,[export] Fix serialization of OpOverload w/ SymInt outputs (#132126),test/export/test_serialize.py torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/132126,angelayi,ydwu4,,,
f467d553299,skip,not user facing,Disable remote cache on test_aot_autograd_cache (#132409),test/dynamo/test_aot_autograd_cache.py,https://github.com/pytorch/pytorch/pull/132409,jamesjwu,masnesral,,,
97eba8e1745,inductor,not user facing,[AOTI] Fix a typo in ExternKernel.codegen_const_args (#132191),torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/132191,desertfire,chenyang78,,,
ce613001415,python_frontend,improvements,Enable oneDNN for tanh based GELU on aarch64 (#130925),aten/src/ATen/native/Activation.cpp,https://github.com/pytorch/pytorch/pull/130925,Ryo-not-rio,malfet,,,
0eea2b39473,inductor,not user facing,Cast inputs to low precision kernels in emulate low precision mode (#132345),torch/_inductor/index_propagation.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/132345,eellison,zou3519,,,
fbf3bc0a602,quantization,bc breaking,Always use high precision for SDPA math backend (#128922),aten/src/ATen/native/transformers/attention.cpp test/test_decomp.py test/test_transformers.py torch/nn/functional.py,https://github.com/pytorch/pytorch/pull/128922,jianyuh,drisspg,xw285cornell,,
93979e70631,dynamo,Untopiced,Skip frame if torch dispatch mode enabled (#131828),test/dynamo/test_modes.py test/dynamo/test_repros.py test/dynamo_expected_failures/TestAutograd.test_checkpointing_without_reentrant_with_context_fn test/dynamo_expected_failures/TestCustomOpTestingCPU.test_missing_functionalization_cpu test/dynamo_expected_failures/TestGenerateOpcheckTests.test_opcheck_bad_op test/dynamo_expected_failures/TestNestedCheckpoint.test_nested_checkpoint_set_early_stop_no_recompution_needed test/test_python_dispatch.py torch/_dynamo/convert_frame.py torch/_subclasses/fake_tensor.py torch/_subclasses/fake_utils.py torch/_subclasses/functional_tensor.py torch/fx/experimental/proxy_tensor.py torch/utils/_python_dispatch.py torch/utils/flop_counter.py,https://github.com/pytorch/pytorch/pull/131828,mlazos,anijain2305,bdhirsh,,
40c8f730994,skip,Untopiced,"Revert ""Fix inlining module-scoped store global (#132224)""",test/dynamo/mock_store_global_crossfile_inline.py test/dynamo/test_global.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/builder.py,,,,,,
9853c048eb5,skip,Untopiced,Ban decorator usage of dynamo_timed (#132328),.lintrunner.toml test/dynamo/test_profiler.py torch/_dynamo/convert_frame.py torch/_dynamo/output_graph.py torch/_dynamo/utils.py torch/_functorch/aot_autograd.py torch/_inductor/codecache.py torch/_inductor/codegen/wrapper.py torch/_inductor/compile_fx.py torch/_inductor/freezing.py torch/_inductor/graph.py torch/_inductor/runtime/runtime_utils.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/scheduler.py torch/fx/experimental/validator.py,https://github.com/pytorch/pytorch/pull/132328,ezyang,albanD,,,
596f568592a,distributed,not user facing,[dtensor][debug] adding js script to pytorch github so that i can host the browser visualizer on pytorch (#132185),torch/distributed/_tensor/debug/comm_mode_broswer_visual.js,https://github.com/pytorch/pytorch/pull/132185,sinhaanshul,XilunWu,,,
12f61e65ebe,Uncategorized,Untopiced,[mtia][sdpa] MTIA SDPA dispatch via _fused_sdp_choice_stub (#132008),aten/src/ATen/native/DispatchStub.cpp aten/src/ATen/native/DispatchStub.h,https://github.com/pytorch/pytorch/pull/132008,nautsimon,mortzur,,,
71e22e0959e,skip,not user facing,Add basic mypy annotations to dynamo (#132415),torch/_dynamo/backends/common.py torch/_dynamo/backends/debugging.py torch/_dynamo/backends/distributed.py torch/_dynamo/codegen.py torch/_dynamo/comptime.py torch/_dynamo/device_interface.py torch/_dynamo/eval_frame.py torch/_dynamo/exc.py torch/_dynamo/external_utils.py torch/_dynamo/profiler.py torch/_dynamo/repro/after_aot.py torch/_dynamo/repro/after_dynamo.py torch/_dynamo/symbolic_convert.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/distributed.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/higher_order_ops.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lazy.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/optimizer.py torch/_dynamo/variables/script_object.py torch/_dynamo/variables/sdpa.py torch/_dynamo/variables/tensor.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/torch_function.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/132415,oulgen,XuehaiPan,jamesjwu,,
78927d37f60,skip,not user facing,Add basic mypy annotations to inductor (#132416),torch/_inductor/autoheuristic/autoheuristic_utils.py torch/_inductor/autotune_process.py torch/_inductor/bounds.py torch/_inductor/codecache.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/codegen/cpp_template.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/codegen/cuda/cuda_cpp_scheduling.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/cuda/cuda_template.py torch/_inductor/codegen/cuda/cutlass_lib_extensions/gemm_operation_extensions.py torch/_inductor/codegen/cuda/gemm_template.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/rocm/ck_universal_gemm_template.py torch/_inductor/codegen/rocm/rocm_benchmark_request.py torch/_inductor/codegen/rocm/rocm_cpp_scheduling.py torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/codegen/rocm/rocm_template.py torch/_inductor/codegen/rocm/rocm_template_buffer.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_split_scan.py torch/_inductor/comms.py torch/_inductor/compile_worker/subproc_pool.py torch/_inductor/constant_folding.py torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py torch/_inductor/dependencies.py torch/_inductor/exc.py torch/_inductor/freezing.py torch/_inductor/fx_passes/group_batch_fusion.py torch/_inductor/fx_passes/joint_graph.py torch/_inductor/fx_passes/mkldnn_fusion.py torch/_inductor/fx_passes/pre_grad.py torch/_inductor/fx_passes/quantization.py torch/_inductor/fx_passes/split_cat.py torch/_inductor/fx_utils.py torch/_inductor/graph.py torch/_inductor/index_propagation.py torch/_inductor/mkldnn_ir.py torch/_inductor/package/package.py torch/_inductor/remote_cache.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py torch/_inductor/sizevars.py torch/_inductor/subgraph_lowering.py,https://github.com/pytorch/pytorch/pull/132416,oulgen,XuehaiPan,jamesjwu,,
c8958f8f842,skip,Untopiced,"Revert ""Ban decorator usage of dynamo_timed (#132328)""",.lintrunner.toml test/dynamo/test_profiler.py torch/_dynamo/convert_frame.py torch/_dynamo/output_graph.py torch/_dynamo/utils.py torch/_functorch/aot_autograd.py torch/_inductor/codecache.py torch/_inductor/codegen/wrapper.py torch/_inductor/compile_fx.py torch/_inductor/freezing.py torch/_inductor/graph.py torch/_inductor/runtime/runtime_utils.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/scheduler.py torch/fx/experimental/validator.py,,,,,,
997f64af38c,skip,not user facing,fastpath FunctionalTensor sizes() (#132084),aten/src/ATen/native/TensorShape.cpp torch/_subclasses/functional_tensor.py,https://github.com/pytorch/pytorch/pull/132084,bdhirsh,ezyang,,,
92bebb46fa9,xpu,not user facing,Support XPU ABI=0 build (#130110),README.md c10/xpu/CMakeLists.txt cmake/Dependencies.cmake cmake/Modules/FindMKLDNN.cmake cmake/Modules/FindSYCLToolkit.cmake,https://github.com/pytorch/pytorch/pull/130110,guangyey,EikanWang,albanD,gujinghui,
a356a03f4a6,skip,not user facing,Fix DEBUG=1 asserts for mvlgamma backward with NJT (#132422),test/test_nestedtensor.py torch/csrc/autograd/FunctionsManual.cpp,https://github.com/pytorch/pytorch/pull/132422,jbschlosser,albanD,,,
2f54c385945,skip,not user facing,[AOTI] Fix bfloat16 in CPU (#132150),test/inductor/test_aot_inductor.py torch/_inductor/codegen/aoti_runtime/implementation.cpp torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cuda.py,https://github.com/pytorch/pytorch/pull/132150,yushangdi,chenyang78,desertfire,,
35400f750f7,skip,not user facing,[torchbind] don't warning for certain skippable methods. (#132306),torch/_library/fake_class_registry.py,https://github.com/pytorch/pytorch/pull/132306,ydwu4,angelayi,,,
1362d51e7d4,inductor,not user facing,[AOTI] Fix number type for AOTI (#132180),test/inductor/test_cuda_cpp_wrapper.py torch/_inductor/codegen/cpp_wrapper_cpu.py,https://github.com/pytorch/pytorch/pull/132180,yushangdi,desertfire,,,
87ddf70fc63,skip,not user facing,Set weights_only=False in export `deserialize_torch_artifact` (#132348),torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/132348,mikaylagawarecki,angelayi,,,
0c3ac428a29,skip,not user facing,[BE][typing] fix types in common pruning (#132309),torch/testing/_internal/common_pruning.py,https://github.com/pytorch/pytorch/pull/132309,henrylhtsang,ColinPeppler,,,
3855ac5a5d5,skip,Untopiced,"Revert ""[export] Add print_readable to unflattener (#128617)""",test/dynamo/test_autograd_function.py test/dynamo/test_higher_order_ops.py test/dynamo/test_subclasses.py test/inductor/test_flex_attention.py torch/export/unflatten.py torch/fx/graph_module.py,,,,,,
0016be8051d,releng,not user facing,[Docker] Replace epel release rpm by yum install (#132449),.ci/docker/conda/Dockerfile .ci/docker/manywheel/Dockerfile .ci/docker/manywheel/Dockerfile_2014 .ci/docker/manywheel/Dockerfile_2_28,https://github.com/pytorch/pytorch/pull/132449,atalman,kit1980,malfet,seemethere,
625af2d27c5,dynamo,not user facing,[dynamo] fix add_push_null callsites with CALL_FUNCTION_EX (#132329),torch/_dynamo/codegen.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/132329,williamwen42,anijain2305,,,
ee1ef066fda,fx,Untopiced,add src map to data-dependent errors (#132393),test/export/test_export.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/132393,avikchaudhuri,BoyuanFeng,,,
56334c854c7,skip,not user facing,"[2/N] Fix clang-tidy warnings in aten/src/ATen/native/*.{cpp,h}  (#131834)",aten/src/ATen/native/AdaptiveAveragePooling3d.cpp aten/src/ATen/native/AveragePool3d.cpp aten/src/ATen/native/BatchLinearAlgebra.cpp aten/src/ATen/native/BatchLinearAlgebraKernel.cpp aten/src/ATen/native/ComparisonUtils.cpp aten/src/ATen/native/ComplexHelper.h aten/src/ATen/native/Convolution.cpp aten/src/ATen/native/Pool.h,https://github.com/pytorch/pytorch/pull/131834,cyyever,ezyang,,,
64235c6a71e,skip,not user facing,Skip test_fp8 in test_aot_inductor to temporarily (#132453),test/inductor/test_aot_inductor.py,https://github.com/pytorch/pytorch/pull/132453,yangsiyu007,henrylhtsang,,,
50ed6ce2777,dynamo,not user facing,Support built-in id function for TensorVariable on parameters (#130100),test/dynamo/test_misc.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/tensor.py,https://github.com/pytorch/pytorch/pull/130100,jerrychenhf,anijain2305,,,
ef426d5183d,distributed,build,[nccl] Wrap nccl code update with version check (#130419),torch/csrc/cuda/nccl.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp,https://github.com/pytorch/pytorch/pull/130419,oraluben,eqy,malfet,,
bc510916fad,distributed,not user facing,Only make wait_tensor as a side_effect op (#132341),test/distributed/test_functional_api.py torch/distributed/_functional_collectives.py,https://github.com/pytorch/pytorch/pull/132341,fegin,yf225,,,
3a355c1891f,python_frontend,Untopiced,Correct sample creation of torch.histogram in UT op_db to align PyTorch defined operator semantics (#131630),torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/131630,majing921201,EikanWang,albanD,,
cd5452aacee,python_frontend,bug fixes,[CUDA] `is_bf16_supported()` should not crash if there are no GPUs (#132313),test/test_torch.py torch/cuda/__init__.py,https://github.com/pytorch/pytorch/pull/132313,malfet,eqy,syed-ahmed,,
6c4ce4331ca,dynamo,not user facing,[dynamo][exception] Raise Observed KeyError exception for dict __getitem__ (#132425),test/dynamo/test_exceptions.py torch/_dynamo/exc.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lists.py,https://github.com/pytorch/pytorch/pull/132425,anijain2305,Skylion007,yanboliang,,
12501718664,skip,not user facing,Use fresh inductor cache on unit tests (#132432),test/dynamo/test_aot_autograd_cache.py,https://github.com/pytorch/pytorch/pull/132432,jamesjwu,masnesral,,,
07fe1dd58f8,jit,not user facing,[13/N] Fix clang-tidy warnings in jit (#132411),torch/csrc/jit/tensorexpr/analysis.h torch/csrc/jit/tensorexpr/block_codegen.cpp torch/csrc/jit/tensorexpr/block_codegen.h torch/csrc/jit/tensorexpr/codegen.h torch/csrc/jit/tensorexpr/cpp_codegen.cpp torch/csrc/jit/tensorexpr/cpp_codegen.h torch/csrc/jit/tensorexpr/cuda_codegen.cpp torch/csrc/jit/tensorexpr/cuda_codegen.h torch/csrc/jit/tensorexpr/eval.cpp torch/csrc/jit/tensorexpr/eval.h torch/csrc/jit/tensorexpr/exceptions.h torch/csrc/jit/tensorexpr/expr.h torch/csrc/jit/tensorexpr/external_functions.h torch/csrc/jit/tensorexpr/external_functions_core.cpp torch/csrc/jit/tensorexpr/external_functions_core.h torch/csrc/jit/tensorexpr/external_functions_registry.h torch/csrc/jit/tensorexpr/fwd_decls.h torch/csrc/jit/tensorexpr/hash_provider.h torch/csrc/jit/tensorexpr/ir.cpp torch/csrc/jit/tensorexpr/ir.h torch/csrc/jit/tensorexpr/ir_cloner.cpp torch/csrc/jit/tensorexpr/ir_mutator.cpp torch/csrc/jit/tensorexpr/ir_simplifier.cpp torch/csrc/jit/tensorexpr/loopnest.cpp torch/csrc/jit/tensorexpr/loopnest.h torch/csrc/jit/tensorexpr/loopnest_randomization.cpp torch/csrc/jit/tensorexpr/loopnest_randomization.h torch/csrc/jit/tensorexpr/lowerings.cpp torch/csrc/jit/tensorexpr/lowerings.h torch/csrc/jit/tensorexpr/operators/quantization.cpp torch/csrc/jit/tensorexpr/stmt.h torch/csrc/jit/tensorexpr/tensor.h torch/csrc/jit/tensorexpr/tensorexpr_init.h torch/csrc/jit/tensorexpr/types.h,https://github.com/pytorch/pytorch/pull/132411,cyyever,Skylion007,,,
2b86a7fcc78,skip,not user facing,fix printing of scores and mods names (#132424),benchmarks/transformer/score_mod.py,https://github.com/pytorch/pytorch/pull/132424,drisspg,Skylion007,,,
5ea0f511878,dynamo,not user facing,[Dynamo] Support abc.MutableMapping.get (#132363),test/dynamo/test_misc.py torch/_dynamo/polyfill.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/132363,yanboliang,anijain2305,mlazos,,
35d14d22a05,quantization,not user facing,Fix some issues detected by static analysis tools (#131989),aten/src/ATen/functorch/BatchRulesBinaryOps.cpp aten/src/ATen/native/EmbeddingBag.cpp aten/src/ATen/native/quantized/FakeQuantPerChannelAffine.cpp aten/src/ATen/native/quantized/cpu/fused_obs_fake_quant.cpp c10/cuda/CUDAMallocAsyncAllocator.cpp torch/csrc/autograd/TraceTypeManual.cpp torch/csrc/lazy/ts_backend/ts_native_functions.cpp torch/csrc/utils/tensor_new.cpp,https://github.com/pytorch/pytorch/pull/131989,cyyever,ezyang,,,
7d8b95e8fb7,skip,not user facing,[easy] more debug in partitioner assert (#132456),torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/132456,davidberard98,Chillee,,,
e5560d10f4e,skip,not user facing,[CUDA][SDPA] Fix expect export on sm90+ (#132194),test/export/test_export.py,https://github.com/pytorch/pytorch/pull/132194,eqy,drisspg,,,
f936e685066,releng,not user facing,[CI] Update CPU inductor smoke test model list and target (#132221),.ci/pytorch/test.sh benchmarks/dynamo/expected_ci_speedup_inductor_torchbench_cpu.csv,https://github.com/pytorch/pytorch/pull/132221,zxd1997066,desertfire,,,
2ee98953046,optim,Untopiced,Support optimizer capturable on hpu and xpu (#132119),torch/optim/optimizer.py,https://github.com/pytorch/pytorch/pull/132119,xinyu-intel,janeyx99,jgong5,,
babb249a89b,skip,not user facing,[dynamo] Track params/buffers and mark them as static (#132334),test/inductor/test_cpu_select_algorithm.py torch/_dynamo/guards.py torch/_dynamo/source.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/132334,anijain2305,ezyang,mlazos,,
a4ea7768814,sparse_frontend,new features,Add pinned memory support to sparse COO/CSR/CSC/BSR/BSC tensors (#129645),aten/src/ATen/SparseCsrTensorUtils.h aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/sparse/SparseCsrTensor.cpp aten/src/ATen/native/sparse/SparseTensor.cpp aten/src/ATen/native/sparse/SparseUnaryOps.cpp test/dynamo_expected_failures/TestNamedTensor.test_info_smoke test/dynamo_expected_failures/TestTorch.test_pin_memory test/test_sparse.py torch/_subclasses/fake_impls.py torch/_torch_docs.py torch/csrc/autograd/python_torch_functions_manual.cpp torch/csrc/utils/tensor_new.cpp torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/129645,pearu,amjames,cpuhrsch,eqy,
d2e9a8bf6d2,dynamo,Untopiced,[Reland] Fix inlining module-scoped store global (#132439),test/dynamo/mock_store_global_crossfile_inline.py test/dynamo/test_global.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/132439,mlazos,anijain2305,,,
8668bc279de,inductor,not user facing,[inductor] contine to fix restrict keyword. (#132463),torch/_inductor/codegen/cpp_micro_gemm.py,https://github.com/pytorch/pytorch/pull/132463,xuhancn,jansel,jgong5,,
290f09f8298,skip,Untopiced,Ban decorator usage of dynamo_timed (#132328),.lintrunner.toml test/dynamo/test_profiler.py torch/_dynamo/convert_frame.py torch/_dynamo/output_graph.py torch/_dynamo/utils.py torch/_functorch/aot_autograd.py torch/_inductor/codecache.py torch/_inductor/codegen/wrapper.py torch/_inductor/compile_fx.py torch/_inductor/freezing.py torch/_inductor/graph.py torch/_inductor/runtime/runtime_utils.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/scheduler.py torch/fx/experimental/validator.py,https://github.com/pytorch/pytorch/pull/132328,ezyang,albanD,,,
d903e664c6b,fx,not user facing,Refactor thunkify to return proper thunk abstraction (#132407),torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/132407,ezyang,albanD,,,
d342dc01799,fx,not user facing,Don't attempt to compute hints for unbacked expressions (#132060),test/dynamo/test_misc.py test/inductor/test_torchinductor.py torch/_refs/__init__.py torch/fx/experimental/sym_node.py,https://github.com/pytorch/pytorch/pull/132060,ezyang,Skylion007,,,
0d9c9716b2d,dynamo,bug fixes,Ensure compiler collective is called even when no graph is compiled (#132163),test/distributed/test_dynamo_distributed.py torch/_dynamo/convert_frame.py torch/_dynamo/output_graph.py,https://github.com/pytorch/pytorch/pull/132163,ezyang,jansel,,,
56f2917bef4,dynamo,not user facing,[dynamo] Bugfix for recently added str handler (#132461),torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/132461,anijain2305,williamwen42,,,
b9cb1abf656,linalg_frontend,Untopiced,[12/N] Use  std::optional  (#132361),aten/src/ATen/core/NamedTensor.cpp aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h aten/src/ATen/core/op_registration/adaption.h aten/src/ATen/functorch/BatchRulesActivation.cpp aten/src/ATen/functorch/BatchRulesBinaryOps.cpp aten/src/ATen/functorch/BatchRulesConvolution.cpp aten/src/ATen/functorch/BatchRulesFactory.cpp aten/src/ATen/functorch/BatchRulesHelper.h aten/src/ATen/functorch/BatchRulesLoss.cpp aten/src/ATen/functorch/BatchRulesModules.cpp aten/src/ATen/functorch/BatchRulesNorm.cpp aten/src/ATen/functorch/BatchRulesPooling.cpp aten/src/ATen/functorch/BatchRulesReduceOps.cpp aten/src/ATen/functorch/BatchRulesScatterOps.cpp aten/src/ATen/functorch/BatchRulesUnaryOps.cpp aten/src/ATen/functorch/BatchRulesViews.cpp aten/src/ATen/functorch/BatchingMetaprogramming.h aten/src/ATen/functorch/DynamicLayer.cpp aten/src/ATen/functorch/PlumbingHelper.cpp aten/src/ATen/functorch/PlumbingHelper.h aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/TensorConversions.cpp aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/flash_attn/flash_api.cpp aten/src/ATen/native/transformers/cuda/flash_attn/flash_api.h c10/core/DeviceGuard.h c10/core/StreamGuard.h c10/core/impl/InlineDeviceGuard.h c10/core/impl/InlineStreamGuard.h c10/cuda/CUDAGuard.h c10/util/Optional.h torch/csrc/api/include/torch/data/dataloader/base.h torch/csrc/api/include/torch/data/dataloader_options.h torch/csrc/api/include/torch/data/datasets/base.h torch/csrc/api/include/torch/data/detail/queue.h torch/csrc/api/include/torch/data/detail/sequencers.h torch/csrc/api/include/torch/data/iterator.h torch/csrc/api/include/torch/data/samplers/base.h torch/csrc/api/include/torch/data/samplers/distributed.h torch/csrc/api/include/torch/data/samplers/random.h torch/csrc/api/include/torch/data/samplers/sequential.h torch/csrc/api/include/torch/data/samplers/stream.h torch/csrc/api/include/torch/nn/modules/container/any.h torch/csrc/api/src/data/datasets/mnist.cpp torch/csrc/api/src/data/samplers/distributed.cpp torch/csrc/api/src/data/samplers/random.cpp torch/csrc/api/src/data/samplers/sequential.cpp torch/csrc/api/src/data/samplers/stream.cpp torch/csrc/autograd/python_engine.cpp torch/csrc/distributed/c10d/Store.hpp torch/csrc/inductor/aoti_eager/kernel_holder.cpp torch/csrc/jit/mobile/train/random.cpp torch/csrc/jit/mobile/train/random.h torch/csrc/jit/mobile/train/sequential.cpp torch/csrc/jit/mobile/train/sequential.h torch/csrc/profiler/unwind/debug_info.h torch/csrc/profiler/unwind/dwarf_symbolize_enums.h torch/csrc/profiler/unwind/line_number_program.h torch/csrc/profiler/unwind/range_table.h torch/csrc/profiler/unwind/sections.h torch/csrc/profiler/unwind/unwind_error.h torchgen/executorch/api/unboxing.py torchgen/gen_vmap_plumbing.py,https://github.com/pytorch/pytorch/pull/132361,cyyever,eqy,,,
48929184e9e,skip,not user facing,AutoHeuristic: mixed_mm heuristic for A100 (#131613),.lintrunner.toml pyproject.toml test/inductor/test_autoheuristic.py test/inductor/test_pattern_matcher.py torch/_inductor/autoheuristic/artifacts/_MixedMMA100.py torch/_inductor/autoheuristic/artifacts/_PadMMA100.py torch/_inductor/autoheuristic/autoheuristic.py torch/_inductor/autoheuristic/autoheuristic_utils.py torch/_inductor/autoheuristic/learnedheuristic_interface.py torch/_inductor/config.py torch/_inductor/kernel/mm.py torchgen/_autoheuristic/mixed_mm/gen_mixedmm_heuristic_a100.sh torchgen/_autoheuristic/mixed_mm/get_mixedmm_dataset.sh torchgen/_autoheuristic/mixed_mm/train_decision_mixedmm.py torchgen/_autoheuristic/pad_mm/gen_pad_mm_a100.sh torchgen/_autoheuristic/pad_mm/gen_pad_mm_h100.sh torchgen/_autoheuristic/pad_mm/train_decision_pad_mm.py torchgen/_autoheuristic/pad_mm/train_pad_mm.py torchgen/_autoheuristic/pad_mm/train_regression_pad_mm.py torchgen/_autoheuristic/train.py torchgen/_autoheuristic/train_decision.py torchgen/_autoheuristic/train_regression.py,https://github.com/pytorch/pytorch/pull/131613,AlnisM,eellison,,,
053e5080f68,dynamo,Untopiced,Enable exception chaining in call_user_compiler (#131186),test/dynamo/test_logging.py torch/_dynamo/output_graph.py,https://github.com/pytorch/pytorch/pull/131186,nicholasw-gc,jansel,,,
ccf9ce8e8c3,skip,not user facing,Change signature of CompilerFn for register_backend decorator (#131880),torch/_dynamo/backends/registry.py,https://github.com/pytorch/pytorch/pull/131880,redradist,,,,
78f4a3919ff,skip,not user facing,Remove duplicate XPU switch case in DispatchStub (#132480),aten/src/ATen/native/DispatchStub.cpp,https://github.com/pytorch/pytorch/pull/132480,ZhiweiYan-96,malfet,nautsimon,,
5aafdc2f870,skip,not user facing,[3/N] Fix clang-tidy warnings in aten/src/ATen/native/  (#132000),aten/src/ATen/native/cpu/UpSampleKernel.cpp,https://github.com/pytorch/pytorch/pull/132000,cyyever,ezyang,,,
63eb06c0512,skip,Untopiced,Disable SymDispatchMode when torch.compile'ing (#132433),torch/_dynamo/convert_frame.py torch/fx/experimental/_sym_dispatch_mode.py,https://github.com/pytorch/pytorch/pull/132433,ezyang,ydwu4,,,
d224857b3af,skip,Untopiced,"Revert ""Change signature of CompilerFn for register_backend decorator (#131880)""",torch/_dynamo/backends/registry.py,,,,,,
fca2dba7ca3,Uncategorized,Untopiced,[pytorch][counters] Pybind for WaitCounter (#132357),c10/util/WaitCounter.h test/test_monitor.py torch/csrc/monitor/python_init.cpp torch/monitor/__init__.py,https://github.com/pytorch/pytorch/pull/132357,andriigrynenko,asiab4,jamesperng,,
9eeb5eebaba,skip,Untopiced,"Revert ""Ensure compiler collective is called even when no graph is compiled (#132163)""",test/distributed/test_dynamo_distributed.py torch/_dynamo/convert_frame.py torch/_dynamo/output_graph.py,,,,,,
82b6480b0ab,autograd_frontend,bug fixes,Update SavedTensorHooks TLS stack to use SafePyObject (#131700),aten/src/ATen/SavedTensorHooks.cpp aten/src/ATen/SavedTensorHooks.h c10/core/SafePyObject.h c10/core/impl/PyInterpreter.cpp c10/core/impl/PyInterpreter.h test/inductor/test_compiled_autograd.py test/test_autograd.py torch/csrc/PyInterpreter.cpp torch/csrc/autograd/python_saved_variable_hooks.cpp,https://github.com/pytorch/pytorch/pull/131700,soulitzer,albanD,,,
296c339f981,dynamo,bug fixes,Ensure compiler collective is called even when no graph is compiled (#132163),test/distributed/test_dynamo_distributed.py torch/_dynamo/convert_frame.py torch/_dynamo/output_graph.py,https://github.com/pytorch/pytorch/pull/132163,ezyang,jansel,,,
1197550876d,skip,Untopiced,"Revert ""Don't attempt to compute hints for unbacked expressions (#132060)""",test/dynamo/test_misc.py test/inductor/test_torchinductor.py torch/_refs/__init__.py torch/fx/experimental/sym_node.py,,,,,,
8fff976355d,skip,Untopiced,"Revert ""Refactor thunkify to return proper thunk abstraction (#132407)""",torch/fx/experimental/proxy_tensor.py,,,,,,
fc32732596d,fx,not user facing,Don't attempt to compute hints for unbacked expressions (#132060),test/dynamo/test_misc.py test/inductor/test_torchinductor.py torch/_refs/__init__.py torch/fx/experimental/sym_node.py,https://github.com/pytorch/pytorch/pull/132060,ezyang,Skylion007,,,
9167113c161,python_frontend,improvements,[easy][MPS] add torch.mps.is_available() (#132426),torch/mps/__init__.py,https://github.com/pytorch/pytorch/pull/132426,davidberard98,malfet,,,
0c491702c4b,onnx,not user facing,[ONNX] Define the `TORCH_ONNX_USE_EXPERIMENTAL_LOGIC` flag (#132299),torch/onnx/_experimental.py torch/onnx/_exporter_states.py torch/onnx/_flags.py,https://github.com/pytorch/pytorch/pull/132299,justinchuby,titaiwangms,,,
207e24ff834,skip,not user facing,Enable clang-tidy on aten/src/ATen/cudnn/* (#130133),.lintrunner.toml aten/src/ATen/cudnn/AutocastRNN.cpp aten/src/ATen/cudnn/Descriptors.cpp aten/src/ATen/cudnn/Descriptors.h aten/src/ATen/cudnn/Handle.cpp aten/src/ATen/cudnn/Handle.h aten/src/ATen/cudnn/Types.cpp aten/src/ATen/cudnn/Types.h aten/src/ATen/cudnn/Utils.h aten/src/ATen/cudnn/cudnn-wrapper.h,https://github.com/pytorch/pytorch/pull/130133,cyyever,Skylion007,eqy,,
87d58cc81f7,mps,Untopiced,[4/N] Fix clang-tidy warnings in aten/src/ATen/native/  (#132001),aten/src/ATen/native/mps/Copy.h aten/src/ATen/native/mps/OperationUtils.h aten/src/ATen/native/mps/OperationUtils.mm aten/src/ATen/native/mps/TensorFactory.cpp,https://github.com/pytorch/pytorch/pull/132001,cyyever,Skylion007,,,
19897a16478,skip,not user facing,[export] change deepcopy to copy in _replace_set_grad_with_hop pass.. (#132181),torch/_export/passes/replace_set_grad_with_hop_pass.py,https://github.com/pytorch/pytorch/pull/132181,ydwu4,BoyuanFeng,,,
df781343e24,build_frontend,not user facing,Link libc10 to pthreads (#132484),c10/CMakeLists.txt,https://github.com/pytorch/pytorch/pull/132484,AlekseiNikiforovIBM,malfet,,,
a9e1133faaa,inductor,Untopiced,[AOTI][refactor] Move set_cpp_kernel to base class (#132319),torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/132319,desertfire,angelayi,chenyang78,,
e0514a5b99e,inductor,Untopiced,[AOTI][refactor] Consolidate how python_kernel_name is set (#132320),torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/132320,desertfire,angelayi,chenyang78,,
b8f7019df09,skip,Untopiced,"Revert ""[dynamo] Track params/buffers and mark them as static (#132334)""",test/inductor/test_cpu_select_algorithm.py torch/_dynamo/guards.py torch/_dynamo/source.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/user_defined.py,,,,,,
193a19ee918,skip,Untopiced,"Revert ""[dynamo] Treat attr of unspecialized buiitin nn modules as static (#132318)""",torch/_dynamo/utils.py torch/_dynamo/variables/builder.py,,,,,,
59b73079a04,skip,Untopiced,"Revert ""Always use high precision for SDPA math backend (#128922)""",aten/src/ATen/native/transformers/attention.cpp test/test_decomp.py test/test_transformers.py torch/nn/functional.py,,,,,,
e4e3575fb0c,skip,Untopiced,"Revert ""[11/N] Use std::nullopt and std::optional (#132396)""",aten/src/ATen/core/IListRef_test.cpp aten/src/ATen/core/function_schema_inl.h aten/src/ATen/core/op_registration/op_registration.h aten/src/ATen/functorch/ADInterpreters.cpp aten/src/ATen/functorch/BatchRulesConvolution.cpp aten/src/ATen/functorch/BatchRulesHelper.cpp aten/src/ATen/functorch/BatchRulesLoss.cpp aten/src/ATen/functorch/BatchRulesModules.cpp aten/src/ATen/functorch/BatchRulesNorm.cpp aten/src/ATen/functorch/BatchRulesReduceOps.cpp aten/src/ATen/functorch/BatchRulesScatterOps.cpp aten/src/ATen/functorch/BatchRulesViews.cpp aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp aten/src/ATen/functorch/PlumbingHelper.cpp aten/src/ATen/functorch/PlumbingHelper.h aten/src/ATen/native/Bucketization.cpp aten/src/ATen/native/NamedTensor.cpp aten/src/ATen/native/TensorAdvancedIndexing.cpp aten/src/ATen/native/TensorConversions.cpp aten/src/ATen/native/cuda/Bucketization.cu aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/templates/LazyIr.h aten/src/ATen/test/ivalue_test.cpp c10/cuda/CUDAGuard.h c10/util/DynamicCounter.cpp functorch/csrc/dim/arena.h functorch/csrc/dim/dim.cpp test/edge/Evalue.h torch/csrc/functorch/init.cpp,,,,,,
e696f17467b,skip,Untopiced,"Revert ""[dynamo] Track builtin nn modules with UnspecializedBuiltinNNModuleVariable (#132314)""",torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/nn_module.py,,,,,,
24d0a32f980,skip,Untopiced,"Revert ""[dynamo] Wrap unspecialized nn module getattr with UnspecializedNNModuleSource (#132308)""",torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/user_defined.py,,,,,,
afca6f5b475,inductor,Untopiced,[PT2][Optimus] Add missing example value for introduced nodes (#132297),test/inductor/test_split_cat_fx_passes.py torch/_inductor/fx_passes/group_batch_fusion.py torch/_inductor/fx_passes/split_cat.py,https://github.com/pytorch/pytorch/pull/132297,mengluy0125,jackiexu1992,,,
bcb4f7c1722,skip,Untopiced,"Revert ""Grouped Query Attention (#128898)""",aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/attention.h aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.h test/cpp_extensions/open_registration_extension.cpp test/distributed/_tensor/test_matrix_ops.py test/dynamo/test_sdpa.py test/test_transformers.py tools/pyi/gen_pyi.py torch/_C/__init__.pyi.in torch/_dynamo/variables/sdpa.py torch/csrc/Module.cpp torch/nested/_internal/sdpa.py torch/nn/attention/bias.py torch/nn/functional.py torch/testing/_internal/common_methods_invocations.py,,,,,,
b71cd149ce5,inductor,not user facing,Fix file lock issue in AotCodeCompiler (#132343),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/132343,masnesral,desertfire,,,
e81e74ca6cb,skip,not user facing,[dynamo] revert map/zip iterator related changes (#132528),test/dynamo/test_functions.py test/dynamo/test_repros.py torch/_dynamo/polyfill.py torch/_dynamo/symbolic_convert.py torch/_dynamo/utils.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/132528,williamwen42,ZainRizvi,,,
f49d5e30eba,skip,not user facing,Change owners of test/test_transformers.py to module: multi-headed-attention (#132519),test/test_transformers.py,https://github.com/pytorch/pytorch/pull/132519,mikaylagawarecki,Skylion007,,,
dccce77935b,skip,not user facing,C++ network flow implementation in c10 (#132188),c10/test/util/NetworkFlow_test.cpp c10/util/NetworkFlow.cpp c10/util/NetworkFlow.h,https://github.com/pytorch/pytorch/pull/132188,davidberard98,Chillee,,,
642257db1a5,skip,not user facing,Update the FQN for auto_functionalized HOO. (#132171),test/export/test_export.py torch/_higher_order_ops/auto_functionalize.py,https://github.com/pytorch/pytorch/pull/132171,zhxchen17,SherlockNoMad,,,
f379bbd46d4,dynamo,not user facing,[dynamo] support inspect.signature.bind (#132330),test/dynamo/test_misc.py torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/132330,williamwen42,jansel,,,
625f494619f,distributed,Untopiced,[Pipelining] Add schedule unshard/reshard pass (#129810),test/distributed/pipelining/test_schedule.py torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/129810,wconstab,H-Huang,kwen2501,,
7c1cca9fdaf,distributed,Untopiced,[pipelining] Add schedule send/recv pass (#130378),test/distributed/pipelining/test_schedule.py torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/130378,wconstab,H-Huang,,,
37c3d503b7d,distributed,not user facing,[pipelining] Make test_schedule quiet (#132369),test/distributed/pipelining/test_schedule.py,https://github.com/pytorch/pytorch/pull/132369,wconstab,H-Huang,,,
1962f9475fb,nested tensor_frontend,bug fixes,"[NJT][flop counter] attention: if offsets are fake, use max seqlen (#132356)",test/test_flop_counter.py torch/utils/flop_counter.py,https://github.com/pytorch/pytorch/pull/132356,davidberard98,soulitzer,,,
bcac71517c4,profiler,not user facing,[Profiler] Test Logging for Empty Traces (#132444),test/profiler/test_profiler.py third_party/kineto,https://github.com/pytorch/pytorch/pull/132444,sraikund16,aaronenyeshi,,,
85f19ce14a4,fx,Untopiced,"Support meta[""val""] that is a dict, for triton kernels and for the partitioner (#132466)",test/functorch/test_ac.py torch/_functorch/partitioners.py torch/_higher_order_ops/triton_kernel_wrap.py torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/132466,davidberard98,zou3519,,,
e3513fb2af7,Uncategorized,Untopiced,"[ts_converter]handle python list append, list add, aten.to.dtype+mutation_op pattern (#132529)",test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/132529,SherlockNoMad,jiashenC,,,
705ac311aae,distributed,Untopiced,Fix Distributed EventList usage (#132448),torch/distributed/rpc/server_process_global_profiler.py,https://github.com/pytorch/pytorch/pull/132448,sraikund16,aaronenyeshi,,,
fcef6cc6d11,jit,not user facing,[13/N] Fix clang-tidy warnings in jit (#132477),torch/csrc/jit/tensorexpr/kernel.h torch/csrc/jit/tensorexpr/operators/matmul.cpp torch/csrc/jit/tensorexpr/operators/misc.cpp torch/csrc/jit/tensorexpr/operators/misc.h torch/csrc/jit/tensorexpr/operators/norm.cpp torch/csrc/jit/tensorexpr/operators/pointwise.cpp torch/csrc/jit/tensorexpr/operators/pointwise.h torch/csrc/jit/tensorexpr/operators/quantization.cpp torch/csrc/jit/tensorexpr/operators/reduction.cpp torch/csrc/jit/tensorexpr/operators/softmax.cpp,https://github.com/pytorch/pytorch/pull/132477,cyyever,Skylion007,,,
64720f3b89a,releng,not user facing,Introduce checks to validate public API tests (#131390),.ci/pytorch/test.sh,https://github.com/pytorch/pytorch/pull/131390,jbschlosser,albanD,,,
a503136583f,onnx,Untopiced,[export] Detect whether case_name is registered in exportdb (#132420),test/onnx/test_fx_to_onnx_with_onnxruntime.py torch/_dynamo/eval_frame.py torch/_export/db/examples/torch_sym_min.py torch/_export/db/examples/unsupported_operator.py torch/_export/db/logging.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/132420,yushangdi,zhxchen17,,,
243a763e1bf,releng,not user facing,ci: Remove split-build CUDA testing from pull.yml (#132537),.github/workflows/pull.yml,https://github.com/pytorch/pytorch/pull/132537,seemethere,ZainRizvi,malfet,,
841cadd5550,skip,not user facing,Fix discrepancies from 129973 (#132545),test/cpp/jit/CMakeLists.txt,https://github.com/pytorch/pytorch/pull/132545,izaitsevfb,kit1980,,,
419b76c4ac8,dynamo,not user facing,"[dynamo] Reland 132308, 132314, 132318, 132334 - Make builtin nn modules attributes static (#132539)",test/inductor/test_cpu_select_algorithm.py torch/_dynamo/guards.py torch/_dynamo/source.py torch/_dynamo/utils.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/132539,anijain2305,Skylion007,mlazos,yanboliang,
25903f3932b,skip,not user facing,[easy] fix f-string messages in torch/_ops.py (#132531),torch/_ops.py,https://github.com/pytorch/pytorch/pull/132531,davidberard98,Skylion007,,,
373e9be457a,inductor,not user facing,[Inductor][FlexAttention] Add kwarg to top level for users to specify kernel params (#132015),test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/132015,yanboliang,Chillee,,,
0b571b10585,skip,not user facing,[codemod][pyre] Add missing Pyre mode headers (#132548),torch/distributed/benchmarks/benchmark_ddp_rpc.py torch/distributed/checkpoint/examples/stateful_example.py,https://github.com/pytorch/pytorch/pull/132548,izaitsevfb,ZainRizvi,kit1980,,
5973aec6712,fx,not user facing,[fx] python_code(verbose=True): show size/strides for all tensors (#132192),test/dynamo/test_higher_order_ops.py test/dynamo/test_subclasses.py torch/fx/graph.py,https://github.com/pytorch/pytorch/pull/132192,davidberard98,Chillee,,,
6966d44eda8,onnx,not user facing,[ONNX] Rename _internal/exporter to _exporter_legacy (#132429),benchmarks/dynamo/common.py test/onnx/dynamo/test_exporter_api.py test/onnx/test_fx_to_onnx.py test/onnx/test_fx_to_onnx_with_onnxruntime.py torch/onnx/__init__.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/exporter.py torch/onnx/_internal/fx/dynamo_graph_extractor.py torch/onnx/_internal/fx/fx_symbolic_graph_extractor.py torch/onnx/_internal/fx/torch_export_graph_extractor.py torch/onnx/_internal/onnxruntime.py,https://github.com/pytorch/pytorch/pull/132429,justinchuby,titaiwangms,,,
1f5dfe00dab,fx,not user facing,Subtracer should always be real to inherit fake/real tensors from parent config (#132488),torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/132488,ezyang,zou3519,,,
ed4493de0ea,Uncategorized,Untopiced,dim name is identifier (#132557),torch/export/dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/132557,avikchaudhuri,pianpwk,,,
b28c01d90d6,skip,Untopiced,[export] Convert autocast to HOO (#131914),test/export/test_passes.py torch/_export/passes/replace_autocast_with_hop_pass.py torch/_export/passes/replace_set_grad_with_hop_pass.py torch/_export/passes/replace_with_hop_pass_util.py torch/_higher_order_ops/wrap.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/131914,yushangdi,angelayi,,,
06581c277ad,dynamo,not user facing,[dynamo][stable-diffusion] Support dict(obj) on constrained subclasses of dict and OrderedDict (#132558),test/dynamo/test_misc.py torch/_dynamo/utils.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/132558,anijain2305,jansel,,,
8ad9f89ccca,inductor,not user facing,[inductor] Reland: Add flag to ignore unsupported @triton.autotune args in user-written kernel compilation (#132562),test/inductor/test_triton_kernels.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_inductor/config.py torch/testing/_internal/triton_utils.py,https://github.com/pytorch/pytorch/pull/132562,aakhundov,chenyang78,,,
36ec0fdf103,inductor,not user facing,[inductor] check compiler exist on Windows. (#132533),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/132533,xuhancn,jansel,,,
159d508f03d,skip,not user facing,[Fix]: prim::If with multiple outputs and input return directly (#131779),test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/131779,jiashenC,SherlockNoMad,angelayi,,
0e7e61f7cec,distributed,deprecation,Deprecate `torch._utils.is_compiling()` and `torch._dynamo.external_utils.is_compiling()` (#127690),test/dynamo/test_skip_non_tensor.py test/export/test_torchbind.py test/functorch/test_memory_efficient_fusion.py test/inductor/test_distributed_patterns.py test/test_optim.py torch/_dynamo/decorators.py torch/_dynamo/external_utils.py torch/_functorch/apis.py torch/_functorch/eager_transforms.py torch/_higher_order_ops/associative_scan.py torch/_utils.py torch/distributed/_composable/fsdp/_fsdp_common.py torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py torch/distributed/tensor/parallel/_utils.py torch/nn/parallel/distributed.py torch/optim/_adafactor.py torch/optim/adadelta.py torch/optim/adagrad.py torch/optim/adam.py torch/optim/adamax.py torch/optim/adamw.py torch/optim/asgd.py torch/optim/nadam.py torch/optim/optimizer.py torch/optim/radam.py torch/optim/rmsprop.py torch/optim/rprop.py torch/optim/sgd.py torch/testing/_internal/optests/generate_tests.py,https://github.com/pytorch/pytorch/pull/127690,XuehaiPan,Skylion007,malfet,,
a896fb1b36a,fx,Untopiced,check unsupported sympy functions for runtime asserts (#132457),test/dynamo/test_repros.py test/export/test_export.py torch/_export/verifier.py torch/fx/experimental/symbolic_shapes.py torch/fx/passes/runtime_assert.py,https://github.com/pytorch/pytorch/pull/132457,pianpwk,avikchaudhuri,,,
21d02f8b4bd,skip,Untopiced,"Revert ""[easy] fix f-string messages in torch/_ops.py (#132531)""",torch/_ops.py,,,,,,
bbce5172219,inductor,not user facing,[Inductor][FlexAttention] TestFlexAttention -> TestFlexDecoding (#132547),test/inductor/test_flex_decoding.py,https://github.com/pytorch/pytorch/pull/132547,yanboliang,Chillee,,,
e3387c67128,inductor,not user facing,[inductor] use uint64_t replace long to add Windows support. (#132491),torch/_inductor/codegen/cpp_utils.py torch/_inductor/codegen/cpp_wrapper_cpu.py,https://github.com/pytorch/pytorch/pull/132491,xuhancn,malfet,,,
00097f3458a,skip,Untopiced,"Revert ""C++ network flow implementation in c10 (#132188)""",c10/test/util/NetworkFlow_test.cpp c10/util/NetworkFlow.cpp c10/util/NetworkFlow.h,,,,,,
bc46f205c44,jit,Untopiced,[15/N] Fix clang-tidy warnings in jit (#132564),torch/csrc/jit/tensorexpr/graph_opt.cpp torch/csrc/jit/tensorexpr/graph_opt.h torch/csrc/jit/tensorexpr/half_support.h torch/csrc/jit/tensorexpr/ir_cloner.cpp torch/csrc/jit/tensorexpr/mem_dependency_checker.cpp torch/csrc/jit/tensorexpr/mem_dependency_checker.h,https://github.com/pytorch/pytorch/pull/132564,cyyever,Skylion007,,,
6c65fd03942,skip,not user facing,[inductor] Add type hints to functions in mkldnn_fusion.py (#131820),torch/_inductor/fx_passes/freezing_patterns.py torch/_inductor/fx_passes/mkldnn_fusion.py torch/_inductor/fx_passes/post_grad.py,https://github.com/pytorch/pytorch/pull/131820,aakhundov,eellison,,,
d9841057486,skip,Untopiced,"Revert ""[export] Convert autocast to HOO (#131914)""",test/export/test_passes.py torch/_export/passes/replace_autocast_with_hop_pass.py torch/_export/passes/replace_set_grad_with_hop_pass.py torch/_export/passes/replace_with_hop_pass_util.py torch/_higher_order_ops/wrap.py torch/export/_trace.py,,,,,,
6ec4af6865d,inductor,not user facing,[Inductor][CPP] Add vectorization support for double (#131886),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_prefix.h,https://github.com/pytorch/pytorch/pull/131886,CaoE,jgong5,peterbell10,,
81b8d3586f3,skip,not user facing,Update torch-xpu-ops pin (ATen XPU implementation) (#132390),third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/132390,fengyuan14,EikanWang,,,
7f8a384a8f6,inductor,not user facing,[inductor] add msvc_cl compiler check (#132571),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/132571,xuhancn,ezyang,,,
8ff310392ed,python_frontend,new features,add __torch_function__ handler to get_device cpp (#132567),torch/csrc/autograd/python_torch_functions_manual.cpp,https://github.com/pytorch/pytorch/pull/132567,exclamaforte,ezyang,,,
2a8e94347fd,distributed,not user facing,[TP] verify numeric parity on Transfromers for multiple iterations (#132543),test/distributed/tensor/parallel/test_tp_examples.py torch/testing/_internal/distributed/_tensor/common_dtensor.py,https://github.com/pytorch/pytorch/pull/132543,weifengpy,tianyu-l,,,
522fa03e911,onnx,not user facing,[Submodule] Bump ONNX to v1.16.2 (#132566),third_party/onnx,https://github.com/pytorch/pytorch/pull/132566,cyyever,justinchuby,,,
2714adce209,caffe2,Untopiced,[caffe2] Fix compiling ATen-hip in non-opt mode (#132581),aten/src/ATen/native/transformers/cuda/sdp_utils.cpp,https://github.com/pytorch/pytorch/pull/132581,danzimm,jianyuh,xw285cornell,,
f3fce597e95,quantization,not user facing,[BE][Easy][17/19] enforce style for empty lines in import segments in `torch/[a-c]*/` and `torch/[e-n]*/` (#129769),tools/linter/adapters/ufmt_linter.py torch/_library/custom_ops.py torch/autograd/__init__.py torch/autograd/_functions/tensor.py torch/autograd/anomaly_mode.py torch/autograd/forward_ad.py torch/autograd/function.py torch/autograd/functional.py torch/autograd/grad_mode.py torch/autograd/gradcheck.py torch/autograd/profiler.py torch/autograd/profiler_legacy.py torch/autograd/profiler_util.py torch/backends/__init__.py torch/backends/_coreml/preprocess.py torch/backends/_nnapi/prepare.py torch/backends/cpu/__init__.py torch/backends/cuda/__init__.py torch/backends/cudnn/__init__.py torch/backends/cudnn/rnn.py torch/backends/mha/__init__.py torch/backends/mkldnn/__init__.py torch/backends/mps/__init__.py torch/backends/nnpack/__init__.py torch/backends/opt_einsum/__init__.py torch/backends/xeon/run_cpu.py torch/cpu/amp/autocast_mode.py torch/cpu/amp/grad_scaler.py torch/csrc/lazy/test_mnist.py torch/cuda/__init__.py torch/cuda/amp/__init__.py torch/cuda/amp/autocast_mode.py torch/cuda/amp/common.py torch/cuda/amp/grad_scaler.py torch/cuda/comm.py torch/cuda/jiterator.py torch/cuda/memory.py torch/cuda/nvtx.py torch/cuda/profiler.py torch/cuda/random.py torch/cuda/streams.py torch/export/__init__.py torch/export/_remove_effect_tokens_pass.py torch/export/_trace.py torch/export/_unlift.py torch/export/dynamic_shapes.py torch/export/exported_program.py torch/fx/_lazy_graph_module.py torch/fx/_pytree.py torch/fx/_utils.py torch/fx/experimental/_config.py torch/fx/experimental/_sym_dispatch_mode.py torch/fx/experimental/shape_inference/infer_symbol_values.py torch/fx/experimental/sym_node.py torch/fx/passes/graph_transform_observer.py torch/fx/passes/runtime_assert.py torch/fx/passes/utils/matcher_with_name_node_map_utils.py torch/jit/__init__.py torch/jit/_async.py torch/jit/_await.py torch/jit/_builtins.py torch/jit/_decompositions.py torch/jit/_logging.py torch/jit/_recursive.py torch/jit/_script.py torch/jit/_shape_functions.py torch/jit/_trace.py torch/jit/annotations.py torch/jit/frontend.py torch/jit/mobile/__init__.py torch/jit/supported_ops.py torch/jit/unsupported_tensor_ops.py torch/masked/_ops.py torch/masked/maskedtensor/_ops_refs.py torch/mps/__init__.py torch/mps/profiler.py torch/mtia/__init__.py torch/multiprocessing/__init__.py torch/multiprocessing/_atfork.py torch/multiprocessing/spawn.py torch/nested/_internal/nested_tensor.py torch/nested/_internal/ops.py torch/nested/_internal/sdpa.py torch/nn/__init__.py torch/nn/attention/flex_attention.py torch/nn/utils/rnn.py,https://github.com/pytorch/pytorch/pull/129769,XuehaiPan,ezyang,,,
f7aeb394b6f,onnx,not user facing,[BE][Easy] Remove empty `ISORT_SKIPLIST` (#132572),pyproject.toml test/higher_order_ops/test_with_effects.py test/onnx/test_fx_op_consistency.py test/onnx/test_fx_to_onnx.py test/test_autograd.py tools/linter/adapters/ufmt_linter.py torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/132572,XuehaiPan,ezyang,justinchuby,,
d2dc173664d,skip,not user facing,Remove lint dependency `ufmt` (#132573),.lintrunner.toml .vscode/extensions.json .vscode/settings_recommended.json tools/linter/adapters/pyfmt_linter.py tools/linter/adapters/ufmt_linter.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/132573,XuehaiPan,ezyang,,,
87d46d70d77,skip,not user facing,[inductor] export kernel for gemm template. (#132580),torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_utils.py,https://github.com/pytorch/pytorch/pull/132580,xuhancn,ezyang,,,
908d2a153b1,skip,not user facing,[easy] fix f-string messages in torch/_ops.py (#132531),torch/_ops.py,https://github.com/pytorch/pytorch/pull/132531,davidberard98,Skylion007,,,
105ba7b58c4,skip,not user facing,[5/N] Fix clang-tidy warnings in aten/src/ATen  (#132565),aten/src/ATen/Context.h aten/src/ATen/DLConvertor.cpp aten/src/ATen/Dispatch.h aten/src/ATen/FunctionalInverses.cpp aten/src/ATen/TensorGeometry.cpp aten/src/ATen/native/CPUBlas.cpp aten/src/ATen/native/CPUFallback.cpp aten/src/ATen/native/Copy.cpp aten/src/ATen/native/Fill.cpp aten/src/ATen/native/FractionalMaxPool2d.cpp aten/src/ATen/native/FractionalMaxPool3d.cpp aten/src/ATen/native/FractionalMaxPooling.h aten/src/ATen/native/FusedAdagrad.cpp aten/src/ATen/native/FusedAdagrad.h aten/src/ATen/native/FusedAdam.cpp aten/src/ATen/native/FusedAdam.h aten/src/ATen/native/FusedSGD.cpp aten/src/ATen/native/FusedSGD.h aten/src/ATen/native/cpu/FlashAttentionKernel.cpp,https://github.com/pytorch/pytorch/pull/132565,cyyever,Skylion007,,,
5dac4d2c780,skip,Untopiced,"Revert ""[easy] fix f-string messages in torch/_ops.py (#132531)""",torch/_ops.py,,,,,,
09fcd792eb4,skip,not user facing,[Fix]: ScriptObject lifting issue (#130952),test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/130952,jiashenC,angelayi,,,
c35061c542b,skip,not user facing,Migrate Python code formatter from `black` to `ruff format` (#132574),.lintrunner.toml pyproject.toml tools/linter/adapters/pyfmt_linter.py,https://github.com/pytorch/pytorch/pull/132574,XuehaiPan,ezyang,,,
4226ed15853,foreach_frontend,not user facing,[BE] Format uncategorized Python files with `ruff format` (#132576),.flake8 docs/source/scripts/onnx/build_onnx_torchscript_supported_aten_op_csv_table.py ios/TestApp/benchmark/coreml_backend.py ios/TestApp/run_on_aws_devicefarm.py scripts/export/update_schema.py scripts/release_notes/commitlist.py test/conftest.py test/pytest_shard_custom.py test/run_test.py test/test_autograd.py test/test_binary_ufuncs.py test/test_bundled_inputs.py test/test_cuda.py test/test_cuda_sanitizer.py test/test_custom_ops.py test/test_dataloader.py test/test_decomp.py test/test_dispatch.py test/test_dynamic_shapes.py test/test_foreach.py test/test_functionalization.py test/test_indexing.py test/test_jit_fuser_te.py test/test_legacy_vmap.py test/test_multiprocessing.py test/test_nestedtensor.py test/test_numpy_interop.py test/test_ops.py test/test_optim.py test/test_public_bindings.py test/test_python_dispatch.py test/test_pytree.py test/test_sort_and_select.py test/test_view_ops.py test/test_weak.py tools/linter/adapters/pyfmt_linter.py torch/_C/_distributed_c10d.pyi torch/_C/_profiler.pyi torch/_VF.py torch/__init__.py torch/_compile.py torch/_guards.py torch/_jit_internal.py torch/_linalg_utils.py torch/_lobpcg.py torch/_lowrank.py torch/_meta_registrations.py torch/_storage_docs.py torch/_tensor.py torch/_tensor_docs.py torch/_tensor_str.py torch/_torch_docs.py torch/functional.py torch/hub.py torch/library.py torch/overrides.py torch/quasirandom.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/132576,XuehaiPan,Skylion007,ezyang,,
fd4b649e6c6,skip,not user facing,[BE]: Simplify some list comps to generators C419 (#132578),benchmarks/gpt_fast/generate.py functorch/notebooks/minifier.ipynb test/functorch/test_ac.py test/test_nestedtensor.py torch/_functorch/partitioners.py torch/autograd/profiler_util.py torch/nested/_internal/nested_tensor.py,https://github.com/pytorch/pytorch/pull/132578,Skylion007,ezyang,,,
0a25666f927,skip,Untopiced,"Revert ""[dynamo] revert map/zip iterator related changes (#132528)""",test/dynamo/test_functions.py test/dynamo/test_repros.py torch/_dynamo/polyfill.py torch/_dynamo/symbolic_convert.py torch/_dynamo/utils.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/user_defined.py,,,,,,
9be33bc5846,skip,Untopiced,"Revert ""[inductor] Add type hints to functions in mkldnn_fusion.py (#131820)""",torch/_inductor/fx_passes/freezing_patterns.py torch/_inductor/fx_passes/mkldnn_fusion.py torch/_inductor/fx_passes/post_grad.py,,,,,,
f2ddd5e9e0e,skip,Untopiced,"Revert ""Add basic mypy annotations to inductor (#132416)""",torch/_inductor/autoheuristic/autoheuristic_utils.py torch/_inductor/autotune_process.py torch/_inductor/bounds.py torch/_inductor/codecache.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/codegen/cpp_template.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/codegen/cuda/cuda_cpp_scheduling.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/cuda/cuda_template.py torch/_inductor/codegen/cuda/cutlass_lib_extensions/gemm_operation_extensions.py torch/_inductor/codegen/cuda/gemm_template.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/rocm/ck_universal_gemm_template.py torch/_inductor/codegen/rocm/rocm_benchmark_request.py torch/_inductor/codegen/rocm/rocm_cpp_scheduling.py torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/codegen/rocm/rocm_template.py torch/_inductor/codegen/rocm/rocm_template_buffer.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_split_scan.py torch/_inductor/comms.py torch/_inductor/compile_worker/subproc_pool.py torch/_inductor/constant_folding.py torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py torch/_inductor/dependencies.py torch/_inductor/exc.py torch/_inductor/freezing.py torch/_inductor/fx_passes/group_batch_fusion.py torch/_inductor/fx_passes/joint_graph.py torch/_inductor/fx_passes/mkldnn_fusion.py torch/_inductor/fx_passes/pre_grad.py torch/_inductor/fx_passes/quantization.py torch/_inductor/fx_passes/split_cat.py torch/_inductor/fx_utils.py torch/_inductor/graph.py torch/_inductor/index_propagation.py torch/_inductor/mkldnn_ir.py torch/_inductor/package/package.py torch/_inductor/remote_cache.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py torch/_inductor/sizevars.py torch/_inductor/subgraph_lowering.py,,,,,,
3558a8cf4a9,skip,Untopiced,"Revert ""Add basic mypy annotations to dynamo (#132415)""",torch/_dynamo/backends/common.py torch/_dynamo/backends/debugging.py torch/_dynamo/backends/distributed.py torch/_dynamo/codegen.py torch/_dynamo/comptime.py torch/_dynamo/device_interface.py torch/_dynamo/eval_frame.py torch/_dynamo/exc.py torch/_dynamo/external_utils.py torch/_dynamo/profiler.py torch/_dynamo/repro/after_aot.py torch/_dynamo/repro/after_dynamo.py torch/_dynamo/symbolic_convert.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/distributed.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/higher_order_ops.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lazy.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/optimizer.py torch/_dynamo/variables/script_object.py torch/_dynamo/variables/sdpa.py torch/_dynamo/variables/tensor.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/torch_function.py torch/_dynamo/variables/user_defined.py,,,,,,
6e799325438,skip,not user facing,Add basic mypy annotations to dynamo (#132415),torch/_dynamo/backends/common.py torch/_dynamo/backends/debugging.py torch/_dynamo/backends/distributed.py torch/_dynamo/codegen.py torch/_dynamo/comptime.py torch/_dynamo/device_interface.py torch/_dynamo/eval_frame.py torch/_dynamo/exc.py torch/_dynamo/external_utils.py torch/_dynamo/profiler.py torch/_dynamo/repro/after_aot.py torch/_dynamo/repro/after_dynamo.py torch/_dynamo/symbolic_convert.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/distributed.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/higher_order_ops.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lazy.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/optimizer.py torch/_dynamo/variables/script_object.py torch/_dynamo/variables/sdpa.py torch/_dynamo/variables/tensor.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/torch_function.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/132415,oulgen,XuehaiPan,jamesjwu,,
09f9c256ada,skip,not user facing,Add basic mypy annotations to inductor (#132416),torch/_inductor/autoheuristic/autoheuristic_utils.py torch/_inductor/autotune_process.py torch/_inductor/bounds.py torch/_inductor/codecache.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/codegen/cpp_template.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/codegen/cuda/cuda_cpp_scheduling.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/cuda/cuda_template.py torch/_inductor/codegen/cuda/cutlass_lib_extensions/gemm_operation_extensions.py torch/_inductor/codegen/cuda/gemm_template.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/rocm/ck_universal_gemm_template.py torch/_inductor/codegen/rocm/rocm_benchmark_request.py torch/_inductor/codegen/rocm/rocm_cpp_scheduling.py torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/codegen/rocm/rocm_template.py torch/_inductor/codegen/rocm/rocm_template_buffer.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_split_scan.py torch/_inductor/comms.py torch/_inductor/compile_worker/subproc_pool.py torch/_inductor/constant_folding.py torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py torch/_inductor/dependencies.py torch/_inductor/exc.py torch/_inductor/freezing.py torch/_inductor/fx_passes/group_batch_fusion.py torch/_inductor/fx_passes/joint_graph.py torch/_inductor/fx_passes/mkldnn_fusion.py torch/_inductor/fx_passes/pre_grad.py torch/_inductor/fx_passes/quantization.py torch/_inductor/fx_passes/split_cat.py torch/_inductor/fx_utils.py torch/_inductor/graph.py torch/_inductor/index_propagation.py torch/_inductor/mkldnn_ir.py torch/_inductor/package/package.py torch/_inductor/remote_cache.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py torch/_inductor/sizevars.py torch/_inductor/subgraph_lowering.py,https://github.com/pytorch/pytorch/pull/132416,oulgen,XuehaiPan,jamesjwu,,
01cdcbf7c83,skip,not user facing,[dynamo] revert map/zip iterator related changes (#132528),test/dynamo/test_functions.py test/dynamo/test_repros.py torch/_dynamo/polyfill.py torch/_dynamo/symbolic_convert.py torch/_dynamo/utils.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/132528,williamwen42,ZainRizvi,,,
c7cfa517213,quantization,bc breaking,Always use high precision for SDPA math backend (#128922),aten/src/ATen/native/transformers/attention.cpp test/test_decomp.py test/test_transformers.py torch/nn/functional.py,https://github.com/pytorch/pytorch/pull/128922,jianyuh,drisspg,xw285cornell,,
1fb498d6e34,inductor,Untopiced,Add try except for _maybe_evaluate_static call in IndexPropagation (#132128),test/inductor/test_cpu_select_algorithm.py torch/_inductor/index_propagation.py torch/_inductor/sizevars.py,https://github.com/pytorch/pytorch/pull/132128,chunyuan-w,jansel,jgong5,,
ae44b8f410b,inductor,not user facing,[inductor] support vectorization for torch.argmax/min(float/int64_t)-> int64_t (#131016),aten/src/ATen/cpu/vec/vec_n.h test/inductor/test_cpu_repro.py test/inductor/test_torchinductor.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_prefix.h,https://github.com/pytorch/pytorch/pull/131016,zhuhaozhe,jansel,jgong5,,
55b0c39d828,inductor,Untopiced,"Reland ""[1/2] PT2 Inductor ComboKernels - Foreach cases (#124969)"" (#132182)",test/inductor/test_foreach.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cuda.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/codegen/triton_foreach.py torch/_inductor/codegen/wrapper.py torch/_inductor/config.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/132182,qchip,Yuzhen11,,,
27f61eba585,Uncategorized,Untopiced,serde sympy functions (#132493),test/export/test_export.py torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/132493,avikchaudhuri,pianpwk,,,
fb87796d4fc,distributed,Untopiced,[DeviceMesh] Add supports for non-continuous slicing (#132310),test/distributed/test_device_mesh.py torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/132310,wz337,wanchaol,,,
b30d0916d9c,distributed,Untopiced,[FSDP2] Added missing event wait (for future) (#132568),torch/distributed/_composable/fsdp/_fsdp_state.py,https://github.com/pytorch/pytorch/pull/132568,awgu,Skylion007,weifengpy,,
c3ee07c71ce,inductor,not user facing,add missing profiler include in cpp code generation (#132419),test/inductor/test_cpu_cpp_wrapper.py test/inductor/test_cuda_cpp_wrapper.py test/inductor/test_torchinductor.py torch/_inductor/codecache.py torch/_inductor/codegen/cpp_wrapper_cpu.py,https://github.com/pytorch/pytorch/pull/132419,exclamaforte,desertfire,jgong5,,
b465a5843b9,distributed,Untopiced,DTensor: add more foreach ops to supported sharding prop list (#132066),torch/distributed/_tensor/ops/_pointwise_ops.py,https://github.com/pytorch/pytorch/pull/132066,bdhirsh,wanchaol,,,
c65cb37657e,fx,not user facing,Refactor thunkify to return proper thunk abstraction (#132407),torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/132407,ezyang,albanD,,,
70cb16b316a,distributed,Untopiced,[DTensor] Added naive replicate strategy for more diagonal ops (#132201),test/distributed/_tensor/test_dtensor_ops.py torch/distributed/_tensor/ops/_math_ops.py,https://github.com/pytorch/pytorch/pull/132201,awgu,wz337,,,
14edd986b3c,skip,not user facing,Fix missing include file (#132647),c10/util/DynamicCounter.cpp,https://github.com/pytorch/pytorch/pull/132647,AlekseiNikiforovIBM,Skylion007,,,
a8490a07623,dynamo,Untopiced,[traced-graph][sparse] propagate sparsity in fx graph (#131920),aten/src/ATen/FunctionalStorageImpl.cpp test/export/test_sparse.py test/test_sparse.py torch/_dynamo/config.py torch/_dynamo/variables/builder.py torch/_functorch/_aot_autograd/functional_utils.py torch/_subclasses/fake_tensor.py torch/_subclasses/functional_tensor.py torch/_subclasses/meta_utils.py torch/csrc/dynamo/guards.cpp torch/fx/experimental/proxy_tensor.py torch/fx/passes/runtime_assert.py torch/fx/passes/shape_prop.py,https://github.com/pytorch/pytorch/pull/131920,aartbik,ezyang,,,
9945caec650,inductor,not user facing,[inductor] Fix autotune non-close attr crash on Windows (#132630),torch/_inductor/autotune_process.py,https://github.com/pytorch/pytorch/pull/132630,xuhancn,desertfire,jgong5,,
a672f6c84e3,inductor,not user facing,[inductor] unificate SUBPROCESS_DECODE_ARGS variable in cpp_builder.py (#132615),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/132615,xuhancn,desertfire,jgong5,malfet,
d532c00c818,skip,not user facing,[test/torch_np] Fix usages of deprecated NumPy 2.0 APIs in numpy_tests (#131909),test/torch_np/numpy_tests/core/test_getlimits.py test/torch_np/numpy_tests/core/test_indexing.py test/torch_np/numpy_tests/core/test_multiarray.py test/torch_np/numpy_tests/core/test_numeric.py test/torch_np/numpy_tests/core/test_scalar_methods.py test/torch_np/numpy_tests/lib/test_function_base.py test/torch_np/numpy_tests/lib/test_type_check.py test/torch_np/numpy_tests/linalg/test_linalg.py torch/_numpy/__init__.py,https://github.com/pytorch/pytorch/pull/131909,kiukchung,Skylion007,atalman,rgommers,
6919e8baaba,mps,improvements,[MPS] Add support for autocast in MPS  (#99272),aten/src/ATen/autocast_mode.cpp aten/src/ATen/autocast_mode.h aten/src/ATen/core/interned_strings.h c10/core/DispatchKey.cpp c10/core/DispatchKey.h c10/core/DispatchKeySet.h test/test_autocast.py test/test_mps.py torch/amp/autocast_mode.py torch/csrc/jit/passes/autocast.cpp torch/csrc/utils/python_dispatch.cpp,https://github.com/pytorch/pytorch/pull/99272,kulinseth,malfet,,,
df59084012a,skip,not user facing,Drop GIL around cudart APIs (#132520),torch/csrc/cuda/shared/cudart.cpp,https://github.com/pytorch/pytorch/pull/132520,wconstab,LucasLLC,teja-rao,,
3d87dfc0888,skip,not user facing,Add basic OpenReg module scaffolding with autograd (#131708),.flake8 test/cpp_extensions/open_registration_extension/README.md test/cpp_extensions/open_registration_extension/pytorch_openreg/__init__.py test/cpp_extensions/open_registration_extension/pytorch_openreg/csrc/Module.cpp test/cpp_extensions/open_registration_extension/pytorch_openreg/csrc/OpenReg.h test/cpp_extensions/open_registration_extension/pytorch_openreg/csrc/OpenRegHooks.cpp test/cpp_extensions/open_registration_extension/setup.py test/cpp_extensions/open_registration_extension/test/test_openreg.py,https://github.com/pytorch/pytorch/pull/131708,albanD,ezyang,,,
e8645fa2b9e,distributed,not user facing,[Doc] fix some typos (found by codespell and typos) (#132544),aten/src/ATen/Context.cpp aten/src/ATen/core/dispatch/OperatorEntry.cpp docs/cpp/source/notes/tensor_basics.rst docs/source/cond.rst docs/source/distributed.checkpoint.rst docs/source/distributed.pipelining.rst docs/source/export.rst docs/source/fx.rst docs/source/jit_language_reference.rst docs/source/jit_language_reference_v2.rst docs/source/notes/extending.rst docs/source/notes/numerical_accuracy.rst docs/source/rpc/rref.rst docs/source/scripts/exportdb/blurb.txt docs/source/torch.compiler_fine_grain_apis.rst test/test_overrides.py test/test_python_dispatch.py torch/_library/fake_class_registry.py,https://github.com/pytorch/pytorch/pull/132544,wdvr,kit1980,,,
d5045cceff5,jit,Untopiced,[16/N] Fix clang-tidy warnings in jit (#132604),torch/csrc/jit/tensorexpr/half_support.h torch/csrc/jit/tensorexpr/kernel.cpp torch/csrc/jit/tensorexpr/kernel.h torch/csrc/jit/tensorexpr/loopnest.cpp torch/csrc/jit/tensorexpr/reduction.cpp torch/csrc/jit/tensorexpr/reduction.h torch/csrc/jit/tensorexpr/registerizer.cpp torch/csrc/jit/tensorexpr/registerizer.h torch/csrc/jit/tensorexpr/tensor.h torch/csrc/jit/tensorexpr/tensorexpr_init.cpp torch/csrc/jit/tensorexpr/types.cpp torch/csrc/jit/tensorexpr/unique_name_manager.h,https://github.com/pytorch/pytorch/pull/132604,cyyever,Skylion007,,,
baa2483ceaf,skip,Untopiced,"Revert ""Refactor thunkify to return proper thunk abstraction (#132407)""",torch/fx/experimental/proxy_tensor.py,,,,,,
7b2664ece6a,python_frontend,bug fixes,Temp disable MKL in  DistributionKernels.cpp (#132532),aten/src/ATen/native/cpu/DistributionKernels.cpp test/distributions/test_distributions.py,https://github.com/pytorch/pytorch/pull/132532,malfet,albanD,,,
1c7dc335f74,inductor,not user facing,[ROCm][CK][Inductor] Enable addmm for CK backend to gemm max autotune (#130576),test/inductor/test_ck_backend.py torch/_inductor/codegen/rocm/ck_template.py torch/_inductor/codegen/rocm/ck_universal_gemm_template.py torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/config.py torch/_inductor/kernel/mm.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/130576,tenpercent,chenyang78,,,
618e2c9de42,Uncategorized,Untopiced,fix torch rec test failure (#132437),test/export/test_unflatten.py torch/export/unflatten.py,https://github.com/pytorch/pytorch/pull/132437,ydwu4,Skylion007,,,
8d9c3a71f61,dataloader_frontend,not user facing,Support IPC for Expandable Segments (#130890),c10/cuda/CUDACachingAllocator.cpp test/test_dataloader.py test/test_multiprocessing.py torch/csrc/StorageSharing.cpp,https://github.com/pytorch/pytorch/pull/130890,zdevito,dsjohns2,,,
aec948adfc2,skip,Untopiced,[export] Convert autocast to HOO (#131914),test/export/test_passes.py torch/_export/passes/replace_autocast_with_hop_pass.py torch/_export/passes/replace_set_grad_with_hop_pass.py torch/_export/passes/replace_with_hop_pass_util.py torch/_higher_order_ops/wrap.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/131914,yushangdi,angelayi,,,
e1c2bdac2f7,skip,not user facing,[easy] fix f-string messages in torch/_ops.py (#132531),torch/_ops.py,https://github.com/pytorch/pytorch/pull/132531,davidberard98,Skylion007,,,
1d34f33d009,inductor,Untopiced,Scale XBLOCK in triton reduction configs to avoid hitting max grid (#128826),test/inductor/test_torchinductor.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/128826,jataylo,jansel,nmacchioni,,
a3ea96b762c,skip,Untopiced,"Revert ""[export] Convert autocast to HOO (#131914)""",test/export/test_passes.py torch/_export/passes/replace_autocast_with_hop_pass.py torch/_export/passes/replace_set_grad_with_hop_pass.py torch/_export/passes/replace_with_hop_pass_util.py torch/_higher_order_ops/wrap.py torch/export/_trace.py,,,,,,
2764bee942e,skip,Untopiced,"Revert ""[MPS] Add support for autocast in MPS  (#99272)""",aten/src/ATen/autocast_mode.cpp aten/src/ATen/autocast_mode.h aten/src/ATen/core/interned_strings.h c10/core/DispatchKey.cpp c10/core/DispatchKey.h c10/core/DispatchKeySet.h test/test_autocast.py test/test_mps.py torch/amp/autocast_mode.py torch/csrc/jit/passes/autocast.cpp torch/csrc/utils/python_dispatch.cpp,,,,,,
b7bcfdaff21,skip,not user facing,Change deprecate warning on dispatch_on_subclass to warn once (#132374),test/test_overrides.py torch/csrc/utils/python_arg_parser.cpp,https://github.com/pytorch/pytorch/pull/132374,basilwong,ezyang,,,
1471473b84b,sparse_frontend,not user facing,Add tests to bsr_dense_addmm_meta. Tune bsr_dense_addmm kernel for ViT shapes. (#132646),test/test_sparse_csr.py torch/sparse/_triton_ops.py torch/sparse/_triton_ops_meta.py,https://github.com/pytorch/pytorch/pull/132646,pearu,cpuhrsch,,,
6f4dc56735d,inductor,not user facing,[inductor] Default to 1 compile thread for internal (#132540),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/132540,masnesral,c00w,,,
6b12dc0224c,skip,not user facing,[Reland] [11/N] Use std::nullopt and std::optional  (#132622),aten/src/ATen/core/IListRef_test.cpp aten/src/ATen/core/function_schema_inl.h aten/src/ATen/core/op_registration/op_registration.h aten/src/ATen/functorch/ADInterpreters.cpp aten/src/ATen/functorch/BatchRulesConvolution.cpp aten/src/ATen/functorch/BatchRulesHelper.cpp aten/src/ATen/functorch/BatchRulesLoss.cpp aten/src/ATen/functorch/BatchRulesModules.cpp aten/src/ATen/functorch/BatchRulesNorm.cpp aten/src/ATen/functorch/BatchRulesReduceOps.cpp aten/src/ATen/functorch/BatchRulesScatterOps.cpp aten/src/ATen/functorch/BatchRulesViews.cpp aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp aten/src/ATen/functorch/PlumbingHelper.cpp aten/src/ATen/functorch/PlumbingHelper.h aten/src/ATen/native/Bucketization.cpp aten/src/ATen/native/NamedTensor.cpp aten/src/ATen/native/TensorAdvancedIndexing.cpp aten/src/ATen/native/TensorConversions.cpp aten/src/ATen/native/cuda/Bucketization.cu aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/templates/LazyIr.h aten/src/ATen/test/ivalue_test.cpp c10/cuda/CUDAGuard.h functorch/csrc/dim/arena.h functorch/csrc/dim/dim.cpp test/edge/Evalue.h torch/csrc/functorch/init.cpp,https://github.com/pytorch/pytorch/pull/132622,cyyever,ezyang,,,
9a1ad3345fe,skip,not user facing,Fix periodic windows test (#132648),test/test_cuda.py,https://github.com/pytorch/pytorch/pull/132648,albanD,janeyx99,malfet,zou3519,
ee0ae11b341,skip,not user facing,Fix a typo in the example code. (#132601),torch/csrc/autograd/custom_function.h,https://github.com/pytorch/pytorch/pull/132601,csukuangfj,soulitzer,,,
4db368a475d,inductor,Untopiced,make functorch CSE respect mutations as barriers (like fsdp.set_) (#132243),test/dynamo/test_repros.py torch/_functorch/compile_utils.py torch/_inductor/pattern_matcher.py,https://github.com/pytorch/pytorch/pull/132243,bdhirsh,albanD,yf225,zou3519,
1a0db299326,onnx,not user facing,move torch._functionalize APIs to pybind. add one for marking storage mutations (#132337),aten/src/ATen/FunctionalTensorWrapper.h test/onnx/test_fx_to_onnx_with_onnxruntime.py torch/csrc/autograd/python_torch_functions_manual.cpp,https://github.com/pytorch/pytorch/pull/132337,bdhirsh,albanD,justinchuby,,
af8b8a47cb7,distributed,Untopiced,fsdp.set_: convey to functionalization that it mutates storage (#132322),test/dynamo/test_repros.py tools/pyi/gen_pyi.py torch/distributed/_composable/fsdp/_fsdp_param.py,https://github.com/pytorch/pytorch/pull/132322,bdhirsh,albanD,yf225,,
2b5e31d099e,skip,not user facing,Move sigmoid run_const_graph HOP to PyTorch core (#132526),torch/_higher_order_ops/run_const_graph.py,https://github.com/pytorch/pytorch/pull/132526,ezyang,SherlockNoMad,,,
ea42027e0ed,distributed,not user facing,[micro_pipeline_tp] support all _scaled_mm args (#131984),test/distributed/test_symmetric_memory.py torch/_inductor/fx_passes/micro_pipeline_tp.py torch/distributed/_symmetric_memory/__init__.py,https://github.com/pytorch/pytorch/pull/131984,yifuwang,weifengpy,,,
4a2cf50edf0,Uncategorized,Untopiced,[export][reland] Convert autocast to HOO (#132677),test/export/test_export.py test/export/test_passes.py torch/_export/passes/replace_autocast_with_hop_pass.py torch/_export/passes/replace_set_grad_with_hop_pass.py torch/_export/passes/replace_with_hop_pass_util.py torch/_export/utils.py torch/_higher_order_ops/wrap.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/132677,yushangdi,angelayi,,,
a4ed8eeb337,dynamo,not user facing,[hop] makes compiled hops not share code objects (#132427),test/functorch/test_control_flow.py test/inductor/test_flex_attention.py torch/_dynamo/trace_rules.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/while_loop.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/132427,ydwu4,anijain2305,jansel,,
a74e5abda49,skip,not user facing,Fix issues in activation_memory_budget for float8 (#132687),torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/132687,y-sq,Chillee,,,
3ef45e56690,jit,Untopiced,Fix ODR (#131032),torch/csrc/jit/tensorexpr/codegen.cpp torch/csrc/jit/tensorexpr/codegen.h,https://github.com/pytorch/pytorch/pull/131032,cyyever,ezyang,,,
1add8c5f1c4,distributed,not user facing,[Easy][DTensor] Rename args_sharding to args_schema for OpSchema __str__ (#132187),torch/distributed/_tensor/_op_schema.py,https://github.com/pytorch/pytorch/pull/132187,wz337,wanchaol,,,
4306eebab18,distributed,Untopiced,[DeviceMesh] Update slicing documentation to include nD and non-continuous slicing (#132311),torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/132311,wz337,wanchaol,,,
eca0cb0fbe8,skip,Untopiced,Conversions between strided and jagged layouts for Nested Tensors (#115749),aten/src/ATen/FunctionalInverses.cpp aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorFactories.cpp aten/src/ATen/native/nested/NestedTensorMath.cpp test/expect/HasDecompTest.test_has_decomposition.expect test/test_nestedtensor.py tools/autograd/derivatives.yaml tools/autograd/gen_inplace_or_view_type.py torch/_subclasses/fake_impls.py torch/nested/__init__.py torch/nested/_internal/nested_tensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/115749,ani300,jbschlosser,,,
26c67861093,skip,not user facing,return_and_correct_aliasing: skip dispatcher when swapping storage (#132524),tools/pyi/gen_pyi.py torch/csrc/autograd/python_torch_functions_manual.cpp torch/utils/_python_dispatch.py,https://github.com/pytorch/pytorch/pull/132524,bdhirsh,ezyang,,,
96471ea47cd,inductor,not user facing,[inductor] support vectorization for torch.any(bool) -> bool (#132472),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/132472,zhuhaozhe,jgong5,,,
d6a24b3b92e,quantization,not user facing,Removed duplicate `__all__` declarations. (#132405),torch/ao/pruning/sparsifier/base_sparsifier.py,https://github.com/pytorch/pytorch/pull/132405,randolf-scholz,soulitzer,,,
38674bcb452,skip,Untopiced,"Revert ""Conversions between strided and jagged layouts for Nested Tensors (#115749)""",aten/src/ATen/FunctionalInverses.cpp aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorFactories.cpp aten/src/ATen/native/nested/NestedTensorMath.cpp test/expect/HasDecompTest.test_has_decomposition.expect test/test_nestedtensor.py tools/autograd/derivatives.yaml tools/autograd/gen_inplace_or_view_type.py torch/_subclasses/fake_impls.py torch/nested/__init__.py torch/nested/_internal/nested_tensor.py torch/nested/_internal/ops.py,,,,,,
2f16e68cab5,skip,not user facing,"[Intel GPU] Allow XPU device in copy, cdist, index_put_impl (#130088)",aten/src/ATen/native/Copy.cpp aten/src/ATen/native/Distance.cpp aten/src/ATen/native/ReduceOps.cpp aten/src/ATen/native/TensorAdvancedIndexing.cpp,https://github.com/pytorch/pytorch/pull/130088,ZhiweiYan-96,EikanWang,albanD,gujinghui,
93fad2f0f2b,Uncategorized,Untopiced,[export] Fix import in D60427208 (#132707),torch/_export/db/logging.py,https://github.com/pytorch/pytorch/pull/132707,yushangdi,ydwu4,,,
345bea01dc6,fx,not user facing,Refactor thunkify to return proper thunk abstraction (#132407),torch/fx/experimental/proxy_tensor.py torch/utils/_thunk.py,https://github.com/pytorch/pytorch/pull/132407,ezyang,albanD,,,
39c9b75a680,skip,not user facing,Add registration mechanism for aoti model runner (#131638),torch/csrc/inductor/aoti_eager/kernel_holder.cpp torch/csrc/inductor/aoti_runner/model_container_runner.cpp torch/csrc/inductor/aoti_runner/model_container_runner.h,https://github.com/pytorch/pytorch/pull/131638,EikanWang,desertfire,jansel,,
4e610924d4d,distributed,Untopiced,[c10d] Add a new API for adding ephemeral timeout for one local rank and the timeout will reset when the first collective finishes (#130905),test/distributed/test_c10d_nccl.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/init.cpp torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/130905,fduwjj,ezyang,,,
4260f365baa,inductor,not user facing,[inductor] Replace torch.allclose with torch.testing.assert_close in test_fx_fusion (#130618),test/inductor/test_fx_fusion.py,https://github.com/pytorch/pytorch/pull/130618,robert-hardwick,jgong5,malfet,,
91df66ee740,caffe2,Untopiced,[caffe2] Wrap constexpr with preprocessor statements (#132582),aten/src/ATen/native/cuda/PointwiseOpsKernel.cu,https://github.com/pytorch/pytorch/pull/132582,danzimm,houseroad,,,
728374d7f7f,skip,not user facing,Changed create_block_mask to just accept BLOCK_SIZE (#132697),torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/132697,Chillee,drisspg,,,
da320214e66,nn_frontend,docs,Format tensor (#127992),torch/nn/modules/pooling.py,https://github.com/pytorch/pytorch/pull/127992,Dmovic,janeyx99,,,
a8f0979962f,dynamo,Untopiced,Add cudagraph static inputs logging (#132726),test/dynamo/test_logging.py torch/_dynamo/variables/builder.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py torch/_functorch/aot_autograd.py torch/_inductor/compile_fx.py torch/_inductor/cudagraph_utils.py torch/_logging/_internal.py torch/_logging/_registrations.py,https://github.com/pytorch/pytorch/pull/132726,mlazos,anijain2305,,,
656a4d14089,skip,not user facing,[6/N] Fix clang-tidy warnings in aten/src/ATen  (#132620),aten/src/ATen/FunctionalInverses.cpp aten/src/ATen/FunctionalStorageImpl.cpp aten/src/ATen/FunctionalStorageImpl.h aten/src/ATen/FunctionalTensorWrapper.cpp aten/src/ATen/FunctionalTensorWrapper.h aten/src/ATen/native/ConvUtils.h aten/src/ATen/native/Convolution.cpp aten/src/ATen/native/Distributions.cpp,https://github.com/pytorch/pytorch/pull/132620,cyyever,Skylion007,,,
7100c36c8a1,skip,Untopiced,"Revert ""[inductor] export kernel for gemm template. (#132580)""",torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_utils.py,,,,,,
6753ee127cc,cuda,Untopiced,Allow torch.cuda.memory.mem_get_info to take a device str argument with an unspecified device index. (#132616),test/test_cuda_multigpu.py torch/cuda/memory.py,https://github.com/pytorch/pytorch/pull/132616,tringwald,soulitzer,,,
679cdf606a3,fx,not user facing,Converted `__all__` literal tuple to literal list. (#132404),torch/fx/experimental/unification/unification_tools.py,https://github.com/pytorch/pytorch/pull/132404,randolf-scholz,soulitzer,,,
fc7849b93fe,skip,Untopiced,[pt2e][quant] Ensure BN node is erased after convert (#131651),test/quantization/pt2e/test_quantize_pt2e_qat.py torch/ao/quantization/pt2e/utils.py,https://github.com/pytorch/pytorch/pull/131651,andrewor14,leslie-fang-intel,yushangdi,,
c803e35c4bb,cuda,Untopiced,Reduce number of guards introduced by check_cudnn_tensor_shapes when cudnn version is higher enough (#132384),aten/src/ATen/native/transformers/cuda/sdp_utils.cpp,https://github.com/pytorch/pytorch/pull/132384,chengzeyi,Skylion007,eqy,,
ca7ce2fca17,skip,not user facing,[ts-migration][1/N]: Add prim::Loop for constant number of iterations and condition (#131418),test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/131418,jiashenC,angelayi,,,
7045bc5a77b,fx,Untopiced,[export] change error message for specializations (#132698),test/export/test_export.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/132698,pianpwk,avikchaudhuri,,,
ffdf48e63b9,releng,not user facing,Consolidate SymDispatchMode into ProxyTensorMode (#132674),.github/labeler.yml docs/source/conf.py docs/source/fx.experimental.rst docs/source/fx.rst torch/_dynamo/compiled_autograd.py torch/_dynamo/convert_frame.py torch/fx/experimental/_sym_dispatch_mode.py torch/fx/experimental/proxy_tensor.py torch/fx/experimental/sym_node.py,https://github.com/pytorch/pytorch/pull/132674,ezyang,bdhirsh,zou3519,,
a94c441e48e,fx,not user facing,Fix symbolic nested int printing (#131916),test/dynamo/test_subclasses.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/131916,soulitzer,Skylion007,jbschlosser,,
406b50835b6,skip,not user facing,Use FakeTensor cache for subclass inner tensors (#131803),test/dynamo/test_subclasses.py test/dynamo_expected_failures/TestAOTAutograd.test_input_mutation_false_aliasing test/functorch/test_aotdispatch.py torch/_subclasses/meta_utils.py,https://github.com/pytorch/pytorch/pull/131803,soulitzer,ezyang,,,
f50621989b8,fx,Untopiced,Construct NJT without graph breaks (#130292),test/dynamo/test_subclasses.py test/test_nestedtensor.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py torch/_subclasses/fake_tensor.py torch/_subclasses/functional_tensor.py torch/_subclasses/meta_utils.py torch/fx/passes/fake_tensor_prop.py torch/nested/_internal/nested_tensor.py torch/nested/_internal/ops.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/130292,soulitzer,bdhirsh,,,
e6eee048750,distributed,Untopiced,dynamo: use equality guards instead of id guards for Placement/DeviceMesh (#124401),test/distributed/_composable/fsdp/test_fully_shard_init.py test/distributed/_tensor/test_dtensor_compile.py torch/_dynamo/guards.py torch/_dynamo/variables/builder.py torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/124401,bdhirsh,anijain2305,,,
af67b8df6d4,skip,not user facing,[export] Fix exportdb test (#132678),test/export/test_serialize.py,https://github.com/pytorch/pytorch/pull/132678,yushangdi,zhxchen17,,,
e3394e5548a,autograd_frontend,Untopiced,"torch.autograd.graph.increment_version: accept List[Tensor], use in AOTDispatcher (#132652)",test/test_autograd.py torch/_C/__init__.pyi.in torch/_functorch/_aot_autograd/runtime_wrappers.py torch/autograd/graph.py torch/csrc/autograd/init.cpp,https://github.com/pytorch/pytorch/pull/132652,bdhirsh,albanD,,,
c6582f11cd6,releng,not user facing,Add get_optin_feature() to allow opt-in to amz2023 (#131792),.github/scripts/runner_determinator.py .github/workflows/_runner-determinator.yml,https://github.com/pytorch/pytorch/pull/131792,zxiiro,ZainRizvi,jeanschmidt,,
8d2c272e5ac,composability,Untopiced,properly register conjugate/neg fallthroughs to prim ops (#132699),test/test_prims.py torch/_prims/__init__.py,https://github.com/pytorch/pytorch/pull/132699,bdhirsh,albanD,,,
db0bd041512,inductor,Untopiced,[AOTI] Switch to use shim v2 for fbcode (#132750),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/132750,desertfire,angelayi,,,
1de4ebc85db,quantization,Untopiced,[Quantizer] Fix Maxpool2d share q params (#132704),torch/ao/quantization/quantizer/xnnpack_quantizer.py torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py,https://github.com/pytorch/pytorch/pull/132704,mcr229,jerryzh168,,,
c2bccfd4311,skip,not user facing,[BE] Simplify code interacting with get_proxy_mode/enable_tracing (#132675),torch/_export/wrappers.py torch/_higher_order_ops/associative_scan.py torch/_higher_order_ops/auto_functionalize.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/effects.py torch/_higher_order_ops/flex_attention.py torch/_higher_order_ops/map.py torch/_higher_order_ops/out_dtype.py torch/_higher_order_ops/run_const_graph.py torch/_higher_order_ops/strict_mode.py torch/_higher_order_ops/torchbind.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_higher_order_ops/while_loop.py torch/_prims/rng_prims.py torch/distributed/_functional_collectives.py,https://github.com/pytorch/pytorch/pull/132675,ezyang,Skylion007,ydwu4,zou3519,
81a5a7a30a8,quantization,Untopiced,[Quantizer] Fix getattr for quantizing constants (#132705),torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py,https://github.com/pytorch/pytorch/pull/132705,mcr229,jerryzh168,,,
1e65ccc3def,skip,not user facing,[inductor] export kernel for gemm template. (#132580),torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_gemm_template.py,https://github.com/pytorch/pytorch/pull/132580,xuhancn,ezyang,,,
4faa5804f61,distributed,Untopiced,[c10d] Used float tensor for PG NCCL barrier all-reduce (#132701),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/132701,awgu,fegin,wanchaol,,
775c310c0c5,Uncategorized,Untopiced,Preserve source_fn_stack in the training IR decomp (#132033),test/export/test_export.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/132033,tugsbayasgalan,angelayi,,,
1954bfacda7,inductor,not user facing,"[Inductor] Small performance, precision, and dependency updates to B2B-GEMM (#132354)",test/inductor/test_b2b_gemm.py torch/_inductor/fx_passes/b2b_gemm.py,https://github.com/pytorch/pytorch/pull/132354,sdingcn,masnesral,,,
de00c795830,dynamo,not user facing,[dynamo][inline_inbuilt_nn_modules] Mark nn module tensor static for cudagraphs (#132736),test/dynamo/test_modules.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/_guards.py,https://github.com/pytorch/pytorch/pull/132736,anijain2305,mlazos,,,
94155ce31bf,Uncategorized,Untopiced,[Torch] Support meta device in checkpoint (#132684),torch/utils/checkpoint.py,https://github.com/pytorch/pytorch/pull/132684,silverlakeli,kit1980,,,
e47b684c334,skip,Untopiced,"Revert ""Temp disable MKL in  DistributionKernels.cpp (#132532)""",aten/src/ATen/native/cpu/DistributionKernels.cpp test/distributions/test_distributions.py,,,,,,
029f8fc701c,skip,not user facing,Bump rexml from 3.2.8 to 3.3.3 in /ios/TestApp (#132469),ios/TestApp/Gemfile.lock,https://github.com/pytorch/pytorch/pull/132469,dependabot,ezyang,,,
2f908ffa4a2,sparse_frontend,Untopiced,[traced-graph][sparse] sparsity propagation for all current tests (#132690),aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp test/export/test_sparse.py torch/_decomp/decompositions.py,https://github.com/pytorch/pytorch/pull/132690,aartbik,ezyang,,,
623d0204f07,autograd_frontend,Untopiced,[NJT] Support Chunk backward for simple cases (#132193),test/test_nestedtensor.py tools/autograd/derivatives.yaml torch/csrc/autograd/FunctionsManual.cpp torch/csrc/autograd/FunctionsManual.h torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/132193,YuqingJ,soulitzer,,,
2073ddfd1c9,skip,not user facing,Actually report the HOP and subclass/mode when there isn't a registration (#132550),test/dynamo/test_higher_order_ops.py torch/_ops.py,https://github.com/pytorch/pytorch/pull/132550,zou3519,ydwu4,,,
3c1033eeb0c,releng,not user facing,Don't auto request review for reopened PRs (#132681),.github/workflows/auto_request_review.yml,https://github.com/pytorch/pytorch/pull/132681,ezyang,albanD,malfet,,
18b678082ef,inductor,not user facing,[Easy] log output code path on cache hit (#132718),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/132718,eellison,masnesral,oulgen,,
1a23ef2ece1,skip,not user facing,[DeviceMesh] Create new group for 1D mesh when default backend is 'gloo' and 'cuda' is available (#132709),test/distributed/test_device_mesh.py torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/132709,wz337,awgu,wanchaol,,
073cee531c9,distributed,not user facing,[Test][Easy] Remove print in test_device_mesh.py (#132780),test/distributed/test_device_mesh.py,https://github.com/pytorch/pytorch/pull/132780,wz337,XilunWu,,,
825002c9c6a,quantization,Untopiced,[export][fx] More robust DCE pass (#132764),test/fx/test_dce_pass.py test/quantization/pt2e/test_x86inductor_quantizer.py torch/_export/serde/serialize.py torch/_export/utils.py torch/ao/quantization/pt2e/qat_utils.py torch/export/_remove_effect_tokens_pass.py torch/export/_trace.py torch/export/_unlift.py torch/fx/node.py torch/fx/proxy.py,https://github.com/pytorch/pytorch/pull/132764,yushangdi,angelayi,,,
179b572fd99,skip,Untopiced,[2/3] 3D Composability - move pp tests (#129801),.ci/pytorch/multigpu-test.sh test/distributed/_composable/test_composability/test_pp_composability.py test/distributed/pipelining/test_composability.py,https://github.com/pytorch/pytorch/pull/129801,mori360,atalman,wconstab,,
cd7f527c59f,skip,Untopiced,[3/3] 3D Composability - move tp dp tests (#129802),.ci/pytorch/multigpu-test.sh .ci/pytorch/test.sh test/distributed/_composable/test_composability/test_2d_composability.py test/distributed/tensor/parallel/test_ddp_2d_parallel.py test/distributed/tensor/parallel/test_fsdp_2d_parallel.py,https://github.com/pytorch/pytorch/pull/129802,mori360,fduwjj,,,
0d6caeb2592,dynamo,not user facing,Add logging + counter for missed reinplacing opportunities (#132758),torch/_dynamo/convert_frame.py torch/_dynamo/utils.py torch/_inductor/fx_passes/reinplace.py,https://github.com/pytorch/pytorch/pull/132758,zou3519,eellison,,,
c7113a61861,skip,Untopiced,"Revert ""[DeviceMesh] Create new group for 1D mesh when default backend is 'gloo' and 'cuda' is available (#132709)""",test/distributed/test_device_mesh.py torch/distributed/device_mesh.py,,,,,,
e98eac76b35,inductor,not user facing,[inductor] switch AotCodeCompiler to new cpp_builder. (take 3) (#132766),torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/132766,henrylhtsang,chenyang78,desertfire,xuhancn,
cbee9c1fd2b,skip,Untopiced,"Revert ""Deprecate `torch._utils.is_compiling()` and `torch._dynamo.external_utils.is_compiling()` (#127690)""",test/dynamo/test_skip_non_tensor.py test/export/test_torchbind.py test/functorch/test_memory_efficient_fusion.py test/inductor/test_distributed_patterns.py test/test_optim.py torch/_dynamo/decorators.py torch/_dynamo/external_utils.py torch/_functorch/apis.py torch/_functorch/eager_transforms.py torch/_higher_order_ops/associative_scan.py torch/_utils.py torch/distributed/_composable/fsdp/_fsdp_common.py torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py torch/distributed/tensor/parallel/_utils.py torch/nn/parallel/distributed.py torch/optim/_adafactor.py torch/optim/adadelta.py torch/optim/adagrad.py torch/optim/adam.py torch/optim/adamax.py torch/optim/adamw.py torch/optim/asgd.py torch/optim/nadam.py torch/optim/optimizer.py torch/optim/radam.py torch/optim/rmsprop.py torch/optim/rprop.py torch/optim/sgd.py torch/testing/_internal/optests/generate_tests.py,,,,,,
cb4d1bfb715,skip,not user facing,Clean up some tflop calc and add option for saving (#132799),benchmarks/transformer/score_mod.py,https://github.com/pytorch/pytorch/pull/132799,drisspg,BoyuanFeng,,,
178dc0c9c7c,skip,not user facing,various doc fixes (#132803),torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/132803,drisspg,BoyuanFeng,Chillee,joydddd,
b01402b0a4c,quantization,Untopiced,[7/N] Fix clang-tidy warnings in aten/src/ATen (#132727),aten/src/ATen/native/quantized/cudnn/BinaryOps.cpp aten/src/ATen/native/quantized/cudnn/Conv.cpp aten/src/ATen/native/quantized/cudnn/ConvPrepack.cpp aten/src/ATen/native/quantized/cudnn/Linear.cpp aten/src/ATen/native/quantized/cudnn/LinearPrepack.cpp aten/src/ATen/native/quantized/cudnn/Pooling.cpp aten/src/ATen/native/quantized/cudnn/utils.h,https://github.com/pytorch/pytorch/pull/132727,cyyever,Skylion007,,,
837898d9c84,inductor,not user facing,Stop using preserve_rng_state as decorator (#132774),.lintrunner.toml torch/_inductor/codegen/triton.py torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/132774,ezyang,albanD,,,
ed224554eb0,skip,not user facing,[BE] Don't unnecessarily suggest -k for rerunning tests locally (#132807),torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/132807,ezyang,malfet,,,
73c083e02cb,skip,not user facing,[Inductor][CPP] Turns on inline_inbuilt_nn_modules for CPP GEMM template testing (#132487),test/inductor/test_cpu_select_algorithm.py,https://github.com/pytorch/pytorch/pull/132487,leslie-fang-intel,anijain2305,jgong5,,
063a45ed27c,skip,not user facing,Fix infinite recursion while walking to submodules (#132763),torch/__init__.py,https://github.com/pytorch/pytorch/pull/132763,XuehaiPan,ezyang,,,
919e3842472,inductor,Untopiced,[PT2][Optimus] Add unbind_stack_to_cat_pass (#132542),test/inductor/test_split_cat_fx_passes.py torch/_inductor/fx_passes/group_batch_fusion.py torch/_inductor/fx_passes/split_cat.py,https://github.com/pytorch/pytorch/pull/132542,mengluy0125,jackiexu1992,,,
03480213dea,quantization,not user facing,[8/N] Fix clang-tidy warnings in aten/src/ATen (#132728),aten/src/ATen/native/quantized/AffineQuantizer.cpp aten/src/ATen/native/quantized/AffineQuantizer.h aten/src/ATen/native/quantized/AffineQuantizerBase.cpp aten/src/ATen/native/quantized/Copy.cpp aten/src/ATen/native/quantized/FakeQuantPerChannelAffine.cpp aten/src/ATen/native/quantized/FakeQuantPerTensorAffine.cpp aten/src/ATen/native/quantized/QTensor.cpp aten/src/ATen/native/quantized/TensorAdvancedIndexing.cpp aten/src/ATen/native/quantized/TensorCompare.cpp aten/src/ATen/native/quantized/TensorFactories.cpp aten/src/ATen/native/quantized/qconv_unpack.cpp aten/src/ATen/native/quantized/qlinear_unpack.cpp,https://github.com/pytorch/pytorch/pull/132728,cyyever,ezyang,,,
bfeb45e46b0,jit,Untopiced,[17/N] Fix clang-tidy warnings in jit  (#132753),torch/csrc/jit/passes/concat_opt.cpp torch/csrc/jit/passes/constant_propagation.cpp torch/csrc/jit/passes/create_autodiff_subgraphs.cpp torch/csrc/jit/passes/dead_code_elimination.cpp torch/csrc/jit/passes/eliminate_no_ops.cpp torch/csrc/jit/passes/eliminate_no_ops.h torch/csrc/jit/passes/erase_number_types.cpp torch/csrc/jit/passes/erase_number_types.h torch/csrc/jit/passes/fixup_trace_scope_blocks.cpp torch/csrc/jit/passes/fixup_trace_scope_blocks.h torch/csrc/jit/passes/fold_conv_bn.cpp torch/csrc/jit/passes/fold_conv_bn.h torch/csrc/jit/passes/fold_linear_bn.cpp torch/csrc/jit/passes/fold_linear_bn.h torch/csrc/jit/passes/freeze_module.cpp torch/csrc/jit/passes/freeze_module.h torch/csrc/jit/passes/frozen_concat_linear.cpp torch/csrc/jit/passes/frozen_concat_linear.h torch/csrc/jit/passes/frozen_conv_add_relu_fusion.cpp torch/csrc/jit/passes/frozen_conv_add_relu_fusion.h torch/csrc/jit/passes/frozen_conv_add_relu_fusion_cuda.cpp torch/csrc/jit/passes/frozen_conv_folding.cpp torch/csrc/jit/passes/frozen_conv_folding.h torch/csrc/jit/passes/frozen_graph_optimizations.cpp torch/csrc/jit/passes/frozen_graph_optimizations.h torch/csrc/jit/passes/frozen_linear_folding.cpp torch/csrc/jit/passes/frozen_linear_folding.h torch/csrc/jit/passes/frozen_linear_transpose.cpp torch/csrc/jit/passes/frozen_linear_transpose.h torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp torch/csrc/jit/passes/frozen_ops_to_mkldnn.h torch/csrc/jit/passes/fuse_linear.cpp torch/csrc/jit/passes/fuse_linear.h torch/csrc/jit/passes/fuse_relu.cpp torch/csrc/jit/passes/fuse_relu.h torch/csrc/jit/passes/graph_fuser.cpp torch/csrc/jit/passes/graph_fuser.h torch/csrc/jit/passes/graph_rewrite_helper.cpp torch/csrc/jit/passes/graph_rewrite_helper.h torch/csrc/jit/passes/guard_elimination.cpp torch/csrc/jit/passes/guard_elimination.h torch/csrc/jit/passes/hoist_conv_packed_params.cpp torch/csrc/jit/passes/hoist_conv_packed_params.h torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp torch/csrc/jit/passes/inline_autodiff_subgraphs.h torch/csrc/jit/passes/inline_fork_wait.cpp torch/csrc/jit/passes/inline_fork_wait.h torch/csrc/jit/passes/inline_forked_closures.cpp torch/csrc/jit/passes/inline_forked_closures.h torch/csrc/jit/passes/inliner.cpp torch/csrc/jit/passes/inliner.h torch/csrc/jit/passes/inplace_check.cpp torch/csrc/jit/passes/inplace_check.h torch/csrc/jit/passes/insert_guards.cpp torch/csrc/jit/passes/insert_guards.h torch/csrc/jit/passes/integer_value_refinement.cpp torch/csrc/jit/passes/integer_value_refinement.h torch/csrc/jit/passes/lift_closures.cpp torch/csrc/jit/passes/lift_closures.h torch/csrc/jit/passes/liveness.cpp torch/csrc/jit/passes/liveness.h torch/csrc/jit/passes/loop_unrolling.cpp torch/csrc/jit/passes/loop_unrolling.h torch/csrc/jit/passes/lower_grad_of.cpp torch/csrc/jit/passes/lower_grad_of.h torch/csrc/jit/passes/lower_graph.cpp torch/csrc/jit/passes/lower_graph.h torch/csrc/jit/passes/lower_tuples.cpp torch/csrc/jit/passes/lower_tuples.h torch/csrc/jit/passes/metal_rewrite.cpp torch/csrc/jit/passes/metal_rewrite.h torch/csrc/jit/passes/mkldnn_rewrite.cpp torch/csrc/jit/passes/mkldnn_rewrite.h torch/csrc/jit/passes/normalize_ops.cpp torch/csrc/jit/passes/normalize_ops.h torch/csrc/jit/passes/onednn_graph_fuser.h torch/csrc/jit/passes/onnx.cpp torch/csrc/jit/passes/onnx.h torch/csrc/jit/passes/pass_manager.cpp torch/csrc/jit/passes/pass_manager.h torch/csrc/jit/passes/peephole.cpp torch/csrc/jit/passes/peephole.h torch/csrc/jit/passes/peephole_alias_sensitive.cpp torch/csrc/jit/passes/peephole_alias_sensitive.h torch/csrc/jit/passes/peephole_dict_idioms.cpp torch/csrc/jit/passes/peephole_dict_idioms.h torch/csrc/jit/passes/peephole_list_idioms.cpp torch/csrc/jit/passes/peephole_list_idioms.h torch/csrc/jit/passes/peephole_non_tensor.cpp torch/csrc/jit/passes/peephole_non_tensor.h torch/csrc/jit/passes/prepack_folding.cpp torch/csrc/jit/passes/prepack_folding.h torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp torch/csrc/jit/passes/refine_tuple_types.cpp torch/csrc/jit/passes/refine_tuple_types.h torch/csrc/jit/passes/remove_dropout.cpp torch/csrc/jit/passes/remove_dropout.h torch/csrc/jit/passes/remove_exceptions.cpp torch/csrc/jit/passes/remove_exceptions.h torch/csrc/jit/passes/remove_expands.cpp torch/csrc/jit/passes/remove_expands.h torch/csrc/jit/passes/remove_inplace_ops.cpp torch/csrc/jit/passes/remove_inplace_ops.h torch/csrc/jit/passes/remove_mutation.cpp torch/csrc/jit/passes/remove_mutation.h torch/csrc/jit/passes/remove_redundant_profiles.cpp torch/csrc/jit/passes/remove_redundant_profiles.h torch/csrc/jit/passes/replacement_of_old_operators.cpp torch/csrc/jit/passes/replacement_of_old_operators.h torch/csrc/jit/passes/requires_grad_analysis.cpp torch/csrc/jit/passes/requires_grad_analysis.h torch/csrc/jit/passes/restore_mutation.cpp torch/csrc/jit/passes/restore_mutation.h torch/csrc/jit/passes/tensorexpr_fuser.cpp torch/csrc/jit/passes/update_differentiable_graph_requires_grad.cpp torch/csrc/jit/passes/update_differentiable_graph_requires_grad.h torch/csrc/jit/passes/value_refinement_utils.cpp torch/csrc/jit/passes/value_refinement_utils.h torch/csrc/jit/passes/variadic_ops.cpp torch/csrc/jit/passes/variadic_ops.h torch/csrc/jit/passes/vulkan_rewrite.cpp torch/csrc/jit/passes/vulkan_rewrite.h torch/csrc/jit/passes/xnnpack_rewrite.cpp torch/csrc/jit/passes/xnnpack_rewrite.h,https://github.com/pytorch/pytorch/pull/132753,cyyever,Skylion007,,,
527f104a699,cuda,Untopiced,add L2 cache size to device properties (#132819),torch/_C/__init__.pyi.in torch/csrc/cuda/Module.cpp,https://github.com/pytorch/pytorch/pull/132819,nmacchioni,eellison,,,
8bc5ef563ea,distributed,Untopiced,Grouped Query Attention (#132689),aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/attention.h aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.h test/cpp_extensions/open_registration_extension.cpp test/distributed/_tensor/test_matrix_ops.py test/dynamo/test_sdpa.py test/test_transformers.py tools/pyi/gen_pyi.py torch/_C/__init__.pyi.in torch/_dynamo/variables/sdpa.py torch/csrc/Module.cpp torch/nested/_internal/sdpa.py torch/nn/attention/bias.py torch/nn/functional.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/132689,jainapurva,drisspg,,,
4faa0e3efb1,inductor,Untopiced,[Inductor] support masked vectorization for the tail_loop (#126526),aten/src/ATen/cpu/vec/vec_n.h test/inductor/test_cpu_repro.py test/inductor/test_torchinductor_opinfo.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_prefix.h,https://github.com/pytorch/pytorch/pull/126526,jiayisunx,jansel,jgong5,leslie-fang-intel,
2206a3de004,dynamo,performance,[Compile] Speedup int8-to-float conversion on aarch64 (#132676),aten/src/ATen/cpu/vec/vec256/vec256_convert.h aten/src/ATen/cpu/vec/vec256/vec256_qint.h,https://github.com/pytorch/pytorch/pull/132676,malfet,desertfire,,,
dc00eeb0f4d,dynamo,Untopiced,[Dynamo] fix incorrect kwargs in create_proxy (#132723),torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/132723,leslie-fang-intel,aorenste,,,
87053132eab,distributed,not user facing,[DeviceMesh] Remove parent mesh concept from _MeshEnv and replace by root mesh (#132339),test/distributed/test_device_mesh.py torch/distributed/_composable/fsdp/_fsdp_param.py torch/distributed/_composable/replicate.py torch/distributed/device_mesh.py torch/distributed/fsdp/_init_utils.py torch/distributed/fsdp/_shard_utils.py torch/distributed/fsdp/_state_dict_utils.py torch/distributed/tensor/parallel/_utils.py torch/distributed/tensor/parallel/fsdp.py torch/nn/parallel/distributed.py,https://github.com/pytorch/pytorch/pull/132339,wz337,wanchaol,,,
c184ac0f6b6,skip,Untopiced,Add support for other backends in get_preferred_device (#132118),torch/distributed/_shard/sharded_tensor/api.py,https://github.com/pytorch/pytorch/pull/132118,jeejakp12,,,,
c8c964f9504,inductor,Untopiced,[inductor] check best templates first for fusions (#132829),torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/132829,nmacchioni,eellison,,,
8333ecf0854,dynamo,not user facing,Support hasattr tracing for more PythonModuleVariable (#132731),torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/132731,xinyu-intel,EikanWang,yanboliang,,
37ab0f33854,skip,not user facing,Loads .pyd instead of .so in MemPool test for windows (#132749),test/test_cuda.py torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/132749,syed-ahmed,albanD,,,
8c8eb9670a1,releng,not user facing,[CI] Enable inductor UT test on avx512 (#132645),.ci/pytorch/test.sh .github/workflows/inductor.yml,https://github.com/pytorch/pytorch/pull/132645,zxd1997066,desertfire,,,
32f9a809c78,inductor,bug fixes,Replace [[unlikely]] with unlikely(x) (#130816),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/130816,Danielmic,jansel,jgong5,malfet,
bb99008c9e7,fx,not user facing,Only thunkify proxies in some situations (#132421),docs/source/fx.experimental.rst test/dynamo/test_export.py test/export/test_export.py test/export/test_torchbind.py test/functorch/test_aotdispatch.py test/test_proxy_tensor.py torch/_dynamo/output_graph.py torch/_functorch/_aot_autograd/traced_function_transforms.py torch/fx/experimental/proxy_tensor.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/132421,ezyang,Skylion007,zou3519,,
7c79e89bc5b,dynamo,not user facing,Stop using clear_frame as decorator (#132778),.lintrunner.toml torch/_dynamo/output_graph.py,https://github.com/pytorch/pytorch/pull/132778,ezyang,albanD,,,
fb146fc3c62,dynamo,not user facing,Only store necessary tensor_dict fields in node meta (#132805),test/dynamo/test_decorators.py test/dynamo/test_misc.py test/dynamo/test_repros.py torch/_dynamo/output_graph.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/132805,jbschlosser,mlazos,,,
ed97fb77f9a,skip,Untopiced,Conversions between strided and jagged layouts for Nested Tensors (#115749),aten/src/ATen/FunctionalInverses.cpp aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorFactories.cpp aten/src/ATen/native/nested/NestedTensorMath.cpp test/expect/HasDecompTest.test_has_decomposition.expect test/test_nestedtensor.py tools/autograd/derivatives.yaml tools/autograd/gen_inplace_or_view_type.py torch/_subclasses/fake_impls.py torch/nested/__init__.py torch/nested/_internal/nested_tensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/115749,ani300,jbschlosser,,,
13fa59580e4,skip,not user facing,Enable clang-tidy on aten/src/ATen/cpu (#132830),.lintrunner.toml aten/src/ATen/cpu/Utils.cpp aten/src/ATen/cpu/vec/vec.h aten/src/ATen/cpu/vec/vec_base.h aten/src/ATen/cpu/vec/vec_convert.h aten/src/ATen/cpu/vml.h,https://github.com/pytorch/pytorch/pull/132830,cyyever,Skylion007,,,
de9b8a42c1b,skip,Untopiced,"Revert ""Add support for other backends in get_preferred_device (#132118)""",torch/distributed/_shard/sharded_tensor/api.py,,,,,,
780310fed7f,skip,Untopiced,"Revert ""Only thunkify proxies in some situations (#132421)""",docs/source/fx.experimental.rst test/dynamo/test_export.py test/export/test_export.py test/export/test_torchbind.py test/functorch/test_aotdispatch.py test/test_proxy_tensor.py torch/_dynamo/output_graph.py torch/_functorch/_aot_autograd/traced_function_transforms.py torch/fx/experimental/proxy_tensor.py torch/fx/experimental/symbolic_shapes.py,,,,,,
258f47fc0b3,cpp_frontend,Untopiced,"Add `padding_side` to `pad_sequence` with `""left""` and `""right""` options (`""right""` as default) (#131884)",aten/src/ATen/native/PackedSequence.cpp aten/src/ATen/native/native_functions.yaml test/cpp/api/nn_utils.cpp test/nn/test_packed_sequence.py test/test_jit.py torch/_C/_nn.pyi.in torch/csrc/api/include/torch/nn/utils/rnn.h torch/nn/utils/rnn.py,https://github.com/pytorch/pytorch/pull/131884,ringohoffman,ezyang,,,
8b50d5398fa,skip,not user facing,[DeviceMesh] Create new group for 1D mesh when default backend is 'gloo' and 'cuda' is available (#132709),test/distributed/test_device_mesh.py torch/distributed/device_mesh.py torch/testing/_internal/distributed/distributed_test.py,https://github.com/pytorch/pytorch/pull/132709,wz337,awgu,wanchaol,,
ffd0d92c189,inductor,not user facing,fix autotuning init issues (#132837),torch/_inductor/kernel/flex_attention.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/132837,Chillee,yanboliang,,,
32a284c2754,skip,not user facing,[9/N] Fix clang-tidy warnings in aten/src/ATen  (#132842),aten/src/ATen/native/cpu/Loops.h,https://github.com/pytorch/pytorch/pull/132842,cyyever,Skylion007,,,
a62710c8203,distributed,not user facing,[FSDP2] Relaxed overlap test to address CI flakiness (#132869),test/distributed/_composable/fsdp/test_fully_shard_overlap.py,https://github.com/pytorch/pytorch/pull/132869,awgu,weifengpy,,,
123d9ec5bfc,skip,Untopiced,"Revert ""Loads .pyd instead of .so in MemPool test for windows (#132749)""",test/test_cuda.py torch/utils/cpp_extension.py,,,,,,
92a17f454ae,distributed,not user facing,[1/N][dtensor] introduce StridedShard placement type and _split_tensor() logic (#126697),test/distributed/_tensor/test_utils.py torch/distributed/_tensor/api.py torch/distributed/_tensor/placement_types.py,https://github.com/pytorch/pytorch/pull/126697,XilunWu,wanchaol,,,
0b0c660c022,distributed,not user facing,[2/N][dtensor] Strided Sharding shard_to_replicate (#130239),test/distributed/_tensor/test_utils.py torch/distributed/_tensor/placement_types.py,https://github.com/pytorch/pytorch/pull/130239,XilunWu,wanchaol,,,
ad0ce890509,distributed,not user facing,[3/N][dtensor] Strided Sharding offset calculation util (#132391),test/distributed/_tensor/test_utils.py torch/distributed/_tensor/_utils.py torch/distributed/_tensor/placement_types.py,https://github.com/pytorch/pytorch/pull/132391,XilunWu,wanchaol,,,
40ce0a53bb4,distributed,not user facing,[FSDP][dtensor] add FSDP2+TP distributed state dict test (#131408),test/distributed/_composable/test_composability/test_2d_composability.py test/distributed/checkpoint/fsdp/test_fsdp_dsd.py torch/distributed/_composable/fsdp/_fsdp_param_group.py,https://github.com/pytorch/pytorch/pull/131408,XilunWu,fegin,,,
25df063f044,dynamo,not user facing,[dynamo][user_defined][stable-diffusion] Raise ObservedAttributeError on UserDefinedObject var_getattr (#132806),test/dynamo/test_exceptions.py test/dynamo_expected_failures/PackedSequenceTest.test_pack_sequence torch/_dynamo/exc.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/132806,anijain2305,williamwen42,,,
17490250812,skip,Untopiced,"Revert ""Fix infinite recursion while walking to submodules (#132763)""",torch/__init__.py,,,,,,
f2ad3c89b0a,python_frontend,improvements,fix dtype mismatch in lobpcg eigen solver (#132762),torch/_lobpcg.py,https://github.com/pytorch/pytorch/pull/132762,sebastroy,albanD,,,
9d476fee537,skip,Untopiced,"Revert ""[BE] Simplify code interacting with get_proxy_mode/enable_tracing (#132675)""",torch/_export/wrappers.py torch/_higher_order_ops/associative_scan.py torch/_higher_order_ops/auto_functionalize.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/effects.py torch/_higher_order_ops/flex_attention.py torch/_higher_order_ops/map.py torch/_higher_order_ops/out_dtype.py torch/_higher_order_ops/run_const_graph.py torch/_higher_order_ops/strict_mode.py torch/_higher_order_ops/torchbind.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_higher_order_ops/while_loop.py torch/_prims/rng_prims.py torch/distributed/_functional_collectives.py,,,,,,
a9ff190867d,skip,Untopiced,"Revert ""Consolidate SymDispatchMode into ProxyTensorMode (#132674)""",.github/labeler.yml docs/source/conf.py docs/source/fx.experimental.rst docs/source/fx.rst torch/_dynamo/compiled_autograd.py torch/_dynamo/convert_frame.py torch/fx/experimental/_sym_dispatch_mode.py torch/fx/experimental/proxy_tensor.py torch/fx/experimental/sym_node.py,,,,,,
ca713b8393a,jit,not user facing,llvm update for backward-breaking APIs in 18 and 19 (#132825),torch/csrc/jit/tensorexpr/llvm_codegen.cpp torch/csrc/jit/tensorexpr/llvm_jit.cpp,https://github.com/pytorch/pytorch/pull/132825,jeffdaily,Skylion007,dcci,,
07551887b8f,skip,Untopiced,"Revert ""Disable SymDispatchMode when torch.compile'ing (#132433)""",torch/_dynamo/convert_frame.py torch/fx/experimental/_sym_dispatch_mode.py,,,,,,
26b0011fb86,profiler,new features,[XPU][Kineto Submodule] Introduce kineto-based XPU profiler (#130811),cmake/Dependencies.cmake third_party/kineto,https://github.com/pytorch/pytorch/pull/130811,zejun-chen,aaronenyeshi,,,
4fe6a5dc349,releng,Untopiced,Move slow tests to be in repo (#132379),.github/workflows/weekly.yml .gitignore scripts/release/apply-release-changes.sh test/slow_tests.json tools/stats/import_test_stats.py tools/testing/test_selections.py tools/testing/update_slow_tests.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/132379,clee2000,huydhn,,,
ff81ca8e0cb,skip,Untopiced,"Revert ""Populate submodules of `torch._C` to `sys.modules` recursively (#132216)""",test/dynamo/test_frame_init.py test/xpu/test_conv.py torch/_C/__init__.pyi.in torch/__init__.py torch/_dynamo/convert_frame.py torch/_dynamo/eval_frame.py torch/_dynamo/guards.py torch/_dynamo/types.py torch/_dynamo/utils.py,,,,,,
83fa7f871f4,cuda,Untopiced,Work around item non-sync issue on AMD (#132772),aten/src/ATen/native/cuda/CUDAScalar.cu,https://github.com/pytorch/pytorch/pull/132772,houseroad,ZhengkaiZ,izaitsevfb,,
7ea8374c0ee,nn_frontend,not user facing,`nn.ModuleList.__getitem__` overloads (#132834),torch/nn/modules/container.py,https://github.com/pytorch/pytorch/pull/132834,ringohoffman,Skylion007,albanD,,
59bbaea3a7d,inductor,not user facing,[inductor] disable capture_pre_autograd_graph related UTs on Windows (#132848),test/inductor/test_cpu_cpp_wrapper.py,https://github.com/pytorch/pytorch/pull/132848,xuhancn,desertfire,jgong5,,
78303736621,skip,not user facing,Update owner for BC test (#132891),CODEOWNERS,https://github.com/pytorch/pytorch/pull/132891,larryliu0820,albanD,,,
e76bd0b6039,skip,not user facing,"[BE] put ""show_dispatch_trace()"" print logic in .cpp file (#132717)",aten/src/ATen/core/dispatch/Dispatcher.cpp aten/src/ATen/core/dispatch/Dispatcher.h,https://github.com/pytorch/pytorch/pull/132717,davidberard98,bdhirsh,soulitzer,,
6a348e5e57e,inductor,not user facing,[CUDAGraph] Warn once if too many distinct sizes (#132832),test/inductor/test_cudagraph_trees.py torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py,https://github.com/pytorch/pytorch/pull/132832,BoyuanFeng,eellison,,,
525fdc0f950,skip,not user facing,[docs] fix incorrect example in `convert_conv3d_weight_memory_format` (#129318),torch/nn/utils/memory_format.py,https://github.com/pytorch/pytorch/pull/129318,a-r-r-o-w,albanD,,,
260e7cb143b,skip,not user facing,Make CUDA device properties's `__repr__` output actually printable (#132863),torch/csrc/cuda/Module.cpp,https://github.com/pytorch/pytorch/pull/132863,int3,Skylion007,eqy,,
48f7bdbbe1e,Uncategorized,Untopiced,aot_autograd: copy metadata from fw to bw nodes (#126573),test/dynamo/test_aot_autograd.py torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py torch/_functorch/_aot_autograd/utils.py,https://github.com/pytorch/pytorch/pull/126573,vkuzo,bdhirsh,ezyang,,
700a11fdd46,inductor,not user facing,Make inductor kernel metadata comments more descriptive (#126698),test/inductor/test_cuda_repro.py test/inductor/test_torchinductor.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/126698,vkuzo,ezyang,,,
acad2050c14,dynamo,not user facing,[easy][dynamo] Add tx as an arg in getitem_const (#132899),torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/optimizer.py,https://github.com/pytorch/pytorch/pull/132899,anijain2305,yanboliang,,,
21d4c480594,distributed,not user facing,Allow distributed breakpoint to skip the first few calls (#129511),torch/distributed/__init__.py,https://github.com/pytorch/pytorch/pull/129511,fegin,c-p-i-o,wconstab,,
c327710a875,Uncategorized,Untopiced,[export] Publicize validate function (#132777),test/export/test_verifier.py torch/_export/serde/serialize.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/132777,angelayi,zhxchen17,,,
383f2ac9148,inductor,not user facing,AutoHeuristic: mixed_mm H100 heuristic (#132685),torch/_inductor/autoheuristic/artifacts/_MixedMMH100.py torchgen/_autoheuristic/mixed_mm/gen_mixedmm_heuristic_h100.sh torchgen/_autoheuristic/mixed_mm/get_mixedmm_dataset.sh,https://github.com/pytorch/pytorch/pull/132685,AlnisM,eellison,,,
c3e51c09ed3,distributed,Untopiced,[PP] Add get_schedule_class util (#132768),test/distributed/pipelining/test_schedule.py torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/132768,H-Huang,c-p-i-o,fduwjj,,
708a99e52a6,inductor,not user facing,Stop using with_fresh_cache_if_config as decorator (#132801),.lintrunner.toml torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/132801,ezyang,albanD,,,
5e4d8eb831b,inductor,not user facing,Don't generate stack entry for DebugContext.wrap (#132802),torch/_inductor/compile_fx.py torch/_inductor/debug.py,https://github.com/pytorch/pytorch/pull/132802,ezyang,albanD,,,
42226ca3a30,dynamo,not user facing,Don't use use_lazy_graph_module as decorator (#132804),.lintrunner.toml torch/_dynamo/convert_frame.py torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/132804,ezyang,albanD,,,
9282e6ca78b,inductor,not user facing,Don't use _disable_current_modes as decorator (#132809),.lintrunner.toml torch/_export/passes/constant_folding.py torch/_inductor/constant_folding.py torch/_inductor/freezing.py torch/_inductor/fx_passes/joint_graph.py,https://github.com/pytorch/pytorch/pull/132809,ezyang,albanD,,,
b73d4b6555d,distributed (pipeline),not user facing,[pipelining] Add schedule runtime for lowered schedule (#130488),test/distributed/_composable/test_composability/test_pp_composability.py test/distributed/pipelining/test_schedule_multiproc.py torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/130488,wconstab,H-Huang,,,
f16d87eeff9,dynamo,not user facing,Print where raw cprofile lives (#132866),torch/_dynamo/convert_frame.py,https://github.com/pytorch/pytorch/pull/132866,ezyang,albanD,,,
374747818d5,skip,not user facing,Run performance test non-alternately (#131935),benchmarks/dynamo/common.py benchmarks/dynamo/torchao_backend.py,https://github.com/pytorch/pytorch/pull/131935,HDCharles,xuzhao9,,,
441c1c03d5d,python_frontend,bug fixes,Prevent an unnecessary device -> host copy for CuPy arrays when not explicitly setting a device in torch.as_tensor.  (#132595),torch/_torch_docs.py torch/csrc/utils/tensor_new.cpp,https://github.com/pytorch/pytorch/pull/132595,tringwald,ezyang,,,
a9036e1cf80,inductor,not user facing,[inductor] raise unsupport msg in capture_pre_autograd_graph on Windows (#132841),torch/_export/__init__.py,https://github.com/pytorch/pytorch/pull/132841,xuhancn,ezyang,jgong5,,
5cb05a82b45,inductor,Untopiced,[BC breaking] move benchmarking + prefer inductor path (#132827),benchmarks/dynamo/microbenchmarks/bench_mm_fusion.py benchmarks/dynamo/microbenchmarks/inductor_mm.py benchmarks/dynamo/microbenchmarks/operatorbench.py benchmarks/dynamo/microbenchmarks/tensor_layout_mini_benchmark.py benchmarks/gpt_fast/benchmark.py benchmarks/sparse/triton_ops.py benchmarks/transformer/score_mod.py test/dynamo/test_logging.py test/inductor/test_b2b_gemm.py test/inductor/test_indexing.py test/inductor/test_inductor_utils.py test/inductor/test_padding.py test/inductor/test_scatter_optimization.py torch/_functorch/partitioners.py torch/_inductor/autotune_process.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/fx_passes/pad_mm.py torch/_inductor/ir.py torch/_inductor/runtime/benchmarking.py torch/_inductor/runtime/runtime_utils.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/select_algorithm.py torch/_inductor/wrapper_benchmark.py torch/_logging/_internal.py torch/_logging/_registrations.py,https://github.com/pytorch/pytorch/pull/132827,nmacchioni,eellison,,,
0f90ffe94a8,distributed,Untopiced,Remove ProcessGroupRoundRobin (#132888),build_variables.bzl test/distributed/test_c10d_gloo.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/ProcessGroupRoundRobin.cpp torch/csrc/distributed/c10d/ProcessGroupRoundRobin.hpp torch/csrc/distributed/c10d/init.cpp torch/distributed/__init__.py,https://github.com/pytorch/pytorch/pull/132888,H-Huang,Skylion007,c-p-i-o,fduwjj,
bbf568aac8e,Uncategorized,Untopiced,"Split of ""[reland] [export] fix zero arg export in training_ir and constant tensor handling"" (#132307)",test/export/test_export.py torch/_export/passes/lift_constants_pass.py torch/_export/utils.py torch/export/_trace.py torch/export/_unlift.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/132307,ydwu4,zhxchen17,,,
59f4725b49a,skip,not user facing,[NJT] manually autocast in SDPA handling (#132835),test/test_nestedtensor.py torch/nested/_internal/sdpa.py,https://github.com/pytorch/pytorch/pull/132835,davidberard98,soulitzer,,,
5709375d565,inductor,Untopiced,[AOTI][tooling][1/n] Add intermediate value debug printer (#132323),torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cuda/cuda_cpp_scheduling.py torch/_inductor/codegen/debug_utils.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/config.py torch/_inductor/graph.py torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/132323,YUNQIUGUO,ColinPeppler,,,
fb6b001cde0,Uncategorized,Untopiced,"Disable expandable segments IPC in fbcode, because some jobs seem to be failing. (#132890)",c10/cuda/CUDACachingAllocator.cpp,https://github.com/pytorch/pytorch/pull/132890,zdevito,eqy,ezyang,,
4ca616e6d45,skip,not user facing,Disable sparse tests in export (#132824),test/export/test_sparse.py,https://github.com/pytorch/pytorch/pull/132824,tugsbayasgalan,BoyuanFeng,,,
45d0e90bd3b,Uncategorized,Untopiced,[export] Allow str outputs (#132808),test/export/test_export.py torch/_export/passes/replace_autocast_with_hop_pass.py torch/_export/passes/replace_set_grad_with_hop_pass.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/132808,angelayi,ydwu4,,,
194ec49d27e,dynamo,not user facing,[dynamo][lists][stable diffusion] Do not add source on list slice (#132912),test/dynamo/test_functions.py torch/_dynamo/variables/lists.py,https://github.com/pytorch/pytorch/pull/132912,anijain2305,williamwen42,,,
c69b2d24e3b,dynamo,not user facing,[dynamo] Support remove method of set (#132943),test/dynamo/test_misc.py torch/_dynamo/variables/dicts.py,https://github.com/pytorch/pytorch/pull/132943,yiming0416,anijain2305,,,
fd874b799fd,inductor,Untopiced,[AOTI][refactor] Update MKLDNN ops cpp wrapper support (#132367),test/inductor/test_cpu_cpp_wrapper.py test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/ir.py torch/_inductor/mkldnn_ir.py torch/_inductor/mkldnn_lowerings.py,https://github.com/pytorch/pytorch/pull/132367,desertfire,angelayi,leslie-fang-intel,,
636a7c4859e,autograd_frontend,not user facing,[13/N] Use std::optional  (#132527),test/cpp_extensions/open_registration_extension.cpp tools/autograd/gen_autograd_functions.py tools/autograd/gen_trace_type.py tools/autograd/gen_variable_factories.py tools/autograd/gen_variable_type.py torch/_inductor/codegen/cpp.py torch/_inductor/mkldnn_ir.py,https://github.com/pytorch/pytorch/pull/132527,cyyever,ezyang,,,
ac6398b630d,distributed,not user facing,[FSDP2] Follow-up fix to correct relaxed overlap test (#132953),test/distributed/_composable/fsdp/test_fully_shard_overlap.py,https://github.com/pytorch/pytorch/pull/132953,awgu,weifengpy,,,
b483ca05a91,skip,not user facing,[inductor]a less ambitious way to slove the scalar tensor (#132702),test/inductor/test_torchinductor.py torch/_inductor/graph.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/132702,FindHao,eellison,,,
4fd0d594a1a,dynamo,not user facing,[sym_shapes] Not eval sym expression for printing storage_offset (#132911),torch/_dynamo/debug_utils.py,https://github.com/pytorch/pytorch/pull/132911,IvanKobzarev,ezyang,,,
5ae979ab106,dynamo,bug fixes,[Dynamo] Support torch.autograd._is_checkpoint_valid (#132611),test/dynamo/test_functions.py torch/_dynamo/variables/torch.py,https://github.com/pytorch/pytorch/pull/132611,xinyu-intel,anijain2305,williamwen42,,
4a1edbe4756,skip,Untopiced,Disable SymDispatchMode when torch.compile'ing (#132433),torch/_dynamo/convert_frame.py torch/fx/experimental/_sym_dispatch_mode.py,https://github.com/pytorch/pytorch/pull/132433,ezyang,ydwu4,,,
aff48f73789,distributed,Untopiced,Autoselect default device in FSDP construction. (#127609),docs/source/torch.rst torch/distributed/fsdp/_init_utils.py,https://github.com/pytorch/pytorch/pull/127609,accelerate321,awgu,,,
745665d8b51,distributed,not user facing,[BE] Using with_temp_dir for test_distributed_checkpoint (#132908),test/distributed/fsdp/test_distributed_checkpoint.py,https://github.com/pytorch/pytorch/pull/132908,fegin,Skylion007,awgu,,
a270800f0b1,fx,Untopiced,[export][reland] Add print_readable to unflattened module (#132817),test/dynamo/test_autograd_function.py test/dynamo/test_higher_order_ops.py test/dynamo/test_subclasses.py test/inductor/test_flex_attention.py torch/export/unflatten.py torch/fx/graph_module.py,https://github.com/pytorch/pytorch/pull/132817,angelayi,pianpwk,,,
9e584d0c052,distributed,not user facing,[BE] Test foreach optimizer for FSDP1 optimizer state_dict (#132933),test/distributed/checkpoint/test_state_dict.py,https://github.com/pytorch/pytorch/pull/132933,fegin,c-p-i-o,,,
0e8541766fe,skip,not user facing,[ts-migration]: Support quantized operation transformation (#131915),test/export/test_converter.py torch/_export/converter.py torch/_export/passes/replace_quantized_ops_with_standard_ops_pass.py torch/fx/graph.py,https://github.com/pytorch/pytorch/pull/131915,jiashenC,angelayi,,,
479d4604711,distributed,not user facing,[DeviceMesh] Add a private _flatten() API for device_mesh (#132632),test/distributed/test_device_mesh.py torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/132632,wz337,fegin,wanchaol,,
83e4af203d3,distributed,Untopiced,[dtensor] rewrite redistribute algorithm for multi-dim mesh (#131210),test/distributed/_tensor/test_redistribute.py torch/distributed/_tensor/_redistribute.py,https://github.com/pytorch/pytorch/pull/131210,wanchaol,tianyu-l,,,
751c744ad0f,skip,not user facing,Optimize sort kernel for contiguous tensors (#132236),aten/src/ATen/native/cpu/SortingKernel.cpp,https://github.com/pytorch/pytorch/pull/132236,abhishek-iitmadras,jgong5,,,
9c5e0d47fe3,xpu,not user facing,Add xpu_cmake_macros.h to xpu build (#132847),c10/xpu/CMakeLists.txt,https://github.com/pytorch/pytorch/pull/132847,guangyey,EikanWang,,,
ac960dced10,skip,not user facing,Skip Reformer for Dynamic size testing (#132468),benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/132468,leslie-fang-intel,ezyang,,,
9e37e73e016,distributed,not user facing,[dtensor] refactor and improve readability of _dispatch.py (#132682),torch/distributed/_tensor/_dispatch.py torch/distributed/_tensor/_op_schema.py,https://github.com/pytorch/pytorch/pull/132682,wanchaol,XilunWu,,,
7f71f2a997a,distributed,Untopiced,[dtensor] improve docs and comments (#132683),torch/distributed/_tensor/api.py torch/distributed/_tensor/experimental/local_map.py torch/distributed/_tensor/placement_types.py torch/distributed/_tensor/random.py,https://github.com/pytorch/pytorch/pull/132683,wanchaol,XilunWu,,,
24dee99cb71,skip,not user facing,Populate submodules of `torch._C` to `sys.modules` recursively (#132216),test/dynamo/test_frame_init.py test/xpu/test_conv.py torch/_C/__init__.pyi.in torch/__init__.py torch/_dynamo/convert_frame.py torch/_dynamo/eval_frame.py torch/_dynamo/guards.py torch/_dynamo/types.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/132216,XuehaiPan,ezyang,,,
ec49796b8f6,inductor,not user facing,"[Inductor] Support use_libdevice_for_f64 for pointwise ops on XPU, align with CUDA. (#132739)",torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/132739,etaf,EikanWang,malfet,,
0f19d4150bc,skip,Untopiced,"Revert ""[inductor]a less ambitious way to slove the scalar tensor (#132702)""",test/inductor/test_torchinductor.py torch/_inductor/graph.py torch/_inductor/scheduler.py,,,,,,
361db32d478,releng,not user facing,Consolidate SymDispatchMode into ProxyTensorMode (#132674),.github/labeler.yml docs/source/conf.py docs/source/fx.experimental.rst docs/source/fx.rst torch/_dynamo/compiled_autograd.py torch/_dynamo/convert_frame.py torch/fx/experimental/_sym_dispatch_mode.py torch/fx/experimental/proxy_tensor.py torch/fx/experimental/sym_node.py,https://github.com/pytorch/pytorch/pull/132674,ezyang,bdhirsh,zou3519,,
54efd430222,skip,not user facing,[BE] Simplify code interacting with get_proxy_mode/enable_tracing (#132675),torch/_export/wrappers.py torch/_higher_order_ops/associative_scan.py torch/_higher_order_ops/auto_functionalize.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/effects.py torch/_higher_order_ops/flex_attention.py torch/_higher_order_ops/map.py torch/_higher_order_ops/out_dtype.py torch/_higher_order_ops/run_const_graph.py torch/_higher_order_ops/strict_mode.py torch/_higher_order_ops/torchbind.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_higher_order_ops/while_loop.py torch/_prims/rng_prims.py torch/distributed/_functional_collectives.py,https://github.com/pytorch/pytorch/pull/132675,ezyang,Skylion007,ydwu4,zou3519,
aec63323564,fx,not user facing,Only thunkify proxies in some situations (#132421),docs/source/fx.experimental.rst test/dynamo/test_export.py test/dynamo/test_subclasses.py test/export/test_export.py test/export/test_torchbind.py test/test_proxy_tensor.py torch/_dynamo/output_graph.py torch/_functorch/_aot_autograd/traced_function_transforms.py torch/fx/experimental/proxy_tensor.py torch/fx/experimental/symbolic_shapes.py torch/fx/proxy.py torch/nested/_internal/nested_tensor.py,https://github.com/pytorch/pytorch/pull/132421,ezyang,Skylion007,zou3519,,
b4e2411f6f1,skip,not user facing,Big enough count to trigger stack overflow (#132062),test/inductor/test_torchinductor_dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/132062,ezyang,bdhirsh,,,
35fd4391bc6,fx,not user facing,Format torch.fx.experimental.proxy_tensor.py (#132767),.lintrunner.toml torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/132767,ezyang,bdhirsh,,,
0e43175e225,fx,not user facing,[BE] Get rid of unnecessary inner_torch_dispatch method (#132769),torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/132769,ezyang,Skylion007,bdhirsh,,
902c6f3a191,fx,not user facing,[BE] Reroute all uses of proxy_tensor.maybe_disable_fake_tensor_mode to fake_tensor.unset_fake_temporarily (#132770),docs/source/conf.py docs/source/torch.compiler_fake_tensor.rst test/allowlist_for_publicAPI.json torch/_dynamo/eval_frame.py torch/_export/__init__.py torch/_inductor/pattern_matcher.py torch/export/exported_program.py torch/fx/experimental/proxy_tensor.py torch/onnx/_internal/fx/_pass.py torch/onnx/_internal/fx/passes/decomp.py torch/onnx/_internal/fx/passes/functionalization.py torch/onnx/_internal/fx/passes/type_promotion.py,https://github.com/pytorch/pytorch/pull/132770,ezyang,bdhirsh,,,
d1f73fd844b,skip,Untopiced,"Revert ""[BE] Reroute all uses of proxy_tensor.maybe_disable_fake_tensor_mode to fake_tensor.unset_fake_temporarily (#132770)""",docs/source/conf.py docs/source/torch.compiler_fake_tensor.rst test/allowlist_for_publicAPI.json torch/_dynamo/eval_frame.py torch/_export/__init__.py torch/_inductor/pattern_matcher.py torch/export/exported_program.py torch/fx/experimental/proxy_tensor.py torch/onnx/_internal/fx/_pass.py torch/onnx/_internal/fx/passes/decomp.py torch/onnx/_internal/fx/passes/functionalization.py torch/onnx/_internal/fx/passes/type_promotion.py,,,,,,
42cd397a0eb,skip,not user facing,Loads .pyd instead of .so in MemPool test for windows (#132749),test/test_cuda.py torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/132749,syed-ahmed,albanD,,,
6f99e97f0ae,skip,Untopiced,"Revert ""[ts-migration]: Support quantized operation transformation (#131915)""",test/export/test_converter.py torch/_export/converter.py torch/_export/passes/replace_quantized_ops_with_standard_ops_pass.py torch/fx/graph.py,,,,,,
51ddcde1101,releng,not user facing,[BE] Introduces runner variants for amzn2023 to simplify lf-scale-config.yml and lf-canary-scale-config.yml (#132918),.github/lf-canary-scale-config.yml .github/lf-scale-config.yml,https://github.com/pytorch/pytorch/pull/132918,jeanschmidt,ZainRizvi,zxiiro,,
9db5bfccdce,inductor,not user facing,[inductor] disable test_torchinductor failed UTs on Windows (#132973),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/132973,xuhancn,desertfire,jgong5,,
c4071c4707d,distributed,Untopiced,Remove noqa: G004 warnings (#132917),torch/distributed/pipelining/schedules.py torch/distributed/pipelining/stage.py,https://github.com/pytorch/pytorch/pull/132917,H-Huang,Skylion007,c-p-i-o,fduwjj,
0ca8f66e3aa,skip,not user facing,[NestedTensor] Modify softmax on ragged dimension to allow for 2D nested tensors (#132812),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/132812,jananisriram,davidberard98,,,
b885ad8fce5,skip,Untopiced,"Revert ""[Inductor][CPP] Turns on inline_inbuilt_nn_modules for CPP GEMM template testing (#132487)""",test/inductor/test_cpu_select_algorithm.py,,,,,,
4ee5547b377,skip,not user facing,[triton_op] Skip HOP dispatch when possible (#132822),test/inductor/test_triton_kernels.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_library/triton.py,https://github.com/pytorch/pytorch/pull/132822,zou3519,SherlockNoMad,,,
9d5c85c499f,Uncategorized,Untopiced,Move exir.delegate to PyTorch core to enforce no out-of-tree HOPs (#132525),torch/_higher_order_ops/executorch_call_delegate.py,https://github.com/pytorch/pytorch/pull/132525,ezyang,Skylion007,zou3519,,
4a4dc9d6d9f,inductor,not user facing,[inductor] Disable remote caching in failing test_cpu_repro tests (#132955),test/inductor/test_cpu_repro.py,https://github.com/pytorch/pytorch/pull/132955,masnesral,leslie-fang-intel,,,
dfc5bb00999,releng,not user facing,Login to Meta's ECR when using non-meta runner (#132870),.github/actions/setup-linux/action.yml,https://github.com/pytorch/pytorch/pull/132870,zxiiro,ZainRizvi,,,
eeb6ad07448,quantization,Untopiced,[quant] Speed up dequantize_per_channel (#132828),torch/ao/quantization/fx/_decomposed.py,https://github.com/pytorch/pytorch/pull/132828,swolchok,cccclai,,,
942ffd1b2d5,Uncategorized,Untopiced,"Make the __module__ name of HOO to be always ""torch.ops.higher_order"" (#132775)",test/dynamo/test_autograd_function.py test/dynamo/test_higher_order_ops.py test/dynamo/test_subclasses.py test/export/test_export.py test/export/test_passes.py test/export/test_torchbind.py test/higher_order_ops/test_with_effects.py test/inductor/test_triton_kernels.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/flex_attention.py torch/_higher_order_ops/out_dtype.py torch/_higher_order_ops/while_loop.py torch/_ops.py,https://github.com/pytorch/pytorch/pull/132775,zhxchen17,ydwu4,zou3519,,
3ec9ec03a80,skip,Untopiced,"Revert ""[pipelining] Add schedule runtime for lowered schedule (#130488)""",test/distributed/_composable/test_composability/test_pp_composability.py test/distributed/pipelining/test_schedule_multiproc.py torch/distributed/pipelining/schedules.py,,,,,,
75eb66afc06,skip,not user facing,Support 'non-contiguous with holes' NJTs for contiguous clone() (#132776),test/test_nestedtensor.py torch/nested/_internal/ops.py torch/testing/_internal/opinfo/definitions/nested.py,https://github.com/pytorch/pytorch/pull/132776,jbschlosser,ani300,soulitzer,,
ac95b2a2f25,releng,not user facing,Migrate slow self-hosted jobs to Amazon2023 AMI (#131771),.github/workflows/slow.yml,https://github.com/pytorch/pytorch/pull/131771,ZainRizvi,seemethere,,,
21906ddaba9,inductor,not user facing,[AOTI] Fix complex64 not defined (#132810),test/cpp/aoti_abi_check/test_dtype.cpp torch/_inductor/codegen/cpp_utils.py torch/_inductor/ir.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/132810,yushangdi,desertfire,,,
322c9d03a00,distributed (fsdp2),not user facing,[FSDP][dtensor] use _StridedShard to represent nested sharding for correct full_tensor() result (#130760),test/distributed/_composable/fsdp/test_fully_shard_init.py test/distributed/_composable/fsdp/test_fully_shard_state_dict.py test/distributed/_composable/fsdp/test_fully_shard_training.py test/distributed/_composable/test_composability/test_2d_composability.py test/distributed/checkpoint/fsdp/test_fsdp_dsd.py torch/distributed/_composable/fsdp/_fsdp_param.py,https://github.com/pytorch/pytorch/pull/130760,XilunWu,fegin,wanchaol,wz337,
de288e2203f,inductor,Untopiced,Fix inf value reduction in non persistent reduction for scans (#132293),test/inductor/test_torchinductor.py torch/_inductor/codegen/triton.py torch/_inductor/runtime/triton_helpers.py,https://github.com/pytorch/pytorch/pull/132293,isuruf,peterbell10,,,
c326533999d,inductor,not user facing,[ROCm][Inductor] Enable AOT Inductor CPP UTs for ROCm (#131521),.ci/pytorch/test.sh test/cpp/aoti_inference/CMakeLists.txt test/cpp/aoti_inference/aoti_custom_class.cpp test/cpp/aoti_inference/test.cpp,https://github.com/pytorch/pytorch/pull/131521,pragupta,jataylo,malfet,pruthvistony,
b845068db26,distributed,not user facing,[dtensor] refactor examples folder (#132914),torch/distributed/_tensor/examples/checkpoint_example.py torch/distributed/_tensor/examples/comm_mode_features_example.py torch/distributed/_tensor/examples/comm_mode_features_example_argparser.py torch/distributed/_tensor/examples/convnext_example.py torch/distributed/_tensor/examples/visualize_sharding_example.py,https://github.com/pytorch/pytorch/pull/132914,wanchaol,wz337,,,
a9954d22f89,dynamo,Untopiced,Raise exception if torch.func.* calls torch.compile functions (#128736),test/dynamo/test_higher_order_ops.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/128736,guilhermeleobas,zou3519,,,
7bd0732cbdc,skip,not user facing,Fix flaky internal mixed_mm tests (#133015),test/inductor/test_autoheuristic.py test/inductor/test_pattern_matcher.py torch/testing/_internal/inductor_utils.py,https://github.com/pytorch/pytorch/pull/133015,AlnisM,Chillee,eellison,,
40767e84685,distributed,not user facing,[BE] rename testHelperPrefix test (#132916),test/cpp/c10d/TCPStoreTest.cpp,https://github.com/pytorch/pytorch/pull/132916,c-p-i-o,Skylion007,,,
cd308618573,inductor,Untopiced,[PT2][Optimus] Update unbind_cat_to_view pass to include more complicated cases (#132831),test/inductor/test_split_cat_fx_passes.py torch/_inductor/fx_passes/split_cat.py,https://github.com/pytorch/pytorch/pull/132831,mengluy0125,jackiexu1992,,,
9cca0494b9d,skip,not user facing,[ROCm] TunableOp logging improvements (#132173),aten/src/ATen/cuda/tunable/Tunable.cpp aten/src/ATen/cuda/tunable/TunableGemm.h,https://github.com/pytorch/pytorch/pull/132173,zixi-qi,eqy,jeffdaily,mxz297,
8707c6dfaca,nn_frontend,improvements,added persistent option to buffers and namedbuffers (#132994),test/test_nn.py torch/distributed/nn/api/remote_module.py torch/nn/modules/module.py,https://github.com/pytorch/pytorch/pull/132994,randolf-scholz,mikaylagawarecki,,,
3d0de6e1cd8,inductor,Untopiced,[Inductor] Add config option to force higher-dimensional tiling (#132937),test/inductor/test_torchinductor.py test/inductor/test_torchinductor_strided_blocks.py torch/_inductor/codegen/simd.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/132937,blaine-rister,eellison,jansel,,
f25df31008e,linalg_frontend,not user facing,TunableOp more unit test follow-up (#130065),test/test_linalg.py,https://github.com/pytorch/pytorch/pull/130065,naromero77amd,jeffdaily,malfet,,
1f66487c698,fx,not user facing,[BE] Reroute all uses of proxy_tensor.maybe_disable_fake_tensor_mode to fake_tensor.unset_fake_temporarily (#132770),docs/source/conf.py docs/source/torch.compiler_fake_tensor.rst test/allowlist_for_publicAPI.json torch/_dynamo/eval_frame.py torch/_export/__init__.py torch/_export/passes/lift_constants_pass.py torch/_inductor/pattern_matcher.py torch/export/exported_program.py torch/fx/experimental/proxy_tensor.py torch/onnx/_internal/fx/_pass.py torch/onnx/_internal/fx/passes/decomp.py torch/onnx/_internal/fx/passes/functionalization.py torch/onnx/_internal/fx/passes/type_promotion.py,https://github.com/pytorch/pytorch/pull/132770,ezyang,bdhirsh,,,
1434e0b1214,autograd_frontend,not user facing,Add a private _safe_softmax (#131060),aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp aten/src/ATen/native/transformers/attention.cpp test/functorch/test_ops.py test/inductor/test_torchinductor_opinfo.py test/test_decomp.py tools/autograd/derivatives.yaml torch/_decomp/__init__.py torch/_decomp/decompositions.py torch/distributed/_tensor/ops/_math_ops.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/131060,drisspg,jbschlosser,,,
45cf8ef5571,skip,not user facing,add impls for required for nt ops (#132710),aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorMath.cpp aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp,https://github.com/pytorch/pytorch/pull/132710,drisspg,jbschlosser,,,
fa8c34301ab,fx,not user facing,[ts-migration]: Quantized ops to standard ops pass. (#133026),test/export/test_converter.py torch/_export/converter.py torch/_export/passes/replace_quantized_ops_with_standard_ops_pass.py torch/fx/graph.py,https://github.com/pytorch/pytorch/pull/133026,jiashenC,angelayi,,,
e2b94923ba7,quantization,Untopiced,[PyTorch] Speed up decomposed quantize_per_channel (#133029),torch/ao/quantization/fx/_decomposed.py,https://github.com/pytorch/pytorch/pull/133029,swolchok,cccclai,,,
3c5b246d3c6,quantization,not user facing,[export] Remove Proxy from exported programs and modules (#132956),test/quantization/pt2e/test_x86inductor_quantizer.py torch/_export/__init__.py torch/_export/utils.py torch/export/_trace.py torch/fx/proxy.py,https://github.com/pytorch/pytorch/pull/132956,yushangdi,angelayi,,,
10c2168b312,dynamo,not user facing,[pt2-bench] use larger multiplier for smaller tensors for a few models (#132952),benchmarks/dynamo/timm_models.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/132952,shunting314,eellison,jansel,,
0ff0bf3d319,distributed,bug fixes,[Replicate] Fix replicate with DeviceMesh initialization (#133024),test/distributed/test_device_mesh.py torch/distributed/_composable/replicate.py,https://github.com/pytorch/pytorch/pull/133024,wz337,Skylion007,fegin,,
4101dd14c29,dynamo,bug fixes,Make debugging backends accept and ignore options kwargs from torch.compile (#132892),test/dynamo/test_compile.py test/inductor/test_config.py torch/_dynamo/backends/common.py torch/_dynamo/backends/debugging.py,https://github.com/pytorch/pytorch/pull/132892,ezyang,anijain2305,jansel,,
313aa151da1,skip,Untopiced,"Revert ""[ROCm] TunableOp logging improvements (#132173)""",aten/src/ATen/cuda/tunable/Tunable.cpp aten/src/ATen/cuda/tunable/TunableGemm.h,,,,,,
8967d55b01c,mobile,not user facing,[18/N] Fix clang-tidy warnings in jit  (#132963),torch/csrc/jit/mobile/code.h torch/csrc/jit/mobile/debug_info.cpp torch/csrc/jit/mobile/debug_info.h torch/csrc/jit/mobile/file_format.h torch/csrc/jit/mobile/flatbuffer_loader.cpp torch/csrc/jit/mobile/flatbuffer_loader.h torch/csrc/jit/mobile/frame.h torch/csrc/jit/mobile/function.cpp torch/csrc/jit/mobile/function.h torch/csrc/jit/mobile/import.cpp torch/csrc/jit/mobile/import.h torch/csrc/jit/mobile/import_data.h torch/csrc/jit/mobile/import_export_common.h torch/csrc/jit/mobile/interpreter.cpp torch/csrc/jit/mobile/interpreter.h torch/csrc/jit/mobile/method.h torch/csrc/jit/mobile/module.cpp torch/csrc/jit/mobile/module.h torch/csrc/jit/mobile/parse_bytecode.cpp torch/csrc/jit/mobile/parse_bytecode.h torch/csrc/jit/mobile/parse_operators.cpp torch/csrc/jit/mobile/parse_operators.h torch/csrc/jit/mobile/prim_ops_registery.cpp torch/csrc/jit/mobile/prim_ops_registery.h torch/csrc/jit/mobile/profiler_edge.cpp torch/csrc/jit/mobile/profiler_edge.h torch/csrc/jit/mobile/promoted_prim_ops.cpp torch/csrc/jit/mobile/promoted_prim_ops.h torch/csrc/jit/mobile/quantization.cpp torch/csrc/jit/mobile/quantization.h torch/csrc/jit/mobile/register_ops_common_utils.cpp torch/csrc/jit/mobile/register_ops_common_utils.h torch/csrc/jit/mobile/type_parser.cpp torch/csrc/jit/mobile/upgrader_mobile.cpp torch/csrc/jit/mobile/upgrader_mobile.h torch/csrc/jit/passes/device_type_analysis.cpp torch/csrc/jit/python/pybind_utils.cpp torch/csrc/jit/python/pybind_utils.h torch/csrc/jit/python/script_init.cpp torch/csrc/utils/python_dispatch.cpp torch/csrc/utils/throughput_benchmark.cpp torch/csrc/utils/throughput_benchmark.h,https://github.com/pytorch/pytorch/pull/132963,cyyever,Skylion007,,,
22ea248aa8e,Uncategorized,Untopiced,dynamic shapes mismatch errors (#132982),test/export/test_export.py torch/_export/non_strict_utils.py torch/export/dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/132982,avikchaudhuri,yushangdi,,,
3b7edc12c6c,distributed,not user facing,[dtensor] more refactor to imports/paths (#133022),test/distributed/_tensor/test_tensor_ops.py torch/distributed/_tensor/debug/__init__.py torch/distributed/_tensor/experimental/__init__.py torch/distributed/_tensor/experimental/func_map.py torch/distributed/_tensor/experimental/local_map.py,https://github.com/pytorch/pytorch/pull/133022,wanchaol,XilunWu,wz337,,
8875226d62a,distributed,not user facing,[dtensor] multi-dim mesh redistribute follow up (#133023),test/distributed/_tensor/test_redistribute.py torch/distributed/_tensor/_redistribute.py,https://github.com/pytorch/pytorch/pull/133023,wanchaol,tianyu-l,,,
bb6eef8ed1d,skip,not user facing,[2/2] PT2 Inductor ComboKernels - automatic horizontal fusing (#131675),test/inductor/test_combo_kernels.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/config.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/131675,qchip,mlazos,,,
05e8e87a692,skip,not user facing,[Submodule] Remove foxi (#132976),.gitmodules BUILD.bazel WORKSPACE cmake/Dependencies.cmake cmake/TorchConfig.cmake.in setup.py third_party/foxi third_party/foxi.BUILD,https://github.com/pytorch/pytorch/pull/132976,cyyever,ezyang,,,
da65cfbdea4,skip,not user facing,Remove unused Caffe2 macros (#132979),BUCK.oss buckbuild.bzl build.bzl caffe2/core/macros.h.in cmake/Dependencies.cmake cmake/MiscCheck.cmake,https://github.com/pytorch/pytorch/pull/132979,cyyever,ezyang,,,
5707c6e9528,skip,not user facing,"[Fake tensor] Align the appearance of `device_put` op in fx_graph generated for CUDA and XPU, which is exposed in the  issue #130823 (#132479)",test/test_fake_tensor.py torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/132479,etaf,EikanWang,eellison,zou3519,
ea00036841b,skip,Untopiced,[BE] Raise when the target model has scalar parameters (#132934),test/distributed/_composable/fsdp/test_fully_shard_init.py test/distributed/_composable/fully_shard/test_fully_shard_init.py torch/distributed/_composable/fsdp/_fsdp_init.py torch/distributed/fsdp/_init_utils.py,https://github.com/pytorch/pytorch/pull/132934,fegin,awgu,wz337,,
7b8ab7eb3e4,dynamo,not user facing,[dynamo] Partially support random.Random class (#133037),torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/133037,yiming0416,anijain2305,,,
927b4c11143,skip,not user facing,[CUDA][CUTLASS][submodule] Fixes for CUTLASS upgrade (#131493),aten/src/ATen/native/cuda/MixedDtypesLinear.cu aten/src/ATen/native/cuda/RowwiseScaledMM.cu aten/src/ATen/native/sparse/cuda/ComputeSparseTile.h aten/src/ATen/native/sparse/cuda/SparseSemiStructuredLinear.cu aten/src/ATen/native/sparse/cuda/SparseSemiStructuredOps.cu aten/src/ATen/native/transformers/cuda/flash_attn/flash_bwd_preprocess_kernel.h aten/src/ATen/native/transformers/cuda/flash_attn/flash_fwd_kernel.h third_party/cutlass torch/_inductor/codegen/cuda/cutlass_utils.py,https://github.com/pytorch/pytorch/pull/131493,eqy,Skylion007,,,
f565d16acbe,cuda,Untopiced,Fix work-around item non-sync issue on AMD (#133054),aten/src/ATen/native/cuda/CUDAScalar.cu,https://github.com/pytorch/pytorch/pull/133054,zoranzhao,houseroad,,,
465e071898e,skip,Untopiced,"Revert ""[CUDA][CUTLASS][submodule] Fixes for CUTLASS upgrade (#131493)""",aten/src/ATen/native/cuda/MixedDtypesLinear.cu aten/src/ATen/native/cuda/RowwiseScaledMM.cu aten/src/ATen/native/sparse/cuda/ComputeSparseTile.h aten/src/ATen/native/sparse/cuda/SparseSemiStructuredLinear.cu aten/src/ATen/native/sparse/cuda/SparseSemiStructuredOps.cu aten/src/ATen/native/transformers/cuda/flash_attn/flash_bwd_preprocess_kernel.h aten/src/ATen/native/transformers/cuda/flash_attn/flash_fwd_kernel.h third_party/cutlass torch/_inductor/codegen/cuda/cutlass_utils.py,,,,,,
f2bacd908a8,distributed,Untopiced,[BE] Move function definitions to .cpp (#132927),torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp,https://github.com/pytorch/pytorch/pull/132927,c-p-i-o,Skylion007,d4l3k,,
4bdb4bbd864,inductor,Untopiced,Fix fbcode AOTI GPU lowering for ARM64 hosts (#133017),torch/_inductor/cpu_vec_isa.py,https://github.com/pytorch/pytorch/pull/133017,amitaga,hl475,,,
065f7aa44b4,inductor,not user facing,[inductor] tensor_is_align fallbacking False if unbacked expr not comptime evaled (#132423),torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/132423,IvanKobzarev,ezyang,,,
50595ecef4a,skip,Untopiced,"Revert ""[BE] Raise when the target model has scalar parameters (#132934)""",test/distributed/_composable/fsdp/test_fully_shard_init.py test/distributed/_composable/fully_shard/test_fully_shard_init.py torch/distributed/_composable/fsdp/_fsdp_init.py torch/distributed/fsdp/_init_utils.py,,,,,,
23b877cb54e,skip,not user facing,[inductor]a less ambitious way to slove the scalar tensor (#132702),test/inductor/test_torchinductor.py torch/_inductor/graph.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/132702,FindHao,eellison,,,
bc57d5b6ff8,skip,not user facing,[Inductor][CPP] Turns on inline_inbuilt_nn_modules for CPP GEMM template testing (#132487),test/inductor/test_cpu_select_algorithm.py,https://github.com/pytorch/pytorch/pull/132487,leslie-fang-intel,anijain2305,jgong5,,
2dbe5cb979f,distributed,Untopiced,[C10D] Clarify warning for concurrent PG usage (#131895),torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/131895,wconstab,fduwjj,,,
574cdf1232e,Uncategorized,Untopiced,[export] Merge functions in replace set_grad/autocast with HOO (#132724),torch/_export/passes/replace_autocast_with_hop_pass.py torch/_export/passes/replace_set_grad_with_hop_pass.py torch/_export/passes/replace_with_hop_pass_util.py,https://github.com/pytorch/pytorch/pull/132724,yushangdi,ydwu4,,,
d13e72fd6ab,distributed,Untopiced,[c10d] set a shorter heartbeat detect timeout to avoid race with NCCL timeout (#133028),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/133028,fduwjj,shuqiangzhang,wconstab,,
655ec075259,skip,not user facing,[ROCm] TunableOp logging improvements (#132173),aten/src/ATen/cuda/tunable/Tunable.cpp aten/src/ATen/cuda/tunable/TunableGemm.h,https://github.com/pytorch/pytorch/pull/132173,zixi-qi,eqy,jeffdaily,mxz297,
6c012f72177,distributed,Untopiced,[c10d][Log] Use pg_id instead of pg_name for logging prefix (#132058),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/132058,fduwjj,shengbao-zheng,shuqiangzhang,,
31ef900a653,skip,Untopiced,"Revert ""added persistent option to buffers and namedbuffers (#132994)""",test/test_nn.py torch/distributed/nn/api/remote_module.py torch/nn/modules/module.py,,,,,,
f3eab23c42c,skip,not user facing,Fix typo in `mypy.ini` (#133097),mypy.ini,https://github.com/pytorch/pytorch/pull/133097,DeinAlptraum,Skylion007,malfet,,
b41d62a3a2c,distributed,Untopiced,Fix typo in docs of `all_gather` (#133066),torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/133066,horizon86,awgu,,,
e66084f9bfd,skip,not user facing,[BUG FIX] Refactor _scale_attn_mask_fusion_kernel to Use Runtime Argument Instead of Template Parameter (#132434),aten/src/ATen/native/cpu/FlashAttentionKernel.cpp,https://github.com/pytorch/pytorch/pull/132434,abhishek-iitmadras,jgong5,,,
472b0daeaa4,distributed,Untopiced,[DDP][FSDP2] keep DTensor params for replicate(fully_shard) (#133059),test/distributed/_composable/test_replicate.py torch/distributed/_composable/replicate.py torch/distributed/tensor/parallel/ddp.py,https://github.com/pytorch/pytorch/pull/133059,weifengpy,Skylion007,fegin,,
78cf8df4a01,inductor,not user facing,[aoti] forward fix of [inductor] switch AotCodeCompiler to new cpp_builder. (take 3) (#133042),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/133042,henrylhtsang,desertfire,hl475,,
cd307fb0b1a,distributed,Untopiced,[FSDP2] reset FSDPParam.sharded_param in lazy_init (#132954),test/distributed/_composable/fsdp/test_fully_shard_init.py torch/distributed/_composable/fsdp/_fsdp_param_group.py,https://github.com/pytorch/pytorch/pull/132954,weifengpy,awgu,,,
c44cb89e062,skip,not user facing,[export] detach constant tensors when they're not registered as buffer or parameter in unlift (#133031),torch/export/_unlift.py,https://github.com/pytorch/pytorch/pull/133031,ydwu4,angelayi,,,
78fa32a77bd,skip,not user facing,Turn off Function Event Accumulation by Default (#133095),test/profiler/test_profiler.py torch/autograd/profiler.py torch/profiler/profiler.py,https://github.com/pytorch/pytorch/pull/133095,sraikund16,LucasLLC,aaronenyeshi,briancoutinho,
dc8bb2636c5,distributed,Untopiced,[c10d][doc] Add docs for ENV variables TORCH_NCCL_ASYNC_ERROR_HANDLING TORCH_NCCL_TRACE_CPP_STACK and TORCH_NCCL_COORD_CHECK_MILSEC (#132920),docs/source/torch_nccl_environment_variables.rst torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,https://github.com/pytorch/pytorch/pull/132920,fduwjj,fegin,wconstab,,
4110cb6ba7a,inductor,not user facing,Add explicit GQA support. (#131559),benchmarks/transformer/score_mod.py test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/131559,joydddd,drisspg,,,
b0b47230624,distributed,Untopiced,[c10d] Rename PG name and PG ID attribute (#132915),torch/csrc/distributed/c10d/Backend.hpp torch/csrc/distributed/c10d/ProcessGroup.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,https://github.com/pytorch/pytorch/pull/132915,fduwjj,fegin,,,
d2ecdcb2f7f,profiler,new features,[Profiler] Add API for Dynamic Activity Toggling [2/n] (#133035),third_party/kineto torch/_C/_autograd.pyi torch/autograd/__init__.py torch/autograd/profiler.py torch/csrc/autograd/init.cpp torch/csrc/autograd/profiler_kineto.cpp torch/csrc/autograd/profiler_kineto.h torch/csrc/profiler/kineto_shim.cpp torch/csrc/profiler/kineto_shim.h torch/csrc/profiler/orchestration/observer.h torch/profiler/profiler.py,https://github.com/pytorch/pytorch/pull/133035,sraikund16,aaronenyeshi,,,
6beb2be2edf,dynamo,not user facing,Fix _dynamo.variables.torch_function.global_mangled_class_name (#132744),test/dynamo/test_modules.py torch/_dynamo/variables/torch_function.py,https://github.com/pytorch/pytorch/pull/132744,rec,zou3519,,,
3b440f358cc,distributed,Untopiced,[elastic collectives API] add missing rank tracing support (#132818),test/distributed/elastic/utils/util_test.py torch/distributed/elastic/utils/store.py,https://github.com/pytorch/pytorch/pull/132818,kurman,d4l3k,,,
e5fa190e017,inductor,not user facing,AutoHeuristic: tuned_mm (#131615),torch/_inductor/autoheuristic/autoheuristic_utils.py torch/_inductor/fx_passes/pad_mm.py torch/_inductor/kernel/mm.py,https://github.com/pytorch/pytorch/pull/131615,AlnisM,eellison,,,
e7512ab752a,inductor,not user facing,inductor mm autotuning: add back previously pruned configs (#131616),torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_common.py,https://github.com/pytorch/pytorch/pull/131616,AlnisM,eellison,,,
21302d58912,skip,not user facing,AutoHeuristic: script to generate data for mm (#131617),torchgen/_autoheuristic/benchmark_runner.py torchgen/_autoheuristic/benchmark_utils.py torchgen/_autoheuristic/mm/gen_data_mm.py torchgen/_autoheuristic/pad_mm/gen_data_pad_mm.py,https://github.com/pytorch/pytorch/pull/131617,AlnisM,eellison,,,
72f2b29bb09,releng,not user facing,[CI] disable xpu kineto build (#133069),.ci/docker/common/install_xpu.sh .ci/pytorch/build.sh .ci/pytorch/test.sh test/run_test.py,https://github.com/pytorch/pytorch/pull/133069,chuanqi129,seemethere,,,
38994652688,Uncategorized,Untopiced,relax unification checks when size-like symbols can be 0 (#133112),torch/_export/utils.py,https://github.com/pytorch/pytorch/pull/133112,avikchaudhuri,angelayi,,,
de48d54042f,distributed,Untopiced,[TorchRec] Add Support for FakeProcessGroup (#133039),torch/_dynamo/variables/distributed.py,https://github.com/pytorch/pytorch/pull/133039,TroyGarden,ezyang,,,
f0378032908,releng,not user facing,"Add ChromiumEventLogger, log FXGraphCache and AOTAutogradCache (#132864)",.ci/pytorch/common_utils.sh test/dynamo/test_structured_trace.py torch/_dynamo/utils.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_inductor/codecache.py torch/_logging/_internal.py,https://github.com/pytorch/pytorch/pull/132864,jamesjwu,aorenste,,,
c89936eaa03,skip,not user facing,[CUDA][SDPA] Bump `grad_key` fudge factor in `test_flash_attention_vs_math_ref_grads` (#133051),test/test_transformers.py,https://github.com/pytorch/pytorch/pull/133051,eqy,Skylion007,drisspg,,
8fbd7d92a81,skip,Untopiced,Inductor-CPU WoQ int8 GEMM micro-kernel with scale epilogue (#131887),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/kernel/mm_common.py torch/_inductor/mkldnn_lowerings.py torch/_inductor/quantized_lowerings.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/131887,sanchitintel,jansel,jgong5,leslie-fang-intel,
e890d888d91,skip,not user facing,[dtensor] move tensor constructors to a separate module (#133129),torch/distributed/_tensor/__init__.py torch/distributed/_tensor/_tensor_constructors.py torch/distributed/_tensor/debug/__init__.py torch/distributed/tensor/parallel/style.py,https://github.com/pytorch/pytorch/pull/133129,wanchaol,awgu,tianyu-l,,
05de2b2d0fd,fx,not user facing,"Revert ""Construct NJT without graph breaks"" (#133145)",test/dynamo/test_subclasses.py test/test_nestedtensor.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py torch/_subclasses/fake_tensor.py torch/_subclasses/functional_tensor.py torch/_subclasses/meta_utils.py torch/fx/passes/fake_tensor_prop.py torch/nested/_internal/nested_tensor.py torch/nested/_internal/ops.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/133145,soulitzer,YuqingJ,,,
e888f401c51,inductor,not user facing,Fix autotuning for flex_decoding (#132157),test/inductor/test_flex_decoding.py torch/_inductor/kernel/flex_decoding.py,https://github.com/pytorch/pytorch/pull/132157,joydddd,drisspg,yanboliang,,
cc5a57d1853,distributed,Untopiced,Return from monitoring thread on TCPStore failure (#133150),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/133150,c-p-i-o,fduwjj,,,
9f0d90655d0,inductor,not user facing,[inductor] cpp_builder add dynamo time trace for compile_file (#133103),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/133103,xuhancn,ezyang,,,
c8275e25a79,Uncategorized,Untopiced,fix requirement for error classification (#133122),torch/_export/db/logging.py,https://github.com/pytorch/pytorch/pull/133122,avikchaudhuri,yushangdi,,,
343071cd967,skip,not user facing,Fix privateuse1 backend name case (#132980),c10/core/DeviceType.cpp c10/test/core/Device_test.cpp,https://github.com/pytorch/pytorch/pull/132980,shink,albanD,,,
2ad011ca73f,inductor,not user facing,[inductor] remove debug code of AotCodeCompiler (#132823),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/132823,xuhancn,henrylhtsang,,,
c860889a650,dynamo,not user facing,[compiled autograd][cpp node] No recompiles from saved int scalars (#132771),test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/csrc/dynamo/compiled_autograd.h torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/132771,xmfan,jansel,,,
b9922f7a5a8,dynamo,Untopiced,[compiled autograd][cpp node] No recaptures from saved float scalars (#133048),test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/csrc/dynamo/compiled_autograd.h torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/133048,xmfan,jansel,,,
32be3e942c3,skip,not user facing,Remove -Wno-error=pedantic from CMake (#133074),CMakeLists.txt,https://github.com/pytorch/pytorch/pull/133074,cyyever,ezyang,,,
4a3a30c36ef,inductor,not user facing,[inductor] remove deprecated cpp_builder implementation. (#133161),test/inductor/test_torchinductor.py torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/133161,xuhancn,ezyang,,,
92f650c5b33,inductor,not user facing,"[Inductor][Intel GPU] Support codegen empty_strided_xpu, align with #118255. (#126678)",caffe2/CMakeLists.txt test/inductor/test_cuda_repro.py torch/_inductor/codegen/wrapper.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/126678,etaf,EikanWang,eellison,jansel,
5b7b3e4af08,skip,not user facing,Fix some issues detected by static analyzer (#132970),aten/src/ATen/cuda/tunable/Tunable.cpp aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp torch/csrc/inductor/aoti_eager/kernel_holder.cpp torch/csrc/profiler/unwind/mem_file.h,https://github.com/pytorch/pytorch/pull/132970,cyyever,ezyang,,,
a7912bf9dc3,skip,not user facing,Make step != 0 test in slice irrefutable (#133091),aten/src/ATen/TensorIndexing.h,https://github.com/pytorch/pytorch/pull/133091,ezyang,bdhirsh,,,
79ca596dc6e,skip,not user facing,Optimize test_transformers.py (#133049),aten/src/ATen/native/transformers/attention.cpp test/test_transformers.py,https://github.com/pytorch/pytorch/pull/133049,jainapurva,drisspg,,,
78ccbad678a,inductor,not user facing,[inductor] remove dtype check/assert for reduction vec and support bool for min/max (#132473),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_prefix.h,https://github.com/pytorch/pytorch/pull/132473,zhuhaozhe,jgong5,,,
04f37ed57db,inductor,not user facing,Add support for returning LSE from FlexAttention (and also differentiating through it) (#133159),test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/133159,Chillee,yanboliang,,,
e7b870c88bc,inductor,not user facing,mixed_mm: fix segfault when allow_tf32=True (#133173),torch/_inductor/kernel/mm.py,https://github.com/pytorch/pytorch/pull/133173,AlnisM,Chillee,,,
2d71f03db12,skip,not user facing,[export] change deepcopy to copy in _replace_with_hop passes (#133142),torch/_export/passes/replace_with_hop_pass_util.py,https://github.com/pytorch/pytorch/pull/133142,yushangdi,ydwu4,,,
456909e5d35,profiler,new features,[Memory Snapshot][Viz] Show event timestamps if collected (#132523),torch/utils/viz/MemoryViz.js,https://github.com/pytorch/pytorch/pull/132523,aaronenyeshi,tianfengfrank,zdevito,,
7f08b73980a,skip,Untopiced,"Revert ""[Memory Snapshot][Viz] Show event timestamps if collected (#132523)""",torch/utils/viz/MemoryViz.js,,,,,,
27c44c884e2,profiler,new features,[Memory Snapshot][Viz] Show event timestamps if collected (#132523),torch/utils/viz/MemoryViz.js,https://github.com/pytorch/pytorch/pull/132523,aaronenyeshi,tianfengfrank,zdevito,,
f5e704a6f25,releng,not user facing,Add instruction count benchmark to run on pull requests (#131475),.ci/pytorch/test.sh .github/workflows/pull.yml benchmarks/__init__.py benchmarks/dynamo/pr_time_benchmarks/__init__.py benchmarks/dynamo/pr_time_benchmarks/benchmark_base.py benchmarks/dynamo/pr_time_benchmarks/benchmark_runner.sh benchmarks/dynamo/pr_time_benchmarks/benchmarks/update_hint_benchmark.py benchmarks/instruction_counts/applications/ci.py benchmarks/instruction_counts/core/api.py benchmarks/instruction_counts/core/expand.py benchmarks/instruction_counts/core/types.py benchmarks/instruction_counts/core/utils.py benchmarks/instruction_counts/definitions/setup.py benchmarks/instruction_counts/definitions/standard.py benchmarks/instruction_counts/execution/runner.py benchmarks/instruction_counts/execution/work.py benchmarks/instruction_counts/main.py benchmarks/operator_benchmark/benchmark_all_other_test.py benchmarks/operator_benchmark/benchmark_all_quantized_test.py benchmarks/operator_benchmark/benchmark_all_test.py benchmarks/operator_benchmark/pt/conv_test.py benchmarks/operator_benchmark/pt/embeddingbag_test.py benchmarks/operator_benchmark/pt/gather_test.py benchmarks/operator_benchmark/pt/index_select_test.py benchmarks/operator_benchmark/pt/linear_test.py benchmarks/operator_benchmark/pt/qatembedding_ops_test.py benchmarks/operator_benchmark/pt/qconv_test.py benchmarks/operator_benchmark/pt/qembedding_bag_lookups_test.py benchmarks/operator_benchmark/pt/qembeddingbag_test.py benchmarks/operator_benchmark/pt/qlinear_test.py build_variables.bzl torch/csrc/Module.cpp torch/csrc/instruction_counter/Module.cpp torch/csrc/instruction_counter/Module.h,https://github.com/pytorch/pytorch/pull/131475,laithsakka,ezyang,,,
26b0a0c2f37,distributed,not user facing,Fix fsdp_state_dict_type_without_warnings (#132621),torch/distributed/checkpoint/state_dict.py,https://github.com/pytorch/pytorch/pull/132621,ppwwyyxx,fegin,,,
e9eb8795bb5,skip,Untopiced,"Revert ""[Memory Snapshot][Viz] Show event timestamps if collected (#132523)""",torch/utils/viz/MemoryViz.js,,,,,,
9641abe97a8,skip,Untopiced,"Revert ""[export] change deepcopy to copy in _replace_with_hop passes (#133142)""",torch/_export/passes/replace_with_hop_pass_util.py,,,,,,
454713fe9d1,releng,not user facing,"Add inductor-cu124, inductor-rocm to upload test stats (#133143)",.github/workflows/upload-test-stats.yml,https://github.com/pytorch/pytorch/pull/133143,clee2000,huydhn,,,
3128640c312,profiler,new features,[Memory Snapshot][Viz] Show event timestamps if collected (#132523),torch/utils/viz/MemoryViz.js,https://github.com/pytorch/pytorch/pull/132523,aaronenyeshi,tianfengfrank,zdevito,,
b06959e6143,skip,not user facing,[export] change deepcopy to copy in _replace_with_hop passes (#133142),torch/_export/passes/replace_with_hop_pass_util.py,https://github.com/pytorch/pytorch/pull/133142,yushangdi,ydwu4,,,
1371c420c32,quantization,not user facing,Migrate binary builds to use Amazon2023 runners (#131826),.github/templates/linux_binary_build_workflow.yml.j2 .github/workflows/_binary-build-linux.yml .github/workflows/_binary-test-linux.yml .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-conda-nightly.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-main.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-main.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-nightly.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/131826,ZainRizvi,malfet,,,
d61815cb7d5,quantization,Untopiced,[torch][ao] Use returned model from Quantizer.transform_for_annotation in prepare_pt2e (#132893),test/quantization/pt2e/test_quantize_pt2e.py torch/ao/quantization/quantize_pt2e.py,https://github.com/pytorch/pytorch/pull/132893,dulinriley,jerryzh168,,,
d51e5467fd6,Uncategorized,Untopiced,TunableOp unconditionally add all validators (#132464),aten/src/ATen/cuda/tunable/Tunable.cpp aten/src/ATen/cuda/tunable/TunableGemm.h,https://github.com/pytorch/pytorch/pull/132464,jeffdaily,zixi-qi,,,
e7d8d735828,skip,not user facing,[minor] Correct in-code documentation for complex numbers and LBFGS (#133020),test/test_optim.py,https://github.com/pytorch/pytorch/pull/133020,janeyx99,lezcano,soulitzer,,
0e4c0ef29fa,optim,Untopiced,fix type of `eta_min` parameter in CosineAnnealing (int -> float) (#132482),torch/optim/lr_scheduler.py,https://github.com/pytorch/pytorch/pull/132482,catwell,janeyx99,,,
d53dfa46806,skip,Untopiced,[BE] Raise when the target model has scalar parameters (#132934),test/distributed/_composable/fsdp/test_fully_shard_init.py test/distributed/_composable/fully_shard/test_fully_shard_init.py torch/distributed/_composable/fsdp/_fsdp_init.py torch/distributed/fsdp/_init_utils.py,https://github.com/pytorch/pytorch/pull/132934,fegin,awgu,wz337,,
afb73d253cf,composability,Untopiced,"[custom_ops] torch.library.{custom_op, register_kernel} disable Dynamo (#133125)",test/dynamo/test_misc.py torch/_library/custom_ops.py torch/library.py,https://github.com/pytorch/pytorch/pull/133125,zou3519,ezyang,,,
fa1d7b02625,skip,Untopiced,"Revert ""Remove unused Caffe2 macros (#132979)""",BUCK.oss buckbuild.bzl build.bzl caffe2/core/macros.h.in cmake/Dependencies.cmake cmake/MiscCheck.cmake,,,,,,
666362865c1,skip,not user facing,[test/profiler] Make test_profiler_pattern_matcher_json_report write … (#133009),test/profiler/test_profiler.py,https://github.com/pytorch/pytorch/pull/133009,kiukchung,zou3519,,,
cc1cc71c460,mps,Untopiced,[MPS] Fix relu for 0-element input case (#133191),aten/src/ATen/native/mps/operations/Activation.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/133191,qqaatw,albanD,,,
cdcc7dc891a,releng,not user facing,update comit pin for xla (#133120),.github/ci_commit_pins/xla.txt,https://github.com/pytorch/pytorch/pull/133120,drisspg,janeyx99,,,
36c4ed8e49f,inductor,not user facing,[inductor] add FreeLibrary to DLLWrapper for Windows. (#133184),torch/_inductor/codecache.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/133184,xuhancn,jansel,jgong5,,
4f0d5f6551e,releng,not user facing,Pin sympy to 1.13.1 (#133235),.ci/docker/requirements-ci.txt .github/requirements/pip-requirements-macOS.txt requirements.txt setup.py,https://github.com/pytorch/pytorch/pull/133235,clee2000,ZainRizvi,atalman,,
80ed3e9ccda,dynamo,not user facing,s/dipatch/dispatch/g (#133192),torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/133192,ezyang,albanD,,,
592682fe221,releng,not user facing,Migrate nightly.yml to use runner determinator (#133225),.github/workflows/nightly.yml,https://github.com/pytorch/pytorch/pull/133225,zxiiro,ZainRizvi,,,
8585dee85df,inductor,not user facing,[inductor] Add some more reinplacing tests (#132839),test/inductor/test_inplacing_pass.py,https://github.com/pytorch/pytorch/pull/132839,zou3519,eellison,,,
e553ef69d08,skip,not user facing,[BE] Fix typo (#133247),tools/testing/update_slow_tests.py,https://github.com/pytorch/pytorch/pull/133247,ZainRizvi,c-p-i-o,zxiiro,,
d4b31f7bcf9,skip,not user facing,Refactor BlockMask constructorr and add Factory func (#132969),test/inductor/test_flex_attention.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/132969,drisspg,Chillee,,,
656465fc775,skip,Untopiced,"Revert ""Conversions between strided and jagged layouts for Nested Tensors (#115749)""",aten/src/ATen/FunctionalInverses.cpp aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorFactories.cpp aten/src/ATen/native/nested/NestedTensorMath.cpp test/expect/HasDecompTest.test_has_decomposition.expect test/test_nestedtensor.py tools/autograd/derivatives.yaml tools/autograd/gen_inplace_or_view_type.py torch/_subclasses/fake_impls.py torch/nested/__init__.py torch/nested/_internal/nested_tensor.py torch/nested/_internal/ops.py,,,,,,
844103197d3,skip,Untopiced,"Revert ""[2/2] PT2 Inductor ComboKernels - automatic horizontal fusing (#131675)""",test/inductor/test_combo_kernels.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/config.py torch/_inductor/scheduler.py,,,,,,
89670d5bdd5,skip,Untopiced,"Revert ""Inductor-CPU WoQ int8 GEMM micro-kernel with scale epilogue (#131887)""",test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/kernel/mm_common.py torch/_inductor/mkldnn_lowerings.py torch/_inductor/quantized_lowerings.py torch/_inductor/utils.py,,,,,,
00aa086298b,skip,Untopiced,"Revert ""[dtensor] move tensor constructors to a separate module (#133129)""",torch/distributed/_tensor/__init__.py torch/distributed/_tensor/_tensor_constructors.py torch/distributed/_tensor/debug/__init__.py torch/distributed/tensor/parallel/style.py,,,,,,
e61def65d5c,nn_frontend,bc breaking,Update fused kernels and call _safe_softmax from SDPA (#131863),aten/src/ATen/native/cpu/FlashAttentionKernel.cpp aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/cuda/mem_eff_attention/epilogue/epilogue_rescale_output.h aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernel_forward.h test/test_nn.py test/test_transformers.py,https://github.com/pytorch/pytorch/pull/131863,drisspg,Chillee,jbschlosser,,
370b072d8d1,inductor,not user facing,[Inductor] support masked vectorization for the tail_loop of the 2d tiles kernel (#130724),aten/src/ATen/cpu/vec/vec512/vec512_bfloat16.h aten/src/ATen/cpu/vec/vec512/vec512_float.h torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/130724,jiayisunx,jansel,jgong5,leslie-fang-intel,
7be77658e98,inductor,not user facing,[Inductor] support masked vectorization for the tail_loop for INT8 datatype (#131155),test/test_mps.py torch/_inductor/codegen/cpp.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/131155,jiayisunx,jansel,jgong5,leslie-fang-intel,
e76f0e0646e,skip,not user facing,Remove QNNPACK reference from setup.py (#133177),setup.py,https://github.com/pytorch/pytorch/pull/133177,cyyever,albanD,,,
f1c439cbedd,skip,not user facing,AutoHeuristic: refactoring (#133170),torchgen/_autoheuristic/train_decision.py,https://github.com/pytorch/pytorch/pull/133170,AlnisM,Chillee,,,
1e9bedf6880,python_frontend,improvements,Add `_codecs.encode` and `builtins.bytearray` to `_get_allowed_globals` to support bytes and bytearray serialization (#133189),test/test_serialization.py torch/_weights_only_unpickler.py,https://github.com/pytorch/pytorch/pull/133189,1ucian0,mikaylagawarecki,,,
fa36eba77d3,inductor,not user facing,Turn off remote caching in unit tests unless explicitly on (#133258),torch/_inductor/compile_fx.py torch/_inductor/runtime/triton_heuristics.py torch/_utils_internal.py,https://github.com/pytorch/pytorch/pull/133258,oulgen,masnesral,,,
4671e98656c,fx,Untopiced,[export] fix node.users when inlining HOOs (#133144),test/export/test_export.py torch/_export/utils.py,https://github.com/pytorch/pytorch/pull/133144,pianpwk,larryliu0820,ydwu4,,
af7830e3534,skip,not user facing,[1/N] Fix clang-tidy warnings in torch/csrc/autograd (#133180),torch/csrc/autograd/functions/accumulate_grad.cpp torch/csrc/autograd/functions/accumulate_grad.h torch/csrc/autograd/functions/basic_ops.cpp torch/csrc/autograd/functions/basic_ops.h torch/csrc/autograd/functions/comm.cpp torch/csrc/autograd/functions/comm.h torch/csrc/autograd/functions/tensor.cpp torch/csrc/autograd/functions/tensor.h torch/csrc/autograd/functions/utils.cpp torch/csrc/autograd/functions/utils.h,https://github.com/pytorch/pytorch/pull/133180,cyyever,albanD,,,
50e837d9c2b,skip,not user facing,[10/N] Fix clang-tidy warnings in aten/src/ATen (#133155),aten/src/ATen/native/cpu/ReduceOpsKernel.cpp aten/src/ATen/native/cpu/SortingKernel.cpp aten/src/ATen/native/cpu/SparseFactories.cpp aten/src/ATen/native/cpu/SpmmReduceKernel.cpp aten/src/ATen/native/cpu/SumKernel.cpp aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp,https://github.com/pytorch/pytorch/pull/133155,cyyever,ezyang,janeyx99,,
9f17037e8ba,skip,not user facing,[dtensor] move tensor constructors to the api module (#133129),torch/distributed/_tensor/__init__.py torch/distributed/_tensor/_utils.py torch/distributed/_tensor/api.py torch/distributed/_tensor/debug/__init__.py torch/distributed/_tensor/ops/_math_ops.py torch/distributed/_tensor/ops/utils.py torch/distributed/tensor/parallel/style.py,https://github.com/pytorch/pytorch/pull/133129,wanchaol,awgu,tianyu-l,,
947a446be43,releng,not user facing,[executorch hash update] update the pinned executorch hash (#131420),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/131420,pytorchupdatebot,pytorchbot,,,
40061bd61ef,Uncategorized,Untopiced,[export] overwrite placeholder names when deepcopying (#133269),torch/export/experimental/__init__.py,https://github.com/pytorch/pytorch/pull/133269,pianpwk,dvorjackz,ydwu4,zhxchen17,
77561752731,Uncategorized,performance,Add Sleef Implementation for maximum Kernel for ARM (#131642),aten/src/ATen/cpu/vec/vec256/vec256_float_neon.h,https://github.com/pytorch/pytorch/pull/131642,akote123,jgong5,snadampal,,
e0a5536cc9a,skip,not user facing,[2/N] Fix clang-tidy warnings in torch/csrc/autograd  (#133295),torch/csrc/autograd/VariableTypeManual.cpp torch/csrc/autograd/VariableTypeUtils.h torch/csrc/autograd/anomaly_mode.cpp torch/csrc/autograd/autograd.cpp torch/csrc/autograd/autograd_meta.cpp torch/csrc/autograd/autograd_not_implemented_fallback.cpp torch/csrc/autograd/cpp_hook.cpp torch/csrc/autograd/custom_function.cpp torch/csrc/autograd/custom_function.h torch/csrc/autograd/forward_grad.cpp torch/csrc/autograd/function.cpp torch/csrc/autograd/init.cpp torch/csrc/autograd/input_buffer.cpp torch/csrc/autograd/input_metadata.cpp torch/csrc/autograd/jit_decomp_interface.cpp torch/csrc/autograd/profiler_legacy.cpp torch/csrc/autograd/profiler_legacy.h torch/csrc/autograd/python_anomaly_mode.cpp torch/csrc/autograd/python_anomaly_mode.h torch/csrc/autograd/python_cpp_function.cpp torch/csrc/autograd/python_function.cpp torch/csrc/autograd/python_hook.cpp torch/csrc/autograd/python_legacy_variable.cpp torch/csrc/autograd/python_nested_functions_manual.cpp torch/csrc/autograd/python_torch_functions_manual.cpp torch/csrc/autograd/python_variable_indexing.h torch/csrc/autograd/record_function_ops.cpp torch/csrc/autograd/utils/error_messages.h torch/csrc/autograd/utils/grad_layout_contract.h torch/csrc/autograd/utils/python_arg_parsing.h torch/csrc/autograd/utils/warnings.cpp torch/csrc/autograd/utils/warnings.h torch/csrc/autograd/utils/wrap_outputs.h,https://github.com/pytorch/pytorch/pull/133295,cyyever,Skylion007,,,
095c5ccf9fa,releng,not user facing,[CD] Change XPU nightly build back to ABI=0 (#132854),.github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/132854,chuanqi129,atalman,,,
4cca18d5b65,skip,Untopiced,"Revert ""Update fused kernels and call _safe_softmax from SDPA (#131863)""",aten/src/ATen/native/cpu/FlashAttentionKernel.cpp aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/cuda/mem_eff_attention/epilogue/epilogue_rescale_output.h aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernel_forward.h test/test_nn.py test/test_transformers.py,,,,,,
cd565bc4555,quantization,Untopiced,Refactor process_inputs outside of create_aot_dispatcher_function (#130962),test/dynamo/test_aot_autograd_cache.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_functorch/aot_autograd.py,https://github.com/pytorch/pytorch/pull/130962,jamesjwu,bdhirsh,,,
c518b50c4cc,skip,not user facing,Remove functorch dispatch keys in `legacyExtractDispatchKey` (#133018),c10/core/DispatchKeySet.h test/functorch/test_eager_transforms.py,https://github.com/pytorch/pytorch/pull/133018,guilhermeleobas,zou3519,,,
2e7d67e6af4,releng,not user facing,Migrate slow.yml jobs to use runner determinator (#133232),.github/workflows/slow.yml,https://github.com/pytorch/pytorch/pull/133232,zxiiro,ZainRizvi,,,
2f30473fba6,mobile,Untopiced,[19/N] Fix clang-tidy warnings in jit (#133067),torch/csrc/jit/mobile/code.h torch/csrc/jit/mobile/compatibility/backport.cpp torch/csrc/jit/mobile/compatibility/backport.h torch/csrc/jit/mobile/compatibility/backport_manager.cpp torch/csrc/jit/mobile/compatibility/backport_manager.h torch/csrc/jit/mobile/compatibility/model_compatibility.cpp torch/csrc/jit/mobile/compatibility/model_compatibility.h torch/csrc/jit/mobile/compatibility/runtime_compatibility.cpp torch/csrc/jit/mobile/compatibility/runtime_compatibility.h torch/csrc/jit/mobile/import.cpp torch/csrc/jit/mobile/import.h torch/csrc/jit/mobile/import_data.cpp torch/csrc/jit/mobile/model_tracer/BuildFeatureTracer.cpp torch/csrc/jit/mobile/model_tracer/BuildFeatureTracer.h torch/csrc/jit/mobile/model_tracer/CustomClassTracer.cpp torch/csrc/jit/mobile/model_tracer/CustomClassTracer.h torch/csrc/jit/mobile/model_tracer/KernelDTypeTracer.cpp torch/csrc/jit/mobile/model_tracer/KernelDTypeTracer.h torch/csrc/jit/mobile/model_tracer/MobileModelRunner.cpp torch/csrc/jit/mobile/model_tracer/MobileModelRunner.h torch/csrc/jit/mobile/model_tracer/OperatorCallTracer.cpp torch/csrc/jit/mobile/model_tracer/OperatorCallTracer.h torch/csrc/jit/mobile/model_tracer/TensorUtils.cpp torch/csrc/jit/mobile/model_tracer/TensorUtils.h torch/csrc/jit/mobile/model_tracer/TracerRunner.cpp torch/csrc/jit/mobile/model_tracer/TracerRunner.h torch/csrc/jit/mobile/model_tracer/tracer.cpp torch/csrc/jit/mobile/module.cpp torch/csrc/jit/mobile/parse_bytecode.cpp torch/csrc/jit/mobile/promoted_prim_ops.cpp torch/csrc/jit/mobile/train/export_data.cpp torch/csrc/jit/mobile/train/export_data.h torch/csrc/jit/mobile/train/optim/sgd.cpp torch/csrc/jit/mobile/train/optim/sgd.h torch/csrc/jit/mobile/train/random.cpp torch/csrc/jit/mobile/train/random.h torch/csrc/jit/mobile/train/sequential.cpp torch/csrc/jit/mobile/train/sequential.h torch/csrc/jit/mobile/upgrader_mobile.h,https://github.com/pytorch/pytorch/pull/133067,cyyever,Skylion007,,,
660436d843e,releng,not user facing,Convert Periodic to use Amazon2023 runners (#133036),.github/workflows/_buck-build-test.yml .github/workflows/_run_android_tests.yml .github/workflows/periodic.yml,https://github.com/pytorch/pytorch/pull/133036,ZainRizvi,clee2000,zxiiro,,
aa4fbba42d2,skip,not user facing,Make q info optional in prep for inference (#133261),test/inductor/test_flex_attention.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/133261,drisspg,Chillee,,,
19270cff616,optim,docs,Add a reference for the LRScheduler class (#133243),docs/source/optim.rst,https://github.com/pytorch/pytorch/pull/133243,spzala,janeyx99,,,
89795da5e30,inductor,not user facing,[inductor] process compile_only case in all build options class. (#129975),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/129975,xuhancn,henrylhtsang,,,
be400ee2b49,inductor,not user facing,[inductor][test] Fix test_vertical_pointwise_reduction_fusion (#133276),test/inductor/test_unbacked_symints.py,https://github.com/pytorch/pytorch/pull/133276,henrylhtsang,ColinPeppler,,,
7172c732d9c,Uncategorized,Untopiced,[Memory Snapshot] Skip C++ warmup unwind() call if context is not set (#133038),torch/csrc/cuda/memory_snapshot.cpp,https://github.com/pytorch/pytorch/pull/133038,aaronenyeshi,eqy,,,
dadb20a9d63,profiler,new features,[Memory Snapshot][Viz] Add Allocator Settings Tab (#132518),torch/utils/viz/MemoryViz.js,https://github.com/pytorch/pytorch/pull/132518,aaronenyeshi,tianfengfrank,zdevito,,
19416bf38b3,inductor,not user facing,"Reland ""[2/2] PT2 Inductor ComboKernels - automatic horizontal fusing (#131675)"" (#133291)",test/inductor/test_combo_kernels.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/config.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/133291,qchip,wdvr,,,
a1ca4dfe0ba,onnx,bug fixes,[ONNX] Fix onnx conversion scaled_dot_product_attention (#133314),torch/onnx/symbolic_opset14.py,https://github.com/pytorch/pytorch/pull/133314,xadupre,Skylion007,justinchuby,,
2fde1934f9e,skip,not user facing,typing for remote_cache (#133299),torch/_inductor/remote_cache.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/133299,aorenste,Skylion007,,,
657d58bbd83,inductor,not user facing,[inductor] Fix test_codecache::test_inductor_counters (#133244),test/inductor/test_codecache.py,https://github.com/pytorch/pytorch/pull/133244,masnesral,eellison,,,
61625a18ef1,profiler,not user facing,[profiler] Only parse kineto requests and build tree when required (#132713),test/profiler/test_profiler.py torch/autograd/profiler.py,https://github.com/pytorch/pytorch/pull/132713,briancoutinho,sraikund16,,,
dfc7c860e41,fx,Untopiced,Allow SymInt input for torch.fx reinplace pass (#133178),test/test_fx_reinplace_pass.py torch/fx/passes/reinplace.py,https://github.com/pytorch/pytorch/pull/133178,YangQun1,ezyang,,,
fa7ae6cdbca,skip,not user facing,can't infer device on benchmarked function with no args or kwargs (#133290),test/inductor/test_inductor_utils.py,https://github.com/pytorch/pytorch/pull/133290,nmacchioni,eellison,,,
69de9e78e99,skip,Untopiced,"Revert ""typing for remote_cache (#133299)""",torch/_inductor/remote_cache.py torch/_inductor/runtime/triton_heuristics.py,,,,,,
a9d34138dfd,skip,not user facing,[traced-graph][sparse] add to_dense() operation to sparse export test (#133175),test/export/test_sparse.py,https://github.com/pytorch/pytorch/pull/133175,aartbik,ezyang,,,
5a1d4f7ddcc,releng,not user facing,Migrate lint.yml to runner determinator (#133320),.github/workflows/lint.yml,https://github.com/pytorch/pytorch/pull/133320,zxiiro,Skylion007,,,
8e074c4583b,distributed,not user facing,[ROCm] skip SymmetricMemory related UTs for ROCm (#133241),test/distributed/test_symmetric_memory.py,https://github.com/pytorch/pytorch/pull/133241,pragupta,malfet,pruthvistony,,
41da5283786,skip,not user facing,[BE] Skip inductor+profiler test for templates if we didn't run select_autotune (#133344),test/inductor/test_profiler.py,https://github.com/pytorch/pytorch/pull/133344,davidberard98,masnesral,,,
c17d26c3c1b,inductor,Untopiced,[AOTI][Tooling] A couple fixes / minor updates for initial debug printer (#133016),torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cuda/cuda_cpp_scheduling.py torch/_inductor/codegen/debug_utils.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/wrapper.py torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/133016,YUNQIUGUO,ColinPeppler,,,
9de023d44d8,dynamo,not user facing,[Dynamo] Make torch.Size can be reconstructed by LOAD_CONST (#133342),test/dynamo/test_functions.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/133342,yanboliang,jansel,mlazos,,
caba37e99b0,nn_frontend,bc breaking,Update fused kernels and call _safe_softmax from SDPA (#131863),aten/src/ATen/native/cpu/FlashAttentionKernel.cpp aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/cuda/mem_eff_attention/epilogue/epilogue_rescale_output.h aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernel_forward.h test/functorch/test_ops.py test/test_nn.py test/test_transformers.py tools/autograd/derivatives.yaml torch/csrc/autograd/FunctionsManual.cpp torch/csrc/autograd/FunctionsManual.h torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/131863,drisspg,Chillee,jbschlosser,,
c91bc499f7a,releng,not user facing,[CI] Do not emit color escape sequence during testing (#133350),.ci/pytorch/test.sh,https://github.com/pytorch/pytorch/pull/133350,malfet,zou3519,,,
5947169c9d5,dynamo,not user facing,Add missing endline in exception message (#133240),torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/133240,guilhermeleobas,Skylion007,,,
fbb0adbc32a,Uncategorized,Untopiced,[TunableOp] lazy init TuningContext singleton (#133347),aten/src/ATen/cuda/tunable/Tunable.cpp,https://github.com/pytorch/pytorch/pull/133347,jeffdaily,zixi-qi,,,
35785984013,skip,not user facing,[11/N] Fix clang-tidy warnings in aten/src/ATen  (#133298),aten/src/ATen/native/cpu/CatKernel.h aten/src/ATen/native/cpu/ChannelShuffleKernel.h aten/src/ATen/native/cpu/CopyKernel.h aten/src/ATen/native/cpu/DistributionTemplates.h aten/src/ATen/native/cpu/GridSamplerKernel.h aten/src/ATen/native/cpu/IndexKernelUtils.h aten/src/ATen/native/cpu/IsContiguous.h aten/src/ATen/native/cpu/LogAddExp.h aten/src/ATen/native/cpu/PixelShuffleKernel.h aten/src/ATen/native/cpu/Reduce.h aten/src/ATen/native/cpu/ReduceUtils.h aten/src/ATen/native/cpu/SampledAddmmKernel.h aten/src/ATen/native/cpu/SerialStackImpl.h aten/src/ATen/native/cpu/StackKernel.h aten/src/ATen/native/cpu/WeightNormKernel.h aten/src/ATen/native/cpu/mixed_data_type.h aten/src/ATen/native/cpu/moments_utils.h aten/src/ATen/native/cpu/utils.h aten/src/ATen/native/cpu/zmath.h,https://github.com/pytorch/pytorch/pull/133298,cyyever,ezyang,,,
7666ef9d9b0,releng,not user facing,[GHF] Fix co-authors attribution (#133372),.github/scripts/trymerge.py,https://github.com/pytorch/pytorch/pull/133372,malfet,kit1980,,,
c2eeda5da0e,cpp_frontend,not user facing,[structural binding][12/N] Replace std::tie with structural binding  (#131031),aten/src/ATen/native/cuda/Normalization.cu test/cpp/api/nn_utils.cpp,https://github.com/pytorch/pytorch/pull/131031,cyyever,ezyang,,,
f23dbefe525,fx,Untopiced,"[export] Support ""custom"" metadata field. (#131912)",test/export/test_serialize.py torch/_export/serde/schema.py torch/_export/serde/schema.yaml torch/_export/serde/serialize.py torch/export/_trace.py torch/fx/proxy.py,https://github.com/pytorch/pytorch/pull/131912,zhxchen17,angelayi,,,
4af4910b1ad,fx,Untopiced,"Reland ""Construct NJT without graph breaks"" (#133196)",test/dynamo/test_subclasses.py test/test_nestedtensor.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py torch/_subclasses/fake_tensor.py torch/_subclasses/functional_tensor.py torch/_subclasses/meta_utils.py torch/fx/passes/fake_tensor_prop.py torch/nested/_internal/nested_tensor.py torch/nested/_internal/ops.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/133196,soulitzer,ezyang,,,
5fff960004a,inductor,Untopiced,[PT2][Optimus] Extend split_stack_to_cats when split and stack have different dims (#133060),test/inductor/test_split_cat_fx_passes.py torch/_inductor/fx_passes/split_cat.py,https://github.com/pytorch/pytorch/pull/133060,mengluy0125,jackiexu1992,,,
14750dd737b,optim,not user facing,Correct return type of grouping helper function in Optimizer (#133360),torch/optim/adadelta.py torch/optim/adagrad.py torch/optim/adam.py torch/optim/adamax.py torch/optim/adamw.py torch/optim/asgd.py torch/optim/nadam.py torch/optim/optimizer.py torch/optim/radam.py torch/optim/rmsprop.py torch/optim/rprop.py torch/optim/sgd.py,https://github.com/pytorch/pytorch/pull/133360,janeyx99,albanD,,,
d1d6b370ce8,releng,not user facing,Upgrade nightly wheels to rocm6.2 - 1 of 2 (docker images) (#132875),.ci/docker/aotriton_version.txt .ci/docker/common/aotriton_version.txt .ci/docker/common/install_miopen.sh .ci/docker/libtorch/Dockerfile .ci/docker/manywheel/Dockerfile .github/workflows/build-libtorch-images.yml .github/workflows/build-manywheel-images.yml,https://github.com/pytorch/pytorch/pull/132875,jithunnair-amd,atalman,,,
1206958d89f,dynamo,not user facing,[Dynamo] add EventVariable reconstruct (#133236),test/dynamo/test_ctx_manager.py torch/_dynamo/variables/ctx_manager.py,https://github.com/pytorch/pytorch/pull/133236,yf225,yifuwang,,,
918367ebb07,releng,not user facing,Add new runner: G4DN Extra Large with T4 for windows binary builds (#133229),.github/actionlint.yaml .github/lf-canary-scale-config.yml .github/lf-scale-config.yml,https://github.com/pytorch/pytorch/pull/133229,wdvr,ZainRizvi,,,
f951fcd1d7c,skip,Untopiced,Inductor-CPU WoQ int8 GEMM micro-kernel with scale epilogue (#131887),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/kernel/mm_common.py torch/_inductor/mkldnn_lowerings.py torch/_inductor/quantized_lowerings.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/131887,sanchitintel,jansel,jgong5,leslie-fang-intel,
2a4304329be,fx,Untopiced,[wip][lowering] Add max_acc_splits (#133041),torch/fx/passes/splitter_base.py,https://github.com/pytorch/pytorch/pull/133041,qxy11,hl475,,,
7aee3376e27,skip,not user facing,[aotd] HOP effect tokens wrapper above SubclassWrapper (#131672),test/higher_order_ops/test_with_effects.py torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/_aot_autograd/traced_function_transforms.py torch/_functorch/_aot_autograd/utils.py,https://github.com/pytorch/pytorch/pull/131672,IvanKobzarev,bdhirsh,zou3519,,
4d11a9b7839,skip,not user facing,[CI] Fix rowwise scaling tests on h100 (#133384),aten/src/ATen/native/cuda/RowwiseScaledMM.cu test/test_matmul_cuda.py,https://github.com/pytorch/pytorch/pull/133384,drisspg,malfet,nWEIdia,,
a30504b2a23,fx,Untopiced,fix silly error when printing diff (#133345),test/export/test_export.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/133345,avikchaudhuri,ezyang,,,
a6413d2924b,dynamo,not user facing,Regression test for S429861 (#133376),test/inductor/s429861_repro.py test/inductor/test_compiled_optimizers.py torch/_dynamo/debug_utils.py,https://github.com/pytorch/pytorch/pull/133376,mlazos,eellison,,,
d143f879e2d,distributed,not user facing,[DTensor] Add more aten._foreach ops to _pointwise_ops.py (#133271),torch/distributed/_tensor/ops/_pointwise_ops.py,https://github.com/pytorch/pytorch/pull/133271,wz337,awgu,,,
ef580a0e5cc,distributed,not user facing,[DeviceMesh] Restrict slicing to be a contiguous or non-contiguous subsequence of the root mesh_dim_names (#133193),test/distributed/test_device_mesh.py torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/133193,wz337,fegin,wanchaol,,
b5711297a0c,dynamo,new features,Add support for SetVariable.discard (#133317),test/dynamo/test_misc.py torch/_dynamo/variables/dicts.py,https://github.com/pytorch/pytorch/pull/133317,ezyang,Skylion007,,,
52741043e71,inductor,not user facing,[Inductor][FlexAttention] Support non-divisible sequence lengths (#133019),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/133019,yanboliang,Chillee,,,
efc6e8457a2,inductor,not user facing,[inductor] [cpp] fix the reindexer from template_buffer to Y (#133070),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py,https://github.com/pytorch/pytorch/pull/133070,chunyuan-w,jgong5,,,
378b12f3ad5,skip,not user facing,Improve namespace for `c10::MemoryFormat::Contiguous` in `torchgen/api/cpp.py` (#131622),torchgen/api/cpp.py torchgen/api/python.py,https://github.com/pytorch/pytorch/pull/131622,shink,zou3519,,,
854a5ba9588,skip,not user facing,[lint] fix lint broken by #131912 (#133428),test/export/test_serialize.py,https://github.com/pytorch/pytorch/pull/133428,nmacchioni,aaronenyeshi,,,
e554f71d7e4,dynamo,Untopiced,Implement filter in dynamo (#131674),test/dynamo/test_functions.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/131674,isuruf,amjames,jansel,,
62cd065de20,jit,bug fixes,Validate that node TK_ASSIGN have field initialized (#127878),torch/csrc/jit/serialization/import_source.cpp,https://github.com/pytorch/pytorch/pull/127878,apach301,malfet,,,
118b2a41390,releng,not user facing,Convert inductor jobs to Linux Amazon 2023 (#133352),.github/workflows/_linux-build.yml .github/workflows/inductor.yml,https://github.com/pytorch/pytorch/pull/133352,ZainRizvi,seemethere,zxiiro,,
7f40ac9be2b,releng,not user facing,Migrate periodic jobs to use runner determinator (#133124),.github/workflows/periodic.yml,https://github.com/pytorch/pytorch/pull/133124,zxiiro,ZainRizvi,,,
d114fd78bd8,distributed,Untopiced,[FSDP2] Enable HSDP + TP (#133335),test/distributed/_composable/fsdp/test_fully_shard_state_dict.py test/distributed/_composable/fsdp/test_fully_shard_training.py torch/distributed/_composable/fsdp/_fsdp_param.py,https://github.com/pytorch/pytorch/pull/133335,fegin,awgu,,,
4bb1650ca39,inductor,not user facing,Bump maxinum num warps (#132458),benchmarks/dynamo/timm_models.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/132458,eellison,Chillee,shunting314,,
7b269cc4843,releng,not user facing,[TD] llm retrieval to not use bash -l {0} (#133464),.github/workflows/llm_td_retrieval.yml,https://github.com/pytorch/pytorch/pull/133464,clee2000,kit1980,,,
07c73a964b8,mps,bc breaking,[MPS][BE] Delete MacOS-12.3 specific checks (#133141),aten/src/ATen/mps/IndexKernels.h aten/src/ATen/mps/MPSDevice.h aten/src/ATen/mps/MPSDevice.mm aten/src/ATen/mps/MPSHooks.mm aten/src/ATen/native/RNN.cpp aten/src/ATen/native/mps/operations/BinaryOps.mm aten/src/ATen/native/mps/operations/Distributions.mm aten/src/ATen/native/mps/operations/Eye.mm aten/src/ATen/native/mps/operations/Indexing.mm aten/src/ATen/native/mps/operations/Pad.mm aten/src/ATen/native/mps/operations/PixelShuffle.mm aten/src/ATen/native/mps/operations/Pooling.mm aten/src/ATen/native/mps/operations/ReduceOps.mm aten/src/ATen/native/mps/operations/Repeat.mm aten/src/ATen/native/mps/operations/ScatterGather.mm aten/src/ATen/native/mps/operations/Shape.mm aten/src/ATen/native/mps/operations/Sort.mm aten/src/ATen/native/mps/operations/TensorCompare.mm aten/src/ATen/native/mps/operations/UnaryOps.mm aten/src/ATen/native/mps/operations/Unique.mm aten/src/ATen/native/mps/operations/UpSample.mm aten/src/ATen/native/mps/operations/View.mm,https://github.com/pytorch/pytorch/pull/133141,malfet,atalman,clee2000,kulinseth,
d46e0761caa,skip,Untopiced,"Revert ""[11/N] Fix clang-tidy warnings in aten/src/ATen  (#133298)""",aten/src/ATen/native/cpu/CatKernel.h aten/src/ATen/native/cpu/ChannelShuffleKernel.h aten/src/ATen/native/cpu/CopyKernel.h aten/src/ATen/native/cpu/DistributionTemplates.h aten/src/ATen/native/cpu/GridSamplerKernel.h aten/src/ATen/native/cpu/IndexKernelUtils.h aten/src/ATen/native/cpu/IsContiguous.h aten/src/ATen/native/cpu/LogAddExp.h aten/src/ATen/native/cpu/PixelShuffleKernel.h aten/src/ATen/native/cpu/Reduce.h aten/src/ATen/native/cpu/ReduceUtils.h aten/src/ATen/native/cpu/SampledAddmmKernel.h aten/src/ATen/native/cpu/SerialStackImpl.h aten/src/ATen/native/cpu/StackKernel.h aten/src/ATen/native/cpu/WeightNormKernel.h aten/src/ATen/native/cpu/mixed_data_type.h aten/src/ATen/native/cpu/moments_utils.h aten/src/ATen/native/cpu/utils.h aten/src/ATen/native/cpu/zmath.h,,,,,,
cf811800078,skip,not user facing,allow `SubConfigProxy` of arbitrary depth (#133418),torch/utils/_config_module.py,https://github.com/pytorch/pytorch/pull/133418,nmacchioni,oulgen,,,
6f51782a591,dynamo,not user facing,Add comptime.sleep (#133362),test/dynamo/test_comptime.py torch/_dynamo/comptime.py,https://github.com/pytorch/pytorch/pull/133362,oulgen,jamesjwu,,,
63e5b092184,distributed,not user facing,Add unit test for asymmetric compilation (#133363),test/distributed/test_dynamo_distributed.py,https://github.com/pytorch/pytorch/pull/133363,oulgen,jamesjwu,,,
6980e9e569f,inductor,Untopiced,[AOTI] Disable split_cat_aten passes (#133014),torch/_inductor/fx_passes/pre_grad.py,https://github.com/pytorch/pytorch/pull/133014,huxintong,frank-wei,,,
bedf96d7ffe,inductor,Untopiced,[AOTI] Switch fbcode HIP to C shim version v2 (#133105),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/133105,desertfire,ColinPeppler,zoranzhao,,
86cb24e6ebf,devx,not user facing,[CI] Change inductor-perf-test-nightly naming (#131476),.github/workflows/inductor-perf-test-nightly.yml .github/workflows/upload-torch-dynamo-perf-stats.yml,https://github.com/pytorch/pytorch/pull/131476,desertfire,huydhn,zou3519,,
2e8c1be9478,skip,not user facing,Update date for 2.5 in RELEASE.md (#133503),RELEASE.md,https://github.com/pytorch/pytorch/pull/133503,kit1980,atalman,,,
c0be0105c76,releng,not user facing,[aarch64] Replace OpenBLAS with NVPL in cuda arm docker (#132811),.ci/docker/common/install_nvpl.sh .ci/docker/manywheel/Dockerfile_cuda_aarch64,https://github.com/pytorch/pytorch/pull/132811,tinglvv,atalman,,,
44eaef46d02,distributed (checkpoint),bug fixes,[DCP] Fix meta tensor loading (#133256),test/distributed/checkpoint/test_tp_checkpoint.py torch/distributed/checkpoint/planner_helpers.py,https://github.com/pytorch/pytorch/pull/133256,wz337,fegin,,,
3fc9ee5a319,distributed,not user facing,[DeviceMesh] Directly retrieve flattened mesh if already created (#133195),test/distributed/test_device_mesh.py torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/133195,wz337,fegin,wanchaol,,
d8c494910bc,releng,not user facing,[EZ] Enable explicitly opting into the old Linux Amazon 2 ami - Pt 1 (#133469),.github/lf-canary-scale-config.yml .github/lf-scale-config.yml,https://github.com/pytorch/pytorch/pull/133469,ZainRizvi,clee2000,,,
3965f118376,inductor,Untopiced,Minor type annotation updates following up D60954888 (#133382),torch/_inductor/codegen/common.py torch/_inductor/codegen/debug_utils.py torch/_inductor/codegen/simd.py,https://github.com/pytorch/pytorch/pull/133382,YUNQIUGUO,ColinPeppler,,,
7a74294786e,skip,not user facing,[sparse] enable meta tests (#133379),test/test_sparse.py,https://github.com/pytorch/pytorch/pull/133379,aartbik,ezyang,,,
6d4287419cc,onnx,not user facing,[ONNX] Disable op_level_debug tests (#133485),test/onnx/dynamo/test_dynamo_with_onnxruntime_backend.py test/onnx/onnx_test_common.py test/onnx/pytorch_test_common.py test/onnx/test_fx_op_consistency.py test/onnx/test_fx_to_onnx.py test/onnx/test_fx_to_onnx_with_onnxruntime.py,https://github.com/pytorch/pytorch/pull/133485,justinchuby,titaiwangms,,,
dcdb25453e0,skip,Untopiced,Make FX Graph Cache work with distributed training (#133374),test/distributed/test_dynamo_distributed.py torch/_inductor/codecache.py torch/_inductor/compile_fx.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/133374,oulgen,ezyang,,,
8676401707f,mps,improvements,[MPS] Enable MPS mm from macOS >= 14.4 (#133494),aten/src/ATen/mps/MPSDevice.mm aten/src/ATen/native/mps/operations/LinearAlgebra.mm,https://github.com/pytorch/pytorch/pull/133494,DenisVieriu97,malfet,,,
c2b2969b5da,skip,not user facing,made some args optional in create_mask (#133413),torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/133413,Chillee,drisspg,yanboliang,,
1120b5ab55b,skip,Untopiced,"Revert ""[CI] Change inductor-perf-test-nightly naming (#131476)""",.github/workflows/inductor-perf-test-nightly.yml .github/workflows/upload-torch-dynamo-perf-stats.yml,,,,,,
8624a571b4e,skip,not user facing,[Inductor][CPP] Support vectorization of remainder (#129849),aten/src/ATen/cpu/vec/functional_base.h aten/src/ATen/native/cpu/BinaryOpsKernel.cpp test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/129849,leslie-fang-intel,jgong5,lezcano,,
448d54ee929,skip,not user facing,AutoHeuristic: instructions (#132894),torchgen/_autoheuristic/README.md,https://github.com/pytorch/pytorch/pull/132894,AlnisM,Chillee,,,
7eb31e5023f,skip,not user facing,Add cache timings info to tlparse (#133504),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/133504,oulgen,jamesjwu,,,
c88174df957,inductor,Untopiced,typing for remote_cache (#133446),torch/_inductor/remote_cache.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/133446,aorenste,oulgen,,,
e6272acaec6,skip,not user facing,C++ network flow implementation in c10 (#132188),c10/test/util/NetworkFlow_test.cpp c10/util/NetworkFlow.cpp c10/util/NetworkFlow.h,https://github.com/pytorch/pytorch/pull/132188,davidberard98,Chillee,,,
cf1fc07bd44,distributed,Untopiced,[DTensor][Easy] Minor fix to Partial Placement Docstring (#133149),torch/distributed/_tensor/placement_types.py,https://github.com/pytorch/pytorch/pull/133149,wz337,XilunWu,tianyu-l,,
b6335cfeab1,inductor,not user facing,Add an option to use do_bench_using_profiling in TORCHINDUCTOR_PROFILE (#133523),torch/_inductor/codegen/triton.py torch/_inductor/config.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/133523,y-sq,nmacchioni,,,
b0fc6aa412a,linalg_frontend,Untopiced,fix a typo in the householder_product docs (#124279),torch/linalg/__init__.py,https://github.com/pytorch/pytorch/pull/124279,melnikovsky,lezcano,,,
142353eca3c,skip,not user facing,AutoHeuristic: util scripts (#133409),torchgen/_autoheuristic/benchmark_runner.py torchgen/_autoheuristic/collect_data.sh torchgen/_autoheuristic/generate_heuristic.sh torchgen/_autoheuristic/merge_data.py torchgen/_autoheuristic/requirements.txt,https://github.com/pytorch/pytorch/pull/133409,AlnisM,Chillee,,,
f32a9e953fe,skip,not user facing,AutoHeuristic: mixed_mm documentation (#133410),torchgen/_autoheuristic/mixed_mm/README.md torchgen/_autoheuristic/mixed_mm/gen_data_mixed_mm.py torchgen/_autoheuristic/mixed_mm/generate_heuristic_mixedmm.sh,https://github.com/pytorch/pytorch/pull/133410,AlnisM,Chillee,,,
9876aa39c02,skip,not user facing,AutoHeuristic: pad_mm documentation (#133411),torchgen/_autoheuristic/pad_mm/README.md torchgen/_autoheuristic/pad_mm/generate_heuristic_pad_mm.sh,https://github.com/pytorch/pytorch/pull/133411,AlnisM,Chillee,,,
bbddde311a6,releng,not user facing,Migrate inductor jobs to runner determinator (#133457),.github/workflows/inductor.yml,https://github.com/pytorch/pytorch/pull/133457,zxiiro,ZainRizvi,,,
32d890745dd,skip,Untopiced,"Revert ""Add cache timings info to tlparse (#133504)""",torch/_inductor/codecache.py,,,,,,
07adae3dac8,skip,Untopiced,"Revert ""Make FX Graph Cache work with distributed training (#133374)""",test/distributed/test_dynamo_distributed.py torch/_inductor/codecache.py torch/_inductor/compile_fx.py torch/_inductor/config.py,,,,,,
59e33cd1f4e,skip,Untopiced,[FSDP2] Set `ctx.set_materialize_grads(False)` for post-backward (#133498),torch/distributed/_composable/fsdp/_fsdp_param_group.py,https://github.com/pytorch/pytorch/pull/133498,awgu,weifengpy,,,
ec49ce5f8ee,cuda,Untopiced,[CUDA]: Add frexp CUDA bfloat16 support (#133313),aten/src/ATen/native/cuda/UnaryOpsKernel.cu torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/133313,Skylion007,eqy,ezyang,,
a6ad834fa8b,skip,not user facing,Fix counting execution time in run_test.py (#133199),test/run_test.py,https://github.com/pytorch/pytorch/pull/133199,zeshengzong,clee2000,,,
57d1ffc5125,onnx,not user facing,Ignore `torch.onnx._internal` in `test_circular_dependencies` (#133110),test/test_testing.py,https://github.com/pytorch/pytorch/pull/133110,justinchuby,malfet,titaiwangms,,
758a0a88a2b,releng,not user facing,[BE][Easy] enable `ruff` rule `PIE790`: unnecessary `pass` statement (#133200),.github/scripts/gitutils.py benchmarks/functional_autograd_benchmark/torchvision_models.py pyproject.toml test/cpp/jit/tests_setup.py test/distributed/_composable/test_replicate_with_compiler.py test/distributed/elastic/rendezvous/rendezvous_backend_test.py test/distributed/fsdp/test_fsdp_comm_hooks.py test/distributed/test_c10d_gloo.py test/export/test_converter.py test/inductor/test_compiled_autograd.py test/inductor/test_flex_attention.py test/inductor/test_group_batch_fusion.py test/inductor/test_perf.py test/jit/test_backends.py test/jit/test_class_type.py test/jit/test_module_interface.py test/scripts/cuda_memcheck_common.py test/test_autograd.py test/test_mobile_optimizer.py test/test_tensorexpr_pybind.py test/test_testing.py tools/linter/adapters/flake8_linter.py tools/testing/target_determination/heuristics/interface.py torch/_awaits/__init__.py torch/_dynamo/debug_utils.py torch/_dynamo/decorators.py torch/_dynamo/repro/after_aot.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/misc.py torch/_export/passes/replace_with_hop_pass_util.py torch/_export/verifier.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_inductor/autoheuristic/autoheuristic.py torch/_inductor/codecache.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/wrapper.py torch/_inductor/cudagraph_trees.py torch/_inductor/fx_passes/group_batch_fusion.py torch/_inductor/virtualized.py torch/_lobpcg.py torch/_meta_registrations.py torch/ao/ns/_numeric_suite.py torch/ao/ns/fx/graph_matcher.py torch/ao/quantization/fx/_model_report/detector.py torch/ao/quantization/fx/quantize_handler.py torch/ao/quantization/quantizer/quantizer.py torch/ao/quantization/utils.py torch/autograd/forward_ad.py torch/autograd/gradcheck.py torch/backends/_nnapi/prepare.py torch/cuda/_sanitizer.py torch/distributed/_shard/sharder.py torch/distributed/_shard/sharding_plan/api.py torch/distributed/_shard/sharding_spec/api.py torch/distributed/_tensor/_op_schema.py torch/distributed/_tensor/ops/_view_ops.py torch/distributed/_tools/fsdp2_mem_tracker.py torch/distributed/_tools/mem_tracker.py torch/distributed/algorithms/join.py torch/distributed/benchmarks/benchmark_ddp_rpc.py torch/distributed/checkpoint/filesystem.py torch/distributed/checkpoint/planner.py torch/distributed/checkpoint/staging.py torch/distributed/checkpoint/storage.py torch/distributed/distributed_c10d.py torch/distributed/elastic/multiprocessing/api.py torch/distributed/elastic/timer/api.py torch/distributed/rpc/backend_registry.py torch/fx/experimental/symbolic_shapes.py torch/fx/operator_schemas.py torch/fx/passes/infra/pass_base.py torch/fx/passes/net_min_base.py torch/jit/__init__.py torch/jit/_recursive.py torch/jit/supported_ops.py torch/mtia/__init__.py torch/nn/utils/prune.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/errors.py torch/optim/optimizer.py torch/package/importer.py torch/package/package_exporter.py torch/testing/_internal/distributed/fake_pg.py torch/testing/_internal/distributed/rpc/rpc_agent_test_fixture.py torch/utils/_sympy/functions.py torch/utils/data/datapipes/datapipe.py,https://github.com/pytorch/pytorch/pull/133200,XuehaiPan,eqy,kit1980,malfet,
dd6ce2fe7c6,inductor,not user facing,Restore mixed dtypes GEMM auto-tuning for Ampere (#129058),test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/gemm_template.py torch/_inductor/kernel/bmm.py torch/_inductor/kernel/mm.py,https://github.com/pytorch/pytorch/pull/129058,alexsamardzic,kadeng,,,
a75248528f1,dynamo,Untopiced,[export] refactor _process_dynamic_shapes (#133391),test/dynamo/test_export.py torch/_dynamo/eval_frame.py torch/_export/non_strict_utils.py torch/export/dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/133391,pianpwk,avikchaudhuri,,,
fc5aa24a6e9,nn_frontend,docs,Rewording doc string for clip_grad_norm_ (#133406),torch/nn/utils/clip_grad.py,https://github.com/pytorch/pytorch/pull/133406,akshay-raj-dhamija,mikaylagawarecki,,,
7470ae85e4b,inductor,Untopiced,Fix triton codegen with math.trunc (#133354),test/inductor/test_torchinductor.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/133354,isuruf,ezyang,jansel,,
3434a54fba5,distributed,Untopiced,[CP] Rewrite ring attention backward algorithm and enablement APIs (#131351),test/distributed/_tensor/test_attention.py torch/distributed/_tensor/experimental/attention.py torch/distributed/_tensor/ops/_matrix_ops.py torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/131351,fegin,wanchaol,,,
c23dceb8f17,optim,performance,Add Adafactor foreach impl (#132336),test/test_optim.py torch/optim/_adafactor.py torch/testing/_internal/common_optimizers.py,https://github.com/pytorch/pytorch/pull/132336,janeyx99,albanD,,,
018e48c3370,Uncategorized,Untopiced,[Reland] Add wrappers for synchronous GPUDirect Storage APIs (#133489),CMakeLists.txt build_variables.bzl caffe2/CMakeLists.txt cmake/Dependencies.cmake cmake/Modules/FindCUDAToolkit.cmake cmake/Summary.cmake cmake/public/cuda.cmake docs/source/cuda.rst setup.py test/test_cuda.py third_party/cuda.BUILD torch/CMakeLists.txt torch/_C/__init__.pyi.in torch/__init__.py torch/csrc/cuda/GdsFile.cpp torch/csrc/cuda/GdsFile.h torch/csrc/cuda/Module.cpp torch/cuda/__init__.py torch/cuda/gds.py,https://github.com/pytorch/pytorch/pull/133489,mikaylagawarecki,albanD,,,
a7c6e30a3ff,distributed,Untopiced,[c10d][ez] Add space between PG ID and PG UID (#133497),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/133497,fduwjj,shengbao-zheng,wz337,,
2236194c6b7,skip,not user facing,[traced-graph][sparse] cleanup test guards (#133375),test/export/test_sparse.py,https://github.com/pytorch/pytorch/pull/133375,aartbik,ezyang,,,
d3b458e6032,Uncategorized,Untopiced,[export] Do not use export.export for `capture_pre_autograd_graph` (#133370),torch/_export/__init__.py,https://github.com/pytorch/pytorch/pull/133370,yushangdi,tugsbayasgalan,,,
cfec69e2a1f,skip,Untopiced,"Revert ""Update fused kernels and call _safe_softmax from SDPA (#131863)""",aten/src/ATen/native/cpu/FlashAttentionKernel.cpp aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/cuda/mem_eff_attention/epilogue/epilogue_rescale_output.h aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernel_forward.h test/functorch/test_ops.py test/test_nn.py test/test_transformers.py tools/autograd/derivatives.yaml torch/csrc/autograd/FunctionsManual.cpp torch/csrc/autograd/FunctionsManual.h torch/testing/_internal/common_methods_invocations.py,,,,,,
5ec9c0bc4a9,fx,Untopiced,Fix `linearize(grad(...))` call (#133364),test/functorch/test_eager_transforms.py torch/_functorch/eager_transforms.py torch/fx/experimental/const_fold.py,https://github.com/pytorch/pytorch/pull/133364,guilhermeleobas,zou3519,,,
59b3f5911dc,jit,not user facing,[sigmoid] Support custom obj deserialization. (#133463),torch/_C/__init__.pyi.in torch/csrc/jit/python/script_init.cpp torch/csrc/jit/serialization/pickle.cpp torch/csrc/jit/serialization/pickle.h,https://github.com/pytorch/pytorch/pull/133463,zhxchen17,ydwu4,,,
546c53b7849,releng,not user facing,Bump max runners for linux.24xlarge to 500 (#133569),.github/lf-canary-scale-config.yml .github/lf-scale-config.yml,https://github.com/pytorch/pytorch/pull/133569,jeanschmidt,ZainRizvi,,,
41d6cabca1b,distributed,Untopiced,[c10d]Control logging c++ traces with a flag (#133490),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,https://github.com/pytorch/pytorch/pull/133490,c-p-i-o,fduwjj,,,
4aa66f68a80,skip,not user facing,[CUDA][CUTLASS][submodule] Fixes for CUTLASS upgrade (#131493),aten/src/ATen/native/cuda/MixedDtypesLinear.cu aten/src/ATen/native/cuda/RowwiseScaledMM.cu aten/src/ATen/native/sparse/cuda/ComputeSparseTile.h aten/src/ATen/native/sparse/cuda/SparseSemiStructuredLinear.cu aten/src/ATen/native/sparse/cuda/SparseSemiStructuredOps.cu aten/src/ATen/native/transformers/cuda/flash_attn/flash_bwd_preprocess_kernel.h aten/src/ATen/native/transformers/cuda/flash_attn/flash_fwd_kernel.h third_party/cutlass torch/_inductor/codegen/cuda/cutlass_utils.py,https://github.com/pytorch/pytorch/pull/131493,eqy,Skylion007,,,
27538671ae6,skip,not user facing,Enable clang-tidy coverage on torch/*.h (#133422),.lintrunner.toml torch/custom_class.h torch/custom_class_detail.h torch/library.h,https://github.com/pytorch/pytorch/pull/133422,cyyever,Skylion007,albanD,,
dff388491bb,nn_frontend,docs,Fix docs for L1Loss and MSELoss (#133501),torch/nn/modules/loss.py,https://github.com/pytorch/pytorch/pull/133501,5trobl,mikaylagawarecki,,,
7673ee54563,releng,not user facing,remove  benchmarks/__init__.py  (#133390),.ci/pytorch/test.sh benchmarks/__init__.py benchmarks/dynamo/pr_time_benchmarks/benchmark_runner.sh benchmarks/dynamo/pr_time_benchmarks/benchmarks/update_hint_benchmark.py benchmarks/operator_benchmark/benchmark_all_other_test.py benchmarks/operator_benchmark/benchmark_all_quantized_test.py benchmarks/operator_benchmark/benchmark_all_test.py benchmarks/operator_benchmark/pt/conv_test.py benchmarks/operator_benchmark/pt/embeddingbag_test.py benchmarks/operator_benchmark/pt/gather_test.py benchmarks/operator_benchmark/pt/index_select_test.py benchmarks/operator_benchmark/pt/linear_test.py benchmarks/operator_benchmark/pt/qatembedding_ops_test.py benchmarks/operator_benchmark/pt/qconv_test.py benchmarks/operator_benchmark/pt/qembedding_bag_lookups_test.py benchmarks/operator_benchmark/pt/qembeddingbag_test.py benchmarks/operator_benchmark/pt/qlinear_test.py,https://github.com/pytorch/pytorch/pull/133390,laithsakka,ezyang,kit1980,malfet,
5dfb22d4c85,inductor,not user facing,AutoHeuristic: tests (#133496),torch/_inductor/autoheuristic/artifacts/_MixedMMH100.py torch/_inductor/autoheuristic/artifacts/_PadMMA100.py torchgen/_autoheuristic/mixed_mm/get_mixedmm_dataset.sh torchgen/_autoheuristic/mixed_mm/test_mixed_mm.py torchgen/_autoheuristic/pad_mm/get_padmm_dataset.sh torchgen/_autoheuristic/pad_mm/test_pad_mm.py torchgen/_autoheuristic/test.sh torchgen/_autoheuristic/test_utils.py torchgen/_autoheuristic/train_regression.py,https://github.com/pytorch/pytorch/pull/133496,AlnisM,eellison,,,
99cf567714d,releng,not user facing,Make SCRIBE_GRAPHQL_ACCESS_TOKEN available to test jobs running on main (#133536),.github/workflows/_linux-test.yml test/run_test.py,https://github.com/pytorch/pytorch/pull/133536,ezyang,albanD,malfet,xuzhao9,
161cc137d21,distributed,not user facing,[DTensor] Add naive replicate strategy for aten.triu.default and aten.tril.default (#133545),test/distributed/_tensor/test_dtensor_ops.py torch/distributed/_tensor/ops/_math_ops.py,https://github.com/pytorch/pytorch/pull/133545,wz337,awgu,,,
cdf217cda17,releng,not user facing,Disable distributed nccl tests to unblock Amazon2023 ami upgrade (#133355),.github/workflows/pull.yml .github/workflows/trunk.yml test/distributed/test_c10d_nccl.py,https://github.com/pytorch/pytorch/pull/133355,ZainRizvi,kit1980,wconstab,,
09a489b177c,Uncategorized,Untopiced,Fix serialization for tensor list output (#133539),torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/133539,SherlockNoMad,zhxchen17,,,
5f1470d45df,dynamo,Untopiced,[export] Add InterpreterModule to trace_rules (#132949),test/export/test_unflatten.py torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/132949,angelayi,zhxchen17,,,
90d2593b3e8,dynamo,Untopiced,"Revert #132806, #132736, #132539, #132487 (#133570)",test/dynamo/test_exceptions.py test/dynamo/test_modules.py test/dynamo_expected_failures/PackedSequenceTest.test_pack_sequence test/inductor/test_cpu_select_algorithm.py torch/_dynamo/exc.py torch/_dynamo/guards.py torch/_dynamo/source.py torch/_dynamo/symbolic_convert.py torch/_dynamo/utils.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/user_defined.py torch/_guards.py,https://github.com/pytorch/pytorch/pull/133570,ezyang,albanD,anijain2305,jansel,
86aa327e4af,distributed,Untopiced,[FSDP2] Added eager fast-path for fp32->bf16 param cast (#133369),torch/distributed/_composable/fsdp/_fsdp_collectives.py,https://github.com/pytorch/pytorch/pull/133369,awgu,weifengpy,,,
29c4b4ea5a2,Uncategorized,Untopiced,[executorch] Refactor delegation code (#132773),torch/_export/verifier.py,https://github.com/pytorch/pytorch/pull/132773,angelayi,cccclai,ydwu4,,
f347174d61f,Uncategorized,Untopiced,Hipify Pytorch3D (#133343),torch/utils/hipify/cuda_to_hip_mappings.py,https://github.com/pytorch/pytorch/pull/133343,jwfromm,houseroad,,,
413416cf335,inductor,Untopiced,[PT2] Consolidate args and kwargs usage in pre_grad passes (#133518),torch/_inductor/fx_passes/pre_grad.py,https://github.com/pytorch/pytorch/pull/133518,huxintong,frank-wei,,,
7ad3108ef2c,skip,not user facing,[CUTLASS][FP8] Skip scaled_mm rowwise test on sm89 (#133612),test/test_matmul_cuda.py,https://github.com/pytorch/pytorch/pull/133612,eqy,Skylion007,,,
0d2be06d94f,skip,Untopiced,[export] fix test for training ir migration (#133587),torch/ao/quantization/pt2e/port_metadata_pass.py,https://github.com/pytorch/pytorch/pull/133587,yushangdi,tugsbayasgalan,,,
c8ad5e37e84,Uncategorized,Untopiced,Fix all RuntimeErrors during weights_only load from being erroneously reported with the weights_only message (#132349),test/test_serialization.py torch/_weights_only_unpickler.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/132349,mikaylagawarecki,albanD,,,
d9576c94404,distributed,Untopiced,Fix failures when default is flipped for weights_only (#127627),test/cpp_api_parity/module_impl_check.py test/distributed/_tensor/test_dtensor.py test/jit/test_torchbind.py test/nn/test_convolution.py test/nn/test_module_hooks.py test/nn/test_pruning.py test/quantization/bc/test_backward_compatibility.py test/quantization/core/test_quantized_module.py test/quantization/core/test_quantized_tensor.py test/quantization/fx/test_quantize_fx.py test/test_cpp_extensions_open_device_registration.py test/test_fx.py test/test_modules.py test/test_nn.py test/test_python_dispatch.py test/test_serialization.py test/test_subclass.py torch/_weights_only_unpickler.py torch/fx/graph_module.py torch/testing/_internal/common_nn.py torch/testing/_internal/common_quantization.py torch/testing/_internal/distributed/distributed_test.py,https://github.com/pytorch/pytorch/pull/127627,mikaylagawarecki,albanD,,,
8f7cf796eaa,autograd_frontend,not user facing,[14/N] Use std::optional  (#133417),aten/src/ATen/SavedTensorHooks.cpp test/cpp_extensions/open_registration_extension.cpp test/test_autograd.py tools/autograd/derivatives.yaml torch/csrc/inductor/aoti_eager/kernel_holder.cpp,https://github.com/pytorch/pytorch/pull/133417,cyyever,ezyang,,,
c22f51ce7c7,inductor,not user facing,[inductor][cpp][gemm] improve large bs perf with better cache blocking (#132729),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/132729,jgong5,chunyuan-w,jansel,leslie-fang-intel,
929d2f82534,skip,not user facing,[3/N] Fix clang-tidy warnings in torch/csrc/autograd (#133389),torch/csrc/autograd/FunctionsManual.cpp torch/csrc/autograd/FunctionsManual.h torch/csrc/autograd/TraceTypeManual.cpp torch/csrc/autograd/engine.cpp torch/csrc/autograd/engine.h torch/csrc/autograd/profiler_python.cpp torch/csrc/autograd/python_engine.cpp torch/csrc/autograd/python_engine.h torch/csrc/autograd/python_variable.cpp torch/csrc/autograd/python_variable_indexing.cpp torch/csrc/autograd/utils/lambda_post_hook.h torch/csrc/autograd/variable.cpp torch/csrc/autograd/variable.h,https://github.com/pytorch/pytorch/pull/133389,cyyever,Skylion007,,,
add0f0085c4,skip,not user facing,AutoHeuristic: Support ranking/pruning choices (#131705),torchgen/_autoheuristic/ah_tree.py torchgen/_autoheuristic/train.py torchgen/_autoheuristic/train_decision.py torchgen/_autoheuristic/train_regression.py,https://github.com/pytorch/pytorch/pull/131705,AlnisM,eellison,,,
3a904d11631,inductor,not user facing,AutoHeuristic: Enable explicit support for ranking (#131710),torch/_inductor/autoheuristic/autoheuristic.py torch/_inductor/autoheuristic/learned_heuristic_controller.py torch/_inductor/autoheuristic/learnedheuristic_interface.py torch/_inductor/kernel/mm.py,https://github.com/pytorch/pytorch/pull/131710,AlnisM,eellison,,,
1653f7786dd,python_frontend,bug fixes,Fix type promotion for `ldexp` (#133519),aten/src/ATen/native/BinaryOps.cpp test/test_binary_ufuncs.py,https://github.com/pytorch/pytorch/pull/133519,malfet,Skylion007,janeyx99,,
a1a869f2f57,Uncategorized,Untopiced,[ts_converter][reland] Add support for LinearOpContext and Conv2dOpContext in quantization pass (#133622),aten/src/ATen/native/xnnpack/RegisterOpContextClass.cpp test/export/test_converter.py torch/_export/passes/replace_quantized_ops_with_standard_ops_pass.py,https://github.com/pytorch/pytorch/pull/133622,angelayi,SherlockNoMad,,,
8a5708ba3dd,dynamo,not user facing,[dynamo] Support object creation of classes with custom __new__ (#132977),test/dynamo/test_misc.py torch/_dynamo/polyfill.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/132977,anijain2305,jansel,,,
d9f17cf4e42,fx,Untopiced,[fx] Do not add Proxy on Tensor (#133470),torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/133470,yushangdi,zou3519,,,
fcc2fc1a70c,distributed,not user facing,[Flight Recorder] Add more basic analysis to the script (#133412),test/distributed/flight_recorder/test_fr_analysis.py tools/flight_recorder/fr_trace.py,https://github.com/pytorch/pytorch/pull/133412,fduwjj,atalman,c-p-i-o,,
8a2b064236c,dynamo,not user facing,[dynamo][user_defined][stable-diffusion] Raise ObservedAttributeError on UserDefinedObject var_getattr (#132806),test/dynamo/test_exceptions.py test/dynamo_expected_failures/PackedSequenceTest.test_pack_sequence torch/_dynamo/exc.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/132806,anijain2305,williamwen42,,,
8ea5b572a63,inductor,Untopiced,[PT2][Optimus] Add missing example value for the nodes introduced in group batch fusion (#133414),torch/_inductor/fx_passes/group_batch_fusion.py,https://github.com/pytorch/pytorch/pull/133414,mengluy0125,jackiexu1992,,,
3d45717219c,inductor,not user facing,[ROCm][CK][Inductor] enable dynamic shapes for CK backend to gemm max autotune (#133285),test/inductor/test_ck_backend.py torch/_inductor/codegen/rocm/ck_universal_gemm_template.py torch/_inductor/codegen/rocm/rocm_benchmark_request.py torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/codegen/rocm/rocm_template.py torch/_inductor/codegen/wrapper.py torch/_inductor/kernel/mm.py,https://github.com/pytorch/pytorch/pull/133285,tenpercent,ColinPeppler,,,
d12bbcd785f,inductor,not user facing,Add auto-tuning for sparse semi-structured MM operator (#123742),test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/cuda_template.py torch/_inductor/codegen/cuda/gemm_template.py torch/_inductor/kernel/mm.py,https://github.com/pytorch/pytorch/pull/123742,alexsamardzic,kadeng,,,
f64146aff0d,fx,Untopiced,Update source matcher to use torch_fn (#133642),test/fx/test_source_matcher_utils.py torch/fx/passes/utils/source_matcher_utils.py,https://github.com/pytorch/pytorch/pull/133642,angelayi,ydwu4,,,
5ed3b70d09a,fx,Untopiced,remove redundant upper bound check at runtime (#133627),test/export/test_export.py torch/fx/passes/runtime_assert.py,https://github.com/pytorch/pytorch/pull/133627,avikchaudhuri,SherlockNoMad,,,
30fbf5b19c4,skip,not user facing,Remove AMD restrictions on triton hashing (#133616),torch/utils/_triton.py,https://github.com/pytorch/pytorch/pull/133616,oulgen,eellison,mxz297,nmacchioni,
15183f5ebfe,inductor,not user facing,overestimate `time_taken_ns` for autotuning (#133633),torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/133633,nmacchioni,eellison,oulgen,,
dd69013c7a5,inductor,deprecation,deprecate `search_autotune_cache` (#133628),test/inductor/test_cpu_cpp_wrapper.py test/inductor/test_cuda_cpp_wrapper.py test/inductor/test_max_autotune.py torch/_inductor/config.py torch/_inductor/select_algorithm.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/133628,nmacchioni,oulgen,,,
762b1b4c172,inductor,not user facing,[inductor] [cpp] fix accuracy when template_buffer has users other than the epilogue nodes (#133073),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_template.py torch/_inductor/codegen/cpp_template_kernel.py,https://github.com/pytorch/pytorch/pull/133073,chunyuan-w,jgong5,,,
b4443430877,inductor,Untopiced,Fix printing symfloat pow in triton (#133614),test/inductor/test_torchinductor.py torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/133614,isuruf,Skylion007,,,
e1b9b89d947,skip,Untopiced,"Revert ""[Flight Recorder] Add more basic analysis to the script (#133412)""",test/distributed/flight_recorder/test_fr_analysis.py tools/flight_recorder/fr_trace.py,,,,,,
a6aa451bde6,releng,not user facing,Move python 3.8 to 3.9 for linux-binary-manywheel workflow (#133621),.github/scripts/generate_ci_workflows.py .github/workflows/generated-linux-binary-manywheel-main.yml,https://github.com/pytorch/pytorch/pull/133621,atalman,Skylion007,kit1980,malfet,
eca8b4220f4,inductor,not user facing,[inductor][cpp][gemm] fix k-slicing bug and add thread blocking config (#132730),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/132730,jgong5,jansel,leslie-fang-intel,,
51e13745be7,jit,not user facing,[BE]: Update ruff to 0.6.0 (#133609),.lintrunner.toml test/test_model_exports_to_core_aten.py torch/jit/_script.py,https://github.com/pytorch/pytorch/pull/133609,Skylion007,malfet,,,
e51c8ad369c,inductor,not user facing,AutoHeuristic: Heuristic that ranks choices for mm (#131714),torch/_inductor/autoheuristic/artifacts/_MMRankingA100.py torch/_inductor/autoheuristic/autoheuristic.py torch/_inductor/kernel/mm.py torchgen/_autoheuristic/mm/gen_heuristic_a100.sh torchgen/_autoheuristic/mm/get_mm_dataset.sh torchgen/_autoheuristic/mm/train_decision_mm.py torchgen/_autoheuristic/train_decision.py,https://github.com/pytorch/pytorch/pull/131714,AlnisM,eellison,,,
0e0077f3b6f,inductor,Untopiced,AutoHeuristic: mm ranking heuristic h100 (#133608),torch/_inductor/autoheuristic/artifacts/_MMRankingH100.py torchgen/_autoheuristic/mm/gen_heuristic_h100.sh torchgen/_autoheuristic/mm/get_mm_dataset.sh,https://github.com/pytorch/pytorch/pull/133608,AlnisM,eellison,,,
8b8b4e5ae9b,skip,not user facing,AutoHeuristic: documentation for mm (#133611),torchgen/_autoheuristic/mm/README.md,https://github.com/pytorch/pytorch/pull/133611,AlnisM,eellison,,,
46af996ce75,distributed,Untopiced,[c10d] Do not call ncclCommAbort if comm is not initialized (#133630),torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp,https://github.com/pytorch/pytorch/pull/133630,shuqiangzhang,wconstab,,,
9c2d119194b,Uncategorized,Untopiced,[Profiler/CPU] Add API for Dynamic Activity Toggling [3/n] (#133353),torch/csrc/autograd/profiler_kineto.cpp torch/csrc/autograd/profiler_python.cpp torch/csrc/profiler/collection.cpp torch/csrc/profiler/collection.h torch/csrc/profiler/orchestration/python_tracer.cpp torch/csrc/profiler/orchestration/python_tracer.h torch/profiler/profiler.py,https://github.com/pytorch/pytorch/pull/133353,sraikund16,aaronenyeshi,sanrise,,
38e5ee1a346,inductor,not user facing,mixed_mm: add more extensive dtype testing (#133292),test/inductor/test_pattern_matcher.py torch/_inductor/kernel/mm.py,https://github.com/pytorch/pytorch/pull/133292,AlnisM,shunting314,,,
770086fe391,dynamo,not user facing,[Dynamo] Support torch.cuda.device ctx manager (#133385),test/dynamo/test_ctx_manager.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/133385,yanboliang,jansel,,,
18705e371d5,releng,not user facing,S390x nightly binaries for python 3.13 (#132984),.github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/132984,AlekseiNikiforovIBM,malfet,,,
6d850771684,skip,not user facing,[Traceable FSDPS] Allow tracing through FSDP2 impl in trace_rules.py (#133532),torch/_dynamo/trace_rules.py torch/distributed/_composable/fsdp/_fsdp_state.py,https://github.com/pytorch/pytorch/pull/133532,yf225,yanboliang,,,
6790eb52f96,distributed,not user facing,[Traceable FSDP2] Set torch._dynamo.config.skip_fsdp_hooks to True by default (#133531),test/distributed/_composable/fsdp/test_fully_shard_compile.py torch/_dynamo/config.py,https://github.com/pytorch/pytorch/pull/133531,yf225,awgu,,,
f735038c8f1,inductor,Untopiced,[PT2][Optimus] Add unbind_stack_to_slices pass (#133420),test/inductor/test_split_cat_fx_passes.py torch/_inductor/fx_passes/split_cat.py,https://github.com/pytorch/pytorch/pull/133420,mengluy0125,jackiexu1992,,,
2cffe82deae,releng,not user facing,Fix triton build failure due to tritonlang.blob.core.windows.net not available (#133694),.github/scripts/build_triton_wheel.py,https://github.com/pytorch/pytorch/pull/133694,atalman,Skylion007,kit1980,malfet,
374c61cc82e,inductor,not user facing,[inductor] make conv template work with symbolic stride/padding (#132938),torch/_inductor/kernel/conv.py torch/_inductor/sizevars.py,https://github.com/pytorch/pytorch/pull/132938,shunting314,eellison,jansel,,
1df1d00ffce,distributed,not user facing,[Traceable FSDP2] Remove usage of tuple() generator and simplify code (#133636),torch/distributed/_composable/fsdp/_fsdp_state.py,https://github.com/pytorch/pytorch/pull/133636,yf225,awgu,,,
4ee65c7e4ed,inductor,Untopiced,Add message text to BypassFxGraphCache exceptions. (#133505),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/133505,byoshimi-github,oulgen,,,
b833990a8f7,skip,Untopiced,"Revert ""[CUDA][CUTLASS][submodule] Fixes for CUTLASS upgrade (#131493)""",aten/src/ATen/native/cuda/MixedDtypesLinear.cu aten/src/ATen/native/cuda/RowwiseScaledMM.cu aten/src/ATen/native/sparse/cuda/ComputeSparseTile.h aten/src/ATen/native/sparse/cuda/SparseSemiStructuredLinear.cu aten/src/ATen/native/sparse/cuda/SparseSemiStructuredOps.cu aten/src/ATen/native/transformers/cuda/flash_attn/flash_bwd_preprocess_kernel.h aten/src/ATen/native/transformers/cuda/flash_attn/flash_fwd_kernel.h third_party/cutlass torch/_inductor/codegen/cuda/cutlass_utils.py,,,,,,
caaa339e0f6,releng,not user facing,"Use dedicated docker-build environment for manywheel, libtorch and conda Docker builds (#133699)",.github/workflows/build-conda-images.yml .github/workflows/build-libtorch-images.yml .github/workflows/build-manywheel-images.yml,https://github.com/pytorch/pytorch/pull/133699,atalman,seemethere,,,
e5baf43b614,inductor,Untopiced,[Inductor] short-term fix for needs_fixed_stride_order silent incorrectness (#133452),test/inductor/test_torchinductor.py torch/_inductor/graph.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/133452,zou3519,eellison,,,
eb3d517605e,distributed,not user facing,[Test] Add SkipIfRocm to test_grad_acc_cpu_offload (#132975),test/distributed/fsdp/test_fsdp_grad_acc.py,https://github.com/pytorch/pytorch/pull/132975,wz337,malfet,,,
90c3669cd9c,skip,not user facing,Make sure T::is_traceable is bool (#133673),torch/csrc/autograd/custom_function.h,https://github.com/pytorch/pytorch/pull/133673,cyyever,Skylion007,,,
5ee070266fb,jit,Untopiced,Workaround ASAN failure (#133623),torch/csrc/jit/tensorexpr/llvm_jit.cpp,https://github.com/pytorch/pytorch/pull/133623,MatzeB,Skylion007,,,
0063e569490,skip,Untopiced,Make FX Graph Cache work with distributed training (#133374),test/distributed/test_dynamo_distributed.py torch/_inductor/codecache.py torch/_inductor/compile_fx.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/133374,oulgen,ezyang,,,
611c1043709,mps,Untopiced,[MPS] Add workaround for nonzero with large/complex inputs  (#126188),aten/src/ATen/native/mps/operations/Indexing.mm,https://github.com/pytorch/pytorch/pull/126188,skotapati,kulinseth,malfet,,
678a8f9e66c,inductor,not user facing,[Inductor][FlexAttention] Small cleanup for FlexAttention kernel template (#133664),torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py,https://github.com/pytorch/pytorch/pull/133664,yanboliang,drisspg,,,
fb59440791c,releng,not user facing,"Use dedicated docker-build environment for manywheel, libtorch and conda Docker builds - 2 (#133709)",.github/workflows/build-manywheel-images.yml,https://github.com/pytorch/pytorch/pull/133709,atalman,Skylion007,seemethere,,
a0cb54ab46f,skip,Untopiced,"Revert ""C++ network flow implementation in c10 (#132188)""",c10/test/util/NetworkFlow_test.cpp c10/util/NetworkFlow.cpp c10/util/NetworkFlow.h,,,,,,
41e66195094,skip,not user facing,[codemod] Del un at::native::metal @ MPSCNNFullyConnectedOp.h:6 (export D59157302) (#133515),aten/src/ATen/native/metal/mpscnn/MPSCNNFullyConnectedOp.h,https://github.com/pytorch/pytorch/pull/133515,izaitsevfb,kit1980,malfet,,
3ac527ac5f3,skip,not user facing,[BE][Ez]: Update cudnn_frontend submodule to 1.6.0 (#133687),third_party/cudnn_frontend,https://github.com/pytorch/pytorch/pull/133687,Skylion007,eqy,malfet,,
88ba50279c3,fx,Untopiced,Consolidate the format for `--max-acc-splits` flag (#133724),torch/fx/passes/splitter_base.py,https://github.com/pytorch/pytorch/pull/133724,izaitsevfb,kit1980,,,
bc9e20b927c,inductor,not user facing,Move the layout constraint registration of aten._scaled_mm.default to module scope (#133669),torch/_inductor/kernel/mm_scaled.py,https://github.com/pytorch/pytorch/pull/133669,yifuwang,drisspg,yangsiyu007,,
f57b00704e4,dynamo,not user facing,[Traceable FSDP2][Dynamo] Support reconstructing CUDA event object within Dynamo graph (#133635),test/dynamo/test_ctx_manager.py torch/_dynamo/__init__.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/133635,yf225,yifuwang,,,
447f428d6d6,skip,not user facing,[ROCm] Fix text_export cudnn_attention UT (#133234),test/export/test_export.py,https://github.com/pytorch/pytorch/pull/133234,jataylo,malfet,,,
861bdf96f4d,mps,Untopiced,[MPS] Add native strided API for MPSNDArray starting with macOS 15 (#128393),aten/src/ATen/mps/MPSDevice.mm aten/src/ATen/mps/MPSHooks.mm aten/src/ATen/native/Convolution.cpp aten/src/ATen/native/mps/MPSGraphSequoiaOps.h aten/src/ATen/native/mps/OperationUtils.h aten/src/ATen/native/mps/OperationUtils.mm aten/src/ATen/native/mps/operations/Activation.mm aten/src/ATen/native/mps/operations/BinaryOps.mm aten/src/ATen/native/mps/operations/Convolution.mm aten/src/ATen/native/mps/operations/Copy.mm aten/src/ATen/native/mps/operations/Indexing.mm aten/src/ATen/native/mps/operations/LinearAlgebra.mm aten/src/ATen/native/mps/operations/LossOps.mm aten/src/ATen/native/mps/operations/PointwiseOps.mm aten/src/ATen/native/mps/operations/Pooling.mm aten/src/ATen/native/mps/operations/RangeFactories.mm aten/src/ATen/native/mps/operations/Shape.mm aten/src/ATen/native/mps/operations/SoftMax.mm aten/src/ATen/native/mps/operations/UnaryOps.mm aten/src/ATen/native/mps/operations/UpSample.mm test/test_mps.py torch/testing/_internal/common_device_type.py torch/testing/_internal/common_modules.py,https://github.com/pytorch/pytorch/pull/128393,DenisVieriu97,albanD,,,
be207af6e19,inductor,Untopiced,Disable unwrapping scalar tensors when used as outputs (#132859),torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_utils.py torch/_inductor/codegen/wrapper.py,https://github.com/pytorch/pytorch/pull/132859,kundaMwiza,jansel,,,
fd33499b0c3,inductor,Untopiced,[PT2][Optimus] Fix mixed precison training problem in decompose mem bound (#133626),test/inductor/test_decompose_mem_bound_mm.py torch/_inductor/fx_passes/decompose_mem_bound_mm.py,https://github.com/pytorch/pytorch/pull/133626,mengluy0125,jackiexu1992,,,
99e789b52b6,quantization,not user facing,[Fix 1/n] GPU Test skips - fbcode/ caffe2/test/quantization (#133158),test/quantization/core/test_quantized_op.py,https://github.com/pytorch/pytorch/pull/133158,HDCharles,jovianjaison,,,
271ee908512,skip,not user facing,[easy] Fix type annotation for `ExportedProgram.run_decompositions` (#133720),torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/133720,justinchuby,Skylion007,,,
98d6a6eb7da,inductor,not user facing,[inductor] clean up TODO comments. (#133718),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/133718,xuhancn,henrylhtsang,,,
19ff9059ebe,skip,Untopiced,"Revert ""[Inductor][CPP] Support vectorization of remainder (#129849)""",aten/src/ATen/cpu/vec/functional_base.h aten/src/ATen/native/cpu/BinaryOpsKernel.cpp test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,,,,,,
d04cd7f3ba8,skip,not user facing,Improvements for associative_scan - Reverse feature (#133011),test/functorch/test_control_flow.py test/higher_order_ops/test_with_effects.py test/inductor/test_control_flow.py torch/_higher_order_ops/associative_scan.py,https://github.com/pytorch/pytorch/pull/133011,bohnstingl,ydwu4,,,
648fc6c9c17,inductor,not user facing,[Inductor][CPP] Refactor the tiling select into a standalone module to enhance its extensibility (#130892),torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_utils.py,https://github.com/pytorch/pytorch/pull/130892,leslie-fang-intel,jgong5,,,
ec281210173,inductor,not user facing,[inductor] Fix test_cudagraph_trees_expandable_segments.py for internal (#133698),test/inductor/test_cudagraph_trees_expandable_segments.py,https://github.com/pytorch/pytorch/pull/133698,masnesral,xuzhao9,,,
b0803129e8c,composability,not user facing,Added meta registration for `_fused_adamw_` (#133728),torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/133728,awgu,fegin,janeyx99,,
4b3ed8bc527,dynamo,Untopiced,[compiled autograd] log aot id for CompiledFunctionBackward (#133115),test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/133115,xmfan,jansel,,,
0a6cc15079c,dynamo,Untopiced,[compiled autograd] use same graph node names as AOTDispatcher (#133148),test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/_functorch/_aot_autograd/runtime_wrappers.py,https://github.com/pytorch/pytorch/pull/133148,xmfan,jansel,,,
983bea399db,dynamo,Untopiced,[compiled autograd] move non-hot path logs into default logger (#133541),test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/133541,xmfan,bdhirsh,yf225,,
dcfa415e6e1,dynamo,not user facing,[Inductor UT] Reuse inductor UT for intel GPU `test/inductor/test_compiled_optimizers.py` (#133083),test/inductor/test_compiled_optimizers.py torch/_dynamo/variables/optimizer.py torch/testing/_internal/common_device_type.py torch/testing/_internal/common_optimizers.py,https://github.com/pytorch/pytorch/pull/133083,hoshibara,etaf,jansel,mlazos,
455f6bda563,skip,not user facing,Add cache timings info to tlparse (#133504),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/133504,oulgen,jamesjwu,,,
695d7db2d6f,fx,Untopiced,remove dead code for suggesting legacy dynamic shapes fixes (#133700),test/test_dynamic_shapes.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/133700,avikchaudhuri,zhxchen17,,,
12b8e29203d,inductor,not user facing,Add a fudge factor to ephemeral NCCL timeout increase (#133722),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/133722,oulgen,aorenste,c00w,,
702c810780b,optim,Untopiced,move param's device check to `_init_group` for fused (#131153),test/test_optim.py torch/optim/adagrad.py torch/optim/adam.py torch/optim/adamw.py torch/optim/optimizer.py torch/optim/sgd.py,https://github.com/pytorch/pytorch/pull/131153,crcrpar,janeyx99,mlazos,,
addee9f4d18,distributed,Untopiced,[dtensor] add missing __all__ to public modules (#133305),torch/distributed/_tensor/debug/comm_mode.py torch/distributed/_tensor/debug/visualize_sharding.py torch/distributed/_tensor/device_mesh.py torch/distributed/_tensor/experimental/attention.py torch/distributed/_tensor/experimental/func_map.py torch/distributed/_tensor/experimental/register_sharding.py torch/distributed/_tensor/experimental/tp_transform.py torch/distributed/_tensor/placement_types.py torch/distributed/_tensor/random.py,https://github.com/pytorch/pytorch/pull/133305,wanchaol,XilunWu,tianyu-l,wz337,
1a4709cef59,distributed,Untopiced,[dtensor] add more documentations (#133306),torch/distributed/_tensor/_dispatch.py torch/distributed/_tensor/api.py torch/distributed/_tensor/placement_types.py,https://github.com/pytorch/pytorch/pull/133306,wanchaol,XilunWu,tianyu-l,wz337,
2ee6b97464d,skip,Untopiced,[dtensor] move DTensor to public namespace (#133113),docs/source/conf.py docs/source/distributed.rst docs/source/distributed.tensor.rst docs/source/index.rst test/allowlist_for_publicAPI.json test/distributed/_tensor/test_common_rules.py test/distributed/_tensor/test_dtensor.py test/distributed/_tensor/test_embedding_ops.py test/distributed/_tensor/test_math_ops.py test/distributed/_tensor/test_op_strategy.py test/distributed/_tensor/test_redistribute.py test/distributed/_tensor/test_tensor_ops.py test/distributed/_tensor/test_view_ops.py test/distributed/test_device_mesh.py torch/_dynamo/guards.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/distributed.py torch/_dynamo/variables/torch.py torch/distributed/_composable/fsdp/_fsdp_collectives.py torch/distributed/_composable/fsdp/_fsdp_common.py torch/distributed/_composable/fsdp/_fsdp_init.py torch/distributed/_composable/fsdp/_fsdp_param.py torch/distributed/_composable/fsdp/fully_shard.py torch/distributed/_functional_collectives.py torch/distributed/_state_dict_utils.py torch/distributed/_tensor/README.md torch/distributed/_tensor/__init__.py torch/distributed/_tensor/_collective_utils.py torch/distributed/_tensor/_dispatch.py torch/distributed/_tensor/_op_schema.py torch/distributed/_tensor/_redistribute.py torch/distributed/_tensor/_sharding_prop.py torch/distributed/_tensor/_shards_wrapper.py torch/distributed/_tensor/_tp_conv.py torch/distributed/_tensor/_utils.py torch/distributed/_tensor/api.py torch/distributed/_tensor/debug/__init__.py torch/distributed/_tensor/debug/_op_coverage.py torch/distributed/_tensor/debug/comm_mode.py torch/distributed/_tensor/debug/comm_mode_broswer_visual.js torch/distributed/_tensor/debug/visualize_sharding.py torch/distributed/_tensor/device_mesh.py torch/distributed/_tensor/examples/comm_mode_features_example.py torch/distributed/_tensor/examples/convnext_example.py torch/distributed/_tensor/examples/torchrec_sharding_example.py torch/distributed/_tensor/examples/visualize_sharding_example.py torch/distributed/_tensor/experimental/__init__.py torch/distributed/_tensor/experimental/attention.py torch/distributed/_tensor/experimental/func_map.py torch/distributed/_tensor/experimental/register_sharding.py torch/distributed/_tensor/experimental/tp_transform.py torch/distributed/_tensor/ops/__init__.py torch/distributed/_tensor/ops/_common_rules.py torch/distributed/_tensor/ops/_conv_ops.py torch/distributed/_tensor/ops/_einsum_strategy.py torch/distributed/_tensor/ops/_embedding_ops.py torch/distributed/_tensor/ops/_experimental_ops.py torch/distributed/_tensor/ops/_math_ops.py torch/distributed/_tensor/ops/_matrix_ops.py torch/distributed/_tensor/ops/_pointwise_ops.py torch/distributed/_tensor/ops/_random_ops.py torch/distributed/_tensor/ops/_tensor_ops.py torch/distributed/_tensor/ops/_view_ops.py torch/distributed/_tensor/ops/utils.py torch/distributed/_tensor/placement_types.py torch/distributed/_tensor/random.py torch/distributed/checkpoint/_traverse.py torch/distributed/checkpoint/default_planner.py torch/distributed/checkpoint/examples/async_checkpointing_example.py torch/distributed/checkpoint/examples/stateful_example.py torch/distributed/checkpoint/optimizer.py torch/distributed/checkpoint/planner_helpers.py torch/distributed/checkpoint/state_dict.py torch/distributed/fsdp/_flat_param.py torch/distributed/fsdp/_fsdp_extensions.py torch/distributed/fsdp/_optim_utils.py torch/distributed/fsdp/_shard_utils.py torch/distributed/fsdp/_state_dict_utils.py torch/distributed/fsdp/fully_sharded_data_parallel.py torch/distributed/tensor/README.md torch/distributed/tensor/__init__.py torch/distributed/tensor/_collective_utils.py torch/distributed/tensor/_dispatch.py torch/distributed/tensor/_op_schema.py torch/distributed/tensor/_ops/__init__.py torch/distributed/tensor/_ops/_common_rules.py torch/distributed/tensor/_ops/_conv_ops.py torch/distributed/tensor/_ops/_einsum_strategy.py torch/distributed/tensor/_ops/_embedding_ops.py torch/distributed/tensor/_ops/_experimental_ops.py torch/distributed/tensor/_ops/_math_ops.py torch/distributed/tensor/_ops/_matrix_ops.py torch/distributed/tensor/_ops/_pointwise_ops.py torch/distributed/tensor/_ops/_random_ops.py torch/distributed/tensor/_ops/_tensor_ops.py torch/distributed/tensor/_ops/_view_ops.py torch/distributed/tensor/_ops/utils.py torch/distributed/tensor/_redistribute.py torch/distributed/tensor/_sharding_prop.py torch/distributed/tensor/_shards_wrapper.py torch/distributed/tensor/_tp_conv.py torch/distributed/tensor/_utils.py torch/distributed/tensor/api.py torch/distributed/tensor/debug/__init__.py torch/distributed/tensor/debug/_op_coverage.py torch/distributed/tensor/debug/comm_mode.py torch/distributed/tensor/debug/comm_mode_broswer_visual.js torch/distributed/tensor/debug/visualize_sharding.py torch/distributed/tensor/device_mesh.py torch/distributed/tensor/examples/comm_mode_features_example.py torch/distributed/tensor/examples/convnext_example.py torch/distributed/tensor/examples/torchrec_sharding_example.py torch/distributed/tensor/examples/visualize_sharding_example.py torch/distributed/tensor/experimental/__init__.py torch/distributed/tensor/experimental/attention.py torch/distributed/tensor/experimental/func_map.py torch/distributed/tensor/experimental/register_sharding.py torch/distributed/tensor/experimental/tp_transform.py torch/distributed/tensor/parallel/_data_parallel_utils.py torch/distributed/tensor/parallel/_utils.py torch/distributed/tensor/parallel/api.py torch/distributed/tensor/parallel/fsdp.py torch/distributed/tensor/parallel/input_reshard.py torch/distributed/tensor/parallel/loss.py torch/distributed/tensor/parallel/style.py torch/distributed/tensor/placement_types.py torch/distributed/tensor/random.py torch/testing/_internal/common_fsdp.py,https://github.com/pytorch/pytorch/pull/133113,wanchaol,XilunWu,,,
4dc9795ebf7,dynamo,not user facing,[refactor][easy] Directly call var_getattr method for PythonModuleVariable (#133745),torch/_dynamo/variables/builtin.py torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/133745,anijain2305,yanboliang,,,
cd89bf77c8e,inductor,not user facing,"[inductor][cpp][gemm] easy: adjust indentation of template, var renaming etc. (#133312)",torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/codegen/cpp_prefix.h,https://github.com/pytorch/pytorch/pull/133312,jgong5,Skylion007,leslie-fang-intel,,
d5f6d68d68f,composability,Untopiced,[PT2] Resolve PT2 compatility issue in slice and diff (#133740),aten/src/ATen/native/ReduceOps.cpp torch/_decomp/decompositions.py,https://github.com/pytorch/pytorch/pull/133740,TroyGarden,ezyang,,,
99b3b58f395,skip,not user facing,[inductor][cpp] complete vectorization for int32/int64 (#122961),test/inductor/test_cpu_repro.py test/inductor/test_torchinductor.py torch/_inductor/codegen/cpp.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/122961,jgong5,jansel,,,
a0ef8888e60,skip,not user facing,[Inductor][CPP] Support vectorization of load_seed and randn (#130317),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_utils.py,https://github.com/pytorch/pytorch/pull/130317,leslie-fang-intel,jgong5,,,
1c6fbae5799,dynamo,not user facing,[Easy][dynamo] fix builtin function names for `itertools` (#133711),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/133711,XuehaiPan,Skylion007,,,
215b14530ae,sparse_frontend,Untopiced,Add Half for sparse.mm reduce (#133672),aten/src/ATen/native/cpu/SpmmReduceKernel.cpp aten/src/ATen/native/cpu/utils.h test/inductor/test_torchinductor_opinfo.py test/test_sparse_csr.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/133672,yanbing-j,Skylion007,,,
b4a1673a674,skip,not user facing,profiler/unwind: include <dlfcn.h> for dladdr (#133582),torch/csrc/profiler/unwind/unwind.cpp,https://github.com/pytorch/pytorch/pull/133582,doughdemon,Skylion007,aaronenyeshi,,
d6368985af9,skip,not user facing,[BE]: Fix setuptools not installed with Python 3.12 (#133561),setup.py,https://github.com/pytorch/pytorch/pull/133561,cbornet,Skylion007,,,
0bde3c4f2f9,quantization,Untopiced,Run cudagraphs on AOTAutograd cache hit (#132294),test/dynamo/test_aot_autograd_cache.py test/functorch/test_aotdispatch.py test/inductor/test_cudagraph_trees.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_functorch/aot_autograd.py torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/132294,jamesjwu,eellison,,,
f8cf1829b5f,skip,not user facing,[Reland] [11/N] Fix clang-tidy warnings in aten/src/ATen (#133758),aten/src/ATen/native/cpu/CatKernel.h aten/src/ATen/native/cpu/ChannelShuffleKernel.h aten/src/ATen/native/cpu/CopyKernel.h aten/src/ATen/native/cpu/DistributionTemplates.h aten/src/ATen/native/cpu/GridSamplerKernel.h aten/src/ATen/native/cpu/IndexKernelUtils.h aten/src/ATen/native/cpu/IsContiguous.h aten/src/ATen/native/cpu/LogAddExp.h aten/src/ATen/native/cpu/PixelShuffleKernel.h aten/src/ATen/native/cpu/SampledAddmmKernel.h aten/src/ATen/native/cpu/SerialStackImpl.h aten/src/ATen/native/cpu/StackKernel.h aten/src/ATen/native/cpu/WeightNormKernel.h aten/src/ATen/native/cpu/mixed_data_type.h aten/src/ATen/native/cpu/moments_utils.h aten/src/ATen/native/cpu/utils.h aten/src/ATen/native/cpu/zmath.h,https://github.com/pytorch/pytorch/pull/133758,cyyever,Skylion007,ezyang,,
fb9d2dc6414,skip,not user facing,Remove Wno-invalid-partial-specialization from CMake (#133398),CMakeLists.txt,https://github.com/pytorch/pytorch/pull/133398,cyyever,ezyang,,,
d717df2071d,dynamo,not user facing,[compiled autograd] fix flaky tests due to torch.cuda.memory_allocated() != 0 (#133733),test/inductor/test_compiled_autograd.py,https://github.com/pytorch/pytorch/pull/133733,xmfan,jansel,,,
27dfd63ee87,skip,not user facing,remove unnecessary slicing in EffectTokensWrapper (#133737),torch/_functorch/_aot_autograd/runtime_wrappers.py,https://github.com/pytorch/pytorch/pull/133737,JackCaoG,IvanKobzarev,,,
d56a395971f,dynamo,not user facing,[dynamo] Support os.fspath (#133747),test/dynamo/test_repros.py torch/_dynamo/polyfill.py torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/133747,anijain2305,Skylion007,jansel,yanboliang,
fed6096e73f,dynamo,not user facing,[dynamo] Support object.__new__ call (#133746),test/dynamo/test_misc.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/133746,anijain2305,Skylion007,jansel,,
fbd020fce64,xpu,Untopiced,Add new prop to _XpuDevicePropertie for triton gemm optimization (#131738),c10/xpu/XPUDeviceProp.h c10/xpu/XPUFunctions.cpp test/test_xpu.py torch/csrc/xpu/Module.cpp,https://github.com/pytorch/pytorch/pull/131738,guangyey,gujinghui,,,
47ed5f57b09,skip,not user facing,[12/N] Fix clang-tidy warnings in aten/src/ATen (#133425),aten/src/ATen/ExpandUtils.h aten/src/ATen/FunctionalStorageImpl.cpp aten/src/ATen/InferSize.h aten/src/ATen/MatrixRef.h aten/src/ATen/NamedTensorUtils.h aten/src/ATen/ParallelNative.h aten/src/ATen/ParallelThreadPoolNative.cpp aten/src/ATen/PythonTorchFunctionTLS.cpp aten/src/ATen/Utils.cpp aten/src/ATen/Utils.h aten/src/ATen/code_template.h aten/src/ATen/detail/CUDAHooksInterface.h aten/src/ATen/detail/HIPHooksInterface.h aten/src/ATen/detail/MPSHooksInterface.h aten/src/ATen/detail/MTIAHooksInterface.h aten/src/ATen/detail/PrivateUse1HooksInterface.h aten/src/ATen/detail/XPUHooksInterface.h aten/src/ATen/templates/Functions.cpp,https://github.com/pytorch/pytorch/pull/133425,cyyever,ezyang,,,
0d4cedaa47c,skip,not user facing,[13/N] Fix clang-tidy warnings in aten/src/ATen  (#133807),aten/src/ATen/native/nested/NestedTensorAliases.cpp aten/src/ATen/native/nested/NestedTensorBackward.cpp aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp aten/src/ATen/native/nested/NestedTensorBinaryOps.h aten/src/ATen/native/nested/NestedTensorFactories.cpp aten/src/ATen/native/nested/NestedTensorFactories.h aten/src/ATen/native/nested/NestedTensorMath.cpp aten/src/ATen/native/nested/NestedTensorMath.h aten/src/ATen/native/nested/NestedTensorMatmul.cpp aten/src/ATen/native/nested/NestedTensorTransformerFunctions.cpp aten/src/ATen/native/nested/NestedTensorTransformerUtils.h aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp aten/src/ATen/native/nested/NestedTensorUtils.cpp aten/src/ATen/native/nested/NestedTensorUtils.h,https://github.com/pytorch/pytorch/pull/133807,cyyever,Skylion007,ezyang,,
cf60fe53a83,skip,not user facing,[BE]: Update Typeguard to TypeIs for better type inference (#133814),torch/__init__.py torch/_dynamo/utils.py torch/_inductor/pattern_matcher.py torch/_subclasses/fake_tensor.py torch/masked/maskedtensor/core.py torch/nn/parameter.pyi torch/serialization.py torch/utils/_python_dispatch.py torch/utils/_sympy/value_ranges.py,https://github.com/pytorch/pytorch/pull/133814,Skylion007,ezyang,,,
c0c82a5f6a0,skip,not user facing,[CUDA][SDPA] Bump tolerances for `test_mem_efficient_attention_attn_mask_vs` (#133738),test/test_transformers.py,https://github.com/pytorch/pytorch/pull/133738,eqy,Skylion007,drisspg,nWEIdia,
e72e924eb5a,skip,not user facing,Add correct typing annotations to rsample() for all distributions (#133516),torch/distributions/beta.py torch/distributions/cauchy.py torch/distributions/continuous_bernoulli.py torch/distributions/dirichlet.py torch/distributions/distribution.py torch/distributions/exponential.py torch/distributions/fishersnedecor.py torch/distributions/gamma.py torch/distributions/independent.py torch/distributions/laplace.py torch/distributions/lowrank_multivariate_normal.py torch/distributions/multivariate_normal.py torch/distributions/normal.py torch/distributions/one_hot_categorical.py torch/distributions/relaxed_bernoulli.py torch/distributions/relaxed_categorical.py torch/distributions/studentT.py torch/distributions/transformed_distribution.py torch/distributions/uniform.py torch/distributions/wishart.py,https://github.com/pytorch/pytorch/pull/133516,chrisyeh96,Skylion007,,,
ae000635700,releng,not user facing,Change default runner's AMI to Amazon 2023 AMI - Part 1 (#133641),.github/lf-canary-scale-config.yml .github/lf-scale-config.yml,https://github.com/pytorch/pytorch/pull/133641,ZainRizvi,jeanschmidt,,,
42e61c783cf,inductor,not user facing,[Inductor][CPP] Align Half load with BFloat16 load (#132011),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/132011,CaoE,jgong5,peterbell10,,
35f36363ec2,skip,Untopiced,"Revert ""[dtensor] move DTensor to public namespace (#133113)""",docs/source/conf.py docs/source/distributed.rst docs/source/distributed.tensor.rst docs/source/index.rst test/allowlist_for_publicAPI.json test/distributed/_tensor/test_common_rules.py test/distributed/_tensor/test_dtensor.py test/distributed/_tensor/test_embedding_ops.py test/distributed/_tensor/test_math_ops.py test/distributed/_tensor/test_op_strategy.py test/distributed/_tensor/test_redistribute.py test/distributed/_tensor/test_tensor_ops.py test/distributed/_tensor/test_view_ops.py test/distributed/test_device_mesh.py torch/_dynamo/guards.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/distributed.py torch/_dynamo/variables/torch.py torch/distributed/_composable/fsdp/_fsdp_collectives.py torch/distributed/_composable/fsdp/_fsdp_common.py torch/distributed/_composable/fsdp/_fsdp_init.py torch/distributed/_composable/fsdp/_fsdp_param.py torch/distributed/_composable/fsdp/fully_shard.py torch/distributed/_functional_collectives.py torch/distributed/_state_dict_utils.py torch/distributed/_tensor/README.md torch/distributed/_tensor/__init__.py torch/distributed/_tensor/_collective_utils.py torch/distributed/_tensor/_dispatch.py torch/distributed/_tensor/_op_schema.py torch/distributed/_tensor/_redistribute.py torch/distributed/_tensor/_sharding_prop.py torch/distributed/_tensor/_shards_wrapper.py torch/distributed/_tensor/_tp_conv.py torch/distributed/_tensor/_utils.py torch/distributed/_tensor/api.py torch/distributed/_tensor/debug/__init__.py torch/distributed/_tensor/debug/_op_coverage.py torch/distributed/_tensor/debug/comm_mode.py torch/distributed/_tensor/debug/comm_mode_broswer_visual.js torch/distributed/_tensor/debug/visualize_sharding.py torch/distributed/_tensor/device_mesh.py torch/distributed/_tensor/examples/comm_mode_features_example.py torch/distributed/_tensor/examples/convnext_example.py torch/distributed/_tensor/examples/torchrec_sharding_example.py torch/distributed/_tensor/examples/visualize_sharding_example.py torch/distributed/_tensor/experimental/__init__.py torch/distributed/_tensor/experimental/attention.py torch/distributed/_tensor/experimental/func_map.py torch/distributed/_tensor/experimental/register_sharding.py torch/distributed/_tensor/experimental/tp_transform.py torch/distributed/_tensor/ops/__init__.py torch/distributed/_tensor/ops/_common_rules.py torch/distributed/_tensor/ops/_conv_ops.py torch/distributed/_tensor/ops/_einsum_strategy.py torch/distributed/_tensor/ops/_embedding_ops.py torch/distributed/_tensor/ops/_experimental_ops.py torch/distributed/_tensor/ops/_math_ops.py torch/distributed/_tensor/ops/_matrix_ops.py torch/distributed/_tensor/ops/_pointwise_ops.py torch/distributed/_tensor/ops/_random_ops.py torch/distributed/_tensor/ops/_tensor_ops.py torch/distributed/_tensor/ops/_view_ops.py torch/distributed/_tensor/ops/utils.py torch/distributed/_tensor/placement_types.py torch/distributed/_tensor/random.py torch/distributed/checkpoint/_traverse.py torch/distributed/checkpoint/default_planner.py torch/distributed/checkpoint/examples/async_checkpointing_example.py torch/distributed/checkpoint/examples/stateful_example.py torch/distributed/checkpoint/optimizer.py torch/distributed/checkpoint/planner_helpers.py torch/distributed/checkpoint/state_dict.py torch/distributed/fsdp/_flat_param.py torch/distributed/fsdp/_fsdp_extensions.py torch/distributed/fsdp/_optim_utils.py torch/distributed/fsdp/_shard_utils.py torch/distributed/fsdp/_state_dict_utils.py torch/distributed/fsdp/fully_sharded_data_parallel.py torch/distributed/tensor/README.md torch/distributed/tensor/__init__.py torch/distributed/tensor/_collective_utils.py torch/distributed/tensor/_dispatch.py torch/distributed/tensor/_op_schema.py torch/distributed/tensor/_ops/__init__.py torch/distributed/tensor/_ops/_common_rules.py torch/distributed/tensor/_ops/_conv_ops.py torch/distributed/tensor/_ops/_einsum_strategy.py torch/distributed/tensor/_ops/_embedding_ops.py torch/distributed/tensor/_ops/_experimental_ops.py torch/distributed/tensor/_ops/_math_ops.py torch/distributed/tensor/_ops/_matrix_ops.py torch/distributed/tensor/_ops/_pointwise_ops.py torch/distributed/tensor/_ops/_random_ops.py torch/distributed/tensor/_ops/_tensor_ops.py torch/distributed/tensor/_ops/_view_ops.py torch/distributed/tensor/_ops/utils.py torch/distributed/tensor/_redistribute.py torch/distributed/tensor/_sharding_prop.py torch/distributed/tensor/_shards_wrapper.py torch/distributed/tensor/_tp_conv.py torch/distributed/tensor/_utils.py torch/distributed/tensor/api.py torch/distributed/tensor/debug/__init__.py torch/distributed/tensor/debug/_op_coverage.py torch/distributed/tensor/debug/comm_mode.py torch/distributed/tensor/debug/comm_mode_broswer_visual.js torch/distributed/tensor/debug/visualize_sharding.py torch/distributed/tensor/device_mesh.py torch/distributed/tensor/examples/comm_mode_features_example.py torch/distributed/tensor/examples/convnext_example.py torch/distributed/tensor/examples/torchrec_sharding_example.py torch/distributed/tensor/examples/visualize_sharding_example.py torch/distributed/tensor/experimental/__init__.py torch/distributed/tensor/experimental/attention.py torch/distributed/tensor/experimental/func_map.py torch/distributed/tensor/experimental/register_sharding.py torch/distributed/tensor/experimental/tp_transform.py torch/distributed/tensor/parallel/_data_parallel_utils.py torch/distributed/tensor/parallel/_utils.py torch/distributed/tensor/parallel/api.py torch/distributed/tensor/parallel/fsdp.py torch/distributed/tensor/parallel/input_reshard.py torch/distributed/tensor/parallel/loss.py torch/distributed/tensor/parallel/style.py torch/distributed/tensor/placement_types.py torch/distributed/tensor/random.py torch/testing/_internal/common_fsdp.py,,,,,,
4bae7ae3d9a,distributed,Untopiced,[DeviceMesh][Easy] Fix typo (#133790),torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/133790,wz337,Skylion007,,,
8b6b1721c86,skip,not user facing,remove StrobelightCompileTimeProfiler.profile_compile_time from stacktrace when strobelight profiling not enabled (#133831),torch/_utils_internal.py,https://github.com/pytorch/pytorch/pull/133831,laithsakka,oulgen,,,
0a976b88992,skip,not user facing,Enable bf16 float32 mkldnn matmul when float32 precision is 'medium' (#130919),aten/src/ATen/native/mkldnn/Matmul.cpp,https://github.com/pytorch/pytorch/pull/130919,robert-hardwick,jgong5,,,
92151c814ba,skip,not user facing,[ROCm] Set _HAS_PYNVML to false if amdsmi not installed (#132990),torch/cuda/__init__.py,https://github.com/pytorch/pytorch/pull/132990,jataylo,eqy,malfet,pruthvistony,
5fab35d77c7,skip,Untopiced,[ONNX] New export logic leveraging ExportedProgram and ONNX IR (#132530),mypy.ini test/onnx/dynamo/test_exporter_api.py test/onnx/exporter/README.md test/onnx/exporter/test_api.py test/test_public_bindings.py torch/onnx/__init__.py torch/onnx/_internal/_lazy_import.py torch/onnx/_internal/exporter/__init__.py torch/onnx/_internal/exporter/_analysis.py torch/onnx/_internal/exporter/_building.py torch/onnx/_internal/exporter/_capture_strategies.py torch/onnx/_internal/exporter/_compat.py torch/onnx/_internal/exporter/_core.py torch/onnx/_internal/exporter/_decomp.py torch/onnx/_internal/exporter/_dispatching.py torch/onnx/_internal/exporter/_fx_passes.py torch/onnx/_internal/exporter/_ir_passes.py torch/onnx/_internal/exporter/_isolated.py torch/onnx/_internal/exporter/_onnx_program.py torch/onnx/_internal/exporter/_registration.py torch/onnx/_internal/exporter/_reporting.py torch/onnx/_internal/exporter/_schemas.py torch/onnx/_internal/exporter/_tensors.py torch/onnx/_internal/exporter/_verification.py torch/onnx/_internal/exporter/errors.py torch/onnx/_internal/fx/fx_onnx_interpreter.py torch/onnx/_internal/fx/serialization.py torch/onnx/utils.py,https://github.com/pytorch/pytorch/pull/132530,justinchuby,malfet,titaiwangms,,
5153550e4bf,releng,not user facing,"[CI] Add FP32 dynamic, AMP static, AMP dynamic for AOT inductor accuracy CPU CI test (#132836)",.github/workflows/inductor.yml benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_torchbench_amp_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_aot_inductor_torchbench_amp_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_aot_inductor_torchbench_freezing_inference.csv,https://github.com/pytorch/pytorch/pull/132836,WeizhuoZhang-intel,desertfire,,,
318d3b39c4e,skip,Untopiced,"Revert ""[Inductor][CPP] Support vectorization of load_seed and randn (#130317)""",test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_utils.py,,,,,,
76b0284744b,skip,Untopiced,"Revert ""[inductor][cpp] complete vectorization for int32/int64 (#122961)""",test/inductor/test_cpu_repro.py test/inductor/test_torchinductor.py torch/_inductor/codegen/cpp.py torch/_inductor/config.py,,,,,,
df6831562c4,distributed,not user facing,[Flight Recorder] Add more basic analysis to the script (#133412),test/distributed/flight_recorder/test_fr_analysis.py tools/flight_recorder/fr_trace.py,https://github.com/pytorch/pytorch/pull/133412,fduwjj,atalman,c-p-i-o,,
32ed4a3beb7,skip,not user facing,Update xpu CD used driver to rolling version (#133454),.ci/docker/common/install_xpu.sh .ci/docker/manywheel/Dockerfile_2_28,https://github.com/pytorch/pytorch/pull/133454,chuanqi129,atalman,,,
68fcd54226f,inductor,not user facing,Lower cache mocking to test more pytorch code (#133579),requirements.txt test/inductor/mock_cache.py test/inductor/test_codecache.py test/inductor/test_max_autotune.py torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/133579,aorenste,oulgen,,,
8d404581fcc,skip,Untopiced,"Revert ""[ONNX] New export logic leveraging ExportedProgram and ONNX IR (#132530)""",mypy.ini test/onnx/dynamo/test_exporter_api.py test/onnx/exporter/README.md test/onnx/exporter/test_api.py test/test_public_bindings.py torch/onnx/__init__.py torch/onnx/_internal/_lazy_import.py torch/onnx/_internal/exporter/__init__.py torch/onnx/_internal/exporter/_analysis.py torch/onnx/_internal/exporter/_building.py torch/onnx/_internal/exporter/_capture_strategies.py torch/onnx/_internal/exporter/_compat.py torch/onnx/_internal/exporter/_core.py torch/onnx/_internal/exporter/_decomp.py torch/onnx/_internal/exporter/_dispatching.py torch/onnx/_internal/exporter/_fx_passes.py torch/onnx/_internal/exporter/_ir_passes.py torch/onnx/_internal/exporter/_isolated.py torch/onnx/_internal/exporter/_onnx_program.py torch/onnx/_internal/exporter/_registration.py torch/onnx/_internal/exporter/_reporting.py torch/onnx/_internal/exporter/_schemas.py torch/onnx/_internal/exporter/_tensors.py torch/onnx/_internal/exporter/_verification.py torch/onnx/_internal/exporter/errors.py torch/onnx/_internal/fx/fx_onnx_interpreter.py torch/onnx/_internal/fx/serialization.py torch/onnx/utils.py,,,,,,
e37eef8a7bd,skip,not user facing,return state dict without optimized module (#132626),test/dynamo/test_hooks.py torch/_dynamo/eval_frame.py,https://github.com/pytorch/pytorch/pull/132626,mayank31398,,,,
d6f30b91e5c,inductor,Untopiced,Add a smaller default config option for decode (#133646),torch/_inductor/kernel/flex_decoding.py,https://github.com/pytorch/pytorch/pull/133646,drisspg,Chillee,joydddd,,
08f14d54929,dynamo,Untopiced,[refactor][dynamo][side-effects] Helper function for __new__ for user defined class (#133799),torch/_dynamo/side_effects.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/133799,anijain2305,jansel,,,
6ca68357b34,dynamo,not user facing,[dynamo] Save class vt in UserDefinedObjectVariable (#133800),test/dynamo/test_misc.py test/dynamo_expected_failures/TestQuantizeFx.test_static_lstm_with_custom_fixed_qparams torch/_dynamo/side_effects.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/133800,anijain2305,jansel,,,
f31404ba6f5,skip,Untopiced,"Revert ""Update xpu CD used driver to rolling version (#133454)""",.ci/docker/common/install_xpu.sh .ci/docker/manywheel/Dockerfile_2_28,,,,,,
da69a28c6f7,distributed (pipeline),not user facing,[pipelining] Add schedule runtime for lowered schedule (#130488),test/distributed/_composable/test_composability/test_pp_composability.py test/distributed/pipelining/test_schedule_multiproc.py torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/130488,wconstab,H-Huang,,,
6564e746ed4,inductor,Untopiced,[PT2] Port remove_noop to PT2 pre_grad passes (#132183),torch/_inductor/fx_passes/pre_grad.py,https://github.com/pytorch/pytorch/pull/132183,huxintong,frank-wei,,,
517aee5369b,jit,Untopiced,[torchscript] Add a sampled logging integration point. (#133484),c10/util/Logging.cpp c10/util/Logging.h torch/csrc/jit/api/function_impl.cpp torch/csrc/jit/frontend/script_type_parser.cpp,https://github.com/pytorch/pytorch/pull/133484,zhxchen17,davidberard98,,,
773a782249e,composability,Untopiced,Decompose _unsafe_index_put into index_put (#133365),test/expect/HasDecompTest.test_has_decomposition.expect torch/_decomp/__init__.py torch/_decomp/decompositions.py,https://github.com/pytorch/pytorch/pull/133365,dvorjackz,pianpwk,,,
ad7dda7b329,releng,not user facing,[CI] Bump up TIMM pin (#133528),.ci/docker/ci_commit_pins/timm.txt,https://github.com/pytorch/pytorch/pull/133528,desertfire,angelayi,,,
64d9afd8a74,composability,Untopiced,Register nll_loss2d decompositions for core aten (#133534),test/expect/HasDecompTest.test_aten_core_operators.expect torch/_decomp/__init__.py,https://github.com/pytorch/pytorch/pull/133534,dvorjackz,JacobSzwejbka,,,
a8619c9a1df,releng,not user facing,"Add nitpicker, which allows adding comments to PRs when they match a file pattern (#133861)",.github/nitpicks.yml .github/workflows/nitpicker.yml,https://github.com/pytorch/pytorch/pull/133861,ezyang,albanD,izaitsevfb,,
f1dc3b108a1,quantization,Untopiced,"Back out ""[export] fix test for training ir migration"" (#133697)",torch/ao/quantization/pt2e/port_metadata_pass.py,https://github.com/pytorch/pytorch/pull/133697,yushangdi,tugsbayasgalan,,,
fb26b843906,nn_frontend,Untopiced,Update fused kernels and call _safe_softmax from SDPA (#133882),aten/src/ATen/native/cpu/FlashAttentionKernel.cpp aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp aten/src/ATen/native/transformers/attention.cpp aten/src/ATen/native/transformers/cuda/mem_eff_attention/epilogue/epilogue_rescale_output.h aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernel_forward.h test/functorch/test_ops.py test/test_nn.py test/test_transformers.py tools/autograd/derivatives.yaml torch/csrc/autograd/FunctionsManual.cpp torch/csrc/autograd/FunctionsManual.h torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/133882,drisspg,soulitzer,,,
565e2ea019a,inductor,Untopiced,Scale XBLOCK in triton for `pointwise` (#133300),torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/133300,oraluben,jansel,,,
0d4eacb9d24,skip,not user facing,[fake tensor] unbacked symint support for binary op fast path (#133584),test/dynamo/test_export.py torch/_subclasses/fake_impls.py,https://github.com/pytorch/pytorch/pull/133584,ColinPeppler,ezyang,,,
66d6d8b1b97,dynamo,not user facing,Support TORCH_COMPILER_COLLECTIVES envvar (#133696),torch/_dynamo/config.py,https://github.com/pytorch/pytorch/pull/133696,ezyang,Skylion007,c-p-i-o,,
d1310480561,releng,not user facing,"Change install_triton to do git checkout, apply patch, pip install (#133878)",.ci/docker/common/install_triton.sh,https://github.com/pytorch/pytorch/pull/133878,atalman,ZainRizvi,malfet,,
432638f5217,releng,not user facing,Remove useless environment in reusable workflow (#133659),.github/workflows/_linux-test.yml,https://github.com/pytorch/pytorch/pull/133659,ezyang,Skylion007,,,
14ddd932fd7,sparse_frontend,new features,Add MaskedTensor support to _is_any_true (#128574),test/test_maskedtensor.py torch/masked/maskedtensor/_ops_refs.py,https://github.com/pytorch/pytorch/pull/128574,nowtryz,cpuhrsch,,,
8de56e29581,sparse_frontend,new features,Add MaskedTensor support to *_like API (#128637),docs/source/masked.rst test/test_maskedtensor.py torch/masked/maskedtensor/__init__.py torch/masked/maskedtensor/_ops_refs.py torch/masked/maskedtensor/like.py,https://github.com/pytorch/pytorch/pull/128637,nowtryz,,,,
641724ed1da,skip,Untopiced,[RFC][dynamo] add decorator to register polyfill for unsupported C++ function to avoid graph break (#133712),.lintrunner.toml docs/source/torch.compiler_api.rst test/dynamo/test_decorators.py torch/_dynamo/__init__.py torch/_dynamo/decorators.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/functions.py torch/compiler/__init__.py,https://github.com/pytorch/pytorch/pull/133712,XuehaiPan,jansel,,,
59ca56e56ca,skip,not user facing,[dynamo] simplify polyfill registration for `builtins.all` and `builtins.any` (#133769),torch/_dynamo/__init__.py torch/_dynamo/polyfill.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/builtins.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/133769,XuehaiPan,jansel,,,
ff9be0eda99,skip,not user facing,[dynamo] simplify implementation for `functools.reduce` (#133778),torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/functools.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/133778,XuehaiPan,anijain2305,jansel,,
1fdeb4e3291,skip,not user facing,[dynamo] simplify implementation for `builtins.sum` (#133779),torch/_dynamo/polyfills/builtins.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/133779,XuehaiPan,anijain2305,jansel,,
afb3e5ed6a9,releng,not user facing,Add onnx and onnxscript to CI requirements (#133647),.ci/docker/requirements-ci.txt torch/onnx/_internal/fx/fx_onnx_interpreter.py torch/onnx/_internal/fx/serialization.py,https://github.com/pytorch/pytorch/pull/133647,justinchuby,kit1980,titaiwangms,,
06faa15194a,inductor,Untopiced,[pytorch][counters] add pytorch.wait_counter.fx_codgen_and_compile (#133107),torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/133107,jamesperng,asiab4,,,
2b95007d12a,dynamo,not user facing,[dynamo] support random.Random (#133725),test/dynamo/test_unspec.py torch/_dynamo/side_effects.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/133725,williamwen42,jansel,,,
3f525c9d5d3,releng,not user facing,Upgrade nightly wheels to rocm6.2 - 2 of 2 (binaries) (#133238),.github/scripts/generate_binary_build_matrix.py .github/workflows/build-triton-wheel.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-nightly.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/133238,jithunnair-amd,atalman,,,
2ec95ffe577,dynamo,not user facing,[cond] support unbacked symbool inputs (#133589),test/functorch/test_control_flow.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/133589,ydwu4,ezyang,,,
65b3e420740,inductor,not user facing,Warn on fx graph cache bypass and log it to tlparse (#133826),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/133826,oulgen,aorenste,,,
9a998d98f17,skip,not user facing,Fix edge case in inductor triton clean script (#130837),test/inductor/test_kernel_benchmark.py torch/utils/_get_clean_triton.py,https://github.com/pytorch/pytorch/pull/130837,ahmadsarvmeily,Chillee,,,
6f738d64346,composability,not user facing,Remove early exit in constant_pad_nd for export (#132679),torch/_refs/__init__.py,https://github.com/pytorch/pytorch/pull/132679,lucylq,ezyang,,,
cfcb9e388d1,inductor,Untopiced,[PT2][Optimus] Add move reshape out of split stack pass (#133710),test/inductor/test_split_cat_fx_passes.py torch/_inductor/fx_passes/split_cat.py,https://github.com/pytorch/pytorch/pull/133710,mengluy0125,jackiexu1992,,,
874ae854eb8,distributed,Untopiced,[c10d] Land CudaEventCache with roll out flags (#133727),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,https://github.com/pytorch/pytorch/pull/133727,fduwjj,eqy,shuqiangzhang,,
c7af2728d35,cuda,Untopiced,Remove aten dispatch to empty in foreach_norm cuda kernel (#133897),aten/src/ATen/native/cuda/ForeachReduceOp.cu,https://github.com/pytorch/pytorch/pull/133897,janeyx99,albanD,drisspg,,
c6ea7b3f21d,skip,not user facing,Update xpu CD used driver to rolling version (#133454),.ci/docker/common/install_xpu.sh .ci/docker/manywheel/Dockerfile_2_28,https://github.com/pytorch/pytorch/pull/133454,chuanqi129,atalman,,,
c51fc7e98eb,skip,not user facing,Enable clang-tidy in aten/src/ATen/native/nested/ (#133829),.lintrunner.toml aten/src/ATen/native/nested/NestedTensorBackward.cpp aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp aten/src/ATen/native/nested/NestedTensorFactories.cpp aten/src/ATen/native/nested/NestedTensorMath.cpp aten/src/ATen/native/nested/NestedTensorMatmul.cpp aten/src/ATen/native/nested/NestedTensorTransformerFunctions.cpp aten/src/ATen/native/nested/NestedTensorUtils.h aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp aten/src/ATen/native/nested/cuda/NestedTensorTransformerUtils.cpp,https://github.com/pytorch/pytorch/pull/133829,cyyever,Skylion007,,,
6c82a1c68cb,inductor,not user facing,[AOTI] Introduce DeferredCudaKernelLine for cuda cpp wrapper (#129135),torch/_inductor/codegen/cpp_wrapper_cuda.py,https://github.com/pytorch/pytorch/pull/129135,desertfire,angelayi,,,
188cb5e67b6,releng,not user facing,Bump scikit-image to 0.22.0 (#133932),.ci/docker/requirements-ci.txt,https://github.com/pytorch/pytorch/pull/133932,atalman,malfet,,,
b0bafd2be51,dynamo,not user facing,remove tensor weak ref from constraint target (#133890),torch/_dynamo/eval_frame.py torch/export/dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/133890,avikchaudhuri,tugsbayasgalan,,,
32f57ac6278,quantization,Untopiced,[BE] Fix lint issues in qlinear_prepack.cpp (#133797),aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp,https://github.com/pytorch/pytorch/pull/133797,hl475,Skylion007,,,
994fcb9acd3,distributed,Untopiced,Killswitch based rollout for flight recorder (#133237),test/distributed/test_c10d_nccl.py torch/testing/_internal/distributed/distributed_test.py,https://github.com/pytorch/pytorch/pull/133237,c-p-i-o,c00w,,,
15addb00e69,skip,not user facing,Update test_control_flow.py to device-agnostic. (#133843),test/inductor/test_control_flow.py,https://github.com/pytorch/pytorch/pull/133843,Stonepia,EikanWang,atalman,desertfire,
b6891f40020,skip,not user facing,[1/N] Refactor fr trace script to make it modulized - config (#133927),tools/flight_recorder/components/config_manager.py tools/flight_recorder/fr_trace.py,https://github.com/pytorch/pytorch/pull/133927,fduwjj,c-p-i-o,,,
f08d4847022,dynamo,not user facing,Add itertools.islice support in dynamo (#133893),test/dynamo/test_misc.py torch/_dynamo/polyfill.py torch/_dynamo/variables/iter.py,https://github.com/pytorch/pytorch/pull/133893,bobrenjc93,oulgen,,,
e8d3c4be362,skip,not user facing,[dynamo][reland][inline-inbuilt-nn-modules] Mark attributes of nn mod… (#133714),test/dynamo/test_modules.py test/dynamo/test_repros.py torch/_dynamo/guards.py torch/_dynamo/source.py torch/_dynamo/utils.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/user_defined.py torch/_guards.py,https://github.com/pytorch/pytorch/pull/133714,anijain2305,jansel,,,
7492da804fb,skip,not user facing,Mark disabled tests as fixed (#133940),test/inductor/mock_cache.py,https://github.com/pytorch/pytorch/pull/133940,aorenste,oulgen,,,
09e366cb576,dynamo,Untopiced,[Dynamo] Add torch function mode stack guard to dynamo (#133130),test/dynamo/test_modes.py torch/_C/_dynamo/guards.pyi torch/_dynamo/guards.py torch/_dynamo/output_graph.py torch/_dynamo/utils.py torch/_dynamo/variables/torch_function.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/133130,mlazos,anijain2305,,,
f1473495689,dynamo,Untopiced,Fix DeviceContext bug (#133729),test/test_overrides.py torch/_dynamo/variables/user_defined.py torch/utils/_device.py,https://github.com/pytorch/pytorch/pull/133729,mlazos,bdhirsh,,,
c0b4aaa8c51,dynamo,Untopiced,[Dynamo] Support pop torch function mode stack (#133131),test/dynamo/test_modes.py torch/_dynamo/side_effects.py torch/_dynamo/source.py torch/_dynamo/symbolic_convert.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/torch_function.py,https://github.com/pytorch/pytorch/pull/133131,mlazos,jansel,,,
d1fdf984c37,dynamo,Untopiced,[Dynamo] Support push torch function mode stack (#133132),test/dynamo/test_modes.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/torch.py,https://github.com/pytorch/pytorch/pull/133132,mlazos,williamwen42,,,
626acaeb16e,dynamo,Untopiced,[Dynamo] Support torch function stack len (#133133),test/dynamo/test_modes.py torch/_dynamo/variables/torch.py,https://github.com/pytorch/pytorch/pull/133133,mlazos,williamwen42,,,
d97ca968cd1,dynamo,not user facing,[Dynamo] Test intermediate tf mode construction (#133134),test/dynamo/test_modes.py,https://github.com/pytorch/pytorch/pull/133134,mlazos,williamwen42,,,
48ee0984ac1,cpp_frontend,Untopiced,Add C API to return all torch function disablement status (#133136),aten/src/ATen/PythonTorchFunctionTLS.cpp aten/src/ATen/PythonTorchFunctionTLS.h test/test_overrides.py torch/_C/__init__.pyi.in torch/csrc/Module.cpp torch/csrc/utils/disable_torch_function.cpp torch/csrc/utils/disable_torch_function.h,https://github.com/pytorch/pytorch/pull/133136,mlazos,bdhirsh,,,
25d5a815f74,dynamo,Untopiced,[Dynamo] Guard on torch function mode global state (#133135),test/dynamo/test_modes.py torch/_dynamo/output_graph.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/133135,mlazos,anijain2305,,,
42097f0ec16,skip,Untopiced,"Revert ""[BE]: Update Typeguard to TypeIs for better type inference (#133814)""",torch/__init__.py torch/_dynamo/utils.py torch/_inductor/pattern_matcher.py torch/_subclasses/fake_tensor.py torch/masked/maskedtensor/core.py torch/nn/parameter.pyi torch/serialization.py torch/utils/_python_dispatch.py torch/utils/_sympy/value_ranges.py,,,,,,
68570fca696,skip,Untopiced,"Revert ""Add MaskedTensor support to *_like API (#128637)""",docs/source/masked.rst test/test_maskedtensor.py torch/masked/maskedtensor/__init__.py torch/masked/maskedtensor/_ops_refs.py torch/masked/maskedtensor/like.py,,,,,,
08b5e07e6c5,skip,Untopiced,"Revert ""[dynamo] simplify implementation for `builtins.sum` (#133779)""",torch/_dynamo/polyfills/builtins.py torch/_dynamo/variables/builtin.py,,,,,,
11af423eca8,distributed,not user facing,"[SymmetricMemory] make buffer_ptrs_dev, signal_pad_ptrs_dev, buffer_size, and signal_pad_size accessible in python (#133680)",torch/csrc/distributed/c10d/init.cpp,https://github.com/pytorch/pytorch/pull/133680,yifuwang,Chillee,,,
241df7e7f80,releng,not user facing,Add multi-cache autotune test (#133868),.ci/docker/requirements-ci.txt requirements.txt test/inductor/mock_cache.py test/inductor/test_codecache.py,https://github.com/pytorch/pytorch/pull/133868,aorenste,oulgen,,,
5109c5ef238,skip,Untopiced,"Revert ""[dynamo] simplify implementation for `functools.reduce` (#133778)""",torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/functools.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builtin.py,,,,,,
91fd270535c,skip,Untopiced,"Revert ""[dynamo] simplify polyfill registration for `builtins.all` and `builtins.any` (#133769)""",torch/_dynamo/__init__.py torch/_dynamo/polyfill.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/builtins.py torch/_dynamo/variables/builtin.py,,,,,,
2bd02e0c82c,skip,Untopiced,"Revert ""[RFC][dynamo] add decorator to register polyfill for unsupported C++ function to avoid graph break (#133712)""",.lintrunner.toml docs/source/torch.compiler_api.rst test/dynamo/test_decorators.py torch/_dynamo/__init__.py torch/_dynamo/decorators.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/functions.py torch/compiler/__init__.py,,,,,,
36376efd06e,skip,not user facing,[2/N] Refactor FR script - add a loader module (#133929),tools/flight_recorder/components/loader.py tools/flight_recorder/fr_trace.py,https://github.com/pytorch/pytorch/pull/133929,fduwjj,c-p-i-o,,,
6590f4fb0ef,releng,not user facing,[CD] Enable python 3.13 for xpu nightly build (#133670),.circleci/scripts/binary_linux_test.sh .circleci/scripts/binary_populate_env.sh .github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/133670,chuanqi129,atalman,malfet,,
b02695d65f9,quantization,Untopiced,"[export] training ir migration, fix export_rle_model (#133937)",torch/ao/quantization/pt2e/port_metadata_pass.py,https://github.com/pytorch/pytorch/pull/133937,yushangdi,tugsbayasgalan,,,
f2b57d8831b,python_frontend,improvements,Fix `torch._C` submodules population (#133919),torch/__init__.py,https://github.com/pytorch/pytorch/pull/133919,malfet,Skylion007,XuehaiPan,albanD,
3a2f7192c3b,skip,Untopiced,"Revert ""return state dict without optimized module (#132626)""",test/dynamo/test_hooks.py torch/_dynamo/eval_frame.py,,,,,,
52dfe99dbf7,skip,not user facing,Skip test_custom_op_add_abi_compatible_cpu_with_stack_allocation internally (#133704),test/inductor/test_aot_inductor.py,https://github.com/pytorch/pytorch/pull/133704,masnesral,desertfire,,,
187d55018ae,inductor,not user facing,[BE] Fix MYPY issues (#133872),torch/_inductor/codegen/simd.py torch/_inductor/dependencies.py torch/_inductor/sizevars.py torch/utils/_ordered_set.py,https://github.com/pytorch/pytorch/pull/133872,aorenste,Skylion007,oulgen,,
8d93fe510e5,skip,not user facing,Remove NestedTensorFactories.h (#133809),aten/src/ATen/native/nested/NestedTensorFactories.cpp aten/src/ATen/native/nested/NestedTensorFactories.h aten/src/ATen/native/nested/README.md,https://github.com/pytorch/pytorch/pull/133809,cyyever,ezyang,,,
33f1ee036ef,dynamo,not user facing,[dynamo][user-defined] Simplify call_hasattr (#133935),torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/133935,anijain2305,jansel,williamwen42,,
c3d02fa3906,skip,not user facing,[Reland2] Update NVTX to NVTX3 (#109843),.ci/pytorch/win-test-helpers/build_pytorch.bat .ci/pytorch/win-test-helpers/setup_pytorch_env.bat .ci/pytorch/win-test-helpers/test_custom_backend.bat .ci/pytorch/win-test-helpers/test_custom_script_ops.bat .ci/pytorch/win-test-helpers/test_libtorch.bat .gitmodules CMakeLists.txt caffe2/CMakeLists.txt cmake/TorchConfig.cmake.in cmake/public/cuda.cmake setup.py third_party/NVTX torch/CMakeLists.txt torch/csrc/cuda/shared/nvtx.cpp torch/csrc/profiler/stubs/cuda.cpp torch/utils/hipify/cuda_to_hip_mappings.py,https://github.com/pytorch/pytorch/pull/109843,cyyever,eqy,peterbell10,,
3caf3baabbc,inductor,not user facing,[inductor] enable inductor backend for dynamo on Windows. (#133921),torch/_dynamo/backends/inductor.py,https://github.com/pytorch/pytorch/pull/133921,xuhancn,jansel,jgong5,,
fbf3fc2a306,inductor,not user facing,[inductor] Use int64_t as index type for all platfroms 4 (#133892),test/inductor/test_indexing.py test/inductor/test_memory_planning.py torch/_inductor/codecache.py torch/_inductor/codegen/cpp_utils.py,https://github.com/pytorch/pytorch/pull/133892,xuhancn,jansel,,,
bce0caba780,skip,not user facing,[BE]: Update Typeguard to TypeIs for better type inference (#133814),setup.py torch/__init__.py torch/_dynamo/utils.py torch/_inductor/pattern_matcher.py torch/_subclasses/fake_tensor.py torch/masked/maskedtensor/core.py torch/nn/parameter.pyi torch/serialization.py torch/utils/_python_dispatch.py,https://github.com/pytorch/pytorch/pull/133814,Skylion007,ezyang,,,
e41b520ee30,distributed,not user facing,[3/N] Refactor FR script - Add a processor module (#133933),test/distributed/flight_recorder/test_fr_analysis.py tools/flight_recorder/components/processor.py tools/flight_recorder/fr_trace.py,https://github.com/pytorch/pytorch/pull/133933,fduwjj,c-p-i-o,,,
333890b701f,releng,not user facing,Enable CUDA 12.4.1 (#132202),.ci/docker/build.sh .ci/docker/common/install_cuda.sh .ci/docker/common/install_cuda_aarch64.sh .ci/pytorch/test.sh .github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/132202,nWEIdia,Skylion007,atalman,eqy,
278bc985d71,skip,not user facing,[report_exportability] Avoid re-exporting duplicated modules (#133930),torch/_export/tools.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/133930,SherlockNoMad,angelayi,,,
43f78bf37a0,mps,Untopiced,[MPS] Gather sliced inputs to batch norm (#133610),aten/src/ATen/native/mps/operations/Normalization.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/133610,hvaara,malfet,,,
49f6ea6dd91,skip,Untopiced,"Revert ""[report_exportability] Avoid re-exporting duplicated modules (#133930)""",torch/_export/tools.py torch/export/_trace.py,,,,,,
81a822ddc9a,Uncategorized,Untopiced,"Back out ""[1/N] Fix clang-tidy warnings in inductor (#131979)"" (#133922)",torch/csrc/inductor/aoti_eager/kernel_holder.cpp torch/csrc/inductor/aoti_eager/kernel_meta_info.cpp torch/csrc/inductor/aoti_eager/kernel_meta_info.h torch/csrc/inductor/aoti_runner/model_container_runner.cpp torch/csrc/inductor/aoti_runner/model_container_runner_cpu.cpp torch/csrc/inductor/aoti_runner/model_container_runner_cuda.cpp torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/mkldnn_tensor.cpp torch/csrc/inductor/aoti_torch/oss_proxy_executor.cpp torch/csrc/inductor/aoti_torch/oss_proxy_executor.h torch/csrc/inductor/aoti_torch/shim_common.cpp torch/csrc/inductor/aoti_torch/tensor_converter.cpp torch/csrc/inductor/inductor_ops.cpp torch/csrc/inductor/resize_storage_bytes.cpp,https://github.com/pytorch/pytorch/pull/133922,ColinPeppler,22quinn,,,
c188d419db9,releng,not user facing,[BE] [EZ] Allow linux-build workflows to run on the default runner type (#133640),.github/workflows/periodic.yml .github/workflows/pull.yml .github/workflows/slow.yml .github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/133640,ZainRizvi,clee2000,jeanschmidt,malfet,
49430bfd5c7,distributed,not user facing,[DeviceMesh] Add a _MeshEnv attr to record the mapping of flatten mesh_dim_name to its mesh dim index in root mesh (#133838),test/distributed/test_device_mesh.py torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/133838,wz337,fegin,,,
71dd52f51a0,skip,Untopiced,[RFC][dynamo] add decorator to register polyfill for unsupported C++ function to avoid graph break (#133712),.lintrunner.toml docs/source/torch.compiler_api.rst test/dynamo/test_decorators.py torch/_dynamo/__init__.py torch/_dynamo/decorators.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/functions.py torch/compiler/__init__.py,https://github.com/pytorch/pytorch/pull/133712,XuehaiPan,jansel,,,
178e8563b8a,skip,not user facing,[dynamo] simplify polyfill registration for `builtins.all` and `builtins.any` (#133769),torch/_dynamo/__init__.py torch/_dynamo/polyfill.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/builtins.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/133769,XuehaiPan,jansel,,,
37b4bc60a4e,skip,not user facing,[dynamo] simplify implementation for `functools.reduce` (#133778),torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/functools.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/133778,XuehaiPan,anijain2305,jansel,,
3f58a8051a9,skip,not user facing,[dynamo] simplify implementation for `builtins.sum` (#133779),torch/_dynamo/polyfills/builtins.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/133779,XuehaiPan,anijain2305,jansel,,
28ce3c02278,dynamo,not user facing,[dynamo][itertools] support `itertools.tee` (#133771),test/dynamo/test_misc.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/itertools.py,https://github.com/pytorch/pytorch/pull/133771,XuehaiPan,jansel,,,
bb0bf09aff8,skip,not user facing,[easy] skip test_sdpa_autocast on windows (#134009),test/test_nestedtensor.py,https://github.com/pytorch/pytorch/pull/134009,davidberard98,Skylion007,YuqingJ,ZainRizvi,
5229b52bf23,dynamo,Untopiced,[dynamo] support `cls.__base__` (#133969),test/dynamo/test_subclasses.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/133969,XuehaiPan,jansel,,,
b03381cac28,dynamo,Untopiced,[dynamo] support `cls.__flags__` (#133970),test/dynamo/test_misc.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/133970,XuehaiPan,jansel,,,
ccc0aa69ce5,onnx,bc breaking,[ONNX] Remove torch.onnx._export (#133824),test/onnx/dynamo/test_exporter_api.py test/onnx/test_export_modes.py test/onnx/verify.py torch/onnx/__init__.py torch/onnx/symbolic_opset9.py,https://github.com/pytorch/pytorch/pull/133824,justinchuby,titaiwangms,,,
2540ee372a7,skip,Untopiced,"Revert ""[dynamo][itertools] support `itertools.tee` (#133771)""",test/dynamo/test_misc.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/itertools.py,,,,,,
98e6a1d8ff1,skip,Untopiced,"Revert ""[dynamo] simplify implementation for `builtins.sum` (#133779)""",torch/_dynamo/polyfills/builtins.py torch/_dynamo/variables/builtin.py,,,,,,
3fa874abbef,skip,Untopiced,"Revert ""[dynamo] simplify implementation for `functools.reduce` (#133778)""",torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/functools.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builtin.py,,,,,,
88ead0afc69,skip,Untopiced,"Revert ""[dynamo] simplify polyfill registration for `builtins.all` and `builtins.any` (#133769)""",torch/_dynamo/__init__.py torch/_dynamo/polyfill.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/builtins.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builtin.py,,,,,,
15b5a0b67fc,skip,Untopiced,"Revert ""[RFC][dynamo] add decorator to register polyfill for unsupported C++ function to avoid graph break (#133712)""",.lintrunner.toml docs/source/torch.compiler_api.rst test/dynamo/test_decorators.py torch/_dynamo/__init__.py torch/_dynamo/decorators.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/functions.py torch/compiler/__init__.py,,,,,,
2e1830c7c8d,nested tensor_frontend,Untopiced,Implement 2D version of masked_select for nestedtensors (#133889),test/test_nestedtensor.py torch/nested/__init__.py torch/nested/_internal/ops.py torch/testing/_internal/opinfo/definitions/nested.py,https://github.com/pytorch/pytorch/pull/133889,krzysztofjordan,soulitzer,,,
a36739f36a3,releng,not user facing,Cherry-Picking don't resolve conflicts (#134047),.github/scripts/cherry_pick.py,https://github.com/pytorch/pytorch/pull/134047,atalman,kit1980,,,
1ae5d5bb621,dynamo,not user facing,[dynamo][user-defined] Improve getattr_static for user_defined objects (#133742),test/dynamo/test_repros.py test/dynamo_expected_failures/FakeTensorOperatorInvariants.test_like_ops test/dynamo_expected_failures/FakeTensorOperatorInvariants.test_non_kwarg_only_device test/dynamo_expected_failures/FakeTensorOperatorInvariants.test_tensor_constructors_all_have_kwarg_device test/dynamo_expected_failures/PackedSequenceTest.test_pack_sequence test/dynamo_expected_failures/PackedSequenceTest.test_unpack_sequence test/dynamo_expected_failures/TestBroadcast.test_broadcast_error_kwargs test/dynamo_expected_failures/TestBroadcast.test_broadcast_single_arg test/dynamo_expected_failures/TestCommonPass.test_correctness_CSEPass_MutationInput_cpu test/dynamo_expected_failures/TestCommonPass.test_correctness_CSEPass_MutationMetadata_cpu test/dynamo_expected_failures/TestCommonPass.test_correctness_CSEPass_MutationTorchTensorCall_cpu test/dynamo_expected_failures/TestCommonPass.test_correctness_CSEPass_Mutation_cpu test/dynamo_expected_failures/TestCommonPass.test_correctness_CSEPass_ReturnList_cpu test/dynamo_expected_failures/TestCommonPass.test_correctness_CSEPass_TakeList_cpu test/dynamo_expected_failures/TestCommonPass.test_correctness_factory_CSEPass_FactoryFunctionCall_cpu test/dynamo_expected_failures/TestCommonPass.test_correctness_factory_CSEPass_MutationFactory_cpu test/dynamo_expected_failures/TestPassManager.test_pass_manager test/dynamo_expected_failures/TestScript.test_dict_str test/dynamo_expected_failures/TestScript.test_none_type_str test/dynamo_expected_failures/TestScript.test_script_pack_padded_sequence test/dynamo_expected_failures/TestScript.test_tuple_str torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/133742,anijain2305,Skylion007,jansel,,
41fab40be77,skip,not user facing,[report_exportability] Avoid re-exporting duplicated modules (#133930),torch/_export/tools.py,https://github.com/pytorch/pytorch/pull/133930,SherlockNoMad,angelayi,,,
f7c1f32803a,skip,not user facing,Fix  partially initialized module error (#134019),torch/cuda/__init__.py,https://github.com/pytorch/pytorch/pull/134019,malfet,seemethere,,,
bc785c2d9af,inductor,not user facing,[Inductor][FlexAttention] Don't trigger dynamic shape on building empty block mask (#133836),test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/133836,yanboliang,Chillee,,,
5a7b544e5c3,skip,not user facing,Update FlexAttention with masking semantic (#133373),test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py,https://github.com/pytorch/pytorch/pull/133373,drisspg,yanboliang,,,
5f3d22a6096,distributed,Untopiced,Avoid GPU syncs by reusing Pre-allocated Zero Tensor (#128069),torch/distributed/fsdp/fully_sharded_data_parallel.py,https://github.com/pytorch/pytorch/pull/128069,quanta42,awgu,,,
c42ac54d9e8,inductor,not user facing,[inductor] prune unused constants in graph scheduling (#132208),torch/_inductor/codegen/cpp_gemm_template.py,https://github.com/pytorch/pytorch/pull/132208,jgong5,leslie-fang-intel,,,
d1abd6241a7,releng,not user facing,[CI][BE] Update retry action to v3.0.0 (#119403),.github/actions/filter-test-configs/action.yml .github/actions/pytest-cache-download/action.yml .github/actions/pytest-cache-upload/action.yml .github/actions/setup-linux/action.yml .github/actions/teardown-win/action.yml .github/templates/macos_binary_build_workflow.yml.j2 .github/workflows/_buck-build-test.yml .github/workflows/_ios-build-test.yml .github/workflows/_mac-build.yml .github/workflows/_run_android_tests.yml .github/workflows/_win-test.yml .github/workflows/docker-builds.yml .github/workflows/generated-macos-arm64-binary-conda-nightly.yml .github/workflows/generated-macos-arm64-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/llm_td_retrieval.yml .github/workflows/nightly-rockset-uploads.yml,https://github.com/pytorch/pytorch/pull/119403,malfet,Skylion007,clee2000,,
06cc2e83f0b,optim,Untopiced,Make optim.swa.util content accessible from the torch.optim doc (#133393),docs/source/optim.rst torch/optim/swa_utils.py,https://github.com/pytorch/pytorch/pull/133393,spzala,janeyx99,,,
e8fc1e0118d,skip,Untopiced,[ONNX] New export logic leveraging ExportedProgram and ONNX IR (#132530),docs/source/onnx_torchscript.rst mypy.ini test/onnx/dynamo/test_exporter_api.py test/onnx/exporter/README.md test/onnx/exporter/test_api.py test/test_public_bindings.py torch/onnx/__init__.py torch/onnx/_internal/_lazy_import.py torch/onnx/_internal/exporter/__init__.py torch/onnx/_internal/exporter/_analysis.py torch/onnx/_internal/exporter/_building.py torch/onnx/_internal/exporter/_capture_strategies.py torch/onnx/_internal/exporter/_compat.py torch/onnx/_internal/exporter/_core.py torch/onnx/_internal/exporter/_decomp.py torch/onnx/_internal/exporter/_dispatching.py torch/onnx/_internal/exporter/_fx_passes.py torch/onnx/_internal/exporter/_ir_passes.py torch/onnx/_internal/exporter/_isolated.py torch/onnx/_internal/exporter/_onnx_program.py torch/onnx/_internal/exporter/_registration.py torch/onnx/_internal/exporter/_reporting.py torch/onnx/_internal/exporter/_schemas.py torch/onnx/_internal/exporter/_tensors.py torch/onnx/_internal/exporter/_verification.py torch/onnx/_internal/exporter/errors.py torch/onnx/utils.py,https://github.com/pytorch/pytorch/pull/132530,justinchuby,malfet,titaiwangms,,
8337b4d96eb,skip,not user facing,[training ir migration] Fix ReorderConvertTest (#134010),torch/_export/__init__.py,https://github.com/pytorch/pytorch/pull/134010,yushangdi,tugsbayasgalan,,,
66d3eb783c3,distributed,not user facing,"[SymmetricMemory] introduce multicast support, multimem_all_reduce_ and multimem_one_shot_all_reduce (#133424)",BUILD.bazel build_variables.bzl c10/cuda/driver_api.cpp c10/cuda/driver_api.h test/distributed/test_symmetric_memory.py torch/csrc/distributed/c10d/CUDASymmetricMemory-inl.h torch/csrc/distributed/c10d/CUDASymmetricMemory.cu torch/csrc/distributed/c10d/CUDASymmetricMemory.hpp torch/csrc/distributed/c10d/CUDASymmetricMemoryOps.cu torch/csrc/distributed/c10d/SymmetricMemory.cpp torch/csrc/distributed/c10d/SymmetricMemory.hpp torch/csrc/distributed/c10d/init.cpp,https://github.com/pytorch/pytorch/pull/133424,yifuwang,weifengpy,yf225,,
b39ec7fbe99,distributed,Untopiced,[1/N] Make NCCL PG error messages more accurate and simpler (#134017),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,https://github.com/pytorch/pytorch/pull/134017,fduwjj,wconstab,,,
af664882dde,inductor,not user facing,Safely infer device type + docstrings + tests (#133668),test/inductor/test_benchmarking.py torch/_inductor/runtime/benchmarking.py,https://github.com/pytorch/pytorch/pull/133668,nmacchioni,eellison,,,
843fdf81c21,distributed,Untopiced,Fix a getenv segfault due to a race (#133744),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/133744,deepshah133,fduwjj,wconstab,,
022cd7c9aa4,skip,Untopiced,[RFC][dynamo] add decorator to register polyfill for unsupported C++ function to avoid graph break (#133712),.lintrunner.toml docs/source/torch.compiler_api.rst test/dynamo/test_decorators.py torch/_dynamo/__init__.py torch/_dynamo/decorators.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/functions.py torch/compiler/__init__.py,https://github.com/pytorch/pytorch/pull/133712,XuehaiPan,jansel,,,
3d8db413373,quantization,Untopiced,Add new op wrapped_quantized_linear (#134024),aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp aten/src/ATen/native/quantized/library.cpp test/quantization/core/test_quantized_op.py,https://github.com/pytorch/pytorch/pull/134024,hl475,houseroad,,,
c2e2602ecdc,inductor,not user facing,"[Inductor] Move `GPU_TYPE`(The runtime avaliable gpu type, cuda or xpu) from (#132740)",torch/_inductor/utils.py torch/testing/_internal/inductor_utils.py,https://github.com/pytorch/pytorch/pull/132740,etaf,EikanWang,jansel,,
6535f112591,inductor,not user facing,[Inductor] Support _check_triton_bf16_support on XPU. (#132748),torch/_dynamo/device_interface.py torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/132748,etaf,EikanWang,eellison,,
19eb14493a8,inductor,not user facing,"[Inductor] Moves intermediary tensors which are constructed on the cpu to XPU when safe, align with CUDA. (#132843)",torch/_inductor/fx_passes/post_grad.py,https://github.com/pytorch/pytorch/pull/132843,etaf,EikanWang,eellison,,
30dc6338c13,composability,not user facing,[effects] Prevent inductor dtype promotions for HOP effects tokens (#134003),torch/_higher_order_ops/effects.py torch/_prims/__init__.py,https://github.com/pytorch/pytorch/pull/134003,IvanKobzarev,bdhirsh,zou3519,,
695291be2f9,skip,not user facing,Fix test flakiness due to not resetting state (#134058),test/test_overrides.py,https://github.com/pytorch/pytorch/pull/134058,mlazos,yanboliang,,,
c929e1e11ff,dynamo,not user facing,[dynamo] fix polyfill for user defined constructor `__new__` (#133822),test/dynamo/test_misc.py torch/_dynamo/polyfill.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/133822,XuehaiPan,jansel,,,
bd0db490bf7,dynamo,not user facing,[dynamo][set] Fix EQUALS_MATCH guard for constant sets and lists (#134016),test/dynamo/test_functions.py test/dynamo_expected_failures/TestOldSerialization.test_serialization_filelike_api_requirements test/dynamo_expected_failures/TestSerialization.test_serialization_filelike_api_requirements torch/_dynamo/guards.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/constant.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/134016,anijain2305,jansel,laithsakka,,
0d79f67a25a,dynamo,not user facing,[dynamo][exception] Support raise exception from None (#134028),test/dynamo/test_exceptions.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/134028,anijain2305,yanboliang,,,
539be0a7695,dynamo,Untopiced,[dynamo] support `ClassMethodDescriptorType` (#133862),torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/133862,XuehaiPan,jansel,,,
313bc119638,skip,not user facing,[inductor][cpp] complete vectorization for int32/int64 (#122961),test/inductor/test_cpu_repro.py test/inductor/test_torchinductor.py torch/_inductor/codegen/cpp.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/122961,jgong5,jansel,,,
2bffbe06bdf,skip,not user facing,[Inductor][CPP] Support vectorization of load_seed and randn (#130317),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_utils.py,https://github.com/pytorch/pytorch/pull/130317,leslie-fang-intel,jgong5,,,
4b1fb3b0ed9,distributed,Untopiced,[PP] pt-native input/weight grad split (#132691),test/distributed/pipelining/schedule_registry.py test/distributed/pipelining/test_backward.py test/distributed/pipelining/test_schedule_multiproc.py test/distributed/pipelining/test_stage.py torch/distributed/pipelining/_backward.py torch/distributed/pipelining/stage.py,https://github.com/pytorch/pytorch/pull/132691,H-Huang,wconstab,,,
683609c631a,skip,not user facing,Skip cpp_extension test internally (#134011),test/dynamo/test_misc.py,https://github.com/pytorch/pytorch/pull/134011,zou3519,masnesral,,,
afaa5fcecb0,inductor,not user facing,"[BE][Ez]: FURB142,FURB92 misc preview fixes (#133880)",torch/_inductor/graph.py torch/distributed/_tensor/ops/_pointwise_ops.py torch/utils/flop_counter.py,https://github.com/pytorch/pytorch/pull/133880,Skylion007,malfet,soulitzer,,
585c049fa37,python_frontend,Untopiced,Fix `Extension` attribute name in `CppExtension` example (#134046),torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/134046,blazej-smorawski,soulitzer,,,
32e052e468b,python_frontend,Untopiced,[docs] improve `torch.stack` example code to be reproducible (#133857),torch/_torch_docs.py,https://github.com/pytorch/pytorch/pull/133857,ooooo-create,soulitzer,,,
68425e68fed,skip,Untopiced,"Revert ""[dynamo][reland][inline-inbuilt-nn-modules] Mark attributes of nn mod… (#133714)""",test/dynamo/test_modules.py test/dynamo/test_repros.py torch/_dynamo/guards.py torch/_dynamo/source.py torch/_dynamo/utils.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/user_defined.py torch/_guards.py,,,,,,
57625baceac,skip,not user facing,[partitioner] Fix must_be_in_backward corner cases (#134002),torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/134002,IvanKobzarev,bdhirsh,,,
2db28a96113,skip,Untopiced,"Revert ""[BE]: Update Typeguard to TypeIs for better type inference (#133814)""",setup.py torch/__init__.py torch/_dynamo/utils.py torch/_inductor/pattern_matcher.py torch/_subclasses/fake_tensor.py torch/masked/maskedtensor/core.py torch/nn/parameter.pyi torch/serialization.py torch/utils/_python_dispatch.py,,,,,,
3ef1cc8583b,skip,not user facing,[export] Implement common_getitem_elimination pass. (#133618),test/export/test_export.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/133618,zhxchen17,tugsbayasgalan,,,
1da3a049dac,skip,not user facing,[dynamo][super] Improve handling of getattr on super (#134039),test/dynamo/test_repros.py torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/134039,anijain2305,jansel,yanboliang,,
dd5a7c83971,inductor,Untopiced,[PT2] Add a pass to convert stack to unsqueeze cat (#133966),torch/_inductor/fx_passes/pre_grad.py,https://github.com/pytorch/pytorch/pull/133966,huxintong,frank-wei,,,
6835f20d20c,skip,not user facing,[HOP] support generating schema for hop (#133521),test/export/test_export.py test/functorch/test_control_flow.py torch/_higher_order_ops/associative_scan.py torch/_higher_order_ops/cond.py torch/_library/utils.py torchgen/gen_schema_utils.py torchgen/model.py,https://github.com/pytorch/pytorch/pull/133521,ydwu4,zou3519,,,
696107efcb8,skip,not user facing,[hop] ban creating hop by directly instantiating HigherOrderOperator. (#133645),test/dynamo/test_higher_order_ops.py test/functorch/test_eager_transforms.py torch/_dynamo/_trace_wrapped_higher_order_op.py torch/_export/wrappers.py torch/_higher_order_ops/executorch_call_delegate.py torch/_higher_order_ops/map.py torch/_higher_order_ops/run_const_graph.py torch/_higher_order_ops/strict_mode.py torch/_higher_order_ops/torchbind.py torch/_ops.py torch/_prims/rng_prims.py,https://github.com/pytorch/pytorch/pull/133645,ydwu4,zou3519,,,
750d68ff70a,releng,not user facing,"Use amazon linux2 for Docker builds, fix build-docker-conda condition (#134116)",.github/actionlint.yaml .github/workflows/build-conda-images.yml .github/workflows/build-manywheel-images.yml,https://github.com/pytorch/pytorch/pull/134116,atalman,ZainRizvi,nWEIdia,,
32e057636c7,releng,not user facing,Enable scribe environment for compile-time benchmarks if requested. (#133891),.ci/pytorch/test.sh .github/workflows/_linux-build.yml .github/workflows/_linux-test.yml .github/workflows/pull.yml .github/workflows/slow.yml .github/workflows/trunk.yml benchmarks/dynamo/pr_time_benchmarks/benchmark_base.py,https://github.com/pytorch/pytorch/pull/133891,ezyang,malfet,,,
05304f59f05,fx,docs,[Doc] Fix typo in `torch/fx/passes/README.md` (#134078),torch/fx/passes/README.md,https://github.com/pytorch/pytorch/pull/134078,spzala,malfet,soulitzer,,
84b3f1900a4,skip,not user facing,C++ network flow implementation in c10 (#132188),c10/test/util/NetworkFlow_test.cpp c10/util/NetworkFlow.cpp c10/util/NetworkFlow.h,https://github.com/pytorch/pytorch/pull/132188,davidberard98,Chillee,,,
18aaceb7be5,releng,not user facing,Update conda-env-iOS.txt (#134068),.github/requirements/conda-env-iOS.txt,https://github.com/pytorch/pytorch/pull/134068,malfet,Skylion007,atalman,wdvr,
5fcfccefc69,export,Untopiced,[export] Migrate `capture_pre_autograd_graph` to `_export_for_training` (#132815),torch/_export/__init__.py,https://github.com/pytorch/pytorch/pull/132815,yushangdi,tugsbayasgalan,,,
1491a617698,skip,Untopiced,"Revert ""[hop] ban creating hop by directly instantiating HigherOrderOperator. (#133645)""",test/dynamo/test_higher_order_ops.py test/functorch/test_eager_transforms.py torch/_dynamo/_trace_wrapped_higher_order_op.py torch/_export/wrappers.py torch/_higher_order_ops/executorch_call_delegate.py torch/_higher_order_ops/map.py torch/_higher_order_ops/run_const_graph.py torch/_higher_order_ops/strict_mode.py torch/_higher_order_ops/torchbind.py torch/_ops.py torch/_prims/rng_prims.py,,,,,,
865facda449,skip,not user facing,[pytorch] Remove thread naming when torch is imported (#134066),torch/csrc/Module.cpp,https://github.com/pytorch/pytorch/pull/134066,valentinandrei,d4l3k,soulitzer,,
6c1e2d2462b,skip,not user facing,[easy] Force inline_inbuilt_nn_modules to remove divergence (#134122),test/inductor/test_compiled_autograd.py,https://github.com/pytorch/pytorch/pull/134122,anijain2305,mlazos,williamwen42,,
a3e1416c05d,skip,not user facing,Fix out_tensor device in diag_test.py (#134020),benchmarks/operator_benchmark/pt/diag_test.py,https://github.com/pytorch/pytorch/pull/134020,pbelevich,soulitzer,,,
345578afb47,sparse_frontend,new features,Add int8 support to bsr_dense_addmm and bsr_dense_mm Triton kernels (#133855),test/test_sparse_csr.py torch/sparse/_triton_ops.py torch/sparse/_triton_ops_meta.py,https://github.com/pytorch/pytorch/pull/133855,pearu,cpuhrsch,,,
b3eef3deafb,releng,not user facing,Triple number of shards for aarch64 cpu inductor tests (#134123),.github/workflows/inductor-perf-test-nightly-aarch64.yml,https://github.com/pytorch/pytorch/pull/134123,malfet,clee2000,,,
5d5a45dc850,releng,not user facing,[CI][dashboard] Collect Export pass rate separately (#134076),.ci/pytorch/test.sh benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/134076,desertfire,angelayi,,,
96dfe95ed03,distributed,docs,Fix DDPLoadBalancingPlanner docstring (#134044),torch/distributed/checkpoint/planner.py,https://github.com/pytorch/pytorch/pull/134044,yingufan,fegin,,,
0870398fa8c,onnx,not user facing,[ONNX] Opt into ruff fmt (#134120),test/onnx/dynamo/test_registry_dispatcher.py test/onnx/internal/test_diagnostics.py test/onnx/internal/test_registraion.py test/onnx/model_defs/dcgan.py test/onnx/pytorch_test_common.py test/onnx/test_models_onnxruntime.py test/onnx/test_onnxscript_no_runtime.py test/onnx/test_onnxscript_runtime.py test/onnx/test_operators.py test/onnx/test_pytorch_onnx_onnxruntime.py test/onnx/test_utility_funs.py test/onnx/test_verification.py test/onnx/torch_export/test_torch_export_with_onnxruntime.py tools/linter/adapters/pyfmt_linter.py torch/onnx/__init__.py torch/onnx/_globals.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/_lazy_import.py torch/onnx/_internal/diagnostics/_diagnostic.py torch/onnx/_internal/diagnostics/infra/sarif/_artifact_content.py torch/onnx/_internal/diagnostics/infra/sarif/_conversion.py torch/onnx/_internal/diagnostics/infra/sarif/_external_properties.py torch/onnx/_internal/diagnostics/infra/sarif/_invocation.py torch/onnx/_internal/diagnostics/infra/sarif/_location.py torch/onnx/_internal/diagnostics/infra/sarif/_physical_location.py torch/onnx/_internal/diagnostics/infra/sarif/_reporting_descriptor.py torch/onnx/_internal/diagnostics/infra/sarif/_reporting_descriptor_reference.py torch/onnx/_internal/diagnostics/infra/sarif/_result.py torch/onnx/_internal/diagnostics/infra/sarif/_result_provenance.py torch/onnx/_internal/diagnostics/infra/sarif/_run.py torch/onnx/_internal/diagnostics/infra/sarif/_tool_component.py torch/onnx/_internal/diagnostics/infra/sarif/_translation_metadata.py torch/onnx/_internal/exporter/_core.py torch/onnx/_internal/exporter/_schemas.py torch/onnx/_internal/fx/_pass.py torch/onnx/_internal/fx/decomposition_skip.py torch/onnx/_internal/fx/dynamo_graph_extractor.py torch/onnx/_internal/fx/fx_onnx_interpreter.py torch/onnx/_internal/fx/fx_symbolic_graph_extractor.py torch/onnx/_internal/fx/onnxfunction_dispatcher.py torch/onnx/_internal/fx/op_validation.py torch/onnx/_internal/fx/passes/_utils.py torch/onnx/_internal/fx/passes/decomp.py torch/onnx/_internal/fx/passes/modularization.py torch/onnx/_internal/fx/passes/type_promotion.py torch/onnx/_internal/fx/torch_export_graph_extractor.py torch/onnx/_internal/fx/type_utils.py torch/onnx/_internal/io_adapter.py torch/onnx/_internal/onnxruntime.py torch/onnx/_internal/registration.py torch/onnx/_type_utils.py torch/onnx/errors.py torch/onnx/symbolic_helper.py torch/onnx/symbolic_opset11.py torch/onnx/symbolic_opset14.py torch/onnx/symbolic_opset16.py torch/onnx/symbolic_opset20.py torch/onnx/symbolic_opset9.py torch/onnx/utils.py torch/onnx/verification.py,https://github.com/pytorch/pytorch/pull/134120,justinchuby,Skylion007,XuehaiPan,,
255cd75a974,Uncategorized,Untopiced,[sparse] Add cuSPARSELt as a backend (#128534),build_variables.bzl docs/source/backends.rst test/test_sparse_semi_structured.py torch/CMakeLists.txt torch/_C/__init__.pyi.in torch/_C/_cusparselt.pyi torch/backends/__init__.py torch/backends/cusparselt/__init__.py torch/csrc/Module.cpp torch/csrc/cuda/Module.cpp torch/csrc/cuda/shared/cusparselt.cpp,https://github.com/pytorch/pytorch/pull/128534,jcaip,cpuhrsch,eqy,syed-ahmed,
d2204d4f0f7,skip,not user facing,Remove skip ci recommendation (#134134),CONTRIBUTING.md,https://github.com/pytorch/pytorch/pull/134134,spzala,soulitzer,,,
8604c0a150b,skip,Untopiced,[inductor] Fix needs_fixed_stride_order silent incorrectness (#133639),test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/133639,zou3519,eellison,,,
a5ef04a3b8c,skip,not user facing,add relevant function (#133946),torch/_C/__init__.pyi.in,https://github.com/pytorch/pytorch/pull/133946,FFFrog,ezyang,,,
5f0bd98767e,skip,not user facing,Increase max total number of dynamo partitions to 15 (#134153),benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/134153,malfet,ZainRizvi,kit1980,seemethere,
2a73ba298c3,skip,not user facing,Upgrade submodule oneDNN to v3.5.3 (#131620),third_party/ideep third_party/mkl-dnn.BUILD,https://github.com/pytorch/pytorch/pull/131620,yanbing-j,jgong5,malfet,,
6bddfb95463,distributed (fsdp2),not user facing,[FSDP2] Add cache for FSDP wrapper class (#134135),torch/distributed/_composable/fsdp/fully_shard.py,https://github.com/pytorch/pytorch/pull/134135,yf225,awgu,,,
df467f8746d,releng,not user facing,[CI] Do not set Intel OMP for aarch64 (#133997),.ci/pytorch/test.sh,https://github.com/pytorch/pytorch/pull/133997,desertfire,angelayi,,,
7b20514f8e3,Uncategorized,Untopiced,[export] Device remapping in export (#133660),test/export/test_passes.py torch/_export/passes/move_to_device_pass.py,https://github.com/pytorch/pytorch/pull/133660,yiming0416,angelayi,,,
7868b65c4d4,dynamo,not user facing,[Dynamo] Support dict.setdefault (#134083),test/dynamo/test_functions.py torch/_dynamo/variables/dicts.py,https://github.com/pytorch/pytorch/pull/134083,yanboliang,williamwen42,,,
e847b6bb9ba,skip,not user facing,[FlexAttention] Enable different qk and v head-dims (#134043),test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/nn/attention/_utils.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/134043,drisspg,Chillee,,,
de06345e9b7,optim,Untopiced,Avoid Host & Device Sync In LR Scheduler (#133663),torch/optim/lr_scheduler.py,https://github.com/pytorch/pytorch/pull/133663,alpha0422,eqy,janeyx99,,
0d7ac1966a1,dynamo,Untopiced,kill sharing of constraints (#134045),test/dynamo/test_export.py torch/_dynamo/guards.py torch/_dynamo/variables/builder.py torch/_export/non_strict_utils.py torch/export/dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/134045,avikchaudhuri,pianpwk,,,
e2ff094008a,inductor,not user facing,[inductor] calibration inductor windows uts (1/N) (#134033),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/134033,xuhancn,jansel,,,
938f37b745d,nn_frontend,Untopiced,"Added batching rule for sdpa_math, sdpa_efficient_attention forward, cudnn, and flash attention (#133964)",aten/src/ATen/functorch/BatchRulesConvolution.cpp aten/src/ATen/functorch/BatchRulesDecompositions.cpp aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp aten/src/ATen/functorch/BatchRulesReduceOps.cpp test/functorch/test_vmap.py torch/testing/_internal/common_methods_invocations.py torchgen/gen_vmap_plumbing.py,https://github.com/pytorch/pytorch/pull/133964,Chillee,Skylion007,,,
24c2dd2002c,inductor,Untopiced,Migrate fuse_chunk_reshape_concat_pass to PT2 (#134026),torch/_inductor/fx_passes/pre_grad.py,https://github.com/pytorch/pytorch/pull/134026,oniononion36,huxintong,,,
cdb9c7d2283,skip,not user facing,Add support for using privateuse1 backend name in `instantiate_device_type_tests()` (#133082),torch/testing/_internal/common_device_type.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/133082,shink,albanD,,,
b7baa062fc0,skip,not user facing,Update torch-xpu-ops pin (ATen XPU implementation) (#133850),third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/133850,fengyuan14,EikanWang,,,
64cfcbd8a31,sparse_frontend,not user facing,Tune _int_bsr_dense_addmm for int8 inputs on A100 (#134035),torch/sparse/_triton_ops_meta.py,https://github.com/pytorch/pytorch/pull/134035,pearu,cpuhrsch,,,
90c821814ed,releng,Untopiced,SparseCsrCUDA: cuDSS backend for linalg.solve (#129856),.ci/docker/common/install_cudss.sh .ci/docker/ubuntu-cuda/Dockerfile CMakeLists.txt aten/src/ATen/cuda/CUDAContextLight.h aten/src/ATen/cuda/Exceptions.cpp aten/src/ATen/cuda/Exceptions.h aten/src/ATen/native/BatchLinearAlgebra.cpp aten/src/ATen/native/cuda/linalg/CudssHandlePool.cpp aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/sparse/cuda/SparseCsrTensorMath.cu build_variables.bzl caffe2/CMakeLists.txt cmake/Modules/FindCUDSS.cmake cmake/Summary.cmake cmake/public/cuda.cmake docs/source/sparse.rst setup.py test/expect/HasDecompTest.test_has_decomposition.expect test/test_sparse_csr.py torch/linalg/__init__.py torch/sparse/__init__.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/129856,zitongzhan,amjames,huydhn,lezcano,
4c8193b8f06,quantization,Untopiced,[14/N] Fix clang-tidy warnings in aten/src/ATen  (#132733),aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,https://github.com/pytorch/pytorch/pull/132733,cyyever,ezyang,,,
b8ea5b01c90,cuda,not user facing,[fp8 rowwise] Allocate workspace as a PyTorch Tensor (#134110),aten/src/ATen/native/cuda/RowwiseScaledMM.cu,https://github.com/pytorch/pytorch/pull/134110,lw,drisspg,,,
15faed60ca3,cuda,not user facing,[fp8 rowwise] Make schedule selection more readable (#134111),aten/src/ATen/native/cuda/RowwiseScaledMM.cu,https://github.com/pytorch/pytorch/pull/134111,lw,drisspg,,,
d64fa110953,cuda,not user facing,[fp8 rowwise] Fix bias calculation being done in low precision (#134112),aten/src/ATen/native/cuda/RowwiseScaledMM.cu,https://github.com/pytorch/pytorch/pull/134112,lw,drisspg,,,
72586ccd143,cuda,not user facing,[fp8 rowwise] Don't build separate kernel for no bias (#134113),aten/src/ATen/native/cuda/RowwiseScaledMM.cu,https://github.com/pytorch/pytorch/pull/134113,lw,drisspg,,,
aea1148d56f,cuda,not user facing,[fp8 rowwise] Clarify dtypes (#134114),aten/src/ATen/native/cuda/RowwiseScaledMM.cu,https://github.com/pytorch/pytorch/pull/134114,lw,drisspg,,,
5fb87544346,inductor,not user facing,[inductor] write cpp code with encoding utf-8 (#134027),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/134027,xuhancn,desertfire,jansel,jgong5,
4c645c04d8f,dynamo,not user facing,Fix type of get_raw_stream (#134187),torch/_dynamo/device_interface.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/134187,int3,jansel,,,
592a1729102,distributed,not user facing,[FSDP2] Resolved strided sharding todo in clipping tests (#134152),test/distributed/_composable/fsdp/test_fully_shard_clip_grad_norm_.py,https://github.com/pytorch/pytorch/pull/134152,awgu,XilunWu,weifengpy,wz337,
cedfac20c7c,skip,Untopiced,"Revert ""[SymmetricMemory] introduce multicast support, multimem_all_reduce_ and multimem_one_shot_all_reduce (#133424)""",BUILD.bazel build_variables.bzl c10/cuda/driver_api.cpp c10/cuda/driver_api.h test/distributed/test_symmetric_memory.py torch/csrc/distributed/c10d/CUDASymmetricMemory-inl.h torch/csrc/distributed/c10d/CUDASymmetricMemory.cu torch/csrc/distributed/c10d/CUDASymmetricMemory.hpp torch/csrc/distributed/c10d/CUDASymmetricMemoryOps.cu torch/csrc/distributed/c10d/SymmetricMemory.cpp torch/csrc/distributed/c10d/SymmetricMemory.hpp torch/csrc/distributed/c10d/init.cpp,,,,,,
108a75b4546,distributed,Untopiced,[PP] Add ZeroBubble schedule (#133467),docs/source/distributed.pipelining.rst test/distributed/pipelining/test_schedule_multiproc.py torch/distributed/pipelining/__init__.py torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/133467,H-Huang,wconstab,,,
c9c84ae3ee6,skip,not user facing,[BE][Ez]: Update CUDNN_frontend submodule to 1.6.1 (#134007),third_party/cudnn_frontend,https://github.com/pytorch/pytorch/pull/134007,Skylion007,eqy,,,
83b5d449a36,cuda,Untopiced,Add full float16/bfloat16 support to MaxUnPool (#133774),aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp aten/src/ATen/native/cuda/MaxUnpooling.cu torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/133774,Skylion007,eqy,ezyang,,
88c973005d2,skip,Untopiced,"Revert ""[FlexAttention] Enable different qk and v head-dims (#134043)""",test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/nn/attention/_utils.py torch/nn/attention/flex_attention.py,,,,,,
1a7e8e5780b,skip,Untopiced,"Revert ""Update FlexAttention with masking semantic (#133373)""",test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py,,,,,,
b459ca78ebc,skip,not user facing,[NJT]Add unit tests that cover the internal use cases using new NJT API (#133513),test/test_nestedtensor.py,https://github.com/pytorch/pytorch/pull/133513,YuqingJ,davidberard98,soulitzer,,
afc2615d33f,skip,not user facing,Add proper casting to fuse_linear_bn_weights (#134105),torch/nn/utils/fusion.py,https://github.com/pytorch/pytorch/pull/134105,vladim0105,mikaylagawarecki,,,
1b10a5c6528,inductor,Untopiced,Allow SymInts and SymFloats as other in div_softmax_pattern (#133989),test/inductor/test_torchinductor.py torch/_inductor/codegen/halide.py torch/_inductor/fx_passes/joint_graph.py,https://github.com/pytorch/pytorch/pull/133989,isuruf,ezyang,,,
3c5485fb7fb,dynamo,Untopiced,[Retry] Log chromium events to scuba (#134118),torch/_dynamo/convert_frame.py torch/_dynamo/utils.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_inductor/codecache.py torch/_utils_internal.py,https://github.com/pytorch/pytorch/pull/134118,jamesjwu,oulgen,,,
828ab84e197,skip,not user facing,Improve error msg on _lazy_init() error (#134159),torch/mtia/__init__.py,https://github.com/pytorch/pytorch/pull/134159,nautsimon,hanzlfs,,,
b0171c39209,skip,Untopiced,"Revert ""[ONNX] Opt into ruff fmt (#134120)""",test/onnx/dynamo/test_registry_dispatcher.py test/onnx/internal/test_diagnostics.py test/onnx/internal/test_registraion.py test/onnx/model_defs/dcgan.py test/onnx/pytorch_test_common.py test/onnx/test_models_onnxruntime.py test/onnx/test_onnxscript_no_runtime.py test/onnx/test_onnxscript_runtime.py test/onnx/test_operators.py test/onnx/test_pytorch_onnx_onnxruntime.py test/onnx/test_utility_funs.py test/onnx/test_verification.py test/onnx/torch_export/test_torch_export_with_onnxruntime.py tools/linter/adapters/pyfmt_linter.py torch/onnx/__init__.py torch/onnx/_globals.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/_lazy_import.py torch/onnx/_internal/diagnostics/_diagnostic.py torch/onnx/_internal/diagnostics/infra/sarif/_artifact_content.py torch/onnx/_internal/diagnostics/infra/sarif/_conversion.py torch/onnx/_internal/diagnostics/infra/sarif/_external_properties.py torch/onnx/_internal/diagnostics/infra/sarif/_invocation.py torch/onnx/_internal/diagnostics/infra/sarif/_location.py torch/onnx/_internal/diagnostics/infra/sarif/_physical_location.py torch/onnx/_internal/diagnostics/infra/sarif/_reporting_descriptor.py torch/onnx/_internal/diagnostics/infra/sarif/_reporting_descriptor_reference.py torch/onnx/_internal/diagnostics/infra/sarif/_result.py torch/onnx/_internal/diagnostics/infra/sarif/_result_provenance.py torch/onnx/_internal/diagnostics/infra/sarif/_run.py torch/onnx/_internal/diagnostics/infra/sarif/_tool_component.py torch/onnx/_internal/diagnostics/infra/sarif/_translation_metadata.py torch/onnx/_internal/exporter/_core.py torch/onnx/_internal/exporter/_schemas.py torch/onnx/_internal/fx/_pass.py torch/onnx/_internal/fx/decomposition_skip.py torch/onnx/_internal/fx/dynamo_graph_extractor.py torch/onnx/_internal/fx/fx_onnx_interpreter.py torch/onnx/_internal/fx/fx_symbolic_graph_extractor.py torch/onnx/_internal/fx/onnxfunction_dispatcher.py torch/onnx/_internal/fx/op_validation.py torch/onnx/_internal/fx/passes/_utils.py torch/onnx/_internal/fx/passes/decomp.py torch/onnx/_internal/fx/passes/modularization.py torch/onnx/_internal/fx/passes/type_promotion.py torch/onnx/_internal/fx/torch_export_graph_extractor.py torch/onnx/_internal/fx/type_utils.py torch/onnx/_internal/io_adapter.py torch/onnx/_internal/onnxruntime.py torch/onnx/_internal/registration.py torch/onnx/_type_utils.py torch/onnx/errors.py torch/onnx/symbolic_helper.py torch/onnx/symbolic_opset11.py torch/onnx/symbolic_opset14.py torch/onnx/symbolic_opset16.py torch/onnx/symbolic_opset20.py torch/onnx/symbolic_opset9.py torch/onnx/utils.py torch/onnx/verification.py,,,,,,
240467adfe9,fx,Untopiced,[fx] Implement deepcopy for Proxy (#133706),test/test_fx.py torch/fx/proxy.py,https://github.com/pytorch/pytorch/pull/133706,yushangdi,angelayi,,,
c95ddd4bf28,dynamo,not user facing,[dynamo] ensure polyfill function has the same signature as the original function in `substitute_in_graph` (#133813),test/dynamo/test_decorators.py torch/_dynamo/decorators.py torch/compiler/__init__.py,https://github.com/pytorch/pytorch/pull/133813,XuehaiPan,jansel,,,
b6abac68ecb,dynamo,not user facing,[BE][dynamo] reorganize polyfill module hierarchy (#133977),torch/_dynamo/__init__.py torch/_dynamo/polyfill.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/loader.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/133977,XuehaiPan,jansel,,,
44fa9f991c5,nested tensor_frontend,Untopiced,[NJT] add aten.to.dtype support (#134164),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/134164,YuqingJ,davidberard98,,,
d95aedf5fd4,dynamo,Untopiced,[BE] typing for decorators - fx/_compatibility (part 1) (#134202),torch/_dynamo/compiled_autograd.py torch/_dynamo/eval_frame.py torch/_dynamo/utils.py torch/_export/converter.py torch/_export/pass_base.py torch/_export/passes/replace_with_hop_pass_util.py torch/_functorch/partitioners.py torch/_higher_order_ops/flex_attention.py torch/_inductor/constant_folding.py torch/_inductor/debug.py torch/_inductor/fx_passes/binary_folding.py torch/_inductor/fx_passes/efficient_conv_bn_eval.py torch/_inductor/fx_passes/freezing_patterns.py torch/_inductor/fx_passes/joint_graph.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/fx_passes/quantization.py torch/_inductor/fx_passes/reinplace.py torch/_inductor/fx_passes/split_cat.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/pattern_matcher.py torch/_inductor/subgraph_lowering.py torch/_inductor/utils.py torch/_subclasses/fake_tensor.py torch/ao/ns/_numeric_suite_fx.py torch/ao/ns/fx/graph_passes.py torch/ao/ns/fx/n_shadows_utils.py torch/ao/pruning/_experimental/pruner/base_structured_sparsifier.py torch/ao/pruning/_experimental/pruner/match_utils.py torch/ao/quantization/fx/_lower_to_native_backend.py torch/ao/quantization/fx/utils.py torch/ao/quantization/pt2e/duplicate_dq_pass.py torch/ao/quantization/pt2e/utils.py torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py torch/distributed/_tensor/experimental/tp_transform.py torch/distributed/_tensor/ops/_view_ops.py torch/distributed/fsdp/_trace_utils.py torch/distributed/pipelining/_IR.py torch/distributed/pipelining/microbatch.py torch/export/_trace.py torch/fx/experimental/meta_tracer.py torch/fx/experimental/proxy_tensor.py torch/fx/node.py torch/fx/passes/graph_drawer.py torch/fx/passes/net_min_base.py torch/fx/passes/split_module.py torch/fx/passes/splitter_base.py torch/fx/passes/utils/matcher_with_name_node_map_utils.py torch/fx/subgraph_rewriter.py torch/nested/_internal/ops.py torch/onnx/_internal/fx/op_validation.py torch/utils/_python_dispatch.py,https://github.com/pytorch/pytorch/pull/134202,aorenste,Skylion007,,,
8f7d66f0c34,releng,not user facing,Enable dynamic rollout for Linux binary workflows (#131472),.github/templates/linux_binary_build_workflow.yml.j2 .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-conda-nightly.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-main.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-main.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-nightly.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/131472,zxiiro,ZainRizvi,,,
fee677eeb6c,dynamo,not user facing,[fbode-testing][dynamo][reland][inline-inbuilt-nn-modules] Mark attri… (#134136),test/dynamo/test_modules.py test/dynamo/test_repros.py torch/_dynamo/guards.py torch/_dynamo/source.py torch/_dynamo/utils.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/user_defined.py torch/_guards.py,https://github.com/pytorch/pytorch/pull/134136,anijain2305,yanboliang,,,
978c5a80a0a,quantization,Untopiced,[export][training ir migration] fix batch norm pattern match in quantization (#134157),torch/ao/quantization/pt2e/utils.py,https://github.com/pytorch/pytorch/pull/134157,yushangdi,tugsbayasgalan,,,
0eb9c870fda,linalg_frontend,Untopiced,[reland][ROCm] TunableOp for gemm_and_bias (#128919),aten/src/ATen/cuda/tunable/GemmCommon.h aten/src/ATen/cuda/tunable/GemmHipblaslt.h aten/src/ATen/cuda/tunable/TunableGemm.h aten/src/ATen/native/cuda/Blas.cpp test/test_linalg.py,https://github.com/pytorch/pytorch/pull/128919,jeffdaily,malfet,,,
56337731889,releng,not user facing,Convert various jobs to be Linux Foundation fleet compatible (#134128),.github/workflows/linux-aarch64.yml .github/workflows/llm_td_retrieval.yml .github/workflows/target_determination.yml .github/workflows/xpu.yml,https://github.com/pytorch/pytorch/pull/134128,ZainRizvi,jeanschmidt,zxiiro,,
ff61f553873,dynamo,not user facing,[Dynamo][autograd.Function] Supports ctx.set_materialize_grads (#133978),test/dynamo/test_autograd_function.py,https://github.com/pytorch/pytorch/pull/133978,yanboliang,zou3519,,,
1b6bbaa016f,distributed,Untopiced,Remove PMI dependencies in PyTorch (#133960),torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,https://github.com/pytorch/pytorch/pull/133960,dsjohns2,shuqiangzhang,,,
f0ba309d78c,releng,not user facing,[CI][dashboard] Add jemalloc back for aarch64 (#134189),.ci/pytorch/test.sh,https://github.com/pytorch/pytorch/pull/134189,desertfire,huydhn,malfet,,
b0cf287b46f,quantization,Untopiced,[export][training ir migration] Fix getitem not exist (#134259),test/quantization/pt2e/test_quantize_pt2e_qat.py torch/_export/__init__.py torch/_utils_internal.py torch/ao/quantization/pt2e/qat_utils.py,https://github.com/pytorch/pytorch/pull/134259,yushangdi,tugsbayasgalan,,,
25499de8147,distributed,Untopiced,Remove ncclIdToCommMap_. (#133961),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,https://github.com/pytorch/pytorch/pull/133961,dsjohns2,shuqiangzhang,,,
b319fa3fd92,onnx,not user facing,[ONNX] Opt into ruff fmt (#134120),test/onnx/dynamo/test_registry_dispatcher.py test/onnx/internal/test_diagnostics.py test/onnx/internal/test_registraion.py test/onnx/model_defs/dcgan.py test/onnx/pytorch_test_common.py test/onnx/test_models_onnxruntime.py test/onnx/test_onnxscript_no_runtime.py test/onnx/test_onnxscript_runtime.py test/onnx/test_operators.py test/onnx/test_pytorch_onnx_onnxruntime.py test/onnx/test_utility_funs.py test/onnx/test_verification.py test/onnx/torch_export/test_torch_export_with_onnxruntime.py tools/linter/adapters/pyfmt_linter.py torch/onnx/__init__.py torch/onnx/_globals.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/_lazy_import.py torch/onnx/_internal/diagnostics/_diagnostic.py torch/onnx/_internal/diagnostics/infra/sarif/_artifact_content.py torch/onnx/_internal/diagnostics/infra/sarif/_conversion.py torch/onnx/_internal/diagnostics/infra/sarif/_external_properties.py torch/onnx/_internal/diagnostics/infra/sarif/_invocation.py torch/onnx/_internal/diagnostics/infra/sarif/_location.py torch/onnx/_internal/diagnostics/infra/sarif/_physical_location.py torch/onnx/_internal/diagnostics/infra/sarif/_reporting_descriptor.py torch/onnx/_internal/diagnostics/infra/sarif/_reporting_descriptor_reference.py torch/onnx/_internal/diagnostics/infra/sarif/_result.py torch/onnx/_internal/diagnostics/infra/sarif/_result_provenance.py torch/onnx/_internal/diagnostics/infra/sarif/_run.py torch/onnx/_internal/diagnostics/infra/sarif/_tool_component.py torch/onnx/_internal/diagnostics/infra/sarif/_translation_metadata.py torch/onnx/_internal/exporter/_core.py torch/onnx/_internal/exporter/_schemas.py torch/onnx/_internal/fx/_pass.py torch/onnx/_internal/fx/decomposition_skip.py torch/onnx/_internal/fx/dynamo_graph_extractor.py torch/onnx/_internal/fx/fx_onnx_interpreter.py torch/onnx/_internal/fx/fx_symbolic_graph_extractor.py torch/onnx/_internal/fx/onnxfunction_dispatcher.py torch/onnx/_internal/fx/op_validation.py torch/onnx/_internal/fx/passes/_utils.py torch/onnx/_internal/fx/passes/decomp.py torch/onnx/_internal/fx/passes/modularization.py torch/onnx/_internal/fx/passes/type_promotion.py torch/onnx/_internal/fx/torch_export_graph_extractor.py torch/onnx/_internal/fx/type_utils.py torch/onnx/_internal/io_adapter.py torch/onnx/_internal/onnxruntime.py torch/onnx/_internal/registration.py torch/onnx/_type_utils.py torch/onnx/errors.py torch/onnx/symbolic_helper.py torch/onnx/symbolic_opset11.py torch/onnx/symbolic_opset14.py torch/onnx/symbolic_opset16.py torch/onnx/symbolic_opset20.py torch/onnx/symbolic_opset9.py torch/onnx/utils.py torch/onnx/verification.py,https://github.com/pytorch/pytorch/pull/134120,justinchuby,Skylion007,XuehaiPan,,
e7929809f31,distributed,Untopiced,[c10d][ez] Add comments to CudaEventCache class (#134172),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/134172,fduwjj,d4l3k,kwen2501,,
629bd6f7184,skip,not user facing,Update FlexAttention with masking semantic (#133373),test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py,https://github.com/pytorch/pytorch/pull/133373,drisspg,yanboliang,,,
30d7e7a1cd1,releng,not user facing,[XPU] Fix patch for old llvm package error for triton xpu (#134204),.github/scripts/build_triton_wheel.py,https://github.com/pytorch/pytorch/pull/134204,Stonepia,EikanWang,atalman,chuanqi129,
b2eb0e8c6a8,releng,not user facing,"docker: Use miniforge, install from pip (#134274)",.github/workflows/docker-release.yml Dockerfile docker.Makefile,https://github.com/pytorch/pytorch/pull/134274,seemethere,atalman,malfet,,
edbadc904bd,distributed,Untopiced,Do not broadcast uniqueId during a split (#133962),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/133962,dsjohns2,shuqiangzhang,,,
b3821f1da1b,dynamo,not user facing,[dynamo][guards][logs] Generate code_parts for debugging (#134181),torch/_dynamo/guards.py,https://github.com/pytorch/pytorch/pull/134181,anijain2305,jansel,youkaichao,,
7c93c4f8cf3,releng,not user facing,[CI][dashboard] Change aarch64 perf run (#134265),.github/workflows/inductor-perf-test-nightly-aarch64.yml,https://github.com/pytorch/pytorch/pull/134265,desertfire,malfet,,,
bf5addb6133,skip,not user facing,[FlexAttention] Enable different qk and v head-dims (#134043),test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/nn/attention/_utils.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/134043,drisspg,Chillee,,,
bcfc560aea0,profiler,not user facing,[Profiler/CPU] Add Test for Dynamic Activity Toggling [4/n] (#134149),test/profiler/test_profiler.py torch/csrc/autograd/profiler_kineto.cpp torch/profiler/profiler.py,https://github.com/pytorch/pytorch/pull/134149,sraikund16,aaronenyeshi,,,
8301add833e,distributed,not user facing,[4/N] Further refactor FR script to make it more modulized (#134196),test/distributed/flight_recorder/test_fr_analysis.py tools/flight_recorder/components/builder.py tools/flight_recorder/components/processor.py tools/flight_recorder/components/types.py tools/flight_recorder/components/utils.py tools/flight_recorder/fr_trace.py,https://github.com/pytorch/pytorch/pull/134196,fduwjj,c-p-i-o,,,
d2c60749ac8,inductor,not user facing,[Inductor][FlexAttention] Respect user's input kernel_options (#134065),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/134065,yanboliang,Chillee,,,
058302494cd,inductor,not user facing,[AOTI][Tooling] Add a test case where `config.debug_intermediate_value_printer=True` to check codegen (#133326),test/inductor/test_aot_inductor.py torch/_inductor/codegen/debug_utils.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/133326,YUNQIUGUO,ColinPeppler,,,
b454c510607,dynamo,Untopiced,remove dynamic_dim (#134211),docs/source/export.rst test/dynamo/test_export.py test/export/test_export.py test/export/test_unflatten.py test/test_dynamic_shapes.py torch/_dynamo/eval_frame.py torch/_export/non_strict_utils.py torch/export/__init__.py torch/export/dynamic_shapes.py torch/fx/experimental/symbolic_shapes.py torch/overrides.py,https://github.com/pytorch/pytorch/pull/134211,avikchaudhuri,angelayi,,,
a699bd11551,skip,not user facing,[dynamo] Cache _dynamo.disable results (#134272),test/dynamo/test_decorators.py torch/_dynamo/__init__.py torch/_dynamo/convert_frame.py torch/_dynamo/decorators.py,https://github.com/pytorch/pytorch/pull/134272,anijain2305,jansel,yf225,,
b23779ef0af,dynamo,bug fixes,[dynamo][fix] always use POSIX-style path in `trace_rule.py` (#133987),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/133987,XuehaiPan,jansel,,,
311af3b9889,quantization,Untopiced,Add new ops wrapped_linear_prepack and wrapped_quantized_linear_prepacked (#134232),aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp aten/src/ATen/native/quantized/library.cpp test/functorch/test_vmap_registrations.py test/quantization/core/test_quantized_op.py torch/overrides.py,https://github.com/pytorch/pytorch/pull/134232,hl475,houseroad,,,
49b9f2d8b0a,inductor,not user facing,[inductor] fix signbit build fail on Windows. (#134229),torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/134229,xuhancn,jansel,,,
80846caa8c1,inductor,not user facing,[inductor] fix dynamic size array(vla) build error on msvc v4 (#134221),torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/134221,xuhancn,jansel,zhuhaozhe,,
673b9bd561f,skip,not user facing,[WIP] [Inductor UT] Reuse inductor UT for intel GPU `test/inductor/test_multi_kernel.py` (#133943),test/inductor/test_multi_kernel.py,https://github.com/pytorch/pytorch/pull/133943,hoshibara,EikanWang,jansel,,
25b2e465730,dynamo,not user facing,[dynamo] add max iterator limit while inlining generators (#134233),test/dynamo/test_misc.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/iter.py,https://github.com/pytorch/pytorch/pull/134233,XuehaiPan,jansel,,,
2f198605ac4,cuda,not user facing,[fp8 rowwise] Simplify epilogue visitor tree via common blocks (#134223),aten/src/ATen/native/cuda/RowwiseScaledMM.cu,https://github.com/pytorch/pytorch/pull/134223,lw,drisspg,,,
9f8d3f511f2,cuda,not user facing,[fp8 rowwise] Some clean-up (#134224),aten/src/ATen/native/cuda/RowwiseScaledMM.cu,https://github.com/pytorch/pytorch/pull/134224,lw,drisspg,,,
0afb4872aa1,cuda,not user facing,[fp8 rowwise] Support non-contiguous inputs and clarify checks (#134225),aten/src/ATen/native/cuda/RowwiseScaledMM.cu,https://github.com/pytorch/pytorch/pull/134225,lw,drisspg,,,
9d81767d439,cuda,not user facing,[fp8 rowwise] Rework dispatch logic (#134226),aten/src/ATen/native/cuda/RowwiseScaledMM.cu,https://github.com/pytorch/pytorch/pull/134226,lw,drisspg,,,
55cdcef0f71,cuda,not user facing,[fp8 rowwise] Work around CUDA Invalid Memory Access bug (#134227),aten/src/ATen/native/cuda/RowwiseScaledMM.cu,https://github.com/pytorch/pytorch/pull/134227,lw,drisspg,eqy,,
ca3f48dd5ba,skip,not user facing,[XPU] Set `make triton` install pre-built whl by default (#130313),scripts/install_triton_wheel.sh,https://github.com/pytorch/pytorch/pull/130313,Stonepia,EikanWang,atalman,chuanqi129,
cc3a76edbac,skip,not user facing,[dynamo] simplify polyfill registration for `builtins.all` and `builtins.any` (#133769),torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/builtins.py torch/_dynamo/polyfills/loader.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/133769,XuehaiPan,jansel,,,
6c0b15e3828,skip,not user facing,[dynamo] simplify implementation for `functools.reduce` (#133778),torch/_dynamo/polyfills/functools.py torch/_dynamo/polyfills/loader.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/133778,XuehaiPan,anijain2305,jansel,,
8d90392fb02,skip,not user facing,[dynamo] simplify implementation for `builtins.sum` (#133779),torch/_dynamo/polyfills/builtins.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/133779,XuehaiPan,anijain2305,jansel,,
0e49b2f18e7,dynamo,not user facing,[dynamo][itertools] support `itertools.tee` (#133771),test/dynamo/test_misc.py torch/_dynamo/polyfills/itertools.py torch/_dynamo/polyfills/loader.py,https://github.com/pytorch/pytorch/pull/133771,XuehaiPan,jansel,,,
75c22dd8bf8,skip,Untopiced,"Revert ""[dynamo][fix] always use POSIX-style path in `trace_rule.py` (#133987)""",torch/_dynamo/trace_rules.py,,,,,,
09127b096c7,skip,Untopiced,"Revert ""[inductor] Fix needs_fixed_stride_order silent incorrectness (#133639)""",test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/lowering.py,,,,,,
7fd3b698868,skip,Untopiced,"Revert ""[dynamo][super] Improve handling of getattr on super (#134039)""",test/dynamo/test_repros.py torch/_dynamo/variables/misc.py,,,,,,
8ae4f822432,skip,not user facing,[aotd] Support HOP effects in backward (#132638),test/dynamo/test_logging.py test/higher_order_ops/test_with_effects.py torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/_aot_autograd/traced_function_transforms.py torch/_functorch/_aot_autograd/utils.py torch/_functorch/partitioners.py torch/_higher_order_ops/effects.py torch/_logging/_registrations.py torch/_subclasses/functional_tensor.py,https://github.com/pytorch/pytorch/pull/132638,IvanKobzarev,bdhirsh,,,
2eef749b313,inductor,not user facing,[Inductor][FlexAttention] Fix IS_DIVISIBLE bug and add unit tests (#134055),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/134055,yanboliang,Chillee,,,
6eae5695466,dynamo,bug fixes,[dynamo][fix] always use POSIX-style path in `trace_rule.py` (#133987),test/functorch/test_eager_transforms.py torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/133987,XuehaiPan,jansel,,,
2553278bae5,releng,not user facing,.github/merge_rules.yaml: added multiprocessing to Distributed (#134262),.github/merge_rules.yaml,https://github.com/pytorch/pytorch/pull/134262,d4l3k,PaliC,wconstab,,
afd081c9d4d,skip,Untopiced,[inductor] Fix needs_fixed_stride_order silent incorrectness (#133639),test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/133639,zou3519,eellison,,,
3546628a2a1,skip,Untopiced,Allow mp.start_processes to create processes in parallel (#133707),test/distributed/elastic/multiprocessing/api_test.py test/test_multiprocessing_spawn.py torch/multiprocessing/__init__.py torch/multiprocessing/spawn.py,https://github.com/pytorch/pytorch/pull/133707,lijia19,d4l3k,ezyang,,
a23d86c1782,skip,not user facing,[hop] ban creating hop by directly instantiating HigherOrderOperator. (#133645),test/dynamo/test_higher_order_ops.py test/functorch/test_eager_transforms.py torch/_dynamo/_trace_wrapped_higher_order_op.py torch/_export/wrappers.py torch/_higher_order_ops/executorch_call_delegate.py torch/_higher_order_ops/map.py torch/_higher_order_ops/run_const_graph.py torch/_higher_order_ops/strict_mode.py torch/_higher_order_ops/torchbind.py torch/_ops.py torch/_prims/rng_prims.py,https://github.com/pytorch/pytorch/pull/133645,ydwu4,zou3519,,,
f8fbfe5846e,dynamo,Untopiced,"Always emit end events even on failure, use thread local storage for stack (#134279)",torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/134279,jamesjwu,aorenste,,,
74a9001adab,Uncategorized,Untopiced,[aoti] Add additional custom op input type support (#132454),test/inductor/custom_ops.cpp test/inductor/test_aot_inductor.py test/inductor/test_aot_inductor_utils.py torch/csrc/inductor/aoti_torch/oss_proxy_executor.cpp torch/csrc/inductor/aoti_torch/oss_proxy_executor.h,https://github.com/pytorch/pytorch/pull/132454,angelayi,desertfire,,,
157de30f53d,releng,Untopiced,[sparse] Update cuSPARSELt to v0.6.2 (#134022),.ci/docker/common/install_cuda.sh .ci/docker/common/install_cusparselt.sh test/test_sparse_semi_structured.py torch/sparse/semi_structured.py,https://github.com/pytorch/pytorch/pull/134022,jcaip,jerryzh168,malfet,,
58e2cf364bc,distributed,Untopiced,Make DTensor sharding propagation for `scaled_dot_product_efficient_attention` and `scaled_dot_product_flash_attention` more conservatively cached (#134146),torch/distributed/_tensor/ops/_matrix_ops.py,https://github.com/pytorch/pytorch/pull/134146,speediedan,tianyu-l,,,
2ca7f0fc5c8,fx,Untopiced,"[Minimizer] for sequential mode, respect find_all setting (#134339)",torch/fx/passes/net_min_base.py,https://github.com/pytorch/pytorch/pull/134339,qcyuan,jfix71,,,
78d69bfe113,distributed,not user facing,"[SymmetricMemory] introduce multicast support, multimem_all_reduce_ and multimem_one_shot_all_reduce (#133424)",BUILD.bazel build_variables.bzl c10/cuda/driver_api.cpp c10/cuda/driver_api.h test/distributed/test_symmetric_memory.py torch/csrc/distributed/c10d/CUDASymmetricMemory-inl.h torch/csrc/distributed/c10d/CUDASymmetricMemory.cu torch/csrc/distributed/c10d/CUDASymmetricMemory.hpp torch/csrc/distributed/c10d/CUDASymmetricMemoryOps.cu torch/csrc/distributed/c10d/SymmetricMemory.cpp torch/csrc/distributed/c10d/SymmetricMemory.hpp torch/csrc/distributed/c10d/init.cpp,https://github.com/pytorch/pytorch/pull/133424,yifuwang,weifengpy,yf225,,
69813dbbfd5,Uncategorized,Untopiced,[export] Schematize nn_module_stack serialization (#134049),test/export/test_serialize.py torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/134049,yiming0416,angelayi,,,
adf0f50cc7f,inductor,performance,[Compile] Add NEON implementation for bf16->fp32 cast (#134297),aten/src/ATen/cpu/vec/vec256/vec256_convert.h,https://github.com/pytorch/pytorch/pull/134297,malfet,kimishpatel,,,
09a82f3d246,releng,not user facing,[EZ][BE] Delete references to non-existing `AWS_SCCACHE` secrets (#134370),.github/templates/macos_binary_build_workflow.yml.j2 .github/workflows/generated-macos-arm64-binary-conda-nightly.yml .github/workflows/generated-macos-arm64-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/134370,malfet,atalman,seemethere,,
eaa2c0e0099,skip,not user facing,Improves error message when passing wrong tensor type to torch.nn.functional.one_hot (#134209),aten/src/ATen/native/Onehot.cpp,https://github.com/pytorch/pytorch/pull/134209,jnt0rrente,mikaylagawarecki,,,
f5a2a22dc49,Uncategorized,Untopiced,[export] Fix unflattener to respect nn.Parameter requires_grad (#134353),test/export/test_unflatten.py torch/export/unflatten.py,https://github.com/pytorch/pytorch/pull/134353,angelayi,pianpwk,,,
8ff3a5be1b2,dynamo,Untopiced,[export] basic auto dynamic shapes (#133620),docs/source/export.rst test/dynamo/test_export.py test/export/test_export.py torch/_dynamo/eval_frame.py torch/_export/non_strict_utils.py torch/_export/utils.py torch/export/_trace.py torch/export/dynamic_shapes.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/133620,pianpwk,avikchaudhuri,,,
1f19ccb5b38,inductor,Untopiced,[Inductor/Triton] Customize triton codegen to optionally preserve input dtype on tl.load (#132406),test/inductor/test_torchinductor.py torch/_inductor/codegen/triton.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/132406,arui-meta,blaine-rister,jfix71,,
c638a40a939,skip,not user facing,[Caffe2] Remove unused AVX512 code (#133160),BUILD.bazel build.bzl caffe2/core/macros.h.in caffe2/perfkernels/CMakeLists.txt caffe2/perfkernels/common_avx512.cc cmake/MiscCheck.cmake,https://github.com/pytorch/pytorch/pull/133160,cyyever,albanD,,,
2cfc2da5273,Uncategorized,Untopiced,[export] Make move_to_device_pass function public (#134263),docs/source/export.rst test/export/test_passes.py torch/_export/passes/move_to_device_pass.py torch/export/passes/__init__.py,https://github.com/pytorch/pytorch/pull/134263,yiming0416,angelayi,,,
286f2dba9ff,distributed,Untopiced,[2/N refactor NCCLPG error logs][c10d] Make msg in monitoring thread in NCCLPG more accurate and simpler (#134036),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,https://github.com/pytorch/pytorch/pull/134036,fduwjj,wconstab,,,
aa9f4cc733f,skip,not user facing,[Inductor][CPP] Support vectorization of remainder (#129849),aten/src/ATen/native/cpu/BinaryOpsKernel.cpp test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_prefix.h,https://github.com/pytorch/pytorch/pull/129849,leslie-fang-intel,jgong5,lezcano,,
54ff3205199,Uncategorized,Untopiced,[export] refactor ExportGraphSignature construction (#134059),torch/export/_trace.py torch/export/graph_signature.py,https://github.com/pytorch/pytorch/pull/134059,pianpwk,angelayi,ydwu4,,
e52e93e8fdb,releng,not user facing,Update scale-config files with linux.24xlarge.ephemeral (#134380),.github/lf-canary-scale-config.yml .github/lf-scale-config.yml,https://github.com/pytorch/pytorch/pull/134380,atalman,ZainRizvi,kit1980,,
4c28a0eb0ba,skip,Untopiced,c10d/logging: add C10D_LOCK_GUARD (#134131),caffe2/CMakeLists.txt test/cpp/c10d/CMakeLists.txt test/cpp/c10d/LoggingTest.cpp test/cpp/c10d/ProcessGroupNCCLErrorsTest.cpp torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupGloo.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/Work.cpp torch/csrc/distributed/c10d/Work.hpp torch/csrc/distributed/c10d/logging.cpp torch/csrc/distributed/c10d/logging.h,https://github.com/pytorch/pytorch/pull/134131,d4l3k,c-p-i-o,fduwjj,,
9dc47f5e62f,inductor,not user facing,[FlexAttention]Fix how we realize input buffers (#134351),torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py,https://github.com/pytorch/pytorch/pull/134351,drisspg,Chillee,,,
4ff1a4dd0f3,dynamo,not user facing,[export] support set_grad_enabled hop in dynamo to enable re-tracing (#134281),test/export/test_export.py torch/_dynamo/variables/higher_order_ops.py,https://github.com/pytorch/pytorch/pull/134281,ydwu4,tugsbayasgalan,,,
1aa0e35a047,inductor,not user facing,[inductor] Remove dead code in multi_kernel.py (#134194),torch/_inductor/codegen/multi_kernel.py,https://github.com/pytorch/pytorch/pull/134194,jansel,eellison,,,
2c8fc3f4ce0,inductor,not user facing,[inductor] Move imports to top of file in generated code (#134195),test/inductor/test_cuda_repro.py torch/_inductor/codegen/wrapper.py,https://github.com/pytorch/pytorch/pull/134195,jansel,eellison,,,
a1d0b4d568a,inductor,not user facing,Add option to skip functional passes in the pattern matcher's replacement graph (#134364),test/inductor/test_inplacing_pass.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/pattern_matcher.py,https://github.com/pytorch/pytorch/pull/134364,zou3519,eellison,,,
e2a87fb1e9d,onnx,new features,[ONNX] Update exporter logic (#134304),test/onnx/exporter/test_api.py torch/onnx/__init__.py torch/onnx/_internal/exporter/__init__.py torch/onnx/_internal/exporter/_building.py torch/onnx/_internal/exporter/_core.py torch/onnx/_internal/exporter/_decomp.py torch/onnx/_internal/exporter/_fx_passes.py torch/onnx/_internal/exporter/_onnx_program.py torch/onnx/_internal/exporter/_reporting.py torch/onnx/_internal/exporter/_testing.py torch/onnx/_internal/exporter/_verification.py,https://github.com/pytorch/pytorch/pull/134304,justinchuby,titaiwangms,,,
519342962da,distributed,Untopiced,Pass process group info into NcclWork (#134269),test/cpp/c10d/ProcessGroupNCCLErrorsTest.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,https://github.com/pytorch/pytorch/pull/134269,shengfukevin,wconstab,,,
195abdb85c6,inductor,Untopiced,ppc64le: VSX Support for Inductor (#132746),aten/src/ATen/cpu/vec/vec256/vsx/vec256_double_vsx.h aten/src/ATen/cpu/vec/vec256/vsx/vec256_float_vsx.h torch/_inductor/codegen/cpp_prefix.h torch/_inductor/cpu_vec_isa.py,https://github.com/pytorch/pytorch/pull/132746,Akashcodes732,jansel,,,
5d39b14b689,distributed,not user facing,[DeviceMesh] Add DeviceMesh slicing support for flatten mesh dim (#133839),test/distributed/test_device_mesh.py torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/133839,wz337,fegin,,,
962e1f6ca7f,distributed,not user facing,"[DTensor] Add aten.any.default,dim,out to linear_reduction_strategy (#134206)",test/distributed/_tensor/test_math_ops.py torch/distributed/_tensor/ops/_math_ops.py,https://github.com/pytorch/pytorch/pull/134206,wz337,chuanhaozhuge,tianyu-l,,
5ae9c017948,distributed,not user facing,[DTensor] Add naive replicate strategy for aten._linalg_eigh.default (#134284),test/distributed/_tensor/test_dtensor_ops.py test/distributed/_tensor/test_math_ops.py torch/distributed/_tensor/ops/_math_ops.py,https://github.com/pytorch/pytorch/pull/134284,wz337,awgu,,,
5ad759ca33b,inductor,not user facing,[inductor] calibration inductor windows uts (2/N) (#134358),test/inductor/test_compile_worker.py,https://github.com/pytorch/pytorch/pull/134358,xuhancn,jansel,,,
8d3c6494aeb,inductor,not user facing,[Inductor][FlexAttention] Rename IS_LAST_BLOCK to CHECK_BLOCK_BOUNDARY (#134378),torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/134378,yanboliang,drisspg,,,
f260cc2edf7,distributed,not user facing,Enable DTensor sharding propagation of `native_layer_norm_backward` to more fully accommodate optional args (#133502),test/distributed/_tensor/test_math_ops.py test/distributed/tensor/parallel/test_tp_examples.py torch/distributed/_tensor/_sharding_prop.py torch/distributed/_tensor/ops/_math_ops.py,https://github.com/pytorch/pytorch/pull/133502,speediedan,XilunWu,,,
0694918aeb1,fx,Untopiced,[export] Temporarily bypass torch_fn in partitioner (#134292),test/fx/test_source_matcher_utils.py torch/fx/passes/utils/source_matcher_utils.py,https://github.com/pytorch/pytorch/pull/134292,yushangdi,angelayi,,,
1034f456ef6,inductor,not user facing,[inductor] fix munge_exc not support windows path (#134348),torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/134348,xuhancn,jansel,,,
eb15b1a016c,skip,not user facing,[dtensor][MTPG] make sharding prop lru cache not shared among threads (#134294),test/distributed/_tensor/test_dtensor_ops.py torch/distributed/_tensor/_sharding_prop.py,https://github.com/pytorch/pytorch/pull/134294,XilunWu,awgu,wz337,,
ccafc93be53,inductor,not user facing,[AOTI][CPU] Make int8 qlinear work (#134368),test/inductor/test_aot_inductor.py torch/_inductor/decomposition.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/134368,hl475,houseroad,,,
0d9208a3980,autograd_frontend,Untopiced,[compiled autograd] match eager behavior for inplace detached activations (#134186),test/inductor/test_compiled_autograd.py tools/autograd/gen_autograd_functions.py torch/csrc/autograd/custom_function.h torch/csrc/autograd/python_function.cpp torch/csrc/dynamo/compiled_autograd.h,https://github.com/pytorch/pytorch/pull/134186,xmfan,jansel,,,
d7a25e1d8ca,dynamo,not user facing,[compiled autograd] add config patching for certain eager tests (#134200),test/inductor/test_compiled_autograd.py,https://github.com/pytorch/pytorch/pull/134200,xmfan,jansel,,,
6cc57c64b2d,dynamo,Untopiced,[compiled autograd] match eager behavior for post acc grad hooks (#134205),test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/_dynamo/external_utils.py,https://github.com/pytorch/pytorch/pull/134205,xmfan,jansel,,,
0b228a2af83,dynamo,Untopiced,[compiled autograd] match eager behavior for ctx.saved_variables (#134286),test/inductor/test_compiled_autograd.py torch/_dynamo/external_utils.py,https://github.com/pytorch/pytorch/pull/134286,xmfan,jansel,,,
1431663693d,dynamo,not user facing,[compiled autograd] finish classifying tests (#134290),test/inductor/test_compiled_autograd.py,https://github.com/pytorch/pytorch/pull/134290,xmfan,yf225,,,
ad8bdfae1ee,skip,not user facing,add compiled_autograd to programmatic set_logs API (#134162),test/inductor/test_compiled_autograd.py torch/_logging/_internal.py,https://github.com/pytorch/pytorch/pull/134162,xmfan,jansel,yf225,,
929de1d0d45,skip,not user facing,Re-enable skipped compiled autograd eager tests (#134163),test/inductor/test_compiled_autograd.py,https://github.com/pytorch/pytorch/pull/134163,xmfan,soulitzer,,,
ff7d94c67ef,dynamo,Untopiced,[compiled autograd] fix saved tensor hook firing count (#134361),test/inductor/test_compiled_autograd.py torch/csrc/dynamo/compiled_autograd.h,https://github.com/pytorch/pytorch/pull/134361,xmfan,jansel,,,
ff77c67d16d,releng,not user facing,Use ephemeral runners for linux nightly builds (#134367),.github/actionlint.yaml .github/templates/linux_binary_build_workflow.yml.j2 .github/workflows/_binary-build-linux.yml .github/workflows/generated-linux-binary-conda-nightly.yml,https://github.com/pytorch/pytorch/pull/134367,atalman,kit1980,malfet,seemethere,
a1061009c9f,inductor,not user facing,[PT2] use statically_known_true in slice_noop (#134270),torch/_inductor/fx_passes/post_grad.py,https://github.com/pytorch/pytorch/pull/134270,TroyGarden,ezyang,,,
d433a603af4,skip,not user facing,[BE] use torch.amp.autocast instead of torch.cuda.amp.autocast (#134291),test/test_ops.py,https://github.com/pytorch/pytorch/pull/134291,davidberard98,YuqingJ,,,
cdb9df5efe7,skip,not user facing,[dynamo][guards] De-dupe DUPLICATE_INPUT guard (#134354),torch/_dynamo/guards.py,https://github.com/pytorch/pytorch/pull/134354,anijain2305,jansel,,,
a63efee5cd4,skip,not user facing,[inductor]Let output or input_as_strided match exact strides  (#130956),test/inductor/test_torchinductor.py torch/_inductor/graph.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/130956,FindHao,blaine-rister,desertfire,eellison,
6245d5b87b0,releng,not user facing,[CI] Update XPU ci test python version to 3.9 (#134214),.ci/docker/build.sh .ci/docker/common/install_conda.sh .github/workflows/xpu.yml,https://github.com/pytorch/pytorch/pull/134214,chuanqi129,EikanWang,malfet,,
f71c3d265ab,skip,not user facing,[ROCm] remove triton-rocm commit pin and merge pins with triton.txt (#133438),.ci/docker/centos-rocm/Dockerfile .ci/docker/ci_commit_pins/triton-rocm.txt .ci/docker/ci_commit_pins/triton.txt .ci/docker/common/install_triton.sh .ci/docker/ubuntu-rocm/Dockerfile .circleci/scripts/binary_populate_env.sh .github/scripts/build_triton_wheel.py .github/workflows/build-triton-wheel.yml CODEOWNERS,https://github.com/pytorch/pytorch/pull/133438,jataylo,jithunnair-amd,malfet,,
50d5aa8c10b,quantization,Untopiced,Enable optimized dynamic quantization on aarch64 (#126687),aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp,https://github.com/pytorch/pytorch/pull/126687,jondea,jgong5,malfet,snadampal,
9cd53b32126,skip,not user facing,Add Arm copyright line to LICENSE (#133982),LICENSE,https://github.com/pytorch/pytorch/pull/133982,jondea,malfet,,,
90fb83749e8,inductor,not user facing,[inductor] fix test torch package working with trace on windows (#134397),test/dynamo/test_misc.py,https://github.com/pytorch/pytorch/pull/134397,xuhancn,ezyang,,,
050aa67e415,sparse_frontend,Untopiced,[traced-graph][sparse] fix restrictive assert for sparse add (#134037),aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp test/export/test_sparse.py,https://github.com/pytorch/pytorch/pull/134037,aartbik,ezyang,,,
1a0d00f1f46,sparse_frontend,Untopiced,[traced-graph][sparse] enable to_dense() for compressed (#133371),test/export/test_sparse.py torch/_subclasses/functional_tensor.py,https://github.com/pytorch/pytorch/pull/133371,aartbik,ezyang,,,
94f92fbd883,composability,not user facing,Use integer divison in arange length calculation when start/end/step are integral (#134296),test/dynamo/test_repros.py torch/_refs/__init__.py,https://github.com/pytorch/pytorch/pull/134296,bobrenjc93,aorenste,,,
af4c87953e4,inductor,not user facing,[inductor] calibration inductor windows uts (5/N) (#134402),test/dynamo/test_repros.py,https://github.com/pytorch/pytorch/pull/134402,xuhancn,ezyang,,,
d33d68e3263,profiler,not user facing,[Profiler] Add test to make sure FunctionEvents are processed lazily (#134359),test/profiler/test_profiler.py,https://github.com/pytorch/pytorch/pull/134359,sraikund16,briancoutinho,,,
74ef74be369,inductor,not user facing,[inductor] calibration inductor windows uts (3/N) (#134400),test/dynamo/test_trace_rules.py,https://github.com/pytorch/pytorch/pull/134400,xuhancn,ezyang,,,
907c32faaca,inductor,not user facing,[inductor] calibration inductor windows uts (4/N) (#134401),test/dynamo/test_unspec.py,https://github.com/pytorch/pytorch/pull/134401,xuhancn,ezyang,,,
8db8ac700d8,fx,Untopiced,line by line logging (#134298),torch/_export/non_strict_utils.py torch/_logging/_registrations.py torch/export/_trace.py torch/fx/experimental/_config.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/134298,avikchaudhuri,angelayi,,,
326db8af4cd,fx,not user facing,Replace sympy Min/Max with reimplementations (#133319),torch/fx/experimental/sym_node.py torch/utils/_sympy/functions.py torch/utils/_sympy/interp.py torch/utils/_sympy/reference.py,https://github.com/pytorch/pytorch/pull/133319,ezyang,Skylion007,,,
268092db838,distributed,not user facing,[DeviceMesh] Allow _flatten() to take in an optional mesh_dim_name (#134048),test/distributed/test_device_mesh.py torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/134048,wz337,fegin,,,
e5563f7ad7f,skip,Untopiced,"Revert ""[dtensor][MTPG] make sharding prop lru cache not shared among threads (#134294)""",test/distributed/_tensor/test_dtensor_ops.py torch/distributed/_tensor/_sharding_prop.py,,,,,,
4648848696d,skip,Untopiced,"Revert ""[ROCm] remove triton-rocm commit pin and merge pins with triton.txt (#133438)""",.ci/docker/centos-rocm/Dockerfile .ci/docker/ci_commit_pins/triton-rocm.txt .ci/docker/ci_commit_pins/triton.txt .ci/docker/common/install_triton.sh .ci/docker/ubuntu-rocm/Dockerfile .circleci/scripts/binary_populate_env.sh .github/scripts/build_triton_wheel.py .github/workflows/build-triton-wheel.yml CODEOWNERS,,,,,,
08d111250aa,distributed,Untopiced,[ez][c10d] change ERROR to WARNING (#134349),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/134349,c-p-i-o,fduwjj,wconstab,,
e93ca12c881,cudnn,bug fixes,[CUDNN][SDPA] Fix unsupported trivial stride-1 transpose case (#134031),aten/src/ATen/native/cudnn/MHA.cpp test/test_transformers.py,https://github.com/pytorch/pytorch/pull/134031,eqy,Skylion007,drisspg,,
816061843a2,distributed,Untopiced,[Distributed/Profiler] Fix input/output dimension overflow (#134360),torch/csrc/distributed/c10d/ParamCommsUtils.cpp torch/csrc/distributed/c10d/ParamCommsUtils.hpp,https://github.com/pytorch/pytorch/pull/134360,sraikund16,fengxizhou,jeanschmidt,,
7940f2428f6,package,Untopiced,[torch/package_importer] add compatibility name mapping (#134376),torch/package/package_importer.py,https://github.com/pytorch/pytorch/pull/134376,igorsugak,SherlockNoMad,,,
b5dd60fa75e,cpp_frontend,not user facing,Fix namespace issues with qnnpack (#134336),aten/src/ATen/native/quantized/cpu/qnnpack/test/convolution-operator-tester.h aten/src/ATen/native/quantized/cpu/qnnpack/test/convolution.cc aten/src/ATen/native/quantized/cpu/qnnpack/test/deconvolution-operator-tester.h aten/src/ATen/native/quantized/cpu/qnnpack/test/test_utils.h,https://github.com/pytorch/pytorch/pull/134336,r-barnes,Skylion007,,,
97fd087cdbb,inductor,not user facing,[inductor] calibration inductor windows uts (6/N) (#134419),test/dynamo/test_aot_autograd_cache.py,https://github.com/pytorch/pytorch/pull/134419,xuhancn,jansel,,,
dc1959e6a78,inductor,not user facing,[inductor] calibration inductor windows uts (7/N) (#134420),test/dynamo/test_misc.py,https://github.com/pytorch/pytorch/pull/134420,xuhancn,jansel,,,
7af38eb98bd,autograd_frontend,docs,Fix unexpected inference_mode interaction with torch.autograd.functional.jacobian (#130307),docs/source/notes/autograd.rst,https://github.com/pytorch/pytorch/pull/130307,tianyeeT,soulitzer,,,
2ec149cd3e6,inductor,not user facing,[inductor] fix test_functional_call_sequential_params_and_buffers expectation on Windows (#134394),test/dynamo/test_higher_order_ops.py torch/_dynamo/testing.py,https://github.com/pytorch/pytorch/pull/134394,xuhancn,jansel,,,
7b6b10417d8,skip,not user facing,Remove ansi escape chars in assertExpectedInline and add options to skip comments and to skip empty lines (#134248),torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/134248,laithsakka,aorenste,,,
ed86ac2f25e,fx,Untopiced,[BE] typing for decorators - fx/_compatibility (#134054),torch/_export/__init__.py torch/fx/_compatibility.py torch/fx/_lazy_graph_module.py torch/fx/_symbolic_trace.py torch/fx/experimental/proxy_tensor.py torch/fx/graph.py torch/fx/graph_module.py torch/fx/interpreter.py torch/fx/node.py torch/fx/operator_schemas.py torch/fx/passes/graph_manipulation.py torch/fx/passes/infra/pass_manager.py torch/fx/passes/operator_support.py torch/fx/passes/param_fetch.py torch/fx/passes/runtime_assert.py torch/fx/passes/split_module.py torch/fx/passes/split_utils.py torch/fx/passes/splitter_base.py torch/fx/passes/tools_common.py torch/fx/passes/utils/common.py torch/fx/passes/utils/fuser_utils.py torch/fx/subgraph_rewriter.py torch/fx/traceback.py torch/nested/_internal/ops.py torch/onnx/_internal/exporter/_core.py torch/onnx/_internal/onnxruntime.py,https://github.com/pytorch/pytorch/pull/134054,aorenste,oulgen,,,
38f97ec8e32,composability,not user facing,[pt2] Add meta for poisson (#134103),torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/134103,australopitek,ezyang,,,
c5f6b720411,skip,not user facing,[dynamo] simplify implementation for `os.fspath` (#133801),torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/loader.py torch/_dynamo/polyfills/os.py torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/133801,XuehaiPan,anijain2305,,,
92c47718538,skip,Untopiced,fix stuck floordiv (#134150),benchmarks/dynamo/pr_time_benchmarks/benchmarks/sum_floordiv_benchmark.py torch/utils/_sympy/functions.py,https://github.com/pytorch/pytorch/pull/134150,avikchaudhuri,ezyang,,,
bf5c7bf06de,distributed,not user facing,"[FR] Fix the bug in FR script (e.g., checking all ranks dump check) (#134383)",test/distributed/flight_recorder/test_fr_analysis.py tools/flight_recorder/components/builder.py tools/flight_recorder/components/types.py tools/flight_recorder/components/utils.py,https://github.com/pytorch/pytorch/pull/134383,fduwjj,c-p-i-o,,,
1ff226d88c7,inductor,not user facing,[inductor] support vec for atomic add (#131314),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_prefix.h,https://github.com/pytorch/pytorch/pull/131314,zhuhaozhe,jgong5,leslie-fang-intel,,
96738c9d756,releng,Untopiced,[CD] fix xpu nightly wheel test env (#134395),.circleci/scripts/binary_linux_test.sh,,,,,,
be96ccf77c4,releng,not user facing,"Revert ""[CD] fix xpu nightly wheel test env (#134395)"" (#134461)",.circleci/scripts/binary_linux_test.sh,https://github.com/pytorch/pytorch/pull/134461,atalman,jeanschmidt,,,
27d97b9649f,python_frontend,Untopiced,Remove unnecessary test skip (#134250),torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/134250,benjaminglass1,amjames,janeyx99,,
bb67ff2ba78,releng,not user facing,Migrate Windows bin jobs to runner determinator (#134231),.github/templates/windows_binary_build_workflow.yml.j2 .github/workflows/generated-windows-binary-conda-nightly.yml .github/workflows/generated-windows-binary-libtorch-debug-main.yml .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-main.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/134231,zxiiro,ZainRizvi,,,
e1fc4362fb0,skip,Untopiced,"Revert ""[dynamo] simplify implementation for `os.fspath` (#133801)""",torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/loader.py torch/_dynamo/polyfills/os.py torch/_dynamo/variables/misc.py,,,,,,
3d7f3f6a555,skip,Untopiced,"Revert ""[dynamo][itertools] support `itertools.tee` (#133771)""",test/dynamo/test_misc.py torch/_dynamo/polyfills/itertools.py torch/_dynamo/polyfills/loader.py,,,,,,
472c7cf962f,skip,Untopiced,"Revert ""[dynamo] simplify implementation for `builtins.sum` (#133779)""",torch/_dynamo/polyfills/builtins.py torch/_dynamo/variables/builtin.py,,,,,,
50e90d7203b,skip,Untopiced,"Revert ""[dynamo] simplify implementation for `functools.reduce` (#133778)""",torch/_dynamo/polyfills/functools.py torch/_dynamo/polyfills/loader.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builtin.py,,,,,,
1c4780e69a4,skip,Untopiced,"Revert ""c10d/logging: add C10D_LOCK_GUARD (#134131)""",caffe2/CMakeLists.txt test/cpp/c10d/CMakeLists.txt test/cpp/c10d/LoggingTest.cpp test/cpp/c10d/ProcessGroupNCCLErrorsTest.cpp torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupGloo.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/Work.cpp torch/csrc/distributed/c10d/Work.hpp torch/csrc/distributed/c10d/logging.cpp torch/csrc/distributed/c10d/logging.h,,,,,,
17e8a51ff2c,skip,Untopiced,"Revert ""[inductor]Let output or input_as_strided match exact strides  (#130956)""",test/inductor/test_torchinductor.py torch/_inductor/graph.py torch/_inductor/ir.py,,,,,,
c507f402f1e,releng,not user facing,Add linux arm64 ephemeral runners (#134469),.github/lf-canary-scale-config.yml .github/lf-scale-config.yml,https://github.com/pytorch/pytorch/pull/134469,atalman,clee2000,jeanschmidt,,
b417e32da2c,releng,not user facing,[CD] fix xpu nightly wheel test env (#134395) (#134464),.circleci/scripts/binary_linux_test.sh,https://github.com/pytorch/pytorch/pull/134464,atalman,jeanschmidt,,,
a6fac0e9692,releng,not user facing,Use ephemeral runners for windows nightly builds (#134463),.github/actionlint.yaml .github/templates/windows_binary_build_workflow.yml.j2 .github/workflows/generated-windows-binary-conda-nightly.yml .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/134463,atalman,jeanschmidt,,,
e94bdc78763,skip,Untopiced,"Revert ""[dynamo][guards] De-dupe DUPLICATE_INPUT guard (#134354)""",torch/_dynamo/guards.py,,,,,,
42955e04f1c,skip,Untopiced,"Revert ""[dynamo] Cache _dynamo.disable results (#134272)""",test/dynamo/test_decorators.py torch/_dynamo/__init__.py torch/_dynamo/convert_frame.py torch/_dynamo/decorators.py,,,,,,
fc61aae70f2,skip,not user facing,Remove color in CI (#133517),pytest.ini,https://github.com/pytorch/pytorch/pull/133517,clee2000,ZainRizvi,,,
d0ac5d55ba9,distributed,not user facing,Memory optimization for DSD for TorchTune LoRA (#134025),torch/distributed/_state_dict_utils.py torch/distributed/checkpoint/state_dict.py,https://github.com/pytorch/pytorch/pull/134025,mori360,fegin,,,
adcce538b78,skip,Untopiced,"Revert ""Allow mp.start_processes to create processes in parallel (#133707)""",test/distributed/elastic/multiprocessing/api_test.py test/test_multiprocessing_spawn.py torch/multiprocessing/__init__.py torch/multiprocessing/spawn.py,,,,,,
7ff576072f3,inductor,not user facing,[inductor] calibration inductor windows uts (8/N) (#134424),test/inductor/test_benchmark_fusion.py,https://github.com/pytorch/pytorch/pull/134424,xuhancn,ezyang,,,
019b80855f1,inductor,not user facing,[inductor] calibration inductor windows uts (10/N) (#134426),test/inductor/test_efficient_conv_bn_eval.py,https://github.com/pytorch/pytorch/pull/134426,xuhancn,jansel,,,
73604eed0ce,jit,Untopiced,[20/N] Fix clang-tidy warnings in jit  (#133399),torch/csrc/jit/codegen/fuser/arg_spec.h torch/csrc/jit/codegen/fuser/codegen.cpp torch/csrc/jit/codegen/fuser/codegen.h torch/csrc/jit/codegen/fuser/compiler.cpp torch/csrc/jit/codegen/fuser/compiler.h torch/csrc/jit/codegen/fuser/cuda/fused_kernel.cpp torch/csrc/jit/codegen/fuser/cuda/fused_kernel.h torch/csrc/jit/codegen/fuser/cuda/resource_strings.h torch/csrc/jit/codegen/fuser/executor.cpp torch/csrc/jit/codegen/fuser/executor.h torch/csrc/jit/codegen/fuser/fallback.cpp torch/csrc/jit/codegen/fuser/fallback.h torch/csrc/jit/codegen/fuser/fused_kernel.h torch/csrc/jit/codegen/fuser/interface.cpp torch/csrc/jit/codegen/fuser/interface.h torch/csrc/jit/codegen/fuser/kernel_cache.cpp torch/csrc/jit/codegen/fuser/kernel_cache.h torch/csrc/jit/codegen/fuser/kernel_spec.h torch/csrc/jit/codegen/fuser/partition_desc.h torch/csrc/jit/codegen/fuser/tensor_desc.h torch/csrc/jit/codegen/fuser/tensor_info.h torch/csrc/jit/cuda/cuda.h torch/csrc/jit/frontend/script_type_parser.cpp torch/csrc/jit/frontend/tree_views.h torch/csrc/jit/ir/alias_analysis.cpp torch/csrc/jit/ir/ir.h torch/csrc/jit/jit_log.cpp torch/csrc/jit/mobile/import_data.cpp torch/csrc/jit/mobile/promoted_prim_ops.cpp torch/csrc/jit/passes/onnx/cast_all_constant_to_floating.cpp torch/csrc/jit/passes/onnx/peephole.cpp torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp torch/csrc/jit/passes/quantization/insert_observers.cpp torch/csrc/jit/runtime/argument_spec.h torch/csrc/jit/runtime/autodiff.cpp torch/csrc/jit/runtime/interpreter/code_impl.h torch/csrc/jit/runtime/register_prim_ops.cpp torch/csrc/jit/runtime/register_special_ops.cpp torch/csrc/jit/runtime/static/fusion.cpp torch/csrc/jit/runtime/static/impl.cpp torch/csrc/jit/runtime/symbolic_script.cpp torch/csrc/jit/tensorexpr/operators/conv2d.cpp,https://github.com/pytorch/pytorch/pull/133399,cyyever,Skylion007,,,
0f5b052dbae,inductor,not user facing,[inductor] calibration inductor windows uts (11/N) (#134427),test/inductor/test_inductor_freezing.py,https://github.com/pytorch/pytorch/pull/134427,xuhancn,jansel,,,
78128cbdd8d,releng,not user facing,[CD] Use ephemeral arm64 runners for nightly and docker builds (#134473),.github/actionlint.yaml .github/templates/linux_binary_build_workflow.yml.j2 .github/workflows/build-manywheel-images.yml .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/134473,atalman,malfet,,,
0a3c064c124,inductor,not user facing,[inductor] fix _maybe_subprocess_run not support Windows path (#134365),torch/_dynamo/test_minifier_common.py,https://github.com/pytorch/pytorch/pull/134365,xuhancn,jansel,jgong5,,
1dd4b9221bf,inductor,bug fixes,[inductor] enable clang for Windows inductor (#134444),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/134444,xuhancn,jansel,jgong5,malfet,
1ccc8f0200c,skip,not user facing,[dynamo][super] Improve handling of getattr on super (#134039),test/dynamo/test_repros.py torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/134039,anijain2305,jansel,yanboliang,,
1d231ff8ba1,dynamo,not user facing,[HOO] add hints_wrapper to support passing context hints (#132860),test/dynamo/test_higher_order_ops.py test/export/test_export.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/__init__.py torch/_higher_order_ops/hints_wrap.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/132860,wuxun-zhang,ydwu4,zou3519,,
3322ee236d2,inductor,not user facing,[aoti] remove c_shim_version v1 logic (#134283),torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/134283,henrylhtsang,desertfire,,,
3541e450af8,inductor,not user facing,Support larger page sizes with `use_mmap_weights` (#131000),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/131000,eqy,malfet,,,
a0e062c6f1a,skip,not user facing,Add mean.dtype_out (#133506),aten/src/ATen/native/ReduceOps.cpp aten/src/ATen/native/native_functions.yaml test/expect/HasDecompTest.test_aten_core_operators.expect torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/133506,larryliu0820,bdhirsh,,,
2588b5e51ae,python_frontend,improvements,Move module_tracker to logging for confused hierarchy (#134467),test/test_module_tracker.py torch/autograd/graph.py torch/utils/module_tracker.py,https://github.com/pytorch/pytorch/pull/134467,albanD,malfet,,,
af82dc816aa,inductor,not user facing,Fix lint failures (#134488),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/134488,malfet,Skylion007,albanD,atalman,
68624cf0894,skip,not user facing,[dynamo][guards] De-dupe DUPLICATE_INPUT guard (#134354),torch/_dynamo/guards.py,https://github.com/pytorch/pytorch/pull/134354,anijain2305,jansel,,,
900c5083edc,inductor,not user facing,[inductor] calibration inductor windows uts (9/N) (#134425),test/inductor/test_binary_folding.py,https://github.com/pytorch/pytorch/pull/134425,xuhancn,ezyang,jansel,,
28a4db84f20,skip,not user facing,[ARM] Fix infinite recursion in unwind (#134387),test/run_test.py torch/csrc/Module.cpp torch/csrc/profiler/unwind/unwind.cpp,https://github.com/pytorch/pytorch/pull/134387,Aidyn-A,eqy,malfet,nWEIdia,
dbef2b05b4d,skip,not user facing,[dynamo] Cache _dynamo.disable results (#134272),test/dynamo/test_decorators.py torch/_dynamo/__init__.py torch/_dynamo/convert_frame.py torch/_dynamo/decorators.py,https://github.com/pytorch/pytorch/pull/134272,anijain2305,jansel,yf225,,
a23dae22d50,skip,not user facing,Update AC pass use_reentrant message (#134472),torch/utils/checkpoint.py,https://github.com/pytorch/pytorch/pull/134472,soulitzer,albanD,,,
3c5883e5502,foreach_frontend,Untopiced,Fix test_parity xfail for sigmoid (#134253),aten/src/ATen/native/cuda/ForeachUnaryOp.cu torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/134253,benjaminglass1,amjames,janeyx99,,
ef8c474fcf1,foreach_frontend,Untopiced,Add the fast path for bfloat16 lgamma (#134344),aten/src/ATen/native/cuda/ForeachUnaryOp.cu torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/134344,benjaminglass1,amjames,janeyx99,,
55236d0cb7d,skip,not user facing,TestForeach::test_parity: Remove check for error message text (#134251),test/test_foreach.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/134251,benjaminglass1,amjames,janeyx99,,
ddd71e34797,skip,Untopiced,[export] enumerate unsupported sympy.Functions (#134271),torch/fx/experimental/symbolic_shapes.py torch/fx/passes/runtime_assert.py torch/utils/_sympy/functions.py,https://github.com/pytorch/pytorch/pull/134271,pianpwk,avikchaudhuri,,,
79b7fff1882,linalg_frontend,docs,Fix docstring for torch.signal.windows.nuttall (#134512),torch/signal/windows/windows.py,https://github.com/pytorch/pytorch/pull/134512,malfet,aorenste,atalman,kit1980,
1565940114a,nn_frontend,not user facing,[MPS] Add `test/test_nn.py` to test suite (#134184),test/run_test.py test/test_nn.py torch/testing/_internal/common_device_type.py,https://github.com/pytorch/pytorch/pull/134184,hvaara,albanD,malfet,,
0be6584203f,skip,not user facing,[Inductor UT] Refine test case `test_codegen_upcast_to_fp32_upcast` to pass on XPU. (#134474),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/134474,etaf,jansel,,,
46ecc673ae0,skip,not user facing,[ROCm] Prevent accidental enablement of efficient attention. (#133331),aten/src/ATen/native/transformers/cuda/sdp_utils.cpp test/test_transformers.py,https://github.com/pytorch/pytorch/pull/133331,xinyazhang,jithunnair-amd,malfet,,
43bbd781f27,dynamo,Untopiced,"Back out ""[Traceable FSDPS] Allow tracing through FSDP2 impl in trace_rules.py (#133532)"" (#134478)",torch/_dynamo/trace_rules.py torch/distributed/_composable/fsdp/_fsdp_state.py,https://github.com/pytorch/pytorch/pull/134478,ckluk2,yf225,,,
1dbd3476de0,dynamo,not user facing,[dynamo][itertools] support `itertools.tee` (#133771),test/dynamo/test_misc.py torch/_dynamo/polyfills/itertools.py torch/_dynamo/polyfills/loader.py,https://github.com/pytorch/pytorch/pull/133771,XuehaiPan,jansel,,,
74341e1150f,skip,not user facing,[dynamo] simplify implementation for `os.fspath` (#133801),torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/loader.py torch/_dynamo/polyfills/os.py torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/133801,XuehaiPan,anijain2305,,,
b98d33c1557,inductor,not user facing,[inductor] calibration inductor windows uts (13/N) (#134429),test/inductor/test_torchinductor_dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/134429,xuhancn,jansel,,,
4df10a63409,skip,not user facing,[FlexAttention]  Fix bug when checking whether to return LSE (#134495),test/inductor/test_flex_attention.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/134495,drisspg,BoyuanFeng,Chillee,yanboliang,
141a9c72046,skip,Untopiced,"Revert ""[export] enumerate unsupported sympy.Functions (#134271)""",torch/fx/experimental/symbolic_shapes.py torch/fx/passes/runtime_assert.py torch/utils/_sympy/functions.py,,,,,,
58771315d3a,inductor,Untopiced,Unify lowerings for auto_functionalized and triton_kernel_wrapper_functional (#134466),test/inductor/test_triton_kernels.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/134466,zou3519,eellison,oulgen,,
c7ca89a11ad,dynamo,not user facing,Improve print stack/locals printing in comptime (#133651),test/dynamo/test_comptime.py torch/_dynamo/comptime.py,https://github.com/pytorch/pytorch/pull/133651,ezyang,anijain2305,,,
bdfc1d3987e,composability,not user facing,Remove unnecessary expect_true in split_with_sizes (#133439),torch/_decomp/decompositions.py,https://github.com/pytorch/pytorch/pull/133439,ezyang,albanD,,,
2ee201a7d06,skip,not user facing,[CMake] Remove BUILDING_WITH_TORCH_LIBS (#134434),CMakeLists.txt scripts/build_windows.bat tools/setup_helpers/cmake.py,https://github.com/pytorch/pytorch/pull/134434,cyyever,ezyang,,,
d0147290d83,dynamo,not user facing,[BE][Easy][dynamo] ensure `trace_rules.MOD_INLINELIST` in alphabetical order (#134246),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/134246,XuehaiPan,yanboliang,,,
02b0b524b50,inductor,not user facing,[inductor] Turn on UT: test_randint_int64_mod (#134510),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/134510,xuhancn,ezyang,,,
de57a6e806e,dynamo,Untopiced,"Back out ""[dynamo][exception] Support raise exception from None (#134028)"" (#134513)",test/dynamo/test_exceptions.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/134513,Microve,anijain2305,,,
0b81f700aa7,skip,Untopiced,[PT2/Profiler] Add Context Info to Torch-Compiled Regions (#132765),aten/src/ATen/record_function.h test/dynamo/test_profiler.py torch/_C/_dynamo/eval_frame.pyi torch/_guards.py torch/csrc/dynamo/cpp_shim.cpp torch/csrc/dynamo/cpp_shim.h torch/csrc/dynamo/eval_frame.c,https://github.com/pytorch/pytorch/pull/132765,sraikund16,anijain2305,,,
4a18fcf7afe,inductor,not user facing,[inductor] calibration inductor windows uts (12/N) (#134428),test/inductor/test_torchinductor_codegen_dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/134428,xuhancn,jansel,,,
136b19b062f,skip,Untopiced,Adding entry-point based support for out-of-tree rendezvous plugins (#132633),test/distributed/elastic/rendezvous/out_of_tree_rendezvous_test.py test/distributed/elastic/rendezvous/out_of_tree_test_package/pyproject.toml test/distributed/elastic/rendezvous/out_of_tree_test_package/src/testbackend/__init__.py torch/distributed/elastic/rendezvous/__init__.py torch/distributed/elastic/rendezvous/registry.py,https://github.com/pytorch/pytorch/pull/132633,sathyanarays,fduwjj,,,
35150900065,skip,not user facing,Fix TypeError when itering NoneType in instantiate_device_type_tests() (#134457),torch/testing/_internal/common_device_type.py,https://github.com/pytorch/pytorch/pull/134457,wizzniu,albanD,shink,,
0916d72e993,quantization,Untopiced,Fix the warning for cat operators with same qparams (#133999),aten/src/ATen/native/quantized/cpu/TensorShape.cpp,https://github.com/pytorch/pytorch/pull/133999,pssrawat,tarun292,,,
f4803852779,releng,not user facing,Remove explicit Amz2023 reference from jobs (#134355),.github/templates/linux_binary_build_workflow.yml.j2 .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-conda-nightly.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-main.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-main.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-nightly.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/inductor.yml .github/workflows/lint.yml .github/workflows/nightly.yml .github/workflows/periodic.yml .github/workflows/pull.yml .github/workflows/slow.yml .github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/134355,ZainRizvi,jeanschmidt,,,
4d0a44d34a4,skip,not user facing,[FlexAttention] Create new variables for the subgraphs (#134507),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/134507,drisspg,BoyuanFeng,yanboliang,,
767c47d3c0e,skip,not user facing,[FlexAttention] Remove unused code (#134511),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/134511,drisspg,yanboliang,,,
a34320a6f22,skip,not user facing,[FlexAttention] Fix Sparse block multiple to ceildiv instead for floor div (#134538),test/inductor/test_flex_decoding.py torch/_inductor/kernel/flex_decoding.py,https://github.com/pytorch/pytorch/pull/134538,drisspg,yanboliang,,,
ef8236f12ba,autograd_frontend,not user facing,Provide default value None for the attn_bias parameter(#133981) (#133986),aten/src/ATen/native/native_functions.yaml tools/autograd/derivatives.yaml,https://github.com/pytorch/pytorch/pull/133986,1274085042,ezyang,,,
dde5974b136,composability,not user facing,Implementation for rng ops on hpu and xpu (#133068),test/inductor/test_torchinductor.py torch/_prims/rng_prims.py,https://github.com/pytorch/pytorch/pull/133068,xinyu-intel,EikanWang,anijain2305,jansel,
c7cbcdad76c,inductor,Untopiced,Update partitioner's is_fusible heuristic to respect auto_functionalized (#134490),test/inductor/test_inplacing_pass.py test/inductor/test_perf.py torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/134490,zou3519,Chillee,,,
3e10a1eb5ae,skip,Untopiced,"Revert ""[FlexAttention] Fix Sparse block multiple to ceildiv instead for floor div (#134538)""",test/inductor/test_flex_decoding.py torch/_inductor/kernel/flex_decoding.py,,,,,,
87a3f664e19,skip,Untopiced,"Revert ""[FlexAttention] Remove unused code (#134511)""",test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py,,,,,,
3418708abfc,skip,Untopiced,"Revert ""[FlexAttention] Create new variables for the subgraphs (#134507)""",test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py,,,,,,
0fa0ac80e4c,distributed,not user facing,Do not use `<filesystem>` on Linux (#134494),torch/csrc/distributed/c10d/control_plane/WorkerServer.cpp torch/csrc/inductor/aoti_runner/model_container_runner.cpp,https://github.com/pytorch/pytorch/pull/134494,malfet,atalman,d4l3k,,
2ac710e6677,python_frontend,improvements,Make torch.serialization.set_default_mmap_options usable as a context manager (#134371),test/test_serialization.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/134371,mikaylagawarecki,albanD,,,
2033934ff88,skip,not user facing,Clarify error messages for NEWOBJ and BUILD in weights_only unpickler (#134346),torch/_weights_only_unpickler.py,https://github.com/pytorch/pytorch/pull/134346,mikaylagawarecki,albanD,,,
d028b810fe3,skip,not user facing,Fix flaky GroupNorm ModuleInfo test (#133899),torch/testing/_internal/common_modules.py,https://github.com/pytorch/pytorch/pull/133899,mikaylagawarecki,albanD,,,
761cf91e3cf,distributed,not user facing,[DeviceMesh] Add get_all_submeshes in _MeshEnv (#134275),test/distributed/test_device_mesh.py torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/134275,wz337,XilunWu,,,
c5826022456,Uncategorized,Untopiced,Update partitioner's is_fusible heuristic to respect triton kernels (#134491),test/inductor/test_inplacing_pass.py test/inductor/test_perf.py torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/134491,zou3519,Chillee,,,
94caba48990,skip,not user facing,[1/N] Move NaN check onto NCCL stream (#134300),BUILD.bazel build_variables.bzl torch/csrc/distributed/c10d/NCCLUtils.cu torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/Utils.cu torch/csrc/distributed/c10d/Utils.hpp,https://github.com/pytorch/pytorch/pull/134300,kwen2501,shuqiangzhang,wconstab,,
9dc4bd74661,fx,Untopiced,Create a JustknobConfig for use in config (#134161),test/test_utils_internal.py torch/_utils_internal.py,https://github.com/pytorch/pytorch/pull/134161,c00w,ezyang,,,
be7752ead38,distributed,bug fixes,[2/N] Add flag to control which rank should perform NaN check (#134345),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,https://github.com/pytorch/pytorch/pull/134345,kwen2501,shuqiangzhang,wconstab,,
13114da4ef9,distributed,bug fixes,[3/N] Set correct device to CUDA guards (#134357),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/134357,kwen2501,shuqiangzhang,wconstab,,
13049cd6e5b,inductor,Untopiced,[aotinductor][UserDefinedTritonKernel] fix case with non-constexpr params declared after autotuned params (#134520),test/inductor/test_aot_inductor.py torch/_inductor/ir.py torch/testing/_internal/triton_utils.py,https://github.com/pytorch/pytorch/pull/134520,ColinPeppler,oulgen,,,
68b1a094223,distributed,Untopiced,Integrate device agnostic APIs in FSDP library [1/n] (#134337),torch/distributed/fsdp/_common_utils.py torch/distributed/fsdp/_limiter_utils.py torch/distributed/fsdp/fully_sharded_data_parallel.py torch/distributed/fsdp/sharded_grad_scaler.py,https://github.com/pytorch/pytorch/pull/134337,jvandebon,LucasLLC,awgu,,
8de0d7690c5,nn_frontend,bug fixes,Use newer `toAccumulateType` signature in `Normalization.cpp` (#134540),aten/src/ATen/native/Normalization.cpp test/test_mps.py test/test_nn.py torch/testing/_internal/common_modules.py,https://github.com/pytorch/pytorch/pull/134540,malfet,Skylion007,albanD,,
0159ebb6545,distributed,not user facing,[dtensor] add test for local_map decorator (#127752),test/distributed/_tensor/experimental/test_local_map.py torch/distributed/_tensor/experimental/func_map.py,https://github.com/pytorch/pytorch/pull/127752,XilunWu,wanchaol,,,
5b392d22c6e,skip,Untopiced,"Revert ""fix stuck floordiv (#134150)""",benchmarks/dynamo/pr_time_benchmarks/benchmarks/sum_floordiv_benchmark.py torch/utils/_sympy/functions.py,,,,,,
5fd670e0ef7,skip,not user facing,[ROCM] Properly disable Flash Attention/Efficient Attention with environment variables (#133866),CMakeLists.txt aten/src/ATen/native/transformers/cuda/sdp_utils.cpp cmake/Dependencies.cmake,https://github.com/pytorch/pytorch/pull/133866,xinyazhang,jeffdaily,jithunnair-amd,malfet,
a4b44dd2ef9,inductor,not user facing,[AOTI] Introduce DeferredCudaGridLine for cuda cpp wrapper (#129268),torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cuda.py torch/_inductor/codegen/wrapper.py,https://github.com/pytorch/pytorch/pull/129268,desertfire,angelayi,,,
d5aefadb17d,releng,not user facing,[CD] Fix docker builds by installing setuptools (#134595),.ci/docker/manywheel/build_scripts/build.sh .github/workflows/build-manywheel-images.yml,https://github.com/pytorch/pytorch/pull/134595,atalman,kit1980,malfet,seemethere,
05ac7cd760f,mps,not user facing,[MPS] Remove superfluous label/link (#134090),aten/src/ATen/native/mps/operations/Indexing.mm,https://github.com/pytorch/pytorch/pull/134090,hvaara,albanD,,,
9d0e0e6f1de,inductor,not user facing,[inductor] calibration inductor windows uts (14/N) (#134585),test/dynamo/test_exc.py,https://github.com/pytorch/pytorch/pull/134585,xuhancn,jansel,,,
b094972051b,inductor,not user facing,[inductor] calibration inductor windows uts (17/N) (#134588),test/inductor/test_minifier_isolate.py,https://github.com/pytorch/pytorch/pull/134588,xuhancn,jansel,,,
ed494603c7d,inductor,not user facing,[inductor] calibration inductor windows uts (16/N) (#134587),test/inductor/test_compiled_autograd.py,https://github.com/pytorch/pytorch/pull/134587,xuhancn,jansel,,,
3ef4c27ab34,quantization,Untopiced,"Update pt2e numeric debugger to use node.meta[""custom""] field (#134040)",docs/source/quantization-support.rst test/quantization/pt2e/test_numeric_debugger.py torch/ao/quantization/__init__.py torch/ao/quantization/fx/convert.py torch/ao/quantization/pt2e/_numeric_debugger.py torch/ao/quantization/pt2e/prepare.py,https://github.com/pytorch/pytorch/pull/134040,jerryzh168,tarun292,,,
534f43ddce2,python_frontend,docs,[Doc] Fix rendering of the unicode characters (#134597),torch/nn/modules/adaptive.py torch/nn/modules/conv.py torch/nn/modules/fold.py,https://github.com/pytorch/pytorch/pull/134597,malfet,Skylion007,aorenste,,
adf401f8222,skip,not user facing,Links to contributors' GitHub accounts (#133787),README.md,https://github.com/pytorch/pytorch/pull/133787,ivanduka,albanD,,,
b744ed68162,skip,not user facing,Add a cpu_dispatch_key parameter to the cpu_fallback function (#134321),aten/src/ATen/native/CPUFallback.cpp aten/src/ATen/native/CPUFallback.h,https://github.com/pytorch/pytorch/pull/134321,xpfjmj,albanD,,,
fe6d0e3a047,skip,not user facing,Do not compute unnecessary `tensor!=0` for bool tensors in `count_nonzero` (#134254),aten/src/ATen/native/TensorAdvancedIndexing.cpp,https://github.com/pytorch/pytorch/pull/134254,fleonce,albanD,,,
f754c0ae1b7,skip,not user facing,[easy] rm duplicate definition for inductor in TORCH_LOGS documentation (#134480),torch/_logging/_internal.py,https://github.com/pytorch/pytorch/pull/134480,ColinPeppler,eellison,mlazos,,
44dadf25065,skip,not user facing,[Fix] Check name when registering privateuse1 backend (#134071),c10/core/DeviceType.cpp c10/test/core/Device_test.cpp,https://github.com/pytorch/pytorch/pull/134071,shink,albanD,,,
16b8146c9e0,distributed,not user facing,Exclude test_transformers and unit tests which require recent GPU arch (#132895),test/distributed/tensor/parallel/test_micro_pipeline_tp.py test/run_test.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/132895,BLOrange-AMD,jithunnair-amd,malfet,pruthvistony,
d23c0150f3b,skip,not user facing,[dynamo][dicts] Support hasattr on dicts (#134590),test/dynamo/test_functions.py torch/_dynamo/variables/dicts.py,https://github.com/pytorch/pytorch/pull/134590,anijain2305,Skylion007,,,
b567ca0f515,linalg_frontend,not user facing,Remove unused imported names in python files (#134438),torch/_awaits/__init__.py torch/_deploy.py torch/_lazy/__init__.py torch/_prims/__init__.py torch/_prims_common/__init__.py torch/_refs/linalg/__init__.py torch/linalg/__init__.py,https://github.com/pytorch/pytorch/pull/134438,cyyever,zou3519,,,
39ca96398b3,releng,not user facing,Update label_to_label with oncall: pt2 hierarchy. (#134582),.github/label_to_label.yml,https://github.com/pytorch/pytorch/pull/134582,zou3519,clee2000,,,
289486d0070,composability,not user facing,Move attention kernels back from fake_impls to meta_registrations (#134288),test/test_ops.py torch/_meta_registrations.py torch/_subclasses/fake_impls.py torch/_subclasses/fake_utils.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/134288,davidberard98,drisspg,,,
b58a0c3c4d9,skip,not user facing,[split build] fix distributed problems (#134502),torch/CMakeLists.txt,https://github.com/pytorch/pytorch/pull/134502,PaliC,yifuwang,,,
1ba39ec1d0b,skip,not user facing,Add test case test_arange_length_with_float32_dtype (#134415),test/dynamo/test_misc.py,https://github.com/pytorch/pytorch/pull/134415,bobrenjc93,ezyang,,,
41e512a4cd1,skip,not user facing,[EZ] Restore `test_unicode_comments` (#134589),test/test_jit.py,https://github.com/pytorch/pytorch/pull/134589,malfet,Skylion007,aorenste,,
856a8410f2d,skip,not user facing,[FlexAttention] Create new variables for the subgraphs (#134507),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/134507,drisspg,BoyuanFeng,yanboliang,,
f5c67917d30,skip,not user facing,[FlexAttention] Remove unused code (#134511),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/134511,drisspg,yanboliang,,,
d966d91e377,skip,not user facing,[FlexAttention] Fix Sparse block multiple to ceildiv instead for floor div (#134538),test/inductor/test_flex_decoding.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py,https://github.com/pytorch/pytorch/pull/134538,drisspg,yanboliang,,,
30094bedbc2,skip,Untopiced,"Revert ""[dynamo][dicts] Support hasattr on dicts (#134590)""",test/dynamo/test_functions.py torch/_dynamo/variables/dicts.py,,,,,,
5ead965026e,Uncategorized,Untopiced,[export] don't duck size for DIM.AUTO (#134486),test/export/test_export.py torch/export/dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/134486,pianpwk,avikchaudhuri,,,
ef0f5919c7d,inductor,not user facing,[ROCm][Inductor][CK] Fix codegen after ck signature change (#134483),torch/_inductor/codegen/rocm/ck_universal_gemm_template.py,https://github.com/pytorch/pytorch/pull/134483,tenpercent,ColinPeppler,,,
d6091c8726e,dynamo,not user facing,Add compile time instruction count metric (#133834),benchmarks/dynamo/pr_time_benchmarks/benchmark_base.py benchmarks/dynamo/pr_time_benchmarks/benchmarks/update_hint_benchmark.py torch/_C/_instruction_counter.pyi torch/_dynamo/config.py torch/_dynamo/convert_frame.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/133834,laithsakka,aorenste,,,
3b33f265132,skip,not user facing,Add device daemon (#131814),.flake8 test/cpp_extensions/open_registration_extension/README.md test/cpp_extensions/open_registration_extension/pytorch_openreg/__init__.py test/cpp_extensions/open_registration_extension/pytorch_openreg/_aten_impl.py test/cpp_extensions/open_registration_extension/pytorch_openreg/_device_daemon.py test/cpp_extensions/open_registration_extension/pytorch_openreg/_meta_parser.py test/cpp_extensions/open_registration_extension/pytorch_openreg/csrc/OpenReg.h test/cpp_extensions/open_registration_extension/pytorch_openreg/csrc/OpenRegHooks.cpp test/cpp_extensions/open_registration_extension/pytorch_openreg/csrc/OpenRegMem.cpp test/cpp_extensions/open_registration_extension/test/test_openreg.py,https://github.com/pytorch/pytorch/pull/131814,albanD,ezyang,,,
0c7856973b2,fx,Untopiced,[export] enumerate unsupported sympy.Functions (#134271) (#134598),torch/fx/experimental/symbolic_shapes.py torch/fx/passes/runtime_assert.py torch/utils/_sympy/functions.py,https://github.com/pytorch/pytorch/pull/134598,pianpwk,angelayi,,,
f7467c3b95f,skip,not user facing,using new device-agnostic api instead of old api like torch.cpu or torch.cuda (#134448),test/test_autocast.py,https://github.com/pytorch/pytorch/pull/134448,FFFrog,albanD,guangyey,shink,
2b95da7ef46,inductor,not user facing,allow conv_bn mixed dtype folding in post-grad (#133968),test/inductor/test_inductor_freezing.py torch/_inductor/fx_passes/binary_folding.py,https://github.com/pytorch/pytorch/pull/133968,jiayisunx,jansel,leslie-fang-intel,,
d96254631ea,releng,not user facing,[CD] Fix docker builds by installing setuptools after python build (#134631),.ci/docker/common/install_cpython.sh .ci/docker/manywheel/build_scripts/build.sh,https://github.com/pytorch/pytorch/pull/134631,atalman,kit1980,,,
c45ca8092dd,skip,not user facing,Refactor caching device allocator utils (#130923),c10/core/CachingDeviceAllocator.h c10/cuda/CUDACachingAllocator.cpp c10/cuda/CUDACachingAllocator.h c10/cuda/CUDAMallocAsyncAllocator.cpp torch/csrc/cuda/CUDAPluggableAllocator.cpp torch/csrc/cuda/CUDAPluggableAllocator.h torch/csrc/cuda/Module.cpp,https://github.com/pytorch/pytorch/pull/130923,guangyey,EikanWang,albanD,eqy,
f33bcbe5fd6,skip,Untopiced,c10d/logging: add C10D_LOCK_GUARD (#134131),build_variables.bzl caffe2/CMakeLists.txt test/cpp/c10d/CMakeLists.txt test/cpp/c10d/LoggingTest.cpp test/cpp/c10d/ProcessGroupNCCLErrorsTest.cpp torch/csrc/distributed/c10d/LockGuard.cpp torch/csrc/distributed/c10d/LockGuard.hpp torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupGloo.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/Work.cpp torch/csrc/distributed/c10d/Work.hpp,https://github.com/pytorch/pytorch/pull/134131,d4l3k,c-p-i-o,fduwjj,,
5beb859e749,distributed,not user facing,[BE] no need to print stream in comm abort (#134362),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/134362,kwen2501,fduwjj,wconstab,,
ec3f52dd278,jit,Untopiced,[21/N] Fix clang-tidy warnings in jit  (#134537),torch/csrc/jit/api/function_impl.h torch/csrc/jit/codegen/fuser/cuda/fused_kernel.cpp torch/csrc/jit/codegen/fuser/cuda/fused_kernel.h torch/csrc/jit/codegen/fuser/partition_desc.h torch/csrc/jit/runtime/jit_trace.cpp torch/csrc/jit/runtime/jit_trace.h torch/csrc/jit/runtime/register_ops_utils.cpp torch/csrc/jit/runtime/register_prim_ops.cpp torch/csrc/jit/runtime/script_profile.cpp torch/csrc/jit/runtime/symbolic_shape_registry.cpp torch/csrc/jit/runtime/symbolic_shape_registry.h torch/csrc/jit/serialization/pickler.h,https://github.com/pytorch/pytorch/pull/134537,cyyever,Skylion007,,,
71d0eff6e76,Uncategorized,Untopiced,"Back out ""[pytorch][PR] [export] Schematize nn_module_stack serialization"" (#134628)",test/export/test_serialize.py torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/134628,yiming0416,angelayi,,,
bb4dfe90b87,skip,not user facing,[Reland] [1/N] Fix clang-tidy warnings in inductor (#134544),torch/csrc/inductor/aoti_eager/kernel_holder.cpp torch/csrc/inductor/aoti_eager/kernel_meta_info.cpp torch/csrc/inductor/aoti_eager/kernel_meta_info.h torch/csrc/inductor/aoti_runner/model_container_runner.cpp torch/csrc/inductor/aoti_runner/model_container_runner_cpu.cpp torch/csrc/inductor/aoti_runner/model_container_runner_cuda.cpp torch/csrc/inductor/aoti_torch/mkldnn_tensor.cpp torch/csrc/inductor/aoti_torch/tensor_converter.cpp torch/csrc/inductor/inductor_ops.cpp torch/csrc/inductor/resize_storage_bytes.cpp,https://github.com/pytorch/pytorch/pull/134544,cyyever,ColinPeppler,,,
e3308d835df,releng,not user facing,[audio hash update] update the pinned audio hash (#134632),.github/ci_commit_pins/audio.txt,https://github.com/pytorch/pytorch/pull/134632,pytorchupdatebot,pytorchbot,,,
ca77f0a9863,releng,not user facing,[executorch hash update] update the pinned executorch hash (#133386),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/133386,pytorchupdatebot,pytorchbot,,,
89929d9abc2,inductor,Untopiced,[AOTI][Tooling][4/n] Add `torch.save()` for individual intermediate tensor (#133871),build_variables.bzl test/inductor/test_aot_inductor.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/debug_utils.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/wrapper.py torch/_inductor/config.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/133871,YUNQIUGUO,ColinPeppler,,,
2ba60a16188,python_frontend,Untopiced,fix torch.prod vectorized path for bool (#128009),aten/src/ATen/cpu/vec/vec256/vec256.h aten/src/ATen/cpu/vec/vec512/vec512.h aten/src/ATen/cpu/vec/vec_base.h test/test_reductions.py,https://github.com/pytorch/pytorch/pull/128009,zhuhaozhe,albanD,jgong5,,
bf7db4e4f9f,skip,not user facing,[Inductor UT] Generalize inductor UT for intel GPU (#133309),test/inductor/test_decompose_mem_bound_mm.py test/inductor/test_inplacing_pass.py,https://github.com/pytorch/pytorch/pull/133309,hoshibara,EikanWang,etaf,jansel,
880e3d18a40,skip,not user facing,[dynamo][exceptions] Use exception subclass whenever possible (#134610),test/dynamo/test_exceptions.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/134610,anijain2305,drisspg,jansel,,
c566f2465f4,skip,not user facing,[dynamo][dicts] Support hasattr on dicts (#134590),test/dynamo/test_functions.py torch/_dynamo/variables/dicts.py,https://github.com/pytorch/pytorch/pull/134590,anijain2305,Skylion007,,,
e96dc3665a1,skip,not user facing,[raland][dynamo][exceptions] Support raise from None (#134621),test/dynamo/test_exceptions.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/134621,anijain2305,jansel,,,
e4a5958ab58,skip,not user facing,[dynamo] Graph break on FSDP flat_param inconsistent tensor and grad dtype (#134614),torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/134614,anijain2305,awgu,yf225,,
d01415409bf,distributed,Untopiced,[PGNCCL] Improve logic to infer device for barrier (#134617),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/134617,kwen2501,shuqiangzhang,yifuwang,,
8693322ef08,dynamo,not user facing,[Dynamo][autograd.Function] Support mark_non_differentiable (#134087),test/dynamo/test_autograd_function.py test/test_autograd.py torch/_dynamo/variables/higher_order_ops.py torch/_dynamo/variables/misc.py torch/_functorch/autograd_function.py,https://github.com/pytorch/pytorch/pull/134087,yanboliang,zou3519,,,
26e392132d3,dynamo,not user facing,[2nd try][Traceable FSDP2] Allow tracing through FSDP2 impl in trace_rules.py (#134539),torch/_dynamo/trace_rules.py torch/distributed/_composable/fsdp/_fsdp_state.py,https://github.com/pytorch/pytorch/pull/134539,yf225,ckluk2,yanboliang,,
97c8a0739e6,dynamo,not user facing,[Dynamo] Support inspect.signature.Parameter getattr (#134636),test/dynamo/test_misc.py test/inductor/test_flex_attention.py torch/_dynamo/variables/misc.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/134636,yanboliang,Chillee,anijain2305,,
bb22132c8dd,skip,not user facing,[aotd] Make effects op registry WeakKeyDictionary (#134470),torch/_higher_order_ops/effects.py,https://github.com/pytorch/pytorch/pull/134470,IvanKobzarev,zou3519,,,
3e42f21eeef,composability,Untopiced,Bucketize fix to include number and tensor inputs (#133652),torch/_refs/__init__.py,https://github.com/pytorch/pytorch/pull/133652,Lazarus42,ezyang,,,
c142af7209a,skip,Untopiced,hang dim hint constants off Dim (#134484),docs/source/export.rst test/export/test_export.py torch/_export/non_strict_utils.py torch/export/dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/134484,avikchaudhuri,angelayi,,,
e6bf1710ffa,inductor,Untopiced,[Inductor][Refactor] Rename CPU benchmark test configs (#134639),.ci/pytorch/test.sh .github/workflows/inductor.yml benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_freezing_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_freezing_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_huggingface_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_timm_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_torchbench_amp_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_torchbench_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_amp_freezing_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_amp_freezing_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_freezing_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_freezing_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_huggingface_amp_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_huggingface_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_timm_amp_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_timm_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_amp_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_aot_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_aot_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_aot_inductor_torchbench_amp_freezing_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_aot_inductor_torchbench_freezing_inference.csv,https://github.com/pytorch/pytorch/pull/134639,desertfire,huydhn,,,
208b9223274,python_frontend,improvements,[Intel GPU] Remove special dispatch logic for xpu in adaptive_avg_pooling (#132217),aten/src/ATen/native/AdaptiveAveragePooling.cpp aten/src/ATen/native/AdaptiveAveragePooling3d.cpp,https://github.com/pytorch/pytorch/pull/132217,retonym,EikanWang,albanD,atalman,
85d9946001c,releng,not user facing,[CI] change conda to miniforge for XPU images (#134455),.ci/docker/common/install_conda.sh .ci/docker/ubuntu-xpu/Dockerfile,https://github.com/pytorch/pytorch/pull/134455,chuanqi129,atalman,,,
d52aff3e736,skip,Untopiced,"Revert ""Adding entry-point based support for out-of-tree rendezvous plugins (#132633)""",test/distributed/elastic/rendezvous/out_of_tree_rendezvous_test.py test/distributed/elastic/rendezvous/out_of_tree_test_package/pyproject.toml test/distributed/elastic/rendezvous/out_of_tree_test_package/src/testbackend/__init__.py torch/distributed/elastic/rendezvous/__init__.py torch/distributed/elastic/rendezvous/registry.py,,,,,,
2c88a923a79,skip,Untopiced,"Revert ""Refactor caching device allocator utils (#130923)""",c10/core/CachingDeviceAllocator.h c10/cuda/CUDACachingAllocator.cpp c10/cuda/CUDACachingAllocator.h c10/cuda/CUDAMallocAsyncAllocator.cpp torch/csrc/cuda/CUDAPluggableAllocator.cpp torch/csrc/cuda/CUDAPluggableAllocator.h torch/csrc/cuda/Module.cpp,,,,,,
13d40f6fc59,skip,Untopiced,"Revert ""hang dim hint constants off Dim (#134484)""",docs/source/export.rst test/export/test_export.py torch/_export/non_strict_utils.py torch/export/dynamic_shapes.py,,,,,,
c7338f457cb,distributed,Untopiced,[DCP] Fixes the BC issue where the traversal doesn't support versions before 2.4 (#134158),test/distributed/checkpoint/test_compatibility.py torch/distributed/checkpoint/_nested_dict.py torch/distributed/checkpoint/_traverse.py torch/distributed/checkpoint/_version.py torch/distributed/checkpoint/default_planner.py,https://github.com/pytorch/pytorch/pull/134158,fegin,pradeepfn,wz337,,
40de63be097,skip,not user facing,parameterized test_graph_optims and test_graph_scaling_fused_optimizers (#133749),test/inductor/test_compiled_optimizers.py test/test_cuda.py torch/testing/_internal/common_optimizers.py,https://github.com/pytorch/pytorch/pull/133749,zero000064,janeyx99,,,
b8859dc4b88,skip,not user facing,[PyTorch Pin Memory Allocator] Optimize the free list implementation and add lock sharding (#134154),aten/src/ATen/core/CachingHostAllocator.h,https://github.com/pytorch/pytorch/pull/134154,banitag1,EikanWang,guangyey,jgong5,
633a9a3b138,skip,not user facing,add back sum_floordiv benchmark.  (#134635),benchmarks/dynamo/pr_time_benchmarks/benchmarks/sum_floordiv.py,https://github.com/pytorch/pytorch/pull/134635,laithsakka,avikchaudhuri,oulgen,,
310eb6d8c61,releng,not user facing,[AOTI] Fix test_aoti_inference CPU build issue (#134675),.ci/pytorch/test.sh test/cpp/aoti_inference/aoti_custom_class.cpp test/cpp/aoti_inference/compile_model.py test/cpp/aoti_inference/test.cpp test/cpp/aoti_inference/test.py,https://github.com/pytorch/pytorch/pull/134675,desertfire,atalman,chunyuan-w,,
ba5aec88c67,skip,not user facing,[reland][dtensor][MTPG] make sharding prop lru cache not shared among threads (#134509),test/distributed/_tensor/test_dtensor_ops.py torch/distributed/_tensor/_sharding_prop.py,https://github.com/pytorch/pytorch/pull/134509,XilunWu,tianyu-l,wz337,,
c352b6aaaf0,dynamo,Untopiced,[compiled autograd][cpp node] point c++ custom autograd functions tracing error to google doc (#134514),test/inductor/test_compiled_autograd.py torch/csrc/autograd/custom_function.h,https://github.com/pytorch/pytorch/pull/134514,xmfan,yf225,,,
47ba47a81f1,dynamo,Untopiced,[compiled autograd] error instead of deadlock on reentrant autograd (#134530),test/inductor/test_compiled_autograd.py torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/134530,xmfan,jansel,,,
aa31e7019a4,distributed,Untopiced,[FSDP] Made `clip_grad_norm_` norm compute order deterministic (#134673),torch/distributed/fsdp/fully_sharded_data_parallel.py,https://github.com/pytorch/pytorch/pull/134673,awgu,weifengpy,,,
042b733ddd7,dynamo,not user facing,[dynamo][freezing] Set is_static_type to false after marking an input static (#134653),torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/134653,anijain2305,mlazos,,,
3b40b07efbc,Uncategorized,Untopiced,Update PyTorch for XNNPACK 87ee0b4 (#134518),cmake/Dependencies.cmake third_party/XNNPACK third_party/generate-xnnpack-wrappers.py third_party/xnnpack.buck.bzl third_party/xnnpack_src_defs.bzl third_party/xnnpack_wrapper_defs.bzl,https://github.com/pytorch/pytorch/pull/134518,GregoryComer,mcr229,,,
23e26b84af5,skip,Untopiced,"Revert ""[3/N] Set correct device to CUDA guards (#134357)""",test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,,,,,,
66c33d59898,skip,Untopiced,"Revert ""[2/N] Add flag to control which rank should perform NaN check (#134345)""",test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,,,,,,
b07d0a22f54,dynamo,not user facing,[hop] require hops to override __call__. (#134352),test/dynamo/test_higher_order_ops.py test/functorch/test_control_flow.py test/functorch/test_eager_transforms.py torch/_dynamo/_trace_wrapped_higher_order_op.py torch/_export/wrappers.py torch/_higher_order_ops/executorch_call_delegate.py torch/_higher_order_ops/map.py torch/_higher_order_ops/run_const_graph.py torch/_higher_order_ops/strict_mode.py torch/_higher_order_ops/torchbind.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_ops.py torch/_prims/rng_prims.py,https://github.com/pytorch/pytorch/pull/134352,ydwu4,zou3519,,,
cae817c8629,skip,not user facing,[ET][CodeGen] Remove TORCH_API from NativeFunctions.h declarations (#134245),torchgen/gen_executorch.py,https://github.com/pytorch/pytorch/pull/134245,manuelcandales,larryliu0820,,,
de35d3062f4,distributed (tools),not user facing,Runtime Estimator for estimating GPU compute time (#134243),test/distributed/_tools/test_runtime_estimator.py torch/distributed/_tools/__init__.py torch/distributed/_tools/runtime_estimator.py,https://github.com/pytorch/pytorch/pull/134243,sanketpurandare,weifengpy,,,
4c16797e71d,distributed,not user facing,[c10d FR analyzer] Output a meaningful debug report for users (#134528),test/distributed/flight_recorder/test_fr_analysis.py tools/flight_recorder/components/builder.py tools/flight_recorder/components/types.py tools/flight_recorder/components/utils.py torch/csrc/distributed/c10d/NCCLUtils.cpp,https://github.com/pytorch/pytorch/pull/134528,fduwjj,c-p-i-o,,,
b6e51711a0e,sparse_frontend,new features,Add MaskedTensor support to *_like API (#128637),docs/source/masked.rst test/test_maskedtensor.py torch/masked/maskedtensor/__init__.py torch/masked/maskedtensor/_ops_refs.py torch/masked/maskedtensor/like.py,https://github.com/pytorch/pytorch/pull/128637,nowtryz,,,,
f685018ea9d,sparse_frontend,new features,"Add MaskedTensor passthrough: unfold, F.Unfold, F.Fold, stack (#125262)",aten/src/ATen/native/Col2Im.cpp aten/src/ATen/native/Im2Col.cpp aten/src/ATen/native/cuda/Col2Im.cu aten/src/ATen/native/cuda/Im2Col.cu docs/source/masked.rst test/test_maskedtensor.py torch/masked/maskedtensor/passthrough.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/125262,nowtryz,cpuhrsch,,,
20b62fed21f,distributed,Untopiced,Create processes in parallel in mp.start_processes for forkserver (#134629),test/distributed/elastic/multiprocessing/api_test.py test/test_multiprocessing_spawn.py torch/multiprocessing/__init__.py torch/multiprocessing/spawn.py,https://github.com/pytorch/pytorch/pull/134629,lijia19,d4l3k,,,
7b3da5f2975,skip,Untopiced,"Revert ""[dynamo] Cache _dynamo.disable results (#134272)""",test/dynamo/test_decorators.py torch/_dynamo/__init__.py torch/_dynamo/convert_frame.py torch/_dynamo/decorators.py,,,,,,
26ec06e45de,Uncategorized,Untopiced,[amd][lowering] hipify shim v2 headers (#134689),build.bzl,https://github.com/pytorch/pytorch/pull/134689,qxy11,zoranzhao,,,
2fe7e332c7a,distributed,bug fixes,[2/N] Add flag to control which rank should perform NaN check (#134345),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,https://github.com/pytorch/pytorch/pull/134345,kwen2501,shuqiangzhang,wconstab,,
5ff97e79ee7,skip,not user facing,Skip test_mutable_custom_op_fixed_layout2 on ROCM (#134690),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/134690,zou3519,FindHao,,,
afc76c6f2d4,distributed,bug fixes,[3/N] Set correct device to CUDA guards (#134357),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/134357,kwen2501,shuqiangzhang,wconstab,,
dd443f418ab,composability,Untopiced,Improve opcheck docs. (#134692),torch/library.py,https://github.com/pytorch/pytorch/pull/134692,zou3519,albanD,,,
a7933acd5a2,composability,Untopiced,Improve custom ops aliasing error message (#134688),torch/_library/custom_ops.py,https://github.com/pytorch/pytorch/pull/134688,zou3519,yushangdi,,,
6dd3f81aafd,Uncategorized,Untopiced,Add export_for_training as public API (#134677),test/export/test_export.py test/export/test_export_training_ir_to_run_decomp.py torch/_export/__init__.py torch/export/__init__.py,https://github.com/pytorch/pytorch/pull/134677,tugsbayasgalan,avikchaudhuri,zhxchen17,,
f997b2b8e6f,skip,Untopiced,"Revert ""Add MaskedTensor passthrough: unfold, F.Unfold, F.Fold, stack (#125262)""",aten/src/ATen/native/Col2Im.cpp aten/src/ATen/native/Im2Col.cpp aten/src/ATen/native/cuda/Col2Im.cu aten/src/ATen/native/cuda/Im2Col.cu docs/source/masked.rst test/test_maskedtensor.py torch/masked/maskedtensor/passthrough.py torch/testing/_internal/common_methods_invocations.py,,,,,,
202600bc238,python_frontend,new features,Add torch.serialization.skip_data context manager (#134504),docs/source/notes/serialization.rst test/test_cpp_extensions_open_device_registration.py test/test_serialization.py torch/_tensor.py torch/_utils.py torch/serialization.py torch/storage.py,https://github.com/pytorch/pytorch/pull/134504,mikaylagawarecki,albanD,,,
7a554e96b4c,inductor,Untopiced,[AOTI][Tooling] Follow up to print location of saved file path for `torch.pickle_save()` (#134651),torch/_inductor/codegen/debug_utils.py torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/134651,YUNQIUGUO,muchulee8,,,
ca03a14cf74,Uncategorized,Untopiced,hang dim hint constants off Dim (#134702),docs/source/export.rst test/export/test_export.py torch/_export/non_strict_utils.py torch/export/dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/134702,avikchaudhuri,pianpwk,,,
e7711d6c7da,mps,bug fixes,[MPS] Fix SDP training (#134719),aten/src/ATen/native/transformers/attention.cpp test/test_mps.py,https://github.com/pytorch/pytorch/pull/134719,qqaatw,malfet,,,
12854439944,skip,Untopiced,"Revert ""Add torch.serialization.skip_data context manager (#134504)""",docs/source/notes/serialization.rst test/test_cpp_extensions_open_device_registration.py test/test_serialization.py torch/_tensor.py torch/_utils.py torch/serialization.py torch/storage.py,,,,,,
503c0dd923f,skip,Untopiced,"Revert ""Add MaskedTensor support to *_like API (#128637)""",docs/source/masked.rst test/test_maskedtensor.py torch/masked/maskedtensor/__init__.py torch/masked/maskedtensor/_ops_refs.py torch/masked/maskedtensor/like.py,,,,,,
43dc17fd007,skip,Untopiced,"Revert ""[3/N] Set correct device to CUDA guards (#134357)""",test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,,,,,,
33d0c11b26f,skip,Untopiced,"Revert ""[2/N] Add flag to control which rank should perform NaN check (#134345)""",test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,,,,,,
cbf5ba1e970,skip,Untopiced,"Revert ""[1/N] Move NaN check onto NCCL stream (#134300)""",BUILD.bazel build_variables.bzl torch/csrc/distributed/c10d/NCCLUtils.cu torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/Utils.cu torch/csrc/distributed/c10d/Utils.hpp,,,,,,
25531eb735c,skip,Untopiced,"Revert ""[2nd try][Traceable FSDP2] Allow tracing through FSDP2 impl in trace_rules.py (#134539)""",torch/_dynamo/trace_rules.py torch/distributed/_composable/fsdp/_fsdp_state.py,,,,,,
c35d1f7b3aa,skip,Untopiced,"Revert ""[dynamo] Graph break on FSDP flat_param inconsistent tensor and grad dtype (#134614)""",torch/_dynamo/variables/builder.py,,,,,,
40cebde3bce,skip,Untopiced,"Revert ""[raland][dynamo][exceptions] Support raise from None (#134621)""",test/dynamo/test_exceptions.py torch/_dynamo/symbolic_convert.py,,,,,,
67d7040fce7,skip,Untopiced,"Revert ""[dynamo][dicts] Support hasattr on dicts (#134590)""",test/dynamo/test_functions.py torch/_dynamo/variables/dicts.py,,,,,,
f0fceed432b,skip,Untopiced,"Revert ""[dynamo][exceptions] Use exception subclass whenever possible (#134610)""",test/dynamo/test_exceptions.py torch/_dynamo/symbolic_convert.py,,,,,,
c5f114747e5,skip,not user facing,fix flakiness in update_hint_benchmark.py (#134649),benchmarks/dynamo/pr_time_benchmarks/benchmark_base.py benchmarks/dynamo/pr_time_benchmarks/benchmarks/update_hint_benchmark.py,https://github.com/pytorch/pytorch/pull/134649,laithsakka,aorenste,,,
cccb121d4ed,inductor,Untopiced,[Inductor] add inductor config: masked_vec (#134566),torch/_inductor/codegen/cpp.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/134566,jiayisunx,jansel,jgong5,,
76f975948e5,inductor,not user facing,[inductor] Cleanup generate_node_schedule (#134306),torch/_inductor/codegen/simd.py,https://github.com/pytorch/pytorch/pull/134306,jansel,shunting314,,,
eaec9e80b80,skip,Untopiced,"Revert ""[dynamo] simplify implementation for `os.fspath` (#133801)""",torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/loader.py torch/_dynamo/polyfills/os.py torch/_dynamo/variables/misc.py,,,,,,
f65df5edae9,skip,Untopiced,"Revert ""[dynamo][itertools] support `itertools.tee` (#133771)""",test/dynamo/test_misc.py torch/_dynamo/polyfills/itertools.py torch/_dynamo/polyfills/loader.py,,,,,,
4811dc3de9d,skip,Untopiced,"Revert ""[dynamo] simplify polyfill registration for `builtins.all` and `builtins.any` (#133769)""",torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/builtins.py torch/_dynamo/polyfills/loader.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builtin.py,,,,,,
4f9c68454af,skip,not user facing,[inductor]Let output or input_as_strided match exact strides  (#130956),test/inductor/test_torchinductor.py torch/_inductor/codegen/wrapper.py torch/_inductor/graph.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/130956,FindHao,blaine-rister,desertfire,eellison,
41e36e2b46e,releng,not user facing,Reflect check_labels status as a signal (#134711),.github/scripts/check_labels.py .github/scripts/test_check_labels.py .github/workflows/check-labels.yml,https://github.com/pytorch/pytorch/pull/134711,izaitsevfb,clee2000,,,
17e9c2d1e7c,skip,not user facing,Add oneDNN support for Half  LSTM on CPU (#132607),aten/src/ATen/native/RNN.cpp test/test_mkldnn.py,https://github.com/pytorch/pytorch/pull/132607,CaoE,jgong5,peterbell10,,
4b4ba7ab06f,nested tensor_frontend,not user facing,[NJT] Support NJT SDPA + meta-device flop counting (#134289),test/test_nestedtensor.py torch/nested/_internal/sdpa.py,https://github.com/pytorch/pytorch/pull/134289,davidberard98,soulitzer,,,
4655eb3ee27,skip,not user facing,Uses MemPoolContext to route allocations from CUDACachingAllocator (#134685),c10/cuda/CUDACachingAllocator.cpp docs/source/cuda.rst test/test_cuda.py torch/_C/__init__.pyi.in torch/csrc/cuda/Module.cpp torch/cuda/__init__.py torch/cuda/memory.py,https://github.com/pytorch/pytorch/pull/134685,syed-ahmed,ezyang,,,
a32255481b5,caffe2,Untopiced,[caffe2][hipify] remove un-used flag from `pybind_utils.h` (#134404),torch/csrc/jit/python/pybind_utils.h,https://github.com/pytorch/pytorch/pull/134404,JenniferWang,malfet,,,
297b42012d3,cuda,not user facing,[PyTorch] Use pinned memory for zero_cuda_out (#134712),aten/src/ATen/native/cuda/Nonzero.cu,https://github.com/pytorch/pytorch/pull/134712,banitag1,zyan0,,,
94db935749b,python_frontend,new features,Add torch.serialization.skip_data context manager (#134504),docs/source/notes/serialization.rst test/test_cpp_extensions_open_device_registration.py test/test_serialization.py torch/_tensor.py torch/_utils.py torch/serialization.py torch/storage.py,https://github.com/pytorch/pytorch/pull/134504,mikaylagawarecki,albanD,,,
da9e61ef70a,skip,not user facing,Get accumulate dtype for Intel GPU (#134465),aten/src/ATen/AccumulateType.cpp,https://github.com/pytorch/pytorch/pull/134465,EikanWang,Skylion007,atalman,,
b0a6d9ad27f,distributed,not user facing,"[DTensor] Add pointwise ops strategy for aten.isinf, aten.isneginf, aten.isposinf (#134699)",test/distributed/_tensor/test_dtensor_ops.py torch/distributed/_tensor/ops/_pointwise_ops.py,https://github.com/pytorch/pytorch/pull/134699,wz337,tianyu-l,,,
834d8b09654,inductor,not user facing,[Inductor][mkldnn] Bug fix: incorrect codegen arg order for qconv (#134579),test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/mkldnn_ir.py,https://github.com/pytorch/pytorch/pull/134579,Xia-Weiwen,jgong5,leslie-fang-intel,,
578b8d75e52,dynamo,not user facing,[2nd try][Traceable FSDP2] Allow tracing through FSDP2 impl in trace_rules.py (#134539),torch/_dynamo/trace_rules.py torch/distributed/_composable/fsdp/_fsdp_state.py,https://github.com/pytorch/pytorch/pull/134539,yf225,ckluk2,yanboliang,,
3645634f3c3,skip,not user facing,[1/N] Move NaN check onto NCCL stream (#134300),BUILD.bazel build_variables.bzl torch/csrc/distributed/c10d/NanCheck.cu torch/csrc/distributed/c10d/NanCheck.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/Utils.cu torch/csrc/distributed/c10d/Utils.hpp,https://github.com/pytorch/pytorch/pull/134300,kwen2501,shuqiangzhang,wconstab,,
cfb642bb6b0,distributed,Untopiced,[DTensor] Extend implicit replication to replicate DTensor for foreach ops so model doesn't have to be fully tp-ed when using 2D (#134551),test/distributed/_tensor/test_dtensor.py test/distributed/checkpoint/fsdp/test_fsdp_dsd.py torch/distributed/_tensor/_dispatch.py,https://github.com/pytorch/pytorch/pull/134551,wz337,tianyu-l,,,
2446dead35e,skip,not user facing,[dynamo][exceptions] Use exception subclass whenever possible (#134610),test/dynamo/test_exceptions.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/134610,anijain2305,drisspg,jansel,,
2bf622685da,skip,not user facing,[dynamo][dicts] Support hasattr on dicts (#134590),test/dynamo/test_functions.py torch/_dynamo/variables/dicts.py,https://github.com/pytorch/pytorch/pull/134590,anijain2305,Skylion007,,,
fb35d1e01f4,skip,not user facing,[raland][dynamo][exceptions] Support raise from None (#134621),test/dynamo/test_exceptions.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/134621,anijain2305,jansel,,,
d01a7a9faa5,skip,not user facing,[dynamo] Graph break on FSDP flat_param inconsistent tensor and grad dtype (#134614),torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/134614,anijain2305,awgu,yf225,,
9e806c1a605,skip,not user facing,[dynamo] simplify implementation for `os.fspath` (#133801),torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/builtins.py torch/_dynamo/polyfills/functools.py torch/_dynamo/polyfills/itertools.py torch/_dynamo/polyfills/loader.py torch/_dynamo/polyfills/os.py torch/_dynamo/polyfills/sys.py torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/133801,XuehaiPan,anijain2305,,,
70853b792a8,dynamo,not user facing,[dynamo][itertools] support `itertools.tee` (#133771),test/dynamo/test_misc.py torch/_dynamo/polyfills/itertools.py,https://github.com/pytorch/pytorch/pull/133771,XuehaiPan,jansel,,,
092349dcddb,inductor,Untopiced,Never CSE aten.empty in the partitioner (#134703),test/inductor/test_perf.py torch/_functorch/compile_utils.py,https://github.com/pytorch/pytorch/pull/134703,zou3519,yf225,,,
92e38a476fc,quantization,Untopiced,preserve aten::to device in export training (#134622),test/export/test_export.py torch/_subclasses/functional_tensor.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/134622,avikchaudhuri,tugsbayasgalan,zhxchen17,,
594162f7ab6,dynamo,not user facing,[dynamo] Support reading attributes from pybind objects (#134630),build_variables.bzl test/dynamo/test_functions.py test/dynamo_expected_failures/TestScript.test_is_after_use torch/_dynamo/variables/user_defined.py torch/csrc/dynamo/init.cpp torch/csrc/dynamo/utils.cpp torch/csrc/dynamo/utils.h,https://github.com/pytorch/pytorch/pull/134630,anijain2305,jansel,,,
4fcd15a667d,skip,not user facing,Fix test_sgd_weight_decay_xpu accuracy error (#134744),test/inductor/test_compiled_optimizers.py,https://github.com/pytorch/pytorch/pull/134744,Stonepia,EikanWang,desertfire,,
0dbc72887bb,composability,not user facing,[CPU][flash attention] make the stride of output align with input (#134656),aten/src/ATen/native/transformers/attention.cpp test/test_transformers.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/134656,Valentine233,drisspg,jgong5,,
387d3fc2968,skip,not user facing,[AOTI] Switch benchmarking to use export non-strict mode (#130977),benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/aot_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/aot_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/aot_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_freezing_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_amp_freezing_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_freezing_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_aot_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_aot_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/update_expected.py benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/130977,desertfire,angelayi,,,
9953f55f4c2,distributed,bug fixes,[2/N] Add flag to control which rank should perform NaN check (#134345),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,https://github.com/pytorch/pytorch/pull/134345,kwen2501,shuqiangzhang,wconstab,,
d503217ea4d,inductor,not user facing,[inductor] calibration inductor windows uts (15/N) (#134586),test/dynamo/test_logging.py,https://github.com/pytorch/pytorch/pull/134586,xuhancn,jansel,,,
26aea277f7f,distributed,bug fixes,[3/N] Set correct device to CUDA guards (#134357),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/134357,kwen2501,shuqiangzhang,wconstab,,
ab646cd8059,skip,Untopiced,"Revert ""[reland][dtensor][MTPG] make sharding prop lru cache not shared among threads (#134509)""",test/distributed/_tensor/test_dtensor_ops.py torch/distributed/_tensor/_sharding_prop.py,,,,,,
d9d95dc55e3,distributed,not user facing,[4/N] Test NaN checker against broadcast (#134701),test/distributed/test_c10d_nccl.py,https://github.com/pytorch/pytorch/pull/134701,kwen2501,wconstab,,,
36a6516290c,export,Untopiced,[export] use single FQN for param_buffer_mapping (#134500),test/export/test_export.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/134500,pianpwk,angelayi,,,
348d02a9836,inductor,not user facing,Changed masked out rows logsumexp to be -inf and not zero (#134650),test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py,https://github.com/pytorch/pytorch/pull/134650,Chillee,BoyuanFeng,drisspg,yanboliang,
ce961466239,inductor,Untopiced,[PT2] Fix node metadata setting in group_batch_fusion_aten (#134543),torch/_inductor/fx_passes/pre_grad.py,https://github.com/pytorch/pytorch/pull/134543,huxintong,frank-wei,,,
5573c178775,skip,not user facing,[BE][Ez]: Update ruff to 0.6.3 (#134769),.lintrunner.toml,https://github.com/pytorch/pytorch/pull/134769,Skylion007,albanD,,,
3fc6e47d42e,inductor,not user facing,[AOTI] Fix cosmetic indentation issue in cuda cpp wrapper codegen for DeferredCudaKernelLine/GridLine (#134705),torch/_inductor/codegen/cpp_wrapper_cuda.py,https://github.com/pytorch/pytorch/pull/134705,YUNQIUGUO,ColinPeppler,,,
d91b49dbaac,skip,not user facing,expandable_segments <-> other allocator options (#134338),c10/cuda/CUDACachingAllocator.cpp test/test_cuda.py,https://github.com/pytorch/pytorch/pull/134338,zdevito,ezyang,,,
5470fcd5b92,distributed,Untopiced,[5/N] Reconcile barrier and NaN checker (#134707),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/134707,kwen2501,shuqiangzhang,wconstab,,
43e1df64f8f,dynamo,Untopiced,register all entry_point backends on first attempt (#132546),test/dynamo/test_backends.py torch/_dynamo/backends/registry.py,https://github.com/pytorch/pytorch/pull/132546,nairbv,jansel,,,
202e5cc87df,inductor,not user facing,[inductor] Fix error in debug_str_extra (#134747),torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/134747,jansel,Skylion007,shunting314,,
6180574771e,releng,not user facing,"Move py 3.8->3.9 pull, trunk, inductor, prerioric CI tests (#133624)",.ci/docker/build.sh .ci/docker/common/install_onnx.sh .ci/docker/requirements-ci.txt .ci/pytorch/test.sh .github/merge_rules.yaml .github/workflows/docker-builds.yml .github/workflows/inductor-perf-test-nightly-x86.yml .github/workflows/inductor.yml .github/workflows/nightly.yml .github/workflows/periodic.yml .github/workflows/pull.yml .github/workflows/slow.yml scripts/compile_tests/download_reports.py scripts/compile_tests/update_failures.py,https://github.com/pytorch/pytorch/pull/133624,atalman,Skylion007,ZainRizvi,malfet,
b977abd5de0,inductor,not user facing,[Inductor] Fix error checking for scaled_mm lowering (#134765),torch/_inductor/kernel/mm_scaled.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/134765,drisspg,Skylion007,,,
e09324e7dae,skip,not user facing,[dynamo] simplify polyfill registration for `builtins.all` and `builtins.any` (#133769),torch/_dynamo/decorators.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/builtins.py torch/_dynamo/polyfills/os.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/functions.py torch/compiler/__init__.py,https://github.com/pytorch/pytorch/pull/133769,XuehaiPan,jansel,,,
b5f1ffa7ab0,skip,not user facing,[dynamo] simplify implementation for `functools.reduce` (#133778),torch/_dynamo/polyfills/functools.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/133778,XuehaiPan,anijain2305,jansel,,
eaa449fbf0f,skip,not user facing,[dynamo] simplify implementation for `builtins.sum` (#133779),torch/_dynamo/polyfills/builtins.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/133779,XuehaiPan,anijain2305,jansel,,
1b703669576,skip,not user facing,[dynamo][itertools] refactor `itertools.chain` and `itertools.chain.from_iterable` to use polyfills (#133864),torch/_dynamo/decorators.py torch/_dynamo/polyfills/itertools.py torch/_dynamo/polyfills/loader.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/iter.py,https://github.com/pytorch/pytorch/pull/133864,XuehaiPan,jansel,,,
a2566adfb60,skip,not user facing,[dynamo] refactor `builtins.enumerate` to use polyfill (#133894),torch/_dynamo/polyfills/builtins.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/133894,XuehaiPan,jansel,,,
7d12e6dceb9,skip,not user facing,[dynamo][itertools] refactor `itertools.islice` to use polyfill (#133876),torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/itertools.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/iter.py,https://github.com/pytorch/pytorch/pull/133876,XuehaiPan,jansel,,,
416a7894fee,skip,not user facing,[Windows][XPU] Disable Kineto PTI on Windows only (#134620),cmake/Dependencies.cmake,https://github.com/pytorch/pytorch/pull/134620,min-jean-cho,guangyey,malfet,,
1e92d7b688e,inductor,not user facing,[inductor] move loop ordering after fusion (#126254),test/dynamo/test_logging.py test/inductor/test_benchmark_fusion.py test/inductor/test_loop_ordering.py torch/_dynamo/utils.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/config.py torch/_inductor/dependencies.py torch/_inductor/ir.py torch/_inductor/metrics.py torch/_inductor/scheduler.py torch/_logging/_registrations.py,https://github.com/pytorch/pytorch/pull/126254,shunting314,jansel,,,
8b4c4875818,inductor,Untopiced,Fix AOTInductor complication on ROCM (#134522),torch/_inductor/cpp_builder.py torch/_inductor/fx_passes/reinplace.py,https://github.com/pytorch/pytorch/pull/134522,zoranzhao,frank-wei,,,
65864d01341,distributed,bc breaking,[c10d] Remove Option for ProcessGroup and Expose backend Options to reflect the correct code structure (#132931),test/distributed/test_c10d_common.py test/distributed/test_device_mesh.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/ProcessGroup.cpp torch/csrc/distributed/c10d/ProcessGroup.hpp torch/csrc/distributed/c10d/init.cpp torch/distributed/device_mesh.py torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/132931,fduwjj,H-Huang,,,
496e57283dd,skip,not user facing,add add_loop benchmarks (#134652),benchmarks/dynamo/pr_time_benchmarks/benchmarks/add_loop.py,https://github.com/pytorch/pytorch/pull/134652,laithsakka,anijain2305,,,
71ff168dbba,jit,Untopiced,pytorch: llvm_codegen: prefix JIT generated functions with 8B of data so jitted code can be called from ASAN+UBSAN on LLVM17 (llvm/llvm-project#65253) (#134572),torch/csrc/jit/tensorexpr/llvm_codegen.cpp,https://github.com/pytorch/pytorch/pull/134572,luciang,atalman,,,
d13ce2e2b5f,distributed,Untopiced,[c10d] release gil lock during eager init (#134779),torch/csrc/distributed/c10d/init.cpp,https://github.com/pytorch/pytorch/pull/134779,shuqiangzhang,c-p-i-o,,,
3775fc982df,inductor,not user facing,[Inductor][CPP] Fix Index name error (#134645),torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/134645,leslie-fang-intel,jgong5,masnesral,,
3402a5d8656,xpu,Untopiced,fix windows xpu build issue (#133845),cmake/Modules/FindSYCLToolkit.cmake,https://github.com/pytorch/pytorch/pull/133845,guangyey,EikanWang,atalman,malfet,
8b68912dfc8,releng,not user facing,"Correctly detect ""Rate limit exceeded"" error (#134785)",.github/scripts/github_utils.py,https://github.com/pytorch/pytorch/pull/134785,izaitsevfb,clee2000,,,
cf11fc0dcbb,dynamo,not user facing,dynamo: Only log if we've disabled eval_frame once. (#134529),torch/_dynamo/eval_frame.py torch/_utils_internal.py,https://github.com/pytorch/pytorch/pull/134529,c00w,chuanhaozhuge,oulgen,,
27ffa679849,dynamo,not user facing,Support __class__ attr for tuple and list variables (#134099),test/dynamo/test_misc.py torch/_dynamo/variables/lists.py,https://github.com/pytorch/pytorch/pull/134099,jerrychenhf,anijain2305,jansel,,
a645a18d2ea,skip,not user facing,[reland][dtensor][MTPG] make sharding prop lru cache not shared among threads (#134509),test/distributed/_tensor/test_dtensor_ops.py torch/distributed/_tensor/_sharding_prop.py,https://github.com/pytorch/pytorch/pull/134509,XilunWu,tianyu-l,wz337,,
0d5f9787959,skip,not user facing,add basic nn modules diff time benchmarks (#134658),benchmarks/dynamo/pr_time_benchmarks/benchmarks/basic_modules_benchmarks.py,https://github.com/pytorch/pytorch/pull/134658,laithsakka,anijain2305,,,
1f1e2eeb9d9,inductor,not user facing,[inductor] Install `tlparse` for test\dynamo\test_structured_trace.py UTs. (#134806),.ci/pytorch/win-test.sh,https://github.com/pytorch/pytorch/pull/134806,xuhancn,ezyang,,,
5e8bf29148a,skip,not user facing,[ROCm] remove triton-rocm commit pin and merge pins with triton.txt (#133438),.ci/docker/centos-rocm/Dockerfile .ci/docker/ci_commit_pins/triton-rocm.txt .ci/docker/ci_commit_pins/triton.txt .ci/docker/common/install_triton.sh .ci/docker/ubuntu-rocm/Dockerfile .circleci/scripts/binary_populate_env.sh .github/scripts/build_triton_wheel.py .github/workflows/build-triton-wheel.yml CODEOWNERS,https://github.com/pytorch/pytorch/pull/133438,jataylo,jithunnair-amd,malfet,,
75b86b15548,releng,not user facing,[executorch hash update] update the pinned executorch hash (#134736),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/134736,pytorchupdatebot,pytorchbot,,,
7a694f66835,skip,not user facing,[justknobs] Override __bool__ method (#134799),test/test_utils_internal.py torch/_utils_internal.py,https://github.com/pytorch/pytorch/pull/134799,anijain2305,ezyang,,,
1011e0ae980,skip,not user facing,Generalize devices specific UTs  for dynamo (#130714),test/dynamo/test_model_output.py,https://github.com/pytorch/pytorch/pull/130714,ankurneog,anijain2305,,,
a5630239ad1,dynamo,not user facing,[dynamo] Improve minifier error message when fp64 not supported (#134737),torch/_dynamo/debug_utils.py,https://github.com/pytorch/pytorch/pull/134737,jansel,anijain2305,,,
0f8bec43991,dynamo,Untopiced,[dynamo] mark_static_nn_module (#134713),test/dynamo/test_decorators.py torch/_dynamo/decorators.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/134713,anijain2305,jansel,,,
50efbb9f1e7,distributed,not user facing,[DeviceMesh][Test] Add a unit test for get_local_rank for flattened mesh (#134603),test/distributed/test_device_mesh.py,https://github.com/pytorch/pytorch/pull/134603,wz337,fduwjj,,,
92f282ca520,mps,Untopiced,Enable batch matmul for result sizes > 2**32 the tensor can be split along batch axis (#133430),aten/src/ATen/native/mps/operations/LinearAlgebra.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/133430,jhavukainen,malfet,,,
c07e566baf5,distributed,not user facing,[CUDA][P2P] Check device capability in `requires_cuda_p2p_access` (#134523),test/distributed/test_symmetric_memory.py,https://github.com/pytorch/pytorch/pull/134523,eqy,Skylion007,yifuwang,,
932c4ca5a05,distributed,not user facing,make make_fx collective test single threaded (#134775),test/distributed/test_functional_api.py,https://github.com/pytorch/pytorch/pull/134775,ydwu4,yifuwang,,,
d261a1751a8,dynamo,not user facing,[HOP] fix export x inline_inbuilt_nn_modules (#133731),test/export/test_export.py torch/_dynamo/convert_frame.py torch/_dynamo/guards.py torch/_dynamo/source.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builder.py torch/_higher_order_ops/utils.py torch/fx/_symbolic_trace.py torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/133731,ydwu4,anijain2305,,,
10c31e96dfd,skip,Untopiced,"Revert ""[dynamo][itertools] refactor `itertools.islice` to use polyfill (#133876)""",torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/itertools.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/iter.py,,,,,,
8aa44e14cf1,skip,Untopiced,"Revert ""[dynamo] refactor `builtins.enumerate` to use polyfill (#133894)""",torch/_dynamo/polyfills/builtins.py torch/_dynamo/variables/builtin.py,,,,,,
1ad08c7a5b8,skip,Untopiced,"Revert ""[dynamo][itertools] refactor `itertools.chain` and `itertools.chain.from_iterable` to use polyfills (#133864)""",torch/_dynamo/decorators.py torch/_dynamo/polyfills/itertools.py torch/_dynamo/polyfills/loader.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/iter.py,,,,,,
7a85c488a8f,skip,Untopiced,"Revert ""[dynamo] simplify implementation for `builtins.sum` (#133779)""",torch/_dynamo/polyfills/builtins.py torch/_dynamo/variables/builtin.py,,,,,,
c6ecf57dd26,skip,Untopiced,"Revert ""[dynamo] simplify implementation for `functools.reduce` (#133778)""",torch/_dynamo/polyfills/functools.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builtin.py,,,,,,
994438040c4,dynamo,not user facing,Improvements for associative_scan - combine_mode (#133012),test/dynamo/test_repros.py test/functorch/test_control_flow.py test/inductor/test_control_flow.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/associative_scan.py,https://github.com/pytorch/pytorch/pull/133012,bohnstingl,ydwu4,,,
351ba3e67c9,skip,Untopiced,"Revert ""[c10d] Remove Option for ProcessGroup and Expose backend Options to reflect the correct code structure (#132931)""",test/distributed/test_c10d_common.py test/distributed/test_device_mesh.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/ProcessGroup.cpp torch/csrc/distributed/c10d/ProcessGroup.hpp torch/csrc/distributed/c10d/init.cpp torch/distributed/device_mesh.py torch/distributed/distributed_c10d.py,,,,,,
f5b0caee711,inductor,not user facing,Rewrite `unsafe_remove_auto_functionalized_pass` using `decompose_auto_functionalized` (#134831),test/export/test_passes.py torch/_inductor/pattern_matcher.py torch/export/_remove_auto_functionalized_pass.py,https://github.com/pytorch/pytorch/pull/134831,laithsakka,zou3519,,,
a19a7524f6c,skip,not user facing,[export] Make sure getitem replacement are synced with module call graph. (#134830),torch/_export/verifier.py torch/export/exported_program.py torch/export/unflatten.py,https://github.com/pytorch/pytorch/pull/134830,zhxchen17,angelayi,,,
08184aa85cf,skip,Untopiced,Add support for 32KB multi_tensor_apply kernel arguments (#134373),aten/src/ATen/native/cuda/AmpKernels.cu aten/src/ATen/native/cuda/ForeachBinaryOpList.cu aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu aten/src/ATen/native/cuda/ForeachBinaryOpScalarTensor.cu aten/src/ATen/native/cuda/ForeachFunctors.cuh aten/src/ATen/native/cuda/ForeachPointwiseOp.cu aten/src/ATen/native/cuda/ForeachReduceOp.cu aten/src/ATen/native/cuda/ForeachTernaryOp.cu aten/src/ATen/native/cuda/ForeachUnaryOp.cu aten/src/ATen/native/cuda/FusedSgdKernel.cu aten/src/ATen/native/cuda/MultiTensorApply.cpp aten/src/ATen/native/cuda/MultiTensorApply.cuh aten/src/ATen/native/cuda/fused_adam_amsgrad_impl.cu aten/src/ATen/native/cuda/fused_adam_impl.cu aten/src/ATen/native/cuda/fused_adam_utils.cuh aten/src/ATen/native/cuda/fused_adamw_amsgrad_impl.cu aten/src/ATen/native/cuda/fused_adamw_impl.cu build_variables.bzl,https://github.com/pytorch/pytorch/pull/134373,yifuwang,crcrpar,eqy,janeyx99,
577a93514f1,dynamo,not user facing,[dynamo][dynamic][heuristic] Mark tuple getitem integers as static (#134734),benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_training.csv test/dynamo/test_modules.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/134734,anijain2305,jansel,,,
e21d7b77ceb,foreach_frontend,Untopiced,Update `ForeachfuncInfo.sample_inputs_func` to yield scalars & scalarlists that are more friendly to test_meta (#134552),test/test_foreach.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/134552,crcrpar,janeyx99,,,
9e0ddc0e14c,inductor,not user facing,[inductor] don't allow triton config pre_hook (#134633),test/inductor/test_triton_heuristics.py torch/_inductor/runtime/runtime_utils.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/134633,davidberard98,jansel,shunting314,,
15f5a4858b3,inductor,Untopiced,[inductor] enable Intel Compiler(icx-cl) for inductor windows (#134772),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/134772,xuhancn,jansel,jgong5,,
1993a2aa9ee,distributed,not user facing,"[FR] Make pg_name unique, show P2P collective status and fix bugs when running the script as command (#134780)",test/distributed/flight_recorder/test_fr_analysis.py tools/flight_recorder/components/builder.py tools/flight_recorder/components/config_manager.py tools/flight_recorder/components/loader.py tools/flight_recorder/components/types.py tools/flight_recorder/components/utils.py tools/flight_recorder/fr_trace.py,https://github.com/pytorch/pytorch/pull/134780,fduwjj,c-p-i-o,,,
a0d0c6b7e6c,skip,not user facing,Used `torch.equal` in `test_foreach_copy_with_multi_dtypes` (#134861),test/test_foreach.py,https://github.com/pytorch/pytorch/pull/134861,awgu,Skylion007,crcrpar,janeyx99,
bdfa94b787f,skip,not user facing,[RFC] Make fr trace script a console scripts (#134729),setup.py,https://github.com/pytorch/pytorch/pull/134729,fduwjj,c-p-i-o,malfet,,
98b813d0d4d,inductor,Untopiced,Enable cudagraphs in cpp wrapper (#133885),test/inductor/test_cudagraph_trees.py torch/_inductor/codegen/cpp_wrapper_cpu.py,https://github.com/pytorch/pytorch/pull/133885,exclamaforte,desertfire,eellison,,
db17a9898d3,releng,not user facing,regenerate ci workflows for binary builds with new g4dn runners (#133404),.github/templates/windows_binary_build_workflow.yml.j2 .github/workflows/generated-windows-binary-conda-nightly.yml .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/133404,wdvr,ZainRizvi,,,
f6398eb0fac,inductor,Untopiced,dynamic shapes for combo_kenel/foreach_kernel (#134477),test/inductor/test_combo_kernels.py test/inductor/test_cuda_cpp_wrapper.py test/inductor/test_foreach.py torch/_inductor/codegen/cpp_wrapper_cuda.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/codegen/wrapper.py torch/_inductor/config.py torch/_inductor/lowering.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/134477,qchip,mlazos,,,
a1ba8e61d1b,skip,Untopiced,"Revert ""[ROCm] remove triton-rocm commit pin and merge pins with triton.txt (#133438)""",.ci/docker/centos-rocm/Dockerfile .ci/docker/ci_commit_pins/triton-rocm.txt .ci/docker/ci_commit_pins/triton.txt .ci/docker/common/install_triton.sh .ci/docker/ubuntu-rocm/Dockerfile .circleci/scripts/binary_populate_env.sh .github/scripts/build_triton_wheel.py .github/workflows/build-triton-wheel.yml CODEOWNERS,,,,,,
8b258b3b144,skip,Untopiced,[Inductor] Allow customizing the padding format (#133939),test/inductor/test_padding.py torch/_inductor/config.py torch/_inductor/graph.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/133939,blaine-rister,shunting314,,,
e688b78791d,dynamo,not user facing,[Dynamo][autograd.Function] Trace fwd graph under no_grad mode (#134872),test/dynamo/test_autograd_function.py torch/_dynamo/variables/higher_order_ops.py,https://github.com/pytorch/pytorch/pull/134872,yanboliang,zou3519,,,
2384f77d764,skip,not user facing,[XPU] Fix Windows XPU build (#134276),caffe2/CMakeLists.txt,https://github.com/pytorch/pytorch/pull/134276,ratnampa,EikanWang,atalman,,
5dad6a5a849,onnx,improvements,[ONNX][DORT] Lazy-import `onnxruntime` (#134662),test/onnx/dynamo/test_dynamo_with_onnxruntime_backend.py test/onnx/test_lazy_import.py torch/onnx/_internal/onnxruntime.py,https://github.com/pytorch/pytorch/pull/134662,oraluben,justinchuby,,,
ebbdeeede19,skip,not user facing,[dynamo][itertools] refactor `itertools.chain` and `itertools.chain.from_iterable` to use polyfills (#133864),torch/_dynamo/decorators.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/itertools.py torch/_dynamo/polyfills/loader.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/iter.py,https://github.com/pytorch/pytorch/pull/133864,XuehaiPan,jansel,,,
a854c3a25ec,skip,not user facing,[dynamo] refactor `builtins.enumerate` to use polyfill (#133894),torch/_dynamo/polyfills/builtins.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/133894,XuehaiPan,jansel,,,
050ad925f3e,skip,not user facing,[benchmark] Add to torchbench relative path search (#134871),benchmarks/dynamo/torchbench.py,https://github.com/pytorch/pytorch/pull/134871,yiming0416,FindHao,,,
f95085fd916,mps,not user facing,[BE][MPS] Prefer xfail to skip (#134858),test/test_nn.py torch/testing/_internal/common_device_type.py torch/testing/_internal/common_modules.py,https://github.com/pytorch/pytorch/pull/134858,malfet,janeyx99,,,
86e03a64e1e,skip,Untopiced,"Revert ""[Inductor] Allow customizing the padding format (#133939)""",test/inductor/test_padding.py torch/_inductor/config.py torch/_inductor/graph.py torch/_inductor/ir.py,,,,,,
aef5da50f4d,skip,not user facing,Cleanup unused `pytorch.version` (#134381),tools/pytorch.version,https://github.com/pytorch/pytorch/pull/134381,eqy,zou3519,,,
64fad53b501,inductor,not user facing,[Inductor] Support passing module map parameter to Triton make_ir API (#134774),torch/_higher_order_ops/triton_kernel_wrap.py,https://github.com/pytorch/pytorch/pull/134774,alexbaden,EikanWang,jansel,zou3519,
34b85d301f1,releng,not user facing,[executorch hash update] update the pinned executorch hash (#134894),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/134894,pytorchupdatebot,pytorchbot,,,
090d9cf4103,dynamo,not user facing,[Dynamo][autograd.Function][vmap] support torch._C._are_functorch_transforms_active (#134889),test/dynamo/test_functions.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/134889,yanboliang,jansel,,,
136badae64f,inductor,not user facing,[inductor] preload icx built in math libs (#134870),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/134870,xuhancn,jansel,,,
d9cc693719b,jit,Untopiced,[jit] Change argument names (#134828),torch/csrc/jit/tensorexpr/external_functions.cpp,https://github.com/pytorch/pytorch/pull/134828,cyyever,janeyx99,,,
ec660c383e4,dynamo,not user facing,[dynamo] reduce overhead for `PolyfilledFunctionVariable.call_function` (#134842),torch/_dynamo/decorators.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/torch.py,https://github.com/pytorch/pytorch/pull/134842,XuehaiPan,jansel,,,
eed0d766825,skip,not user facing,[dynamo][itertools] refactor `itertools.islice` to use polyfill (#133876),torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/itertools.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/iter.py,https://github.com/pytorch/pytorch/pull/133876,XuehaiPan,jansel,,,
471c33f007a,dynamo,Untopiced,[dynamo] Rewrite foreach_lerp to avoid aten item call (#134166),test/dynamo/test_functions.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/variables/torch.py,https://github.com/pytorch/pytorch/pull/134166,mlazos,anijain2305,,,
3fb4c6bc383,dynamo,Untopiced,[dynamo] Rewrite foreach pow to broadcast scalar argument (#134167),test/dynamo/test_functions.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/variables/torch.py,https://github.com/pytorch/pytorch/pull/134167,mlazos,anijain2305,,,
4e714185660,dynamo,Untopiced,[dynamo] rewrite addcmul_ to remove graph break (#134168),test/dynamo/test_functions.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/variables/tensor.py,https://github.com/pytorch/pytorch/pull/134168,mlazos,anijain2305,,,
16f119e62a5,skip,not user facing,Update compiled optimizer tests for tensor betas (#134169),test/inductor/test_compiled_optimizers.py,https://github.com/pytorch/pytorch/pull/134169,mlazos,anijain2305,eellison,,
f4641ca481e,inductor,not user facing,[Inductor] Remove VecChecker and fallback non-supported Vec op to Scalar impl with a for loop (#134569),test/inductor/test_cpu_repro.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_utils.py,https://github.com/pytorch/pytorch/pull/134569,zhuhaozhe,jansel,jgong5,,
590d96be641,inductor,not user facing,[inductor] move test_fuse_large_params to slow test. (#134900),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/134900,xuhancn,jansel,,,
7239b8a4f1a,inductor,not user facing,Clean up RemoteCache classes (#134032),test/inductor/mock_cache.py test/inductor/test_codecache.py test/inductor/test_max_autotune.py torch/_VF.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_inductor/codecache.py torch/_inductor/remote_cache.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/134032,aorenste,bhack,oulgen,,
29b7852dc1a,cuda,Untopiced,drop gil in couple places (leads to deadlocks) (#134910),torch/csrc/cuda/Event.cpp torch/csrc/cuda/Module.cpp,https://github.com/pytorch/pytorch/pull/134910,ngimel,eqy,,,
caa04e0caeb,skip,not user facing,[ET] codegen: bool array as array ref (#134886),tools/test/test_executorch_types.py torchgen/executorch/api/et_cpp.py torchgen/executorch/api/unboxing.py,https://github.com/pytorch/pytorch/pull/134886,manuelcandales,larryliu0820,,,
c25b64a0570,cuda,Untopiced,"expose host_emptyCache to python, fix a bug in freeing cudaHostRegist… (#134919)",aten/src/ATen/cuda/CachingHostAllocator.cpp test/test_cuda.py torch/csrc/cuda/Module.cpp,https://github.com/pytorch/pytorch/pull/134919,ngimel,eqy,,,
d03f767cae4,vulkan,not user facing,Check function declarations of Vulkan code (#134550),aten/src/ATen/native/vulkan/api/Pipeline.cpp aten/src/ATen/native/vulkan/api/QueryPool.cpp aten/src/ATen/native/vulkan/api/Resource.cpp aten/src/ATen/native/vulkan/api/Tensor.cpp aten/src/ATen/native/vulkan/impl/Packing.cpp aten/src/ATen/native/vulkan/ops/Batchnorm.cpp aten/src/ATen/native/vulkan/ops/BinaryOp.cpp aten/src/ATen/native/vulkan/ops/Convolution.cpp aten/src/ATen/native/vulkan/ops/Copy.cpp aten/src/ATen/native/vulkan/ops/Factory.cpp aten/src/ATen/native/vulkan/ops/Gru.cpp aten/src/ATen/native/vulkan/ops/Layernorm.cpp aten/src/ATen/native/vulkan/ops/Lstm.cpp aten/src/ATen/native/vulkan/ops/QuantizedTensor.cpp aten/src/ATen/native/vulkan/ops/Random.cpp aten/src/ATen/native/vulkan/ops/Shape.cpp aten/src/ATen/native/vulkan/ops/Upsample.cpp aten/src/ATen/native/vulkan/ops/Utils.cpp caffe2/CMakeLists.txt,https://github.com/pytorch/pytorch/pull/134550,cyyever,ezyang,,,
b1a00b7b6dc,cuda,not user facing,Abate `-Wsign-compare` warning spam in `Indexing.cu` (#134805),aten/src/ATen/native/cuda/Indexing.cu,https://github.com/pytorch/pytorch/pull/134805,eqy,janeyx99,,,
1595e755af8,skip,not user facing,[Reland] [Torchgen] Pass mutable to cpp.valuetype_type (#134549),torchgen/api/cpp.py torchgen/api/structured.py,https://github.com/pytorch/pytorch/pull/134549,cyyever,ezyang,,,
2dadc2c8fc8,inductor,not user facing,Log fx graph cache bypass reasons (#134792),torch/_inductor/codecache.py torch/_utils_internal.py,https://github.com/pytorch/pytorch/pull/134792,oulgen,jamesjwu,,,
ea01aec8b17,skip,not user facing,Move FunctionSchema implementations to cpp file (#133856),aten/src/ATen/core/function_schema.cpp aten/src/ATen/core/function_schema.h aten/src/ATen/core/function_schema_inl.h,https://github.com/pytorch/pytorch/pull/133856,ezyang,albanD,bdhirsh,,
208442ea189,nn_frontend,not user facing,Don't setup try-except handler when Dynamo compiling (#133239),test/dynamo/test_exceptions.py torch/nn/modules/module.py,https://github.com/pytorch/pytorch/pull/133239,ezyang,anijain2305,,,
0cbcef12bdc,dynamo,not user facing,"Stop adding useless prefix to error message here, you're pushing the important info off the screen. (#133108)",test/export/test_export.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/133108,ezyang,Skylion007,,,
c140fa14266,inductor,not user facing,Reorg cache code to make it simpler (#134911),torch/_inductor/runtime/autotune_cache.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/134911,aorenste,oulgen,,,
3cb5d251224,skip,not user facing,[Inductor] Apply loop split optimization in codegen_node (#132389),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/ir.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/132389,jiayisunx,jansel,leslie-fang-intel,,
de3a641476c,releng,not user facing,[executorch hash update] update the pinned executorch hash (#134914),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/134914,pytorchupdatebot,pytorchbot,,,
3daca187aa1,skip,Untopiced,[Inductor] Allow customizing the padding format (#133939),test/inductor/test_padding.py torch/_inductor/config.py torch/_inductor/graph.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/133939,blaine-rister,shunting314,,,
16de25b1dc3,skip,not user facing,fix tensor_repr(at::Tensor)  (#134762) (#134764),torch/csrc/utils.cpp,https://github.com/pytorch/pytorch/pull/134764,1274085042,ezyang,,,
ee03530fd90,inductor,not user facing,Add a test to avoid decorator based regression for cprofile traces (#133086),test/inductor/test_torchinductor.py torch/_inductor/config.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/133086,ezyang,aorenste,,,
76710d4f95d,linalg_frontend,docs,Corrected docstring of ``solve_triangular`` (#129766),torch/linalg/__init__.py,https://github.com/pytorch/pytorch/pull/129766,JonathanWenger,lezcano,,,
4c1dd13ba33,skip,not user facing,[BE] better type annotation for `torch.types` (#129559),torch/types.py,https://github.com/pytorch/pytorch/pull/129559,XuehaiPan,ezyang,,,
45f11094b6c,onnx,bc breaking,[ONNX] Delete `op_level_debug` from `torch.onnx.ExportOptions` (#134961),test/onnx/test_fx_to_onnx.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/fx/fx_onnx_interpreter.py torch/onnx/_internal/fx/op_validation.py torch/onnx/_internal/onnxruntime.py,https://github.com/pytorch/pytorch/pull/134961,titaiwangms,justinchuby,,,
a00fad01771,skip,not user facing,Add specializations for vectorized conversion between float and BF16/FP16 (#126500),aten/src/ATen/cpu/vec/vec256/vec256_convert.h aten/src/ATen/cpu/vec/vec512/vec512_convert.h,https://github.com/pytorch/pytorch/pull/126500,CaoE,jansel,jgong5,,
d14fe3ffedd,skip,not user facing,[Inductor][CPP] Turns on inline_inbuilt_nn_modules for CPP GEMM template testing (#132487),test/inductor/test_cpu_select_algorithm.py,https://github.com/pytorch/pytorch/pull/132487,leslie-fang-intel,anijain2305,jgong5,,
db193d1e29e,skip,not user facing,add msg to _assert_async (#134813),aten/src/ATen/native/cuda/TensorCompare.cu c10/macros/Macros.h,https://github.com/pytorch/pytorch/pull/134813,Chillee,albanD,eqy,ezyang,
6fce1faa107,skip,not user facing,change multinomial to use async asserts instead of a synchronization (#134818),aten/src/ATen/native/Distributions.cpp torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/134818,Chillee,ezyang,,,
9856bc50a25,skip,not user facing,Switch nanmedian to not cuda synchronize (#134819),aten/src/ATen/native/cuda/Sorting.cpp,https://github.com/pytorch/pytorch/pull/134819,Chillee,Skylion007,eqy,,
23a2161ad12,inductor,not user facing,Changed addmv to be a decomposition and not a fallback (#134823),torch/_inductor/decomposition.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/134823,Chillee,jansel,,,
39935e0fdef,skip,not user facing,Update cpuinfo submodule (#134891),third_party/cpuinfo,https://github.com/pytorch/pytorch/pull/134891,malfet,Skylion007,,,
2443507acc9,skip,not user facing,Update torch-xpu-ops pin (ATen XPU implementation) (#134983),third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/134983,fengyuan14,EikanWang,,,
2a49296d756,fx,not user facing,Fix set_unbacked_bindings when list of Tensors is returned (#133585),test/export/test_export.py torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/133585,ezyang,albanD,,,
33ba952e317,skip,not user facing,[subclasses] Do not fakeTensor const prop subclass args (#134855),test/functorch/test_aotdispatch.py torch/_subclasses/fake_tensor.py torch/testing/_internal/common_subclass.py,https://github.com/pytorch/pytorch/pull/134855,IvanKobzarev,zou3519,,,
6eed63c8b9c,skip,not user facing,[ONNX] Bump onnxscript version in CI; temporarily remove op test (#133748),.ci/docker/common/install_onnx.sh test/onnx/dynamo/test_registry_dispatcher.py test/onnx/test_fx_op_consistency.py test/onnx/test_fx_to_onnx.py test/onnx/test_fx_to_onnx_with_onnxruntime.py torch/onnx/_internal/_exporter_legacy.py,https://github.com/pytorch/pytorch/pull/133748,justinchuby,titaiwangms,,,
f927bcb934b,skip,Untopiced,"Revert ""[Inductor] Apply loop split optimization in codegen_node (#132389)""",test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/ir.py torch/_inductor/scheduler.py,,,,,,
a258844a322,skip,not user facing,Properly handle empty CPUINFO variable (#134916),cmake/Modules/FindARM.cmake,https://github.com/pytorch/pytorch/pull/134916,ezyang,Skylion007,,,
27677ead7c8,skip,Untopiced,"Revert ""[ONNX] Bump onnxscript version in CI; temporarily remove op test (#133748)""",.ci/docker/common/install_onnx.sh test/onnx/dynamo/test_registry_dispatcher.py test/onnx/test_fx_op_consistency.py test/onnx/test_fx_to_onnx.py test/onnx/test_fx_to_onnx_with_onnxruntime.py torch/onnx/_internal/_exporter_legacy.py,,,,,,
1b9f51bd881,skip,not user facing,[ONNX] Bump onnxscript version in CI; temporarily remove op test (#133748),.ci/docker/common/install_onnx.sh test/onnx/dynamo/test_registry_dispatcher.py test/onnx/test_fx_op_consistency.py test/onnx/test_fx_to_onnx.py test/onnx/test_fx_to_onnx_with_onnxruntime.py torch/onnx/_internal/_exporter_legacy.py,https://github.com/pytorch/pytorch/pull/133748,justinchuby,titaiwangms,,,
7804c089c63,releng,not user facing,[BE] Update numpy version to 2.0.2 (#134875),.ci/pytorch/build.sh,https://github.com/pytorch/pytorch/pull/134875,malfet,Skylion007,atalman,clee2000,
2e0b114c06f,skip,not user facing,add a new Guage API with an empty backend to PyTorch core (#134883),c10/util/Gauge.cpp c10/util/Gauge.h,https://github.com/pytorch/pytorch/pull/134883,haibchen,c-p-i-o,,,
6c3767452d1,skip,not user facing,Move auto functionalize tests in their own test file (#134834),test/dynamo/test_misc.py test/inductor/test_auto_functionalize.py,https://github.com/pytorch/pytorch/pull/134834,laithsakka,zou3519,,,
758d7879016,autograd_frontend,Untopiced,Added complex support for `torch.logsumexp` (#133187),aten/src/ATen/native/ReduceOps.cpp test/test_mps.py test/test_reductions.py tools/autograd/gen_variable_type.py torch/_refs/__init__.py torch/csrc/autograd/FunctionsManual.cpp torch/masked/_ops.py torch/testing/_internal/common_methods_invocations.py torch/testing/_internal/opinfo/definitions/_masked.py,https://github.com/pytorch/pytorch/pull/133187,tringwald,amjames,lezcano,,
71383dd3dae,mps,bug fixes,[MPS] Fix bachnorm_2d for channels last (#134618),aten/src/ATen/native/mps/operations/Normalization.mm torch/testing/_internal/common_modules.py,https://github.com/pytorch/pytorch/pull/134618,malfet,albanD,,,
e7731b3f8a4,distributed,Untopiced,[TorchElastic] make torch elastic not have to realize TCPStore backend type and rely on c10d to decide which backend to use (#134882),test/distributed/elastic/rendezvous/c10d_rendezvous_backend_test.py test/distributed/elastic/utils/distributed_test.py torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py torch/distributed/elastic/rendezvous/dynamic_rendezvous.py torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py torch/distributed/elastic/utils/distributed.py,https://github.com/pytorch/pytorch/pull/134882,XilunWu,shuqiangzhang,,,
272f3b9fe13,skip,not user facing,[FlexAttention] Update tolerance for failing test (#135035),test/inductor/test_flex_attention.py,https://github.com/pytorch/pytorch/pull/135035,drisspg,Chillee,,,
c40e622966a,inductor,not user facing,[inductor] add openmp config for intel conpiler on Linux. (#134973),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/134973,xuhancn,jansel,jgong5,,
14c8ef5198b,releng,not user facing,autolabel aotinductor->export (#135040),.github/label_to_label.yml,https://github.com/pytorch/pytorch/pull/135040,zou3519,ydwu4,,,
85fa0196971,nn_frontend,docs,[Docs] Fix call to deprecated function (#135037),torch/nn/functional.py,https://github.com/pytorch/pytorch/pull/135037,drisspg,janeyx99,jbschlosser,,
2fd36086bc5,skip,Untopiced,"Revert ""Add torch.serialization.skip_data context manager (#134504)""",docs/source/notes/serialization.rst test/test_cpp_extensions_open_device_registration.py test/test_serialization.py torch/_tensor.py torch/_utils.py torch/serialization.py torch/storage.py,,,,,,
c044deb9ce1,skip,Untopiced,"Revert ""c10d/logging: add C10D_LOCK_GUARD (#134131)""",build_variables.bzl caffe2/CMakeLists.txt test/cpp/c10d/CMakeLists.txt test/cpp/c10d/LoggingTest.cpp test/cpp/c10d/ProcessGroupNCCLErrorsTest.cpp torch/csrc/distributed/c10d/LockGuard.cpp torch/csrc/distributed/c10d/LockGuard.hpp torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupGloo.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/Work.cpp torch/csrc/distributed/c10d/Work.hpp,,,,,,
469429b959a,releng,not user facing,Refactor runner determinator (#134796),.github/scripts/runner_determinator.py .github/workflows/_runner-determinator.yml,https://github.com/pytorch/pytorch/pull/134796,ZainRizvi,PaliC,zxiiro,,
f05b716d6d7,releng,not user facing,Add validator to ensure runner determinator script is kept in sync (#134800),.github/workflows/_runner-determinator.yml .github/workflows/runner-determinator-validator.yml,https://github.com/pytorch/pytorch/pull/134800,ZainRizvi,PaliC,zxiiro,,
9ffcca70601,profiler,not user facing,[Profiler] Handle Tensor Sizes/Strides Parsing Error (#134862),torch/csrc/profiler/collection.cpp torch/csrc/profiler/collection.h torch/csrc/profiler/python/init.cpp,https://github.com/pytorch/pytorch/pull/134862,sraikund16,aaronenyeshi,,,
45743019cff,inductor,Untopiced,[PT2][Optimus] Skip meta update on symblic shape (#134975),torch/_inductor/fx_passes/group_batch_fusion.py,https://github.com/pytorch/pytorch/pull/134975,mengluy0125,xuzhao9,,,
e000cf0ad98,skip,not user facing,Fix license metadata in setup.py (#129219),setup.py,https://github.com/pytorch/pytorch/pull/129219,vizmo,kit1980,malfet,,
4ebf6b04a8b,skip,not user facing,Turn on expanded index path for Half on CPU (#133553),aten/src/ATen/native/TensorAdvancedIndexing.cpp aten/src/ATen/native/cpu/ScatterGatherKernel.cpp,https://github.com/pytorch/pytorch/pull/133553,CaoE,jgong5,peterbell10,yanbing-j,
15c25c45801,dynamo,bug fixes,Fix dim mismatch logic automatic dynamic not working with compiler collectives (#135025),test/distributed/test_dynamo_distributed.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/135025,ezyang,albanD,,,
175485097a2,inductor,not user facing,[EASY] Typofix (#135022),torch/_inductor/cudagraph_trees.py,https://github.com/pytorch/pytorch/pull/135022,ezyang,albanD,,,
ea89f012817,composability,not user facing,Remove unused comment (#135034),torch/_decomp/decompositions.py,https://github.com/pytorch/pytorch/pull/135034,bobrenjc93,albanD,,,
ae3aa8ff73f,inductor,not user facing,[AOTI][Tooling][5/n] Refactor the debug printer call to a level lower (#134789),test/inductor/test_aot_inductor.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cuda.py torch/_inductor/codegen/debug_utils.py torch/_inductor/codegen/simd.py,https://github.com/pytorch/pytorch/pull/134789,YUNQIUGUO,ColinPeppler,,,
982e27e532a,skip,not user facing,[halide-backend] Update CI pin (#130258),.ci/docker/ci_commit_pins/halide.txt,https://github.com/pytorch/pytorch/pull/130258,jansel,eellison,,,
7600e9b36fd,onnx,new features,[ONNX] Use the stable APIs in onnxscript and sync the latest logic (#134782),torch/onnx/_internal/_lazy_import.py torch/onnx/_internal/exporter/_analysis.py torch/onnx/_internal/exporter/_compat.py torch/onnx/_internal/exporter/_core.py torch/onnx/_internal/exporter/_dispatching.py torch/onnx/_internal/exporter/_onnx_program.py torch/onnx/_internal/exporter/_registration.py torch/onnx/_internal/exporter/_schemas.py torch/onnx/_internal/exporter/_verification.py torch/onnx/utils.py,https://github.com/pytorch/pytorch/pull/134782,justinchuby,titaiwangms,,,
362ecd9817c,inductor,not user facing,[inductor] Skip the sub-process pool until it's ready (#133508),torch/_inductor/async_compile.py,https://github.com/pytorch/pytorch/pull/133508,masnesral,Chillee,,,
367a78495f0,releng,Untopiced,Bump actions/download-artifact from 2 to 4.1.7 in /.github/workflows (#135068),.github/workflows/_binary-test-linux.yml .github/workflows/_binary-upload.yml .github/workflows/_ios-build-test.yml .github/workflows/build-triton-wheel.yml .github/workflows/create_release.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-nightly.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-windows-binary-conda-nightly.yml .github/workflows/generated-windows-binary-libtorch-debug-main.yml .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-main.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,,,,,,
9e6f4f3f772,dynamo,not user facing,[dynamo] Use __eq__ for backend match (#135039),test/dynamo/test_backends.py torch/__init__.py torch/csrc/dynamo/cache_entry.cpp torch/csrc/dynamo/extra_state.cpp torch/csrc/dynamo/extra_state.h,https://github.com/pytorch/pytorch/pull/135039,anijain2305,jansel,,,
c818ecd1698,skip,not user facing,Remove Caffe2 code from tool scripts (#134941),tools/amd_build/build_amd.py tools/build_pytorch_libs.py,https://github.com/pytorch/pytorch/pull/134941,cyyever,ezyang,,,
ffd1e214df6,distributed,Untopiced,"Back out ""[FSDP2] Set `ctx.set_materialize_grads(False)` for post-backward (#133498)"" (#135059)",torch/distributed/_composable/fsdp/_fsdp_param_group.py,https://github.com/pytorch/pytorch/pull/135059,ckluk2,awgu,,,
6b05aafc571,skip,not user facing,Add specializations for VecMaskLoad and VecMaskCast (#126501),aten/src/ATen/cpu/vec/vec256/vec256_mask.h aten/src/ATen/cpu/vec/vec512/vec512_mask.h aten/src/ATen/cpu/vec/vec_mask.h aten/src/ATen/cpu/vec/vec_n.h aten/src/ATen/test/vec_test_all_types.cpp aten/src/ATen/test/vec_test_all_types.h,https://github.com/pytorch/pytorch/pull/126501,CaoE,jansel,jgong5,,
2c9b4d2052f,releng,not user facing,[executorch hash update] update the pinned executorch hash (#135077),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/135077,pytorchupdatebot,pytorchbot,,,
6448d351db2,inductor,not user facing,[inductor] clean up cpp_builder code. (#134909),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/134909,xuhancn,henrylhtsang,,,
9809080b9ed,skip,not user facing,[Reland] Refactor caching device allocator utils (#130923),c10/core/CachingDeviceAllocator.h c10/cuda/CUDACachingAllocator.cpp c10/cuda/CUDACachingAllocator.h c10/cuda/CUDAMallocAsyncAllocator.cpp torch/csrc/cuda/CUDAPluggableAllocator.cpp torch/csrc/cuda/CUDAPluggableAllocator.h torch/csrc/cuda/Module.cpp,https://github.com/pytorch/pytorch/pull/130923,guangyey,EikanWang,albanD,eqy,
9f003179977,export,Untopiced,rationalize STATIC vs. None (#134877),test/export/test_export.py torch/_export/non_strict_utils.py torch/_logging/_registrations.py torch/export/_trace.py torch/export/dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/134877,avikchaudhuri,pianpwk,,,
1dfb1052395,skip,Untopiced,restore CSE'd node metadata in runtime asserts pass (#134516),test/distributed/test_dynamo_distributed.py torch/fx/passes/runtime_assert.py,https://github.com/pytorch/pytorch/pull/134516,pianpwk,ezyang,,,
679b8fe426b,skip,not user facing,Update generate-xnnpack-wrappers.py parsing to handle build identifier (#134724),third_party/generate-xnnpack-wrappers.py third_party/xnnpack.buck.bzl,https://github.com/pytorch/pytorch/pull/134724,GregoryComer,mcr229,,,
eec8fa038ed,cuda,not user facing,[fp8 rowwise] Support transposing operands in order to change output layout (#134773),aten/src/ATen/native/cuda/RowwiseScaledMM.cu,https://github.com/pytorch/pytorch/pull/134773,lw,drisspg,,,
c2ff9fe042f,cuda,not user facing,[fp8 rowwise] Retune the tile heuristics to increase perf (#134781),aten/src/ATen/native/cuda/RowwiseScaledMM.cu,https://github.com/pytorch/pytorch/pull/134781,lw,drisspg,eqy,,
80a6d60829f,releng,Untopiced,Moving _run_autocast_outofplace to basic class named TestAutocast to reduce redundance (#134460),test/test_autocast.py test/test_cuda.py test/test_xpu.py torch/testing/_internal/autocast_test_lists.py,https://github.com/pytorch/pytorch/pull/134460,FFFrog,EikanWang,ezyang,,
a8467c17c36,autograd_frontend,not user facing,Remove specific lazy initialization of PrivateUse1 (#135002),tools/autograd/templates/python_variable_methods.cpp torch/csrc/Storage.cpp,https://github.com/pytorch/pytorch/pull/135002,FFFrog,albanD,,,
dcf05fcb143,releng,not user facing,Fix stale job using non-existant ARC runner (#134863),.github/actionlint.yaml .github/workflows/stale.yml,https://github.com/pytorch/pytorch/pull/134863,zxiiro,ZainRizvi,,,
5690f003a6d,skip,not user facing,C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED and C10_DIAGNOST should be used in pairs (#135004),aten/src/ATen/detail/AcceleratorHooksInterface.h aten/src/ATen/detail/MPSHooksInterface.h aten/src/ATen/detail/XPUHooksInterface.h aten/src/ATen/native/mkl/LinearAlgebra.cpp c10/core/SymNodeImpl.h torch/csrc/profiler/stubs/itt.cpp,https://github.com/pytorch/pytorch/pull/135004,FFFrog,aaronenyeshi,,,
46cb2af7d82,fx,not user facing,Compute and do renamings even when ignoring fresh unbacked symbols (#134407),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/134407,ezyang,ydwu4,,,
a178a053ad2,skip,not user facing,Ignore fresh unbacked when doing recursive make_fx inside HOPs (#135053),test/export/test_export.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py,https://github.com/pytorch/pytorch/pull/135053,ezyang,ydwu4,,,
6c5669903f4,skip,not user facing,Fix Invalid NaN comparison due to infinity-zero multiply on latest sympy (#135044),test/test_dynamic_shapes.py torch/utils/_sympy/value_ranges.py,https://github.com/pytorch/pytorch/pull/135044,ezyang,zou3519,,,
67208f08bd5,releng,not user facing,[CD] Enable XPU nightly build on Windows (#134312),.circleci/scripts/binary_windows_build.sh .circleci/scripts/binary_windows_test.sh .github/scripts/generate_binary_build_matrix.py .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/134312,chuanqi129,atalman,,,
8bfd4916d64,skip,not user facing,fast path for sympy gcd in floordiv (#134880),test/test_sympy_utils.py torch/utils/_sympy/functions.py,https://github.com/pytorch/pytorch/pull/134880,avikchaudhuri,ezyang,,,
60dfe1b35ec,releng,not user facing,Fix lint after Bump actions/download-artifact update (#135109),.github/templates/common.yml.j2 .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/135109,atalman,ezyang,huydhn,,
195ac85fb6e,profiler,not user facing,[Profiler] Allow kwinputs to be non-string values (#134893),test/profiler/test_profiler.py torch/csrc/autograd/profiler_kineto.cpp torch/csrc/profiler/util.cpp torch/csrc/profiler/util.h,https://github.com/pytorch/pytorch/pull/134893,sraikund16,izaitsevfb,sanrise,,
c8ab9b06a28,inductor,Untopiced,Redesign custom op functionlaization for better re-inplace  (#134409),test/inductor/test_auto_functionalize.py test/inductor/test_inplacing_pass.py torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py torch/_functorch/partitioners.py torch/_higher_order_ops/auto_functionalize.py torch/_inductor/config.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/fx_passes/reinplace.py torch/_subclasses/functional_tensor.py torch/export/_remove_auto_functionalized_pass.py,https://github.com/pytorch/pytorch/pull/134409,laithsakka,zou3519,,,
fc07e6bf566,skip,Untopiced,"Revert ""Ignore fresh unbacked when doing recursive make_fx inside HOPs (#135053)""",test/export/test_export.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py,,,,,,
8759ed2ac50,skip,Untopiced,"Revert ""Compute and do renamings even when ignoring fresh unbacked symbols (#134407)""",torch/fx/experimental/symbolic_shapes.py,,,,,,
7858045491f,skip,Untopiced,"Revert ""Fix set_unbacked_bindings when list of Tensors is returned (#133585)""",test/export/test_export.py torch/fx/experimental/proxy_tensor.py,,,,,,
cff11582001,inductor,not user facing,"[inductor] Pass to fix device on index(..., [iota]) (#134748)",test/inductor/test_cuda_repro.py torch/_inductor/fx_passes/joint_graph.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/134748,jansel,shunting314,,,
2ddf3ed7075,inductor,not user facing,[inductor] Allow cudagraphs with unused CPU inputs (#134749),test/inductor/test_cuda_repro.py torch/_inductor/graph.py,https://github.com/pytorch/pytorch/pull/134749,jansel,shunting314,,,
193c547461c,inductor,not user facing,[inductor] Refactor simplify erase_nodes() (#134822),torch/_inductor/fx_passes/joint_graph.py torch/_inductor/fx_passes/mkldnn_fusion.py torch/_inductor/pattern_matcher.py,https://github.com/pytorch/pytorch/pull/134822,jansel,shunting314,,,
0d193a0adf0,mobile,Untopiced,Add ExecuTorch warning to mobile_optimizer (#134697),docs/source/mobile_optimizer.rst,https://github.com/pytorch/pytorch/pull/134697,svekars,ali-khosh,malfet,,
d7b57c4d63e,dynamo,Untopiced,Fix tensor.data access under inference_mode and compile (#134878),c10/core/TensorImpl.cpp test/dynamo/test_misc.py,https://github.com/pytorch/pytorch/pull/134878,zou3519,bdhirsh,,,
873abfc18e8,inductor,not user facing,[inductor] fix compile time regression due the (disabled) loop ordering after fusion (#135071),torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/135071,shunting314,jansel,,,
c88c19c6dee,skip,Untopiced,"Revert ""restore CSE'd node metadata in runtime asserts pass (#134516)""",test/distributed/test_dynamo_distributed.py torch/fx/passes/runtime_assert.py,,,,,,
eb0fd17bc45,profiler,not user facing,[Profiler] Fix Raw Metadata Iterator (#135096),torch/csrc/profiler/collection.cpp torch/csrc/profiler/collection.h,https://github.com/pytorch/pytorch/pull/135096,sraikund16,aaronenyeshi,,,
dd7cd182abf,distributed (checkpoint),bug fixes,[AIInfra][DCP] All gather keys checkpoint utils bug fix (#135045),torch/distributed/checkpoint/utils.py,https://github.com/pytorch/pytorch/pull/135045,saumishr,LucasLLC,MeetVadakkanchery,,
741d52c69fd,skip,Untopiced,"Revert ""Add support for 32KB multi_tensor_apply kernel arguments (#134373)""",aten/src/ATen/native/cuda/AmpKernels.cu aten/src/ATen/native/cuda/ForeachBinaryOpList.cu aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu aten/src/ATen/native/cuda/ForeachBinaryOpScalarTensor.cu aten/src/ATen/native/cuda/ForeachFunctors.cuh aten/src/ATen/native/cuda/ForeachPointwiseOp.cu aten/src/ATen/native/cuda/ForeachReduceOp.cu aten/src/ATen/native/cuda/ForeachTernaryOp.cu aten/src/ATen/native/cuda/ForeachUnaryOp.cu aten/src/ATen/native/cuda/FusedSgdKernel.cu aten/src/ATen/native/cuda/MultiTensorApply.cpp aten/src/ATen/native/cuda/MultiTensorApply.cuh aten/src/ATen/native/cuda/fused_adam_amsgrad_impl.cu aten/src/ATen/native/cuda/fused_adam_impl.cu aten/src/ATen/native/cuda/fused_adam_utils.cuh aten/src/ATen/native/cuda/fused_adamw_amsgrad_impl.cu aten/src/ATen/native/cuda/fused_adamw_impl.cu build_variables.bzl,,,,,,
09a339fc060,skip,not user facing,[Flex Attention] update __getitem__ without tree_map_only to support compile (#134627),test/inductor/test_flex_attention.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/134627,BoyuanFeng,Chillee,,,
a8611da86f4,dynamo,not user facing,[dynamo][backend match] Optimize backend match for common case (#135121),torch/csrc/dynamo/extra_state.cpp,https://github.com/pytorch/pytorch/pull/135121,anijain2305,williamwen42,,,
4e6df83d195,cpp_frontend,Untopiced,[PT] Add out variant for avg_pool1d and adaptive_avg_pool1d (#135051),aten/src/ATen/native/native_functions.yaml,https://github.com/pytorch/pytorch/pull/135051,manuelcandales,SS-JIA,,,
2276940f8cc,dynamo,Untopiced,Make Dynamo inline through torch._library.custom_ops.autograd (#135066),test/inductor/test_compiled_autograd.py torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/135066,zou3519,bdhirsh,williamwen42,yanboliang,
fb1c5808929,optimizer_frontend,improvements,[BE][optim] Make pyright recognize exported symbols (#135043),torch/optim/__init__.py,https://github.com/pytorch/pytorch/pull/135043,malfet,janeyx99,,,
ed06772e350,distributed,Untopiced,"[TorchElastic] add warning when users try to pass a ""use_libuv"" argument to create_c10d_store (#135062)",torch/distributed/elastic/utils/distributed.py,https://github.com/pytorch/pytorch/pull/135062,XilunWu,shuqiangzhang,,,
d9ae92cd6ea,dynamo,Untopiced,[Dynamo] Support for proxying frozen dataclasses (#134846),test/dynamo/test_misc.py torch/_dynamo/side_effects.py torch/_dynamo/testing.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/134846,mlazos,anijain2305,bdhirsh,,
87842cc6588,dynamo,not user facing,[dynamo][super] Corner case where the class is not present in the __mro__ (#135129),torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/135129,anijain2305,yanboliang,,,
13a4a0c60df,skip,not user facing,[Inductor] Apply loop split optimization in codegen_node (#132389),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/ir.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/132389,jiayisunx,jansel,leslie-fang-intel,,
804852c1f99,dynamo,not user facing,[dynamo] Search for _torchdynamo_inline only for functions (#135130),torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/135130,anijain2305,williamwen42,yanboliang,,
9810ce9ca71,distributed,not user facing,[PP] Go back to export instead of _export (#134299),torch/distributed/pipelining/_IR.py,https://github.com/pytorch/pytorch/pull/134299,kwen2501,lessw2020,,,
359077fa43f,skip,not user facing,[export] Fix indentation (#135128),torch/_export/__init__.py,https://github.com/pytorch/pytorch/pull/135128,yushangdi,tugsbayasgalan,,,
4f70b3cfae2,complex_frontend,not user facing,[CUDA][complex][TF32] Update `test_noncontiguous_samples` tolerances for `complex64` (#134526),torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/134526,eqy,ezyang,,,
00a86667087,onnx,bug fixes,[ONNX] Support output_names in dynamic_axes when dynamo=True (#135134),test/onnx/exporter/test_api.py torch/onnx/_internal/exporter/_compat.py,https://github.com/pytorch/pytorch/pull/135134,titaiwangms,justinchuby,,,
43c9b4e0e69,skip,not user facing,Fix unintentional deduplication of returned tensors (#134726),test/dynamo/test_repros.py torch/_functorch/compile_utils.py,https://github.com/pytorch/pytorch/pull/134726,benjaminglass1,amjames,bdhirsh,zou3519,
b3ef0c99f56,distributed,Untopiced,[PP] Fix zero bubble composability with DP (#134052),test/distributed/_composable/test_composability/test_pp_composability.py test/distributed/pipelining/test_backward.py torch/distributed/pipelining/_backward.py torch/distributed/pipelining/stage.py,https://github.com/pytorch/pytorch/pull/134052,H-Huang,kwen2501,,,
977a909250e,releng,not user facing,[CI] Build pytorch wheel with Torch XPU Operators on Windows (#133151),.ci/pytorch/win-test-helpers/build_pytorch.bat .ci/pytorch/win-test-helpers/installation-helpers/install_xpu.bat .github/workflows/_win-build.yml .github/workflows/xpu.yml,https://github.com/pytorch/pytorch/pull/133151,XinyueLunaFan,atalman,chuanqi129,,
956da79bda8,skip,not user facing,[CUDA][AMP] Fix autocast_dtype (#133938),aten/src/ATen/ThreadLocalState.cpp aten/src/ATen/ThreadLocalState.h test/test_autocast.py,https://github.com/pytorch/pytorch/pull/133938,Aidyn-A,soulitzer,,,
560f449d8ff,skip,not user facing,Fix: use clone_preserve_strides in auto_functionalized_v2 (#135142),test/inductor/test_auto_functionalize.py torch/_higher_order_ops/auto_functionalize.py,https://github.com/pytorch/pytorch/pull/135142,laithsakka,zou3519,,,
105ac2418c6,releng,not user facing,Fix binary builds artifact download (#135139),.github/templates/common.yml.j2 .github/templates/macos_binary_build_workflow.yml.j2 .github/workflows/_binary-build-linux.yml .github/workflows/generated-macos-arm64-binary-conda-nightly.yml .github/workflows/generated-macos-arm64-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-conda-nightly.yml .github/workflows/generated-windows-binary-libtorch-debug-main.yml .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-main.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/135139,malfet,atalman,huydhn,,
4a661e089a4,skip,not user facing,[FR] Add version based logic to FR script and make traces print can be filtered (#135154),tools/flight_recorder/components/builder.py tools/flight_recorder/components/config_manager.py tools/flight_recorder/components/loader.py tools/flight_recorder/components/utils.py tools/flight_recorder/fr_trace.py,https://github.com/pytorch/pytorch/pull/135154,fduwjj,wconstab,,,
32f45f01a9a,dynamo,not user facing,[dynamo] Retire CompileProfiler (#135133),docs/source/torch.compiler_faq.rst docs/source/torch.compiler_troubleshooting.rst test/distributed/test_dynamo_distributed.py test/dynamo/test_misc.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/135133,anijain2305,ezyang,,,
27d86f93fe9,skip,not user facing,Remove redundant code (#134955),aten/src/ATen/Context.h aten/src/ATen/EmptyTensor.cpp aten/src/ATen/detail/PrivateUse1HooksInterface.cpp aten/src/ATen/detail/PrivateUse1HooksInterface.h aten/src/ATen/native/Resize.cpp test/cpp_extensions/open_registration_extension.cpp,https://github.com/pytorch/pytorch/pull/134955,FFFrog,Skylion007,,,
a3e0d4bf079,skip,not user facing,[FlexAttention] Fix mismatched backward strides for eager impl (#135152),test/inductor/test_flex_attention.py torch/_higher_order_ops/flex_attention.py,https://github.com/pytorch/pytorch/pull/135152,drisspg,Chillee,,,
04e11c7eed7,releng,not user facing,Update current scripts used for setting up s390x runners (#129866),.github/scripts/s390x-ci/README.md .github/scripts/s390x-ci/self-hosted-builder/actions-runner.Dockerfile .github/scripts/s390x-ci/self-hosted-builder/actions-runner@.service .github/scripts/s390x-ci/self-hosted-builder/fs/usr/bin/actions-runner .github/scripts/s390x-ci/self-hosted-builder/helpers/app_token.sh .github/scripts/s390x-ci/self-hosted-builder/helpers/gh_token_generator.sh,https://github.com/pytorch/pytorch/pull/129866,AlekseiNikiforovIBM,huydhn,malfet,,
724faac2607,distributed,Untopiced,[FSDP] casting input args with dataclass(frozen=True) (#135067),test/distributed/fsdp/test_utils.py torch/distributed/utils.py,https://github.com/pytorch/pytorch/pull/135067,weifengpy,awgu,,,
30b98940b8c,skip,not user facing,Fix typo in comment (#135111),torch/_functorch/_aot_autograd/autograd_cache.py,https://github.com/pytorch/pytorch/pull/135111,bobrenjc93,aorenste,oulgen,,
a7a53b796b1,skip,not user facing,[Intel GPU]device guard codegen for XPU (#133980),torchgen/gen.py torchgen/model.py,https://github.com/pytorch/pytorch/pull/133980,ZhiweiYan-96,EikanWang,malfet,,
8fb1281db94,distributed,not user facing,"[Traceable FSDP2] Skip _backward_prefetch under compile, and rely on compiler pass to have prefetching (#135163)",torch/distributed/_composable/fsdp/_fsdp_param_group.py,https://github.com/pytorch/pytorch/pull/135163,yf225,awgu,,,
f2a7228aed1,releng,not user facing,[executorch hash update] update the pinned executorch hash (#135162),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/135162,pytorchupdatebot,pytorchbot,,,
7b280c31bac,export,Untopiced,"[export] dynamic_shapes serialization, load/dump (#134718)",test/export/test_export.py torch/_export/serde/dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/134718,pianpwk,avikchaudhuri,,,
05feb6e4ed0,inductor,not user facing,[Inductor] support masked vectorization for the tail_loop for dynamic shapes (#131745),aten/src/ATen/cpu/vec/vec512/vec512_bfloat16.h aten/src/ATen/cpu/vec/vec512/vec512_float.h aten/src/ATen/cpu/vec/vec_base.h aten/src/ATen/cpu/vec/vec_mask.h test/inductor/test_torchinductor.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/131745,jiayisunx,jansel,jgong5,leslie-fang-intel,
9d705605dda,skip,not user facing,Fix decomp behaviour in export training IR (#134801),test/export/test_experimental.py test/export/test_export.py torch/_export/utils.py torch/_subclasses/fake_impls.py torch/export/_trace.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/134801,tugsbayasgalan,angelayi,avikchaudhuri,,
731fd3172ac,inductor,not user facing,[inductor] [cpp] generate reindexer for each epilogue_node (#134984),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py,https://github.com/pytorch/pytorch/pull/134984,chunyuan-w,jgong5,,,
81a86242965,skip,not user facing,"[Intel GPU] Customized XPU behaviour in indexing, group norm (#134453)",aten/src/ATen/native/TensorAdvancedIndexing.cpp aten/src/ATen/native/group_norm.cpp,https://github.com/pytorch/pytorch/pull/134453,ZhiweiYan-96,EikanWang,albanD,,
5a0e7a408fd,skip,Untopiced,restore CSE'd node metadata in runtime asserts pass (#134516),test/distributed/test_dynamo_distributed.py torch/fx/passes/runtime_assert.py,https://github.com/pytorch/pytorch/pull/134516,pianpwk,ezyang,,,
eaeae0ac950,distributed,Untopiced,[c10d] Change collective to take in a list of tensors so it work fully for all collectives (#135049),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,https://github.com/pytorch/pytorch/pull/135049,fduwjj,kwen2501,,,
749dc6ceda2,inductor,not user facing,[inductor] [cpp] use_local_acc if template_buffer_has_other_users (#135081),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py,https://github.com/pytorch/pytorch/pull/135081,chunyuan-w,jgong5,,,
c7328dff7f7,skip,not user facing,Enhance the stability of the complex divide code (#134647),c10/util/complex.h,https://github.com/pytorch/pytorch/pull/134647,xytintel,EikanWang,albanD,,
8a5c8e5db96,skip,not user facing,Update unbacked symints in masked_select more precisely (#134899),test/export/test_export.py torch/_subclasses/fake_impls.py,https://github.com/pytorch/pytorch/pull/134899,dvorjackz,ezyang,,,
b99ef1a02e7,skip,not user facing,Update torch-xpu-ops pin (ATen XPU implementation) (#135185),third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/135185,fengyuan14,EikanWang,,,
50d1e370798,inductor,not user facing,[AOTI] Fix a unbacked symint retrieve bug (#134670),test/inductor/test_aot_inductor.py torch/_inductor/codegen/cpp_wrapper_cuda.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/134670,desertfire,22quinn,angelayi,chenyang78,
43dcb4bb611,dynamo,Untopiced,Revise CPU vectorization ISA support API (#135075),aten/src/ATen/cpu/Utils.cpp aten/src/ATen/cpu/Utils.h torch/_C/_cpu.pyi torch/_dynamo/trace_rules.py torch/_inductor/cpu_vec_isa.py torch/ao/quantization/qconfig.py torch/cpu/__init__.py torch/csrc/cpu/Module.cpp,https://github.com/pytorch/pytorch/pull/135075,sanchitintel,ezyang,jgong5,leslie-fang-intel,
58f2477a26e,dynamo,not user facing,[Dynamo] Support builtin function frozenset (#134563),test/dynamo/test_functions.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/dicts.py,https://github.com/pytorch/pytorch/pull/134563,xinyu-intel,EikanWang,anijain2305,jansel,
ecbd7153635,skip,not user facing,[Intel GPU][Windows] Fix overriding default CMAKE_CXX_FLAGS (#135093),cmake/Modules/FindMKLDNN.cmake,https://github.com/pytorch/pytorch/pull/135093,min-jean-cho,EikanWang,atalman,,
9d24f945baa,releng,not user facing,[CI] Use larger instance for building triton whl (#135201),.github/workflows/build-triton-wheel.yml,https://github.com/pytorch/pytorch/pull/135201,Stonepia,atalman,chuanqi129,,
2e2fb668fa6,releng,not user facing,Upgrade expecttest to 0.2.1 (#135136),.ci/docker/requirements-ci.txt .github/requirements/pip-requirements-macOS.txt .github/workflows/lint.yml .lintrunner.toml requirements.txt,https://github.com/pytorch/pytorch/pull/135136,ezyang,Skylion007,albanD,atalman,
0043dcd79eb,quantization,not user facing,Switch torch pt2e xnnpack tests to use export_for_training (#134788),test/quantization/pt2e/test_xnnpack_quantizer.py,https://github.com/pytorch/pytorch/pull/134788,tarun292,mergennachin,,,
2c99f17a32a,dynamo,not user facing,Implement VariableTracker.python_type() (#134215),torch/_dynamo/variables/base.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/tensor.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/134215,rec,amjames,jansel,,
bb3c2408f49,inductor,not user facing,"[inductor][test] in test_unbacked_symints, replace inductor's skipCUDAIf with common device type's skipcudaif (#133936)",test/inductor/test_torchinductor_dynamic_shapes.py test/inductor/test_unbacked_symints.py torch/testing/_internal/inductor_utils.py,https://github.com/pytorch/pytorch/pull/133936,henrylhtsang,ColinPeppler,desertfire,,
b1f72e29840,distributed,Untopiced,Gradient scaler for DTensor (#132816),test/distributed/_composable/fsdp/test_fully_shard_grad_scaler.py torch/distributed/_tensor/_dispatch.py torch/distributed/_tensor/ops/_pointwise_ops.py,https://github.com/pytorch/pytorch/pull/132816,mori360,XilunWu,wanchaol,weifengpy,
dbeb8a1691a,skip,not user facing,Render log filepaths that are not anchored in torch's directory in a reasonable way (#135165),torch/_logging/_internal.py,https://github.com/pytorch/pytorch/pull/135165,ezyang,Skylion007,,,
a096f2899d3,python_frontend,new features,Add torch.serialization.skip_data context manager (#134504),docs/source/notes/serialization.rst test/test_cpp_extensions_open_device_registration.py test/test_serialization.py torch/_tensor.py torch/_utils.py torch/serialization.py torch/storage.py,https://github.com/pytorch/pytorch/pull/134504,mikaylagawarecki,albanD,,,
1efd341d15c,skip,not user facing,[fake_tensor] Move unrecognized_type NotImplemented before ConstProp (#135033),torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/135033,IvanKobzarev,bdhirsh,zou3519,,
52c7c89ea48,inductor,not user facing,[Inductor][CPP] Leverage full bits for BF16/FP16 vectorization (#126502),aten/src/ATen/cpu/vec/vec256/vec256_float.h aten/src/ATen/cpu/vec/vec512/vec512_float.h test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/126502,CaoE,jansel,jgong5,,
be660ea2d3f,inductor,not user facing,[PT2] Directly set meta.val in group_batch_fusion_aten (#135078),torch/_inductor/fx_passes/pre_grad.py,https://github.com/pytorch/pytorch/pull/135078,huxintong,frank-wei,,,
098431a29d7,skip,not user facing,Update Resize.cpp with new device type (#135117),aten/src/ATen/native/Resize.cpp,https://github.com/pytorch/pytorch/pull/135117,hanzlfs,egienvalue,,,
82d00acfee9,composability,Untopiced,Allow cross-device copies for cpu scalars in refs (#135140),test/dynamo/test_misc.py torch/_prims_common/wrappers.py,https://github.com/pytorch/pytorch/pull/135140,zou3519,williamwen42,yanboliang,,
8efe5470461,releng,not user facing,Use actions/upload-artifact@v4.4.0 for triton builds (#135263),.github/workflows/build-triton-wheel.yml,https://github.com/pytorch/pytorch/pull/135263,atalman,huydhn,kit1980,,
9c38b00999a,skip,not user facing,[export] Add ability to run eagerly on UnflattenedModule (#133996),test/export/test_unflatten.py torch/export/unflatten.py,https://github.com/pytorch/pytorch/pull/133996,angelayi,pianpwk,,,
034717a0290,skip,not user facing,[ROCm] remove triton-rocm commit pin and merge pins with triton.txt (#133438),.ci/docker/centos-rocm/Dockerfile .ci/docker/ci_commit_pins/triton-rocm.txt .ci/docker/ci_commit_pins/triton.txt .ci/docker/common/install_triton.sh .ci/docker/ubuntu-rocm/Dockerfile .circleci/scripts/binary_populate_env.sh .github/scripts/build_triton_wheel.py .github/workflows/build-triton-wheel.yml CODEOWNERS,https://github.com/pytorch/pytorch/pull/133438,jataylo,jithunnair-amd,malfet,,
9c0b03020b7,skip,not user facing,Use actions/upload-artifact@v4.4.0 for rest of workflows (#135264),.github/actions/upload-test-artifacts/action.yml .github/workflows/_ios-build-test.yml .github/workflows/_mac-build.yml .github/workflows/_rocm-test.yml .github/workflows/_xpu-test.yml .github/workflows/scorecards.yml .github/workflows/target_determination.yml,https://github.com/pytorch/pytorch/pull/135264,atalman,kit1980,malfet,,
a4cf9653eec,skip,Untopiced,"Revert ""Remove Caffe2 code from tool scripts (#134941)""",tools/amd_build/build_amd.py tools/build_pytorch_libs.py,,,,,,
e55c0f59e55,skip,Untopiced,"Revert ""[Reland] Refactor caching device allocator utils (#130923)""",c10/core/CachingDeviceAllocator.h c10/cuda/CUDACachingAllocator.cpp c10/cuda/CUDACachingAllocator.h c10/cuda/CUDAMallocAsyncAllocator.cpp torch/csrc/cuda/CUDAPluggableAllocator.cpp torch/csrc/cuda/CUDAPluggableAllocator.h torch/csrc/cuda/Module.cpp,,,,,,
bc5ecf83d7c,quantization,not user facing,[training ir migration] Fix quantization tests (#135184),test/quantization/pt2e/test_quantize_pt2e.py,https://github.com/pytorch/pytorch/pull/135184,yushangdi,tugsbayasgalan,,,
e4920a13648,dynamo,not user facing,[Traceable FSDP2][Dynamo] allow tracing through auto_functionalized HOP (#135169),test/inductor/test_compiled_autograd.py torch/_dynamo/variables/higher_order_ops.py,https://github.com/pytorch/pytorch/pull/135169,yf225,zou3519,,,
24a223c49d8,releng,Untopiced,Run inductor micro benchmark on x86 metal runner (#135042),.ci/pytorch/test.sh .github/pytorch-probot.yml .github/workflows/inductor-micro-benchmark-x86.yml .github/workflows/upload-test-stats.yml benchmarks/gpt_fast/benchmark.py benchmarks/gpt_fast/generate.py,https://github.com/pytorch/pytorch/pull/135042,huydhn,yanboliang,,,
38fead8f7c4,skip,not user facing,[hop] preserve metadata in re-tracing hop subgraph by running with interpreter (#135159),test/export/test_export.py torch/_higher_order_ops/associative_scan.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/map.py torch/_higher_order_ops/while_loop.py,https://github.com/pytorch/pytorch/pull/135159,ydwu4,tugsbayasgalan,,,
f63571060ca,skip,Untopiced,"Revert ""Use actions/upload-artifact@v4.4.0 for rest of workflows (#135264)""",.github/actions/upload-test-artifacts/action.yml .github/workflows/_ios-build-test.yml .github/workflows/_mac-build.yml .github/workflows/_rocm-test.yml .github/workflows/_xpu-test.yml .github/workflows/scorecards.yml .github/workflows/target_determination.yml,,,,,,
7fe819d9171,skip,not user facing,[PyTorch] Fix -Wshadow -Werror build in BFloat16-inl.h (#135031),c10/util/BFloat16-math.h,https://github.com/pytorch/pytorch/pull/135031,swolchok,albanD,,,
a5d70cf5451,skip,not user facing,[PyTorch] Add isfinite to BFloat16-math.h (#135052),c10/util/BFloat16-math.h,https://github.com/pytorch/pytorch/pull/135052,swolchok,PaliC,albanD,,
116fd474da0,skip,not user facing,[export] Expand coverage to more copied sym ops for unflattener. (#135119),torch/export/unflatten.py,https://github.com/pytorch/pytorch/pull/135119,zhxchen17,yushangdi,,,
6c5920d5158,inductor,not user facing,Tune int8 AMX WoQ micro-kernel for CPU (#134832),torch/_inductor/codegen/cpp_micro_gemm.py,https://github.com/pytorch/pytorch/pull/134832,sanchitintel,jgong5,,,
058a69d91a3,dynamo,not user facing,[fbcode][dynamo] Turn on guard_nn_modules using justknobs_check (#134928),torch/_dynamo/config.py torch/_dynamo/guards.py,https://github.com/pytorch/pytorch/pull/134928,anijain2305,ezyang,,,
fc890b55b51,skip,not user facing,Support rolling over a percentage of workflows (#134816),.github/scripts/runner_determinator.py .github/scripts/test_runner_determinator.py .github/workflows/_runner-determinator.yml,https://github.com/pytorch/pytorch/pull/134816,ZainRizvi,PaliC,zxiiro,,
3c8f71ff93f,skip,not user facing,[cuDNN][64-bit indexing] cuDNN v9.3+ supports non-batch-splittable convolutions with > 2**31 elements (#134890),aten/src/ATen/native/Convolution.cpp,https://github.com/pytorch/pytorch/pull/134890,eqy,Skylion007,,,
38256071446,releng,not user facing,Add torch._logging.scribe (#135224),.ci/docker/requirements-ci.txt .github/requirements/pip-requirements-macOS.txt .github/workflows/lint.yml torch/_logging/scribe.py,https://github.com/pytorch/pytorch/pull/135224,ezyang,Skylion007,,,
4262755b5a6,inductor,not user facing,[cond] fix typo in cond codegen (#134708),torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/134708,ydwu4,jansel,,,
830247c3553,releng,not user facing,[Intel Triton] Update Intel Triton to release/2.5.0 (#134074),.ci/docker/ci_commit_pins/triton-xpu.txt,https://github.com/pytorch/pytorch/pull/134074,Stonepia,EikanWang,,,
65e1c340615,distributed,Untopiced,[rfc] scuba for flight recorder (#134794),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,https://github.com/pytorch/pytorch/pull/134794,c-p-i-o,fduwjj,,,
43f4947d445,skip,not user facing,fix fake tensor tolist implementation (#135131),test/dynamo/test_subclasses.py test/export/test_export.py test/test_nestedtensor.py torch/_subclasses/fake_tensor.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/135131,avikchaudhuri,ezyang,,,
144fde4fd20,mps,improvements,[MPS] Add support for autocast in MPS  (#99272),aten/src/ATen/autocast_mode.cpp aten/src/ATen/autocast_mode.h aten/src/ATen/core/interned_strings.h c10/core/DispatchKey.cpp c10/core/DispatchKey.h c10/core/DispatchKeySet.h test/test_autocast.py test/test_mps.py torch/amp/autocast_mode.py torch/csrc/jit/passes/autocast.cpp torch/csrc/utils/python_dispatch.cpp,https://github.com/pytorch/pytorch/pull/99272,kulinseth,malfet,,,
8f66995459d,skip,Untopiced,"Revert ""Support rolling over a percentage of workflows (#134816)""",.github/scripts/runner_determinator.py .github/scripts/test_runner_determinator.py .github/workflows/_runner-determinator.yml,,,,,,
ea231300d11,inductor,Untopiced,[inductor] Improve compile time regression from MemoryDep.normalize (#135070),torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/135070,jansel,Chillee,,,
70779dded83,fx,Untopiced,[fx] Compile time optimization in Node.__update_args_kwargs (#135076),torch/fx/node.py,https://github.com/pytorch/pytorch/pull/135076,jansel,Chillee,,,
bdfc8d9f964,fx,Untopiced,[fx] Don't use generators in map_aggregate (#135082),torch/fx/node.py,https://github.com/pytorch/pytorch/pull/135082,jansel,oulgen,,,
b2386bdca1a,dynamo,not user facing,[debug] Add helper to run cProfile on a function (#135084),torch/_dynamo/debug_utils.py,https://github.com/pytorch/pytorch/pull/135084,jansel,oulgen,,,
28ccfba2483,onnx,bc breaking,[ONNX] Delete ONNXProgramSerializer (#135261),docs/source/onnx_dynamo.rst test/onnx/dynamo/test_exporter_api.py torch/onnx/__init__.py torch/onnx/_internal/_exporter_legacy.py,https://github.com/pytorch/pytorch/pull/135261,titaiwangms,justinchuby,,,
c83cdf068bf,distributed,not user facing,[DTensor] Fix view op replicating on tensor dim when the size of the tensor dim = 1 (#135054),test/distributed/_tensor/test_view_ops.py torch/distributed/_tensor/ops/_view_ops.py,https://github.com/pytorch/pytorch/pull/135054,wz337,tianyu-l,wanchaol,,
cc28634172b,skip,not user facing,[Submodule] Bump pybind11 to v2.13.5 (#135202),third_party/pybind11,https://github.com/pytorch/pytorch/pull/135202,cyyever,Skylion007,,,
66dd4577b13,skip,not user facing,Track base of FunctionalTensor in inference mode.  (#135141),aten/src/ATen/FunctionalTensorWrapper.cpp aten/src/ATen/FunctionalTensorWrapper.h test/inductor/test_auto_functionalize.py tools/pyi/gen_pyi.py torch/_higher_order_ops/auto_functionalize.py torch/_subclasses/functional_tensor.py torch/csrc/autograd/python_torch_functions_manual.cpp,https://github.com/pytorch/pytorch/pull/135141,laithsakka,zou3519,,,
62b221d5cc3,skip,not user facing,Add Percentages to Function Events (#135155),torch/autograd/profiler_util.py,https://github.com/pytorch/pytorch/pull/135155,sraikund16,kit1980,shanw-meta,,
0b96dfb736c,inductor,not user facing,[AOTI] Support MKLDNN conv ops in cpp wrapper (#134475),test/inductor/test_cpu_cpp_wrapper.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/mkldnn_ir.py,https://github.com/pytorch/pytorch/pull/134475,desertfire,angelayi,chunyuan-w,leslie-fang-intel,
614b86d6021,inductor,not user facing,[AOTI] Support MKLDNN qlinear ops in cpp wrapper (#134783),test/inductor/test_cpu_cpp_wrapper.py test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/mkldnn_ir.py,https://github.com/pytorch/pytorch/pull/134783,desertfire,angelayi,chunyuan-w,leslie-fang-intel,
1e57ef08fa2,inductor,not user facing,[AOTI] Support MKLDNN qconv ops in cpp wrapper (#134795),test/inductor/test_cpu_cpp_wrapper.py torch/_inductor/mkldnn_ir.py,https://github.com/pytorch/pytorch/pull/134795,desertfire,angelayi,chunyuan-w,leslie-fang-intel,
8f6e73f068e,onnx,new features,[ONNX] Enable experimental exporter logic to dynamo_export and support refine dynamic_shapes (#134976),test/onnx/exporter/test_api.py test/onnx/pytorch_test_common.py test/onnx/test_fx_to_onnx.py test/onnx/test_fx_to_onnx_decomp_skip.py test/onnx/test_fx_to_onnx_with_onnxruntime.py test/onnx/torch_export/test_torch_export_with_onnxruntime.py torch/onnx/__init__.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/exporter/_capture_strategies.py torch/onnx/_internal/exporter/_compat.py torch/onnx/_internal/fx/_pass.py torch/onnx/_internal/fx/torch_export_graph_extractor.py,https://github.com/pytorch/pytorch/pull/134976,titaiwangms,justinchuby,,,
96880148204,linalg_frontend,Untopiced,aarch64: extend matmul heuristic checks to all neoverse platforms (#134548),aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/mkldnn/Utils.h,https://github.com/pytorch/pytorch/pull/134548,snadampal,malfet,,,
b46a1b9e2d0,releng,not user facing,Use Python 3.9 on all libtorch jobs (#135245),.github/templates/upload.yml.j2 .github/workflows/generated-macos-arm64-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-windows-binary-libtorch-debug-main.yml .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-main.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml,https://github.com/pytorch/pytorch/pull/135245,atalman,izaitsevfb,,,
9e5a797771a,skip,not user facing,Improve test_public_bindings import module error reporting (#135258),test/test_public_bindings.py,https://github.com/pytorch/pytorch/pull/135258,ezyang,albanD,,,
e162414963c,skip,not user facing,add instrumentation of CCA stats for reserved and allocated memory size (#135231),c10/cuda/CUDACachingAllocator.cpp,https://github.com/pytorch/pytorch/pull/135231,haibchen,c-p-i-o,,,
ad01fc194d3,dynamo,not user facing,Consolidate raise and rewrap raise error branches (#135148),torch/_dynamo/convert_frame.py,https://github.com/pytorch/pytorch/pull/135148,ezyang,albanD,anijain2305,malfet,
d8543e31629,dynamo,not user facing,Include exception type qualname when rewrapping InternalTorchDynamoError (#135145),test/dynamo/test_exc.py torch/_dynamo/convert_frame.py,https://github.com/pytorch/pytorch/pull/135145,ezyang,anijain2305,drisspg,,
d9a18173fab,dynamo,not user facing,Report qualname of exception type rather than <class 'RuntimeError'> (#135146),torch/_dynamo/convert_frame.py,https://github.com/pytorch/pytorch/pull/135146,ezyang,Skylion007,albanD,yanboliang,
06a7dc21c10,fx,Untopiced,Remove dead expect_rational (#135105),torch/fx/experimental/sym_node.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/135105,ezyang,malfet,,,
07689a38bf6,inductor,not user facing,[Inductor] Fix AOT weight alignment issue on CPU (#135205),benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_aot_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_aot_inductor_freezing_torchbench_inference.csv torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/135205,leslie-fang-intel,desertfire,jgong5,,
3b1a334c0f2,inductor,not user facing,[Inductor][CPP] Avoid mistake wgt tensor delete (#135100),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py,https://github.com/pytorch/pytorch/pull/135100,leslie-fang-intel,jgong5,,,
29d72c1100f,inductor,not user facing,[inductor] check intel compiler minimal version (#135209),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/135209,xuhancn,ezyang,,,
5f57be75713,distributed,not user facing,[Distributed] Change function call in test to non-deprecated to eliminate warning (#134938),test/distributed/checkpoint/test_fsdp_model_state.py test/distributed/checkpoint/test_fsdp_tp_checkpoint_conversion.py test/distributed/checkpoint/test_hsdp_checkpoint.py,https://github.com/pytorch/pytorch/pull/134938,zeshengzong,fegin,wz337,,
c05a7adb36d,inductor,not user facing,[inductor][debug] fix draw_buffers (#135266),torch/_inductor/debug.py,https://github.com/pytorch/pytorch/pull/135266,xuanzhang816,yf225,,,
e40a0a93597,skip,not user facing,Add randomness checking for sdpa vmap (#135176),aten/src/ATen/functorch/BatchRulesDecompositions.cpp aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp test/functorch/test_vmap.py test/functorch/test_vmap_registrations.py,https://github.com/pytorch/pytorch/pull/135176,Chillee,zou3519,,,
41e653456e4,inductor,not user facing,"[RDP] Fix ""No module named 'libfb’"" (#135244)",torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/135244,laithsakka,aorenste,,,
66da3b3b2ac,fx,Untopiced,[fx] Bypass custom __setattr__ in Node.__init__ (#135079),torch/fx/graph.py torch/fx/node.py,https://github.com/pytorch/pytorch/pull/135079,jansel,oulgen,,,
f946bf88c46,inductor,not user facing,[inductor] Skip retracing an existing LoopBody (#135235),torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/135235,jansel,oulgen,,,
7ffb3b201c8,inductor,not user facing,"[inductor] Remove LoopBody.reads,writes,other (#135256)",torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/135256,jansel,oulgen,,,
e020a8755a7,skip,not user facing,[Fix][FR][ez] Remove debugging logs (#135308),tools/flight_recorder/components/builder.py,https://github.com/pytorch/pytorch/pull/135308,fduwjj,wz337,,,
67f98a99a47,distributed,not user facing,[DeviceMesh][Easy] Make RuntimeError a bit more descriptive by including the actual world_size (#135271),torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/135271,wz337,fduwjj,,,
764ee6e3f94,inductor,not user facing,[FlexAttention] Specify padding_value for boundary checked loads (#134573),torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/134573,drisspg,Chillee,,,
590a3e9f8a0,quantization,Untopiced,[export][training ir migration] quantized_decomposed.quantize_per_tensor decomposition (#134525),test/quantization/pt2e/test_xnnpack_quantizer.py torch/testing/_internal/common_quantization.py,https://github.com/pytorch/pytorch/pull/134525,yushangdi,jerryzh168,tugsbayasgalan,,
60d98b4cfbc,skip,not user facing,Update torch-xpu-ops pin (ATen XPU implementation) (#135300),third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/135300,fengyuan14,EikanWang,,,
758d515d982,inductor,not user facing,[Inductor][CPP] Select tiling factor for lower precision data types (#133830),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/133830,CaoE,jansel,jgong5,,
217ba7b2abf,skip,not user facing,[Docs] Update FileCheck doc (#135199),test/HowToWriteTestsUsingFileCheck.md,https://github.com/pytorch/pytorch/pull/135199,penguin-wwy,soulitzer,,,
67c7924ea14,inductor,not user facing,[inductor] Fix gen_transposed_tile_load_store (#135307),torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/135307,jiayisunx,jansel,jgong5,,
041960a1ce5,dynamo,Untopiced,[Dynamo] Automatically in-graph traceable tensor subclass ctors (#135151),test/dynamo/test_subclasses.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/user_defined.py torch/utils/_python_dispatch.py,https://github.com/pytorch/pytorch/pull/135151,mlazos,bdhirsh,,,
b5dea061c8d,skip,not user facing,check compilation status before query cudnn version in conv (#135332),aten/src/ATen/native/Convolution.cpp,https://github.com/pytorch/pytorch/pull/135332,ZhiweiYan-96,EikanWang,atalman,,
d0591f46582,skip,not user facing,Ignore fresh unbacked when doing recursive make_fx inside HOPs (#135053),test/export/test_export.py test/inductor/test_torchinductor_dynamic_shapes.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py torch/fx/experimental/proxy_tensor.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/135053,ezyang,ydwu4,,,
d6b9bd3e60a,dynamo,bug fixes,Also handle compiler collective when input variable doesn't exist on all ranks (#135147),test/distributed/test_dynamo_distributed.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/135147,ezyang,jansel,,,
4ef6c05f65f,inductor,not user facing,[inductor][cpp][gemm] fix autotune runtime error from linear_binary fusion (#135275),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py,https://github.com/pytorch/pytorch/pull/135275,jgong5,leslie-fang-intel,,,
13bae39e226,inductor,not user facing,[inductor] [cpp] improve cache blocking for is_dynamic_M (#131306),torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_prefix.h,https://github.com/pytorch/pytorch/pull/131306,chunyuan-w,jgong5,leslie-fang-intel,,
60a097a0715,releng,not user facing,[CD] Update binary_linux_test.sh to include calling builder smoke test (#133869),.circleci/scripts/binary_linux_test.sh,https://github.com/pytorch/pytorch/pull/133869,juliagmt-google,atalman,,,
84ae6b7d6b8,distributed,not user facing,AOTDispatcher: limit cases when we detach() graph inputs to non-leaves (#134193),test/distributed/test_inductor_collectives.py test/inductor/test_distributed_patterns.py test/inductor/test_torchinductor.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py torch/testing/_internal/optests/aot_autograd.py,https://github.com/pytorch/pytorch/pull/134193,bdhirsh,yf225,,,
a086882d72c,inductor,bug fixes,[inductor][triton] mark workspace args as mutated (#134648),test/inductor/test_torchinductor.py torch/_inductor/codegen/triton.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/134648,davidberard98,jansel,peterbell10,,
3a9e33dca8b,distributed,Untopiced,[torchelastic] Don't do signal handling when off the main thread (#135088),test/distributed/elastic/multiprocessing/api_test.py torch/distributed/elastic/multiprocessing/api.py,https://github.com/pytorch/pytorch/pull/135088,evenlion,d4l3k,,,
ad29a2c0dc8,inductor,Untopiced,Add Inductor config for default stride behavior (#135238),aten/src/ATen/native/tags.yaml test/test_custom_ops.py torch/_inductor/config.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/135238,zou3519,albanD,,,
de74aafff46,export,Untopiced,error on exporting ScriptModule (#135302),test/export/test_export.py torch/export/__init__.py,https://github.com/pytorch/pytorch/pull/135302,avikchaudhuri,yushangdi,,,
771dcce11d7,skip,not user facing,[AOTI][Tooling][6/n] Fix long dtype input tensors calling `mean()` in `aoti_torch_print_tensor_handle` (#135072),test/inductor/test_aot_inductor.py torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/135072,YUNQIUGUO,ColinPeppler,hl475,,
7074de43c01,skip,not user facing,Porting to GCC 15 (#135188),caffe2/utils/string_utils.cc,https://github.com/pytorch/pytorch/pull/135188,atupone,Skylion007,,,
c0ec599f274,skip,not user facing,Update submodule ideep to include aarch64 change (#134897),third_party/ideep,https://github.com/pytorch/pytorch/pull/134897,yanbing-j,atalman,jgong5,snadampal,
24482e5c68c,fx,Untopiced,[torch][fx] Set maximum warning count during fx.Graph.lint (#135069),torch/fx/graph.py,https://github.com/pytorch/pytorch/pull/135069,dulinriley,yushangdi,,,
04118d86172,skip,not user facing,[export] Record the global torch version in serialization. (#135243),torch/_export/serde/schema.py torch/_export/serde/schema.yaml torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/135243,zhxchen17,yushangdi,,,
3988b3468bf,inductor,not user facing,[aoti][easy] remove breakpoint() in wrapper.py (#134807),torch/_inductor/codegen/wrapper.py,https://github.com/pytorch/pytorch/pull/134807,henrylhtsang,YUNQIUGUO,,,
177e4f4218d,composability,Untopiced,remove _check call on item() for torch.istft (#135234),test/export/test_export.py torch/_refs/__init__.py,https://github.com/pytorch/pytorch/pull/135234,pianpwk,tugsbayasgalan,,,
196748d4919,distributed,Untopiced,[elastic] support local_addr across all rendezvous impls (#135262),test/distributed/elastic/rendezvous/c10d_rendezvous_backend_test.py test/distributed/elastic/rendezvous/dynamic_rendezvous_test.py test/distributed/elastic/timer/file_based_local_timer_test.py torch/distributed/elastic/rendezvous/api.py torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py torch/distributed/elastic/rendezvous/dynamic_rendezvous.py torch/distributed/elastic/rendezvous/etcd_rendezvous.py torch/distributed/elastic/timer/file_based_local_timer.py,https://github.com/pytorch/pytorch/pull/135262,d4l3k,fduwjj,wz337,,
8520ce5f782,dynamo,not user facing,Fix incorrect trace of post-accumulate grad hook on tensor with zero dims (#135226),torch/_dynamo/compiled_autograd.py,https://github.com/pytorch/pytorch/pull/135226,wdziurdz,xmfan,,,
13ba0a2e5c7,inductor,not user facing,Run bypassed graph compile outside the except block to avoid chaining of exceptions (#135175),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/135175,oulgen,ezyang,masnesral,,
b143426db39,inductor,not user facing,[Inductor] Use argument names as the key for the `constants` dict and the `signature` dict (#135170),torch/_higher_order_ops/triton_kernel_wrap.py,https://github.com/pytorch/pytorch/pull/135170,Jokeren,htyu,,,
a15aabc975c,sparse_frontend,new features,"Add MaskedTensor passthrough: unfold, F.Unfold, F.Fold, stack (#125262)",aten/src/ATen/native/Col2Im.cpp aten/src/ATen/native/Im2Col.cpp aten/src/ATen/native/cuda/Col2Im.cu aten/src/ATen/native/cuda/Im2Col.cu docs/source/masked.rst test/test_maskedtensor.py torch/masked/maskedtensor/passthrough.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/125262,nowtryz,cpuhrsch,,,
5eebd9315a7,onnx,bc breaking,[ONNX] Refactor exporter errors (#135180),docs/source/onnx.rst docs/source/onnx_dynamo.rst test/onnx/dynamo/test_exporter_api.py test/onnx/onnx_test_common.py test/onnx/test_fx_to_onnx.py test/onnx/test_pytorch_onnx_no_runtime.py test/onnx/test_pytorch_onnx_onnxruntime.py torch/onnx/__init__.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/exporter/_building.py torch/onnx/_internal/exporter/_core.py torch/onnx/_internal/exporter/errors.py torch/onnx/errors.py torch/onnx/utils.py,https://github.com/pytorch/pytorch/pull/135180,justinchuby,titaiwangms,,,
32fd29c1ead,onnx,bug fixes,[ONNX] Properly handle Attributes in traceable functions (#135367),torch/onnx/_internal/exporter/_building.py,https://github.com/pytorch/pytorch/pull/135367,titaiwangms,justinchuby,,,
7f2d20e6876,autograd_frontend,Untopiced,Run all autograd node post hooks (#134728),test/inductor/test_compiled_autograd.py test/test_autograd.py torch/autograd/graph.py torch/csrc/autograd/engine.cpp,https://github.com/pytorch/pytorch/pull/134728,janeyx99,albanD,soulitzer,,
3ce433aef24,distributed,Untopiced,[TCPStore] use wait counters (#135283),torch/csrc/distributed/c10d/TCPStore.cpp torch/csrc/distributed/c10d/TCPStore.hpp torch/csrc/distributed/c10d/init.cpp,https://github.com/pytorch/pytorch/pull/135283,d4l3k,c-p-i-o,,,
2a4890e3153,onnx,not user facing,[ONNX] Clean up the missed lines from previous PRs (#135368),test/onnx/test_fx_to_onnx.py torch/onnx/_internal/_exporter_legacy.py,https://github.com/pytorch/pytorch/pull/135368,titaiwangms,justinchuby,,,
22e1fb6faa4,skip,not user facing,[test][easy] Add debug utils for cpu select algorithm test (#135038),test/inductor/test_cpu_select_algorithm.py,https://github.com/pytorch/pytorch/pull/135038,henrylhtsang,XuehaiPan,jgong5,,
a4030e37bef,dynamo,not user facing,[dynamo] reland map/zip iterator related changes (#135074),test/dynamo/test_functions.py test/dynamo/test_repros.py torch/_dynamo/symbolic_convert.py torch/_dynamo/utils.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/135074,williamwen42,anijain2305,jansel,mlazos,
a7643baceb6,skip,not user facing,Revert expectFailureIf condition on tests with torch.compile on Windows (#134759),test/functorch/test_eager_transforms.py,https://github.com/pytorch/pytorch/pull/134759,vadym-janea,malfet,,,
306ac44eaa5,skip,not user facing,[ez][TD] Fix request for issue body returns None (#135389),tools/testing/target_determination/heuristics/utils.py,https://github.com/pytorch/pytorch/pull/135389,clee2000,malfet,,,
95e976a63f0,dynamo,not user facing,[dynamo] recursively skip frames when Dynamo cache limit is hit (#135144),test/dynamo/test_modules.py test/dynamo/test_recompiles.py torch/_C/_dynamo/eval_frame.pyi torch/_dynamo/config.py torch/_dynamo/convert_frame.py torch/_dynamo/exc.py torch/csrc/dynamo/eval_frame.c torch/csrc/dynamo/extra_state.cpp torch/csrc/dynamo/extra_state.h,https://github.com/pytorch/pytorch/pull/135144,williamwen42,anijain2305,jansel,,
a681260cafe,skip,Untopiced,"Revert ""[ONNX] Refactor exporter errors (#135180)""",docs/source/onnx.rst docs/source/onnx_dynamo.rst test/onnx/dynamo/test_exporter_api.py test/onnx/onnx_test_common.py test/onnx/test_fx_to_onnx.py test/onnx/test_pytorch_onnx_no_runtime.py test/onnx/test_pytorch_onnx_onnxruntime.py torch/onnx/__init__.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/exporter/_building.py torch/onnx/_internal/exporter/_core.py torch/onnx/_internal/exporter/errors.py torch/onnx/errors.py torch/onnx/utils.py,,,,,,
06e414d7fe9,skip,not user facing,[FR] Make trace_dir a required argument (#135157),tools/flight_recorder/components/config_manager.py tools/flight_recorder/fr_trace.py,https://github.com/pytorch/pytorch/pull/135157,wconstab,c-p-i-o,fduwjj,,
dc0e8187388,skip,not user facing,[FR] Automatically infer a common filename prefix (#135158),tools/flight_recorder/components/config_manager.py tools/flight_recorder/components/loader.py,https://github.com/pytorch/pytorch/pull/135158,wconstab,c-p-i-o,fduwjj,,
b1612569f6d,optim,docs,[BE] Clarify defaulting behavior in optimizer (#135384),torch/optim/optimizer.py,https://github.com/pytorch/pytorch/pull/135384,janeyx99,drisspg,jainapurva,,
2ab26806f16,skip,not user facing,Require tlparse for failing tests in test_structured_trace.py (#135376),test/dynamo/test_structured_trace.py,https://github.com/pytorch/pytorch/pull/135376,masnesral,ezyang,,,
993b5647ab6,skip,not user facing,[export] fix placeholder name collision tests by removing map call (#135366),test/export/test_export.py,https://github.com/pytorch/pytorch/pull/135366,ydwu4,angelayi,,,
2f5b40c0995,skip,not user facing,[aoti test] Disable FP8 funz dtypes in fp8 runtime check test (#135373),test/inductor/test_aot_inductor.py,https://github.com/pytorch/pytorch/pull/135373,henrylhtsang,chenyang78,,,
ead4407f575,inductor,not user facing,[inductor] Fix loop split optimization (#135303),torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/135303,jiayisunx,jansel,jgong5,leslie-fang-intel,
2c7e3148036,inductor,not user facing,[Inductor][CPP] Fix the issue of view dtype (#135301),test/inductor/test_cpu_repro.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/135301,leslie-fang-intel,jansel,jgong5,,
0c661f3e1a4,releng,not user facing,[Split Build] Refactor split build binary builds into their own workflows and move split build binary builds to periodic (#134624),.github/scripts/generate_binary_build_matrix.py .github/scripts/generate_ci_workflows.py .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-manywheel-main-split.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly-split.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/134624,PaliC,malfet,,,
b1a934741ed,quantization,not user facing,Change test_constant_prop_preserve_metadata (#135268),test/quantization/pt2e/test_quantize_pt2e.py,https://github.com/pytorch/pytorch/pull/135268,yushangdi,angelayi,,,
941d094dd1b,dynamo,not user facing,[Dynamo][DTensor] Fixes SymNodeVariable() is not a constant error in Compiled DDP + TP unit test (#135315),test/distributed/_composable/test_replicate_with_compiler.py test/distributed/_tensor/test_dtensor_compile.py torch/_dynamo/variables/torch.py,https://github.com/pytorch/pytorch/pull/135315,yf225,bdhirsh,,,
d42b0c8f225,skip,not user facing,Add release matrix for 2.5 (#135383),RELEASE.md,https://github.com/pytorch/pytorch/pull/135383,kit1980,huydhn,,,
a6b9d444fbd,onnx,bc breaking,[ONNX] Refactor exporter errors (#135180),docs/source/onnx_dynamo.rst test/onnx/dynamo/test_exporter_api.py test/onnx/onnx_test_common.py test/onnx/test_fx_to_onnx.py test/onnx/test_pytorch_onnx_no_runtime.py test/onnx/test_pytorch_onnx_onnxruntime.py torch/onnx/__init__.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/exporter/_building.py torch/onnx/_internal/exporter/_core.py torch/onnx/_internal/exporter/_errors.py torch/onnx/_internal/exporter/errors.py torch/onnx/errors.py torch/onnx/utils.py,https://github.com/pytorch/pytorch/pull/135180,justinchuby,titaiwangms,,,
e6a0221fc61,inductor,Untopiced,[Inductor] Optionally allow padding on non-GPU devices (#135280),torch/_inductor/compile_fx.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/135280,blaine-rister,jfix71,shunting314,,
c92227c41aa,quantization,not user facing,[quant][pt2e] fix placeholder typo and related quantization tests (#135379),test/quantization/pt2e/test_numeric_debugger.py torch/ao/quantization/pt2e/_numeric_debugger.py,https://github.com/pytorch/pytorch/pull/135379,yiming0416,jerryzh168,,,
3d734d837be,onnx,bug fixes,[ONNX] Handle mixed sequence inputs properly (#135378),torch/onnx/_internal/exporter/_building.py,https://github.com/pytorch/pytorch/pull/135378,justinchuby,titaiwangms,,,
ebab5c85c43,skip,not user facing,[FlexAttention] Skip very small block size unit tests on H100 due to Triton bug (#135393),test/inductor/test_flex_attention.py,https://github.com/pytorch/pytorch/pull/135393,yanboliang,BoyuanFeng,,,
32f3af72b77,onnx,bug fixes,[ONNX] Support FakeTensor in ONNXProgram (#135399),test/onnx/pytorch_test_common.py test/onnx/test_fx_to_onnx.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/exporter/_core.py torch/onnx/_internal/exporter/_onnx_program.py,https://github.com/pytorch/pytorch/pull/135399,justinchuby,titaiwangms,,,
692faa9bc62,inductor,Untopiced,[inductor][cpp][gemm] reduce memory alloc overhead by allocating local acc once per thread (#135277),torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_prefix.h,https://github.com/pytorch/pytorch/pull/135277,jgong5,leslie-fang-intel,,,
be9f4ffe888,inductor,Untopiced,[inductor][cpp][gemm] enable dynamic M for k-slicing (#133447),torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_prefix.h,https://github.com/pytorch/pytorch/pull/133447,jgong5,leslie-fang-intel,,,
d7c97e7245b,inductor,not user facing,[inductor][cpp][gemm] cache blocking config for dynamic shapes (#133538),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_prefix.h,https://github.com/pytorch/pytorch/pull/133538,jgong5,leslie-fang-intel,,,
6c1da66407b,skip,not user facing,[Reland] Refactor caching device allocator utils (#130923),c10/core/CachingDeviceAllocator.h c10/cuda/CUDACachingAllocator.cpp c10/cuda/CUDACachingAllocator.h c10/cuda/CUDAMallocAsyncAllocator.cpp torch/csrc/cuda/CUDAPluggableAllocator.cpp torch/csrc/cuda/CUDAPluggableAllocator.h torch/csrc/cuda/Module.cpp,https://github.com/pytorch/pytorch/pull/130923,guangyey,EikanWang,albanD,eqy,
b53d97c7be2,xpu,Untopiced,[Intel GPU] Add XPU memory-related APIs (#129919),c10/xpu/XPUCachingAllocator.cpp c10/xpu/XPUCachingAllocator.h docs/source/xpu.rst test/test_xpu.py torch/_C/__init__.pyi.in torch/csrc/xpu/Module.cpp torch/xpu/__init__.py torch/xpu/memory.py,https://github.com/pytorch/pytorch/pull/129919,guangyey,EikanWang,abhilash1910,albanD,
f7c0c066924,skip,not user facing,Add oneDNN BRGEMM support on CPU (#131878),BUILD.bazel aten/src/ATen/native/CPUBlas.cpp aten/src/ATen/native/CPUBlas.h cmake/Modules/FindMKLDNN.cmake third_party/mkl-dnn.BUILD,https://github.com/pytorch/pytorch/pull/131878,CaoE,jgong5,peterbell10,,
18479c5f702,inductor,Untopiced,[Doc] update max-autotune for CPU (#134986),torch/__init__.py,https://github.com/pytorch/pytorch/pull/134986,chunyuan-w,jgong5,malfet,,
eac5e125488,inductor,not user facing,[inductor] Move LoopBody to its own file (#135257),torch/_inductor/bounds.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/ir.py torch/_inductor/loop_body.py torch/_inductor/optimize_indexing.py torch/_inductor/scheduler.py torch/_inductor/virtualized.py,https://github.com/pytorch/pytorch/pull/135257,jansel,oulgen,,,
a2db22e6bb2,inductor,not user facing,[inductor] Catch BrokenProcessPool and print a more helpful message. (#135120),torch/_inductor/async_compile.py,https://github.com/pytorch/pytorch/pull/135120,masnesral,Chillee,,,
3bebc09be98,inductor,not user facing,[FlexAttention] Align the matmul tensorcore usage (#135168),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/135168,drisspg,Chillee,,,
e72ed4717ea,dynamo,not user facing,[Dynamo] Fix Huggingface PretrainedConfig get non const attr (#135413),test/dynamo/test_model_output.py torch/_dynamo/variables/dicts.py,https://github.com/pytorch/pytorch/pull/135413,yanboliang,anijain2305,,,
8334cb2fb91,skip,not user facing,remove commented out breakpoints (#135363),test/dynamo/test_sources.py test/torch_np/check_tests_conform.py,https://github.com/pytorch/pytorch/pull/135363,bobrenjc93,oulgen,,,
fd494dd4266,quantization,Untopiced,Change wrapped_linear_prepack and wrapped_quantized_linear_prepacked to private by adding _ as prefix (#135401),aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp aten/src/ATen/native/quantized/library.cpp test/forward_backward_compatibility/check_forward_backward_compatibility.py test/quantization/core/test_quantized_op.py torch/_inductor/decomposition.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp torch/overrides.py,https://github.com/pytorch/pytorch/pull/135401,hl475,houseroad,,,
042f2f7746a,onnx,bug fixes,[ONNX] Re-raise the exception if the dynamic shapes cannot be refined (#135418),torch/onnx/_internal/exporter/_capture_strategies.py,https://github.com/pytorch/pytorch/pull/135418,justinchuby,titaiwangms,,,
a6fae2e8111,skip,not user facing,Use BRGEMM for Half flash attention forward kernel (#131879),aten/src/ATen/native/CPUBlas.cpp aten/src/ATen/native/CPUBlas.h aten/src/ATen/native/cpu/FlashAttentionKernel.cpp aten/src/ATen/native/cpu/utils.h test/test_transformers.py,https://github.com/pytorch/pytorch/pull/131879,CaoE,jgong5,peterbell10,,
20cab91a123,dynamo,not user facing,[dynamo] Remove skip from jit freeze tests (#135281),test/jit/test_freezing.py,https://github.com/pytorch/pytorch/pull/135281,anijain2305,zou3519,,,
cfc227ad43c,distributed,Untopiced,[reland][dtensor] move DTensor to public namespace (#134203),docs/source/distributed.rst docs/source/distributed.tensor.rst docs/source/index.rst test/allowlist_for_publicAPI.json test/distributed/_composable/fsdp/test_fully_shard_clip_grad_norm_.py test/distributed/_composable/fsdp/test_fully_shard_comm.py test/distributed/_composable/fsdp/test_fully_shard_init.py test/distributed/_composable/fsdp/test_fully_shard_training.py test/distributed/_composable/test_composability/test_2d_composability.py test/distributed/_tensor/debug/test_comm_mode.py test/distributed/_tensor/debug/test_comm_mode_features.py test/distributed/_tensor/debug/test_op_coverage.py test/distributed/_tensor/experimental/test_local_map.py test/distributed/_tensor/experimental/test_tp_transform.py test/distributed/_tensor/test_attention.py test/distributed/_tensor/test_common_rules.py test/distributed/_tensor/test_dtensor.py test/distributed/_tensor/test_embedding_ops.py test/distributed/_tensor/test_math_ops.py test/distributed/_tensor/test_matrix_ops.py test/distributed/_tensor/test_op_strategy.py test/distributed/_tensor/test_random_ops.py test/distributed/_tensor/test_redistribute.py test/distributed/_tensor/test_tensor_ops.py test/distributed/_tensor/test_utils.py test/distributed/_tensor/test_view_ops.py test/distributed/checkpoint/test_state_dict_utils.py test/distributed/fsdp/test_fsdp_tp_integration.py test/distributed/tensor/parallel/test_tp_examples.py test/distributed/tensor/parallel/test_tp_random_state.py test/distributed/tensor/parallel/test_tp_style.py test/distributed/test_device_mesh.py torch/_dynamo/guards.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/distributed.py torch/_dynamo/variables/torch.py torch/distributed/_composable/fsdp/_fsdp_collectives.py torch/distributed/_composable/fsdp/_fsdp_common.py torch/distributed/_composable/fsdp/_fsdp_init.py torch/distributed/_composable/fsdp/_fsdp_param.py torch/distributed/_composable/fsdp/fully_shard.py torch/distributed/_functional_collectives.py torch/distributed/_state_dict_utils.py torch/distributed/_tensor/README.md torch/distributed/_tensor/__init__.py torch/distributed/_tensor/_collective_utils.py torch/distributed/_tensor/_dispatch.py torch/distributed/_tensor/_op_schema.py torch/distributed/_tensor/_redistribute.py torch/distributed/_tensor/_sharding_prop.py torch/distributed/_tensor/_shards_wrapper.py torch/distributed/_tensor/_tp_conv.py torch/distributed/_tensor/_utils.py torch/distributed/_tensor/api.py torch/distributed/_tensor/debug/__init__.py torch/distributed/_tensor/debug/_op_coverage.py torch/distributed/_tensor/debug/comm_mode.py torch/distributed/_tensor/debug/comm_mode_broswer_visual.js torch/distributed/_tensor/debug/visualize_sharding.py torch/distributed/_tensor/device_mesh.py torch/distributed/_tensor/examples/comm_mode_features_example.py torch/distributed/_tensor/examples/convnext_example.py torch/distributed/_tensor/examples/torchrec_sharding_example.py torch/distributed/_tensor/examples/visualize_sharding_example.py torch/distributed/_tensor/experimental/__init__.py torch/distributed/_tensor/experimental/attention.py torch/distributed/_tensor/experimental/func_map.py torch/distributed/_tensor/experimental/register_sharding.py torch/distributed/_tensor/experimental/tp_transform.py torch/distributed/_tensor/ops/__init__.py torch/distributed/_tensor/ops/_common_rules.py torch/distributed/_tensor/ops/_conv_ops.py torch/distributed/_tensor/ops/_einsum_strategy.py torch/distributed/_tensor/ops/_embedding_ops.py torch/distributed/_tensor/ops/_experimental_ops.py torch/distributed/_tensor/ops/_math_ops.py torch/distributed/_tensor/ops/_matrix_ops.py torch/distributed/_tensor/ops/_pointwise_ops.py torch/distributed/_tensor/ops/_random_ops.py torch/distributed/_tensor/ops/_tensor_ops.py torch/distributed/_tensor/ops/_view_ops.py torch/distributed/_tensor/ops/utils.py torch/distributed/_tensor/placement_types.py torch/distributed/_tensor/random.py torch/distributed/checkpoint/_traverse.py torch/distributed/checkpoint/default_planner.py torch/distributed/checkpoint/examples/async_checkpointing_example.py torch/distributed/checkpoint/examples/stateful_example.py torch/distributed/checkpoint/optimizer.py torch/distributed/checkpoint/planner_helpers.py torch/distributed/checkpoint/state_dict.py torch/distributed/fsdp/_flat_param.py torch/distributed/fsdp/_fsdp_extensions.py torch/distributed/fsdp/_optim_utils.py torch/distributed/fsdp/_shard_utils.py torch/distributed/fsdp/_state_dict_utils.py torch/distributed/fsdp/fully_sharded_data_parallel.py torch/distributed/tensor/README.md torch/distributed/tensor/__init__.py torch/distributed/tensor/_api.py torch/distributed/tensor/_collective_utils.py torch/distributed/tensor/_dispatch.py torch/distributed/tensor/_dtensor_spec.py torch/distributed/tensor/_op_schema.py torch/distributed/tensor/_ops/__init__.py torch/distributed/tensor/_ops/_common_rules.py torch/distributed/tensor/_ops/_conv_ops.py torch/distributed/tensor/_ops/_einsum_strategy.py torch/distributed/tensor/_ops/_embedding_ops.py torch/distributed/tensor/_ops/_experimental_ops.py torch/distributed/tensor/_ops/_math_ops.py torch/distributed/tensor/_ops/_matrix_ops.py torch/distributed/tensor/_ops/_pointwise_ops.py torch/distributed/tensor/_ops/_random_ops.py torch/distributed/tensor/_ops/_tensor_ops.py torch/distributed/tensor/_ops/_view_ops.py torch/distributed/tensor/_ops/utils.py torch/distributed/tensor/_random.py torch/distributed/tensor/_redistribute.py torch/distributed/tensor/_sharding_prop.py torch/distributed/tensor/_shards_wrapper.py torch/distributed/tensor/_tp_conv.py torch/distributed/tensor/_utils.py torch/distributed/tensor/debug/__init__.py torch/distributed/tensor/debug/_comm_mode.py torch/distributed/tensor/debug/_op_coverage.py torch/distributed/tensor/debug/_visualize_sharding.py torch/distributed/tensor/debug/comm_mode_broswer_visual.js torch/distributed/tensor/device_mesh.py torch/distributed/tensor/examples/comm_mode_features_example.py torch/distributed/tensor/examples/convnext_example.py torch/distributed/tensor/examples/torchrec_sharding_example.py torch/distributed/tensor/examples/visualize_sharding_example.py torch/distributed/tensor/experimental/__init__.py torch/distributed/tensor/experimental/_attention.py torch/distributed/tensor/experimental/_func_map.py torch/distributed/tensor/experimental/_register_sharding.py torch/distributed/tensor/experimental/_tp_transform.py torch/distributed/tensor/parallel/_data_parallel_utils.py torch/distributed/tensor/parallel/_utils.py torch/distributed/tensor/parallel/api.py torch/distributed/tensor/parallel/fsdp.py torch/distributed/tensor/parallel/input_reshard.py torch/distributed/tensor/parallel/loss.py torch/distributed/tensor/parallel/style.py torch/distributed/tensor/placement_types.py torch/testing/_internal/common_fsdp.py,https://github.com/pytorch/pytorch/pull/134203,wanchaol,tianyu-l,,,
2196f324753,onnx,not user facing,[22/N] Fix clang-tidy warnings in jit  (#135319),torch/csrc/jit/passes/onnx.cpp torch/csrc/jit/passes/onnx/cast_all_constant_to_floating.cpp torch/csrc/jit/passes/onnx/cast_all_constant_to_floating.h torch/csrc/jit/passes/onnx/constant_fold.cpp torch/csrc/jit/passes/onnx/constant_fold.h torch/csrc/jit/passes/onnx/constant_map.cpp torch/csrc/jit/passes/onnx/constant_map.h torch/csrc/jit/passes/onnx/deduplicate_initializers.cpp torch/csrc/jit/passes/onnx/deduplicate_initializers.h torch/csrc/jit/passes/onnx/eliminate_unused_items.cpp torch/csrc/jit/passes/onnx/eliminate_unused_items.h torch/csrc/jit/passes/onnx/eval_peephole.cpp torch/csrc/jit/passes/onnx/eval_peephole.h torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.cpp torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.h torch/csrc/jit/passes/onnx/function_extraction.cpp torch/csrc/jit/passes/onnx/function_extraction.h torch/csrc/jit/passes/onnx/function_substitution.cpp torch/csrc/jit/passes/onnx/function_substitution.h torch/csrc/jit/passes/onnx/helper.cpp torch/csrc/jit/passes/onnx/helper.h torch/csrc/jit/passes/onnx/list_model_parameters.cpp torch/csrc/jit/passes/onnx/list_model_parameters.h torch/csrc/jit/passes/onnx/naming.cpp torch/csrc/jit/passes/onnx/naming.h torch/csrc/jit/passes/onnx/onnx_log.cpp torch/csrc/jit/passes/onnx/onnx_log.h torch/csrc/jit/passes/onnx/peephole.cpp torch/csrc/jit/passes/onnx/peephole.h torch/csrc/jit/passes/onnx/prepare_division_for_onnx.cpp torch/csrc/jit/passes/onnx/prepare_division_for_onnx.h torch/csrc/jit/passes/onnx/preprocess_for_onnx.cpp torch/csrc/jit/passes/onnx/preprocess_for_onnx.h torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.h torch/csrc/jit/passes/onnx/scalar_type_analysis.cpp torch/csrc/jit/passes/onnx/scalar_type_analysis.h torch/csrc/jit/passes/onnx/shape_type_inference.cpp torch/csrc/jit/passes/onnx/shape_type_inference.h torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp torch/csrc/jit/passes/onnx/unpack_quantized_weights.h,https://github.com/pytorch/pytorch/pull/135319,cyyever,titaiwangms,,,
3bdc54ed188,inductor,not user facing,[inductor] Refactor LoopBody.memory_usage (#135286),torch/_inductor/codegen/cpp.py torch/_inductor/ir.py torch/_inductor/loop_body.py,https://github.com/pytorch/pytorch/pull/135286,jansel,oulgen,,,
37144be03dc,inductor,not user facing,[inductor] Remove ReadWrites.op_counts (#135306),torch/_inductor/codegen/cpp.py torch/_inductor/dependencies.py torch/_inductor/loop_body.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/135306,jansel,oulgen,,,
16f5155992c,inductor,not user facing,[inductor] Fast path for extract_read_writes without tracing (#135377),torch/_inductor/dependencies.py torch/_inductor/ir.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/135377,jansel,oulgen,,,
53290ca00b8,inductor,not user facing,[inductor] Refactor BaseSchedulerNode.__init__ (#135400),torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/135400,jansel,oulgen,,,
31c4e0d37d8,inductor,not user facing,[inductor] Cleanup analysis done at lowering time (#135412),torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/ops_handler.py,https://github.com/pytorch/pytorch/pull/135412,jansel,oulgen,,,
defb515306f,nested tensor_frontend,Untopiced,[NJT]Add permute ops support (#135336),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/135336,YuqingJ,davidberard98,,,
c1ae78be928,inductor,not user facing,[inductor] calibration inductor windows uts (18/N) (#135449),test/inductor/test_cpu_select_algorithm.py,https://github.com/pytorch/pytorch/pull/135449,xuhancn,ezyang,,,
b7eb7256fb9,python_frontend,docs,docs: `torch.nn.utils.rnn.pack_padded_sequence`: docs improve (#135417),torch/nn/utils/rnn.py,https://github.com/pytorch/pytorch/pull/135417,kuraga,ezyang,,,
e1306962708,releng,Untopiced,[RELEASE-ONLY CHANGES] Branch Cut for Release 2.5 (#135506),.ci/pytorch/common_utils.sh .github/ci_commit_pins/xla.txt .github/scripts/filter_test_configs.py .github/templates/common.yml.j2 .github/templates/linux_binary_build_workflow.yml.j2 .github/templates/macos_binary_build_workflow.yml.j2 .github/templates/windows_binary_build_workflow.yml.j2 .github/workflows/_android-build-test.yml .github/workflows/_android-full-build-test.yml .github/workflows/_bazel-build-test.yml .github/workflows/_binary-build-linux.yml .github/workflows/_binary-test-linux.yml .github/workflows/_binary-upload.yml .github/workflows/_buck-build-test.yml .github/workflows/_docs.yml .github/workflows/_ios-build-test.yml .github/workflows/_linux-build.yml .github/workflows/_linux-test.yml .github/workflows/_mac-build.yml .github/workflows/_mac-test-mps.yml .github/workflows/_mac-test.yml .github/workflows/_rocm-test.yml .github/workflows/_run_android_tests.yml .github/workflows/_runner-determinator.yml .github/workflows/_win-build.yml .github/workflows/_win-test.yml .github/workflows/_xpu-test.yml .github/workflows/build-conda-images.yml .github/workflows/build-libtorch-images.yml .github/workflows/build-manywheel-images.yml .github/workflows/build-triton-wheel.yml .github/workflows/check-labels.yml .github/workflows/close-nonexistent-disable-issues.yml .github/workflows/docker-builds.yml .github/workflows/docker-release.yml .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-conda-nightly.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-main.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-main.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-nightly.yml .github/workflows/generated-linux-binary-manywheel-main-split.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly-split.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml .github/workflows/generated-macos-arm64-binary-conda-nightly.yml .github/workflows/generated-macos-arm64-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-conda-nightly.yml .github/workflows/generated-windows-binary-libtorch-debug-main.yml .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-main.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml .github/workflows/lint-bc.yml .github/workflows/lint.yml .github/workflows/llm_td_retrieval.yml .github/workflows/nightly-rockset-uploads.yml .github/workflows/nightly.yml .github/workflows/nitpicker.yml .github/workflows/target-determination-indexer.yml .github/workflows/target_determination.yml .github/workflows/update-viablestrict.yml .github/workflows/update_pytorch_labels.yml .github/workflows/upload-alerts.yml .github/workflows/upload-test-stats.yml .github/workflows/upload-torch-dynamo-perf-stats.yml .github/workflows/upload_test_stats_intermediate.yml .github/workflows/weekly.yml tools/stats/import_test_stats.py,,,,,,
