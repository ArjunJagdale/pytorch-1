import dataclasses
from torch.fx import Node, Graph
import torch
from typing import Optional
from .utils import use_tangent, compute_tensor_size
from torch._inductor import config

aten = torch.ops.aten

class CantChunk(RuntimeError):
    pass

@dataclasses.dataclass
class ChunkingMeta:
    # The value of the node should be scaled by the specified scalar
    # tensor. Need this sine we pretend tangent to be 1 first and
    # scale the affected tensor later. Need propagate such information
    # to downstream.
    scale_by: Node = None 

    # The dimension of the current tensor that get chunked.
    # Can be None if the current tensor is not chunked. E.g., when
    # the current tensor is a scalar tensor generated by summing a chunked tensor.
    #
    # To recover a tensor with non-None chunk_dim, we need concat each
    # chunk at the 'chunk_dim' dimension.
    chunk_dim: Optional[int] = None

    # The original tensor is the sum of each tensor in the chunk subgraph.
    # chunk_dim should be None if need_sum is True
    #
    # Note for some special cases like the tangent placeholder node, both
    # chunk_dim can be None and need_sum can be false, but scale_by
    # in that case is the tangent node itself.
    need_sum: bool = False

    def copy(self):
        return ChunkingMeta(**self.__dict__)

eligible_amplifier_node = {
    aten.mm.default,
    aten.addmm.default,
}

def find_amplifier_node(graph: Graph) -> Optional[Node]:
    r"""
    Find the 'amplifier' node which is a not that generates large
    output with small/medium input.

    If there are multiple amplifier nodes, return the one with the largest
    amplification ratio.
    """

    amplifier_nodes_ratio = []
    for node in graph.nodes:
        # We only look for amplifier nodes in the fwd part of the graph
        if use_tangent(node):
            break

    # A source user is the user of a source node that we want to
    # chunk. The source node is node we start chunking.
    source_users = []
    for node in graph.nodes:
        if use_tangent(node):
            # enter backward part of the graph
            break

        # Only trigger chunking for a small set of nodes like matmul for now
        if node.op != "call_function" or node.target not in eligible_amplifier_node:
            continue

        input_size = compute_tensor_size(node.args, node.kwargs)
        output_size = compute_tensor_size(node)

        if input_size == 0:
            continue

        ratio = output_size / input_size
        if output_size > config.AutoChunker.output_size_threshold and ratio > config.AutoChunker.amplify_ratio_threshold:
            amplifier_nodes_ratio.append((node, ratio))

    amplifier_nodes_ratio = sorted(amplifier_nodes_ratio, key=lambda x: x[1], reverse=True)
    return amplifier_nodes_ratio[0][0] if len(amplifier_nodes_ratio) > 0 else None
